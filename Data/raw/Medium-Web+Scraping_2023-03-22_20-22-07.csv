blogTitle,blogSubheading,blogContent
Learning Python: From Zero to Hero,earning Python: From Zero to Her,"This post was originally published at  TK's Blog . First of all, what is Python? According to its creator, Guido van Rossum, Python is a: “high-level programming language, and its core design philosophy is all about code readability and a syntax which allows programmers to express concepts in a few lines of code.” For me, the first reason to learn Python was that it is, in fact, a beautiful   programming language. It was really natural to code in it and express my thoughts. Another reason was that we can use coding in Python in multiple ways: data science, web development, and machine learning all shine here. Quora, Pinterest and Spotify all use Python for their backend web development. So let’s learn a bit about it. You can think about variables as words that store a value. Simple as that. In Python, it is really easy to define a variable and set a value to it. Imagine you want to store number 1 in a variable called “one.” Let’s do it: How simple was that? You just assigned the value 1 to the variable “one.” And you can assign any other  value  to whatever other  variables  you want. As you see in the table above, the variable “ two ” stores the integer  2 , and “ some_number ” stores  10,000 . Besides integers, we can also use booleans (True / False), strings, float, and so many other data types. “ If ” uses an expression to evaluate whether a statement is True or False. If it is True, it executes what is inside the “if” statement. For example: 2  is greater than  1 , so the “ print ” code is executed. The “ else ” statement will be executed if the “ if ” expression is  false . 1  is not greater than  2 , so the code inside the “ else ” statement will be executed. You can also use an “ elif ” statement: In Python, we can iterate in different forms. I’ll talk about two:  while   and  for . While  Looping: while the statement is True, the code inside the block will be executed. So, this code will print the number from  1  to  10 . The  while  loop needs a “ loop condition. ” If it stays True, it continues iterating. In this example, when  num  is  11  the  loop condition  equals  False . Another basic bit of code to better understand it: The  loop condition  is  True  so it keeps iterating — until we set it to  False . For Looping : you apply the variable “ num ” to the block, and the “ for ” statement will iterate it for you. This code will print the same as  while  code: from  1  to  10 . See? It is so simple. The range starts with  1  and goes until the  11 th element ( 10  is the  10 th element). Imagine you want to store the integer 1 in a variable. But maybe now you want to store 2. And 3, 4, 5 … Do I have another way to store all the integers that I want, but not in  millions of variables ? You guessed it — there is indeed another way to store them. List  is a collection that can be used to store a list of values (like these integers that you want). So let’s use it: It is really simple. We created an array and stored it on  my_integer . But maybe you are asking: “How can I get a value from this array?” Great question.  List  has a concept called  index . The first element gets the index 0 (zero). The second gets 1, and so on. You get the idea. To make it clearer, we can represent the array and each element with its index. I can draw it: Using the Python syntax, it’s also simple to understand: Imagine that you don’t want to store integers. You just want to store strings, like a list of your relatives’ names. Mine would look something like this: It works the same way as integers. Nice. We just learned how  Lists  indices work. But I still need to show you how we can add an element to the  List  data structure (an item to a list). The most common method to add a new value to a  List  is  append . Let’s see how it works: append  is super simple. You just need to apply the element (eg. “ The Effective Engineer ”) as the  append  parameter. Well, enough about  Lists .  Let’s talk about another data structure. Now we know that  Lists  are indexed with integer numbers. But what if we don’t want to use integer numbers as indices? Some data structures that we can use are numeric, string, or other types of indices. Let’s learn about the  Dictionary  data structure.  Dictionary  is a collection of key-value pairs. Here’s what it looks like: The  key  is the index pointing to the   value . How do we access the  Dictionary   value ? You guessed it — using the  key . Let’s try it: I created a  Dictionary  about me. My name, nickname, and nationality. Those attributes are the  Dictionary   keys . As we learned how to access the  List  using index, we also use indices ( keys  in the  Dictionary  context) to access the  value  stored in the  Dictionary . In the example, I printed a phrase about me using all the values stored in the  Dictionary . Pretty simple, right? Another cool thing about  Dictionary  is that we can use anything as the value. In the  Dictionary   I created, I want to add the  key  “age” and my real integer age in it: Here we have a  key  (age)  value  (24) pair using string as the  key  and integer as the  value . As we did with  Lists , let’s learn how to add elements to a  Dictionary . The  key   pointing to a   value  is a big part of what  Dictionary  is. This is also true when we are talking about adding elements to it: We just need to assign a  value  to a  Dictionary   key . Nothing complicated here, right? As we learned in the  Python Basics , the  List  iteration is very simple. We  Python   developers commonly use  For  looping. Let’s do it: So for each book in the bookshelf, we ( can do everything with it ) print it. Pretty simple and intuitive. That’s Python. For a hash data structure, we can also use the  for  loop, but we apply the  key  : This is an example how to use it. For each  key  in the  dictionary  , we  print  the  key  and its corresponding  value . Another way to do it is to use the  iteritems  method. We did name the two parameters as  key  and  value , but it is not necessary. We can name them anything. Let’s see it: We can see we used attribute as a parameter for the  Dictionary   key , and it works properly. Great! Objects  are a representation of real world objects like cars, dogs, or bikes. The objects share two main characteristics:  data  and  behavior . Cars have  data,  like number of wheels, number of doors, and seating capacity They also exhibit  behavior : they can accelerate, stop, show how much fuel is left, and so many other things. We identify  data  as  attributes  and  behavior  as  methods  in object-oriented programming. Again: Data → Attributes and Behavior → Methods And a  Class  is the blueprint from which individual objects are created. In the real world, we often find many objects with the same type. Like cars. All the same make and model (and all have an engine, wheels, doors, and so on). Each car was built from the same set of blueprints and has the same components. Python, as an Object-Oriented programming language, has these concepts:  class  and  object . A class is a blueprint, a model for its objects. So again, a class it is just a model, or a way to define  attributes  and  behavior  (as we talked about in the theory section). As an example, a vehicle  class  has its own  attributes  that define what  objects  are vehicles. The number of wheels, type of tank, seating capacity, and maximum velocity are all attributes of a vehicle. With this in mind, let’s look at Python syntax for  classes : We define classes with a  class statement —  and that’s it. Easy, isn’t it? Objects  are instances of a  class . We create an instance by naming the class. Here  car  is an  object  (or instance) of the  class   Vehicle . Remember that our vehicle  class  has four  attributes : number of wheels, type of tank, seating capacity, and maximum velocity. We set all these  attributes  when creating a vehicle  object . So here, we define our  class  to receive data when it initiates it: We use the  init   method . We call it a constructor method. So when we create the vehicle  object , we can define these  attributes . Imagine that we love the  Tesla Model S,  and we want to create this kind of  object . It has four wheels, runs on electric energy, has space for five seats, and the maximum velocity is 250km/hour (155 mph). Let’s create this  object: Four wheels + electric “tank type” + five seats + 250km/hour maximum speed. All attributes are set. But how can we access these attributes’ values? We  send a message to the object asking about them . We call it a  method . It’s the  object’s behavior . Let’s implement it: This is an implementation of two methods:  number_of_wheels  and  set_number_of_wheels . We call it  getter  &  setter . Because the first gets the attribute value, and the second sets a new value for the attribute. In Python, we can do that using  @property  ( decorators ) to define  getters  and  setters . Let’s see it with code: And we can use these methods as attributes: This is slightly different than defining methods. The methods work as attributes. For example, when we set the new number of wheels, we don’t apply two as a parameter, but set the value 2 to  number_of_wheels . This is one way to write  pythonic   getter  and  setter  code. But we can also use methods for other things, like the “ make_noise ” method. Let’s see it: When we call this method, it just returns a string  “ VRRRRUUUUM. ” Encapsulation is a mechanism that restricts direct access to objects’ data and methods. But at the same time, it facilitates operation on that data (objects’ methods). “Encapsulation can be used to hide data members and members function. Under this definition, encapsulation means that the internal representation of an  object  is generally hidden from view outside of the object’s definition.” — Wikipedia All internal representation of an object is hidden from the outside. Only the object can interact with its internal data. First, we need to understand how  public  and  non-public  instance variables and methods work. For a Python class, we can initialize a  public instance variable  within our constructor method. Let’s see this: Within the constructor method: Here we apply the  first_name  value as an argument to the  public instance variable . Within the class: Here, we do not need to apply the  first_name  as an argument, and all instance objects will have a  class attribute  initialized with  TK . Cool. We have now learned that we can use  public instance variables  and  class attributes . Another interesting thing about the  public  part is that we can manage the variable value. What do I mean by that? Our  object  can manage its variable value:  Get  and  Set  variable values. Keeping the  Person  class in mind, we want to set another value to its  first_name  variable: There we go. We just set another value ( kaio ) to the  first_name  instance variable and it updated the value. Simple as that. Since it’s a  public  variable, we can do that. We don’t use the term “private” here, since no attribute is really private in Python (without a generally unnecessary amount of work). —  PEP 8 As the  public instance variable  , we can define the  non-public instance variable  both within the constructor method or within the class. The syntax difference is: for  non-public instance variables  , use an underscore ( _ ) before the  variable  name. “‘Private’ instance variables that cannot be accessed except from inside an object don’t exist in Python. However, there is a convention that is followed by most Python code: a name prefixed with an underscore (e.g.  _spam ) should be treated as a non-public part of the API (whether it is a function, a method or a data member)” —  Python Software Foundation Here’s an example: Did you see the  email  variable? This is how we define a  non-public variable  : We can access and update it.  Non-public variables  are just a convention and should be treated as a non-public part of the API. So we use a method that allows us to do it inside our class definition. Let’s implement two methods ( email  and  update_email ) to understand it: Now we can update and access  non-public variables  using those methods. Let’s see: With  public methods , we can also use them out of our class: Let’s test it: Great — we can use it without any problem. But with  non-public methods  we aren’t able to do it. Let’s implement the same  Person  class, but now with a  show_age   non-public method  using an underscore ( _ ). And now, we’ll try to call this  non-public method  with our object: We can access and update it.  Non-public methods  are just a convention and should be treated as a non-public part of the API. Here’s an example for how we can use it: Here we have a  _get_age   non-public method  and a  show_age   public method . The  show_age  can be used by our object (out of our class) and the  _get_age  only used inside our class definition (inside  show_age  method). But again: as a matter of convention. With encapsulation we can ensure that the internal representation of the object is hidden from the outside. Certain objects have some things in common: their behavior and characteristics. For example, I inherited some characteristics and behaviors from my father. I inherited his eyes and hair as characteristics, and his impatience and introversion as behaviors. In object-oriented programming, classes can inherit common characteristics (data) and behavior (methods) from another class. Let’s see another example and implement it in Python. Imagine a car. Number of wheels, seating capacity and maximum velocity are all attributes of a car. We can say that an   ElectricCar  class inherits these same attributes from the regular  Car  class. Our  Car  class implemented: Once initiated, we can use all  instance variables  created. Nice. In Python, we apply a  parent class  to the  child class  as a parameter. An  ElectricCar  class can inherit from our  Car  class. Simple as that. We don’t need to implement any other method, because this class already has it (inherited from  Car  class). Let’s prove it: Beautiful. We learned a lot of things about Python basics: Congrats! You completed this dense piece of content about Python. If you want a complete Python course, learn more real-world coding skills and build projects, try  One Month Python Bootcamp . See you there ☺ For more stories and posts about my journey learning & mastering programming, follow my publication  The Renaissance Developer . Have fun, keep learning, and always keep coding. I hope you liked this content. Support my work on Ko-Fi My  Twitter  &  Github . ☺"
An A-Z of useful Python tricks,n A-Z of useful Python trick,"Python is one of the world’s most popular, in-demand programming languages. This is for many reasons: I use Python daily as an integral part of my job as a data scientist. Along the way, I’ve picked up a few useful tricks and tips. Here, I’ve made an attempt at sharing some of them in an A-Z format. Most of these ‘tricks’ are things I’ve used or stumbled upon during my day-to-day work. Some I found while browsing the  Python Standard Library docs . A few others I found searching through  PyPi . However, credit where it is due — I discovered four or five of them over at  awesome-python.com . This is a curated list of hundreds of interesting Python tools and modules. It is worth browsing for inspiration! One of the many reasons why Python is such a popular language is because it is readable and expressive. It is often joked that Python is ‘ executable pseudocode ’. But when you can write code like this, it’s difficult to argue otherwise: You want to plot graphs in the console? You can have graphs in the console. Python has some great default datatypes, but sometimes they just won’t behave exactly how you’d like them to. Luckily, the Python Standard Library offers  the collections module . This handy add-on provides you with further datatypes. Ever wondered how you can look inside a Python object and see what attributes it has? Of course you have. From the command line: This can be a really useful feature when running Python interactively, and for dynamically exploring objects and modules you are working with. Read more  here . Yes,  really . Don’t pretend you’re not gonna try it out… 👍 One consequence of Python’s popularity is that there are always new versions under development. New versions mean new features — unless your version is out-of-date. Fear not, however. The  __future__ module  lets you import functionality from future versions of Python. It’s literally like time travel, or magic, or something. Why not have a go  importing curly braces ? Geography can be a challenging terrain for programmers to navigate (ha, a pun!). But  the geopy module  makes it unnervingly easy. It works by abstracting the APIs of a range of different geocoding services. It enables you to obtain a place’s full street address, latitude, longitude, and even altitude. There’s also a useful distance class. It calculates the distance between two locations in your favorite unit of measurement. Stuck on a coding problem and can’t remember that solution you saw before? Need to check StackOverflow, but don’t want to leave the terminal? Then you need  this useful command line tool . Ask it whatever question you have, and it’ll do its best to return an answer. Be aware though — it scrapes code from top answers from StackOverflow. It might not always give the most helpful information… Python’s  inspect module  is great for understanding what is happening behind the scenes. You can even call its methods on itself! The code sample below uses  inspect.getsource()  to print its own source code. It also uses  inspect.getmodule()  to print the module in which it was defined. The last line of code prints out its own line number. Of course, beyond these trivial uses, the inspect module can prove useful for understanding what your code is doing. You could also use it for writing self-documenting code. The Jedi library is an autocompletion and code analysis library. It makes writing code quicker and more productive. Unless you’re developing your own IDE, you’ll probably be most interested in  using Jedi as an editor plugin . Luckily, there are already loads available! You may already be using Jedi, however. The IPython project makes use of Jedi for its code autocompletion functionality. When learning any language, there are many milestones along the way. With Python, understanding the mysterious  **kwargs  syntax probably counts as one. The double-asterisk in front of a dictionary object lets you pass the contents of that dictionary as  named arguments to a function . The dictionary’s keys are the argument names, and the values are the values passed to the function. You don’t even need to call it  kwargs ! This is useful when you want to write functions that can handle named arguments not defined in advance. One of my favourite things about programming in Python are its  list comprehensions . These expressions make it easy to write very clean code that reads almost like natural language. You can read more about how to use them  here . Python supports functional programming through a number of inbuilt features. One of the most useful is the  map()  function — especially in combination with  lambda functions . In the example above,  map()  applies a simple lambda function to each element in  x . It returns a map object, which can be converted to some iterable object such as a list or tuple. If you haven’t seen it already, then be prepared to have your mind blown by  Python’s newspaper module . It lets you retrieve news articles and associated meta-data from a range of leading international publications. You can retrieve images, text and author names. It even has some  inbuilt NLP functionality . So if you were thinking of using BeautifulSoup or some other DIY webscraping library for your next project, save yourself the time and effort and  $ pip install newspaper3k  instead. Python provides support for  operator overloading , which is one of those terms that make you sound like a legit computer scientist. It’s actually a simple concept. Ever wondered why Python lets you use the  +  operator to add numbers and also to concatenate strings? That’s operator overloading in action. You can define objects which use Python’s standard operator symbols in their own specific way. This lets you use them in contexts relevant to the objects you’re working with. Python’s default  print  function does its job. But try printing out any large, nested object, and the result is rather ugly. Here’s where the  Standard Library’s pretty-print module  steps in. This prints out complex structured objects in an easy-to-read format. A must-have for any Python developer who works with non-trivial data structures. Python supports multithreading, and this is facilitated by the Standard Library’s Queue module. This module lets you implement queue data structures. These are data structures that let you add and retrieve entries according to a specific rule. ‘First in, first out’ (or FIFO) queues let you retrieve objects in the order they were added. ‘Last in, first out’ (LIFO) queues let you access the most recently added objects first. Finally, priority queues let you retrieve objects according to the order in which they are sorted. Here’s an example of how to use queues  for multithreaded programming in Python. When defining a class or an object in Python, it is useful to provide an ‘official’ way of representing that object as a string. For example: This makes debugging code a lot easier. Add it to your class definitions as below: Python makes a great scripting language. Sometimes using the standard os and subprocess libraries can be a bit of a headache. The  sh library  provides a neat alternative. It lets you call any program as if it were an ordinary function — useful for automating workflows and tasks, all from within Python. Python is a dynamically-typed language. You don’t need to specify datatypes when you define variables, functions, classes etc. This allows for rapid development times. However, there are few things more annoying than a runtime error caused by a simple typing issue. Since Python 3.5 , you have the option to provide type hints when defining functions. You can also define type aliases: Although not compulsory, type annotations can make your code easier to understand. They also allow you to use type checking tools to catch those stray TypeErrors before runtime. Probably worthwhile if you are working on large, complex projects! A quick and easy way to generate Universally Unique IDs (or ‘UUIDs’) is through the  Python Standard Library’s uuid module . This creates a randomized 128-bit number that will almost certainly be unique. In fact, there are over 2¹²² possible UUIDs that can be generated. That’s over five undecillion (or 5,000,000,000,000,000,000,000,000,000,000,000,000). The probability of finding duplicates in a given set is extremely low. Even with a trillion UUIDs, the probability of a duplicate existing is much, much less than one-in-a-billion. Pretty good for two lines of code. This is probably my favorite Python thing of all. Chances are you are working on multiple Python projects at any one time. Unfortunately, sometimes two projects will rely on different versions of the same dependency. Which do you install on your system? Luckily, Python’s  support for virtual environments  lets you have the best of both worlds. From the command line: Now you can have standalone versions and installations of Python running on the same machine. Sorted! Wikipedia has a great API that allows users programmatic access to an unrivalled body of completely free knowledge and information. The  wikipedia module  makes accessing this API almost embarrassingly convenient. Like the real site, the module provides support for multiple languages, page disambiguation, random page retrieval, and even has a  donate()  method. Humour is a key feature of the Python language — after all, it is named after the British comedy sketch show  Monty Python’s Flying Circus . Much of Python’s official documentation references the show’s most famous sketches. The sense of humour isn’t restricted to the docs, though. Have a go running the line below: Never change, Python. Never change. YAML stands for ‘ YAML Ain’t Markup Language ’. It is a data formatting language, and is a superset of JSON. Unlike JSON, it can store more complex objects and refer to its own elements. You can also write comments, making it particularly suited to writing configuration files. The  PyYAML module  lets you use YAML with Python. Install with: And then import into your projects: PyYAML lets you store Python objects of any datatype, and instances of any user-defined classes also. One last trick for ya, and it really is a cool one. Ever needed to form a dictionary out of two lists? The  zip()  inbuilt function takes a number of iterable objects and returns a list of tuples. Each tuple groups the elements of the input objects by their positional index. You can also ‘unzip’ objects by calling  *zip()  on them. So there you have it, an A-Z of Python tricks — hopefully you’ve found something useful for your next project. Python’s a very diverse and well-developed language, so there’s bound to be many features I haven’t got round to including. Please share any of your own favorite Python tricks by leaving a response below!"
How to build your own Neural Network from scratch in Python,ow to build your own Neural Network from scratch in Pytho,"Update : When I wrote this article a year ago, I did not expect it to be  this  popular. Since then, this article has been viewed more than 450,000 times, with more than 30,000 claps. It has also made it to the front page of Google, and it is among the first few search results for ‘ Neural Network ’. Many of you have reached out to me, and I am deeply humbled by the impact of this article on your learning journey. This article also caught the eye of the editors at Packt Publishing. Shortly after this article was published, I was offered to be the sole author of the book  Neural Network Projects with Python .  Today, I am happy to share with you that my book has been published! The book is a continuation of this article, and it covers end-to-end implementation of neural network projects in areas such as face recognition, sentiment analysis, noise removal etc. Every chapter features a unique neural network architecture, including Convolutional Neural Networks, Long Short-Term Memory Nets and Siamese Neural Networks. If you’re looking to create a strong machine learning portfolio with deep learning projects, do consider getting the book! You can get the book from Amazon:  Neural Network Projects with Python Motivation:  As part of my personal journey to gain a better understanding of Deep Learning, I’ve decided to build a Neural Network from scratch without a deep learning library like TensorFlow. I believe that understanding the inner workings of a Neural Network is important to any aspiring Data Scientist. This article contains what I’ve learned, and hopefully it’ll be useful for you as well! Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output. Neural Networks consist of the following components The diagram below shows the architecture of a 2-layer Neural Network ( note that the input layer is typically excluded when counting the number of layers in a Neural Network ) Creating a Neural Network class in Python is easy. Training the Neural Network The output  ŷ  of a simple 2-layer Neural Network is: You might notice that in the equation above, the weights  W  and the biases  b  are the only variables that affects the output  ŷ. Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as  training the Neural Network. Each iteration of the training process consists of the following steps: The sequential graph below illustrates the process. As we’ve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is: Let’s add a feedforward function in our python code to do exactly that. Note that for simplicity, we have assumed the biases to be 0. However, we still need a way to evaluate the “goodness” of our predictions (i.e. how far off are our predictions)? The  Loss Function  allows us to do exactly that. There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we’ll use a simple  sum-of-sqaures error  as our loss function. That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference. Our goal in training is to find the best set of weights and biases that minimizes the loss function. Now that we’ve measured the error of our prediction (loss), we need to find a way to  propagate  the error back, and to update our weights and biases. In order to know the appropriate amount to adjust the weights and biases by, we need to know the  derivative of the loss function with respect to the weights and biases . Recall from calculus that the derivative of a function is simply the slope of the function. If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as  gradient descent . However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the  chain rule  to help us calculate it. Phew! That was ugly but it allows us to get what we needed — the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly. Now that we have that, let’s add the backpropagation function into our python code. For a deeper understanding of the application of calculus and the chain rule in backpropagation, I strongly recommend this tutorial by 3Blue1Brown. Now that we have our complete python code for doing feedforward and backpropagation, let’s apply our Neural Network on an example and see how well it does. Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn’t exactly trivial for us to work out the weights just by inspection alone. Let’s train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss  monotonically decreasing towards a minimum.  This is consistent with the gradient descent algorithm that we’ve discussed earlier. Let’s look at the final prediction (output) from the Neural Network after 1500 iterations. We did it! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values. Note that there’s a slight difference between the predictions and the actual values. This is desirable, as it prevents  overfitting  and allows the Neural Network to  generalize  better to unseen data. Fortunately for us, our journey isn’t over. There’s still  much  to learn about Neural Networks and Deep Learning. For example: I’ll be writing more on these topics soon, so do follow me on Medium and keep and eye out for them! I’ve certainly learnt a lot writing my own Neural Network from scratch. Although Deep Learning libraries such as TensorFlow and Keras makes it easy to build deep nets without fully understanding the inner workings of a Neural Network, I find that it’s beneficial for aspiring data scientist to gain a deeper understanding of Neural Networks. This exercise has been a great investment of my time, and I hope that it’ll be useful for you as well!"
What exactly can you do with Python? Here are Python’s 3 main applications.,hat exactly can you do with Python? Here are Python’s 3 main applications,"If you’re thinking of learning Python — or if you recently started learning it — you may be asking yourself: “What exactly can I use Python for?” Well that’s a tricky question to answer, because there are so many applications for Python. But over time, I have observed that there are 3 main popular applications for Python: Let’s talk about each of them in turn. Web frameworks that are based on Python like  Django  and  Flask  have recently become very popular for web development. These web frameworks help you create server-side code (backend code) in Python. That’s the code that runs on your server, as opposed to on users’ devices and browsers (front-end code). If you’re not familiar with the difference between backend code and front-end code, please see my footnote below. That’s because a web framework makes it easier to build common backend logic. This includes mapping different URLs to chunks of Python code, dealing with databases, and generating HTML files users see on their browsers. Django and Flask are two of the most popular Python web frameworks. I’d recommend using one of them if you’re just getting started. There’s an  excellent article  about this topic by Gareth Dwyer, so let me quote it here: <begin quote> Main contrasts: You should probably choose: </end quote> In other words, If you’re a beginner, Flask is probably a better choice because it has fewer components to deal with. Also, Flask is a better choice if you want more customization. On the other hand, if you’re looking to build something straight-forward, Django will probably let you get there faster. Now, if you’re looking to learn Django, I recommend the book called Django for Beginners. You can find it  here . You can also find the free sample chapters of that book  here . Okay, let’s go to the next topic! I think the best way to explain what machine learning is would be to give you a simple example. Let’s say you want to develop a program that automatically detects what’s in a picture. So, given this picture below (Picture 1), you want your program to recognize that it’s a dog. Given this other one below (Picture 2), you want your program to recognize that it’s a table. You might say, well, I can just write some code to do that. For example, maybe if there are a lot of light brown pixels in the picture, then we can say that it’s a dog. Or maybe, you can figure out how to detect edges in a picture. Then, you might say, if there are many straight edges, then it’s a table. However, this kind of approach gets tricky pretty quickly. What if there’s a white dog in the picture with no brown hair? What if the picture shows only the round parts of the table? This is where machine learning comes in. Machine learning typically implements an algorithm that automatically detects a pattern in the given input. You can give, say, 1,000 pictures of a dog and 1,000 pictures of a table to a machine learning algorithm. Then, it will learn the difference between a dog and a table. When you give it a new picture of either a dog or a table, it will be able to recognize which one it is. I think this is somewhat similar to how a baby learns new things. How does a baby learn that one thing looks like a dog and another a table? Probably from a bunch of examples. You probably don’t explicitly tell a baby, “If something is furry and has light brown hair, then it’s probably a dog.” You would probably just say, “That’s a dog. This is also a dog. And this one is a table. That one is also a table.” Machine learning algorithms work much the same way. You can apply the same idea to: among other applications. Popular machine learning algorithms you might have heard about include: You can use any of the above algorithms to solve the picture-labeling problem I explained earlier. There are popular machine learning libraries and frameworks for Python. Two of the most popular ones are  scikit-learn  and  TensorFlow . If you’re just getting started with a machine learning project, I would recommend that you first start with scikit-learn. If you start running into efficiency issues, then I would start looking into TensorFlow. To learn machine learning fundamentals, I would recommend either  Stanford’s  or  Caltech’s  machine learning course. Please note that you need basic knowledge of calculus and linear algebra to understand some of the materials in those courses. Then, I would practice what you’ve learned from one of those courses with  Kaggle . It’s a website where people compete to build the best machine learning algorithm for a given problem. They have nice tutorials for beginners, too. To help you understand what these might look like, let me give you a simple example here. Let’s say you’re working for a company that sells some products online. Then, as a data analyst, you might draw a bar graph like this. From this graph, we can tell that men bought over 400 units of this product and women bought about 350 units of this product this particular Sunday. As a data analyst, you might come up with a few possible explanations for this difference. One obvious possible explanation is that this product is more popular with men than with women. Another possible explanation might be that the sample size is too small and this difference was caused just by chance. And yet another possible explanation might be that men tend to buy this product more only on Sunday for some reason. To understand which of these explanations is correct, you might draw another graph like this one. Instead of showing the data for Sunday only, we’re looking at the data for a full week. As you can see, from this graph, we can see that this difference is pretty consistent over different days. From this little analysis, you might conclude that the most convincing explanation for this difference is that this product is simply more popular with men than with women. On the other hand, what if you see a graph like this one instead? Then, what explains the difference on Sunday? You might say, perhaps men tend to buy more of this product only on Sunday for some reason. Or, perhaps it was just a coincidence that men bought more of it on Sunday. So, this is a simplified example of what data analysis might look like in the real world. The data analysis work I did when I was working at Google and Microsoft was very similar to this example — only more complex. I actually used Python at Google for this kind of analysis, while I used JavaScript at Microsoft. I used SQL at both of those companies to pull data from our databases. Then, I would use either Python and Matplotlib (at Google) or JavaScript and D3.js (at Microsoft) to visualize and analyze this data. One of the most popular libraries for data visualization is  Matplotlib . It’s a good library to get started with because: How should I learn data analysis / visualization with Python? You should first learn the fundamentals of data analysis and visualization. When I looked for good resources for this online, I couldn’t find any. So, I ended up making a YouTube video on this topic: I also ended up making a  full course on this topic on Pluralsight , which you can take for free by signing up to their 10-day free trial. I’d recommend both of them. After learning the fundamentals of data analysis and visualization, learning fundamentals of statistics from websites like Coursera and Khan Academy will be helpful, as well. Scripting usually refers to writing small programs that are designed to automate simple tasks. So, let me give you an example from my personal experience here. I used to work at a small startup in Japan where we had an email support system. It was a system for us to respond to questions customers sent us via email. When I was working there, I had the task of counting the numbers of emails containing certain keywords so we could analyze the emails we received. We could have done it manually, but instead, I wrote a simple program / simple script to automate this task. Actually, we used Ruby for this back then, but Python is also a good language for this kind of task. Python is suited for this type of task mainly because it has relatively simple syntax and is easy to write. It’s also quick to write something small with it and test it. I’m not an expert on embedded applications, but I know that Python works with Rasberry Pi. It seems like a popular application among hardware hobbyists. You could use the library called PyGame to develop games, but it’s not the most popular gaming engine out there. You could use it to build a hobby project, but I personally wouldn’t choose it if you’re serious about game development. Rather, I would recommend getting started with Unity with C#, which is one of the most popular gaming engines. It allows you to build a game for many platforms, including Mac, Windows, iOS, and Android. You could make one with Python using Tkinter, but it doesn’t seem like the most popular choice either. Instead, it seems like languages like  Java, C#, and C++  are more popular for this. Recently, some companies have started using JavaScript to create Desktop applications, too. For example, Slack’s desktop app was built with something called Electron . It allows you to build desktop applications with JavaScript. Personally, if I was building a desktop application, I would go with a JavaScript option. It allows you to reuse some of the code from a web version if you have it. However, I’m not an expert on desktop applications either, so please let me know in a comment if you disagree or agree with me on this. I would recommend Python 3 since it’s more modern and it’s a more popular option at this point. Let’s say you want to make something like Instagram. Then, you’d need to create front-end code for each type of device you want to support. You might use, for example: Each set of code will run on each type of device / browser. This will be the set of code that determines what the layout of the app will be like, what the buttons should look like when you click them, etc. However, you will still need the ability to store users’ info and photos. You will want to store them on your server and not just on your users’ devices so each user’s followers can view his/her photos. This is where the backend code / server-side code comes in. You’ll need to write some backend code to do things like: So, this is the difference between backend code and front-end code. By the way, Python is not the only good choice for writing backend / server-side code. There are many other popular choices, including Node.js, which is based on JavaScript. I have a programming education YouTube channel called  CS Dojo  with 440,000+ subscribers, where I produce more content like this article. For example, you might like these videos:"
Why Python is not the programming language of the future,hy Python is not the programming language of the futur,"It  took the programming community a couple of decades to appreciate Python. But since the early 2010’s, it has been booming — and eventually surpassing C, C#, Java and JavaScript in popularity. But until when will that trend continue? When will Python eventually be replaced by other languages, and why? Putting an exact expiry date on Python would be so much speculation, it might as well pass as Science-Fiction. Instead, I will assess the virtues that are boosting Python’s popularity right now, and the weak points that will break it in the future. Python’s success is reflected in the  Stack Overflow trends , which measure the count of tags in posts on the platform. Given the size of StackOverflow, this is quite a good indicator for language popularity. While R has been plateauing over the last few years, and many other languages are on a steady decline, Python’s growth seems unstoppable. Almost 14% of all StackOverflow questions are tagged “python”, and the trend is going up. And there are several reasons for that. Python has been around since the nineties. That doesn’t only mean that it has had plenty of time to grow. It has also acquired a large and supportive community. So if you have any issue while you’re coding in Python, the odds are high that you’ll be able to solve it with a single Google search. Simply because somebody will have already encountered your problem and written something helpful about it. It’s not only the fact that it has been around for decades, giving programmers the time to make brilliant tutorials. More than that, the syntax of Python is very human-readable. For a start, there’s no need to specify the data type. You just declare a variable; Python will understand from the context whether it’s an integer, a float value, a boolean or something else. This is a huge edge for beginners. If you’ve ever had to program in C++, you know how frustrating it is your program won’t compile because you swapped a float for an integer. And if you’ve ever had to read Python and C++ code side-by-side, you’ll know how understandable Python is. Even though C++ was designed with English in mind, it’s a rather bumpy read compared to Python code. medium.com Since Python has been around for so long, developers have made a package for every purpose. These days, you can find a package for almost everything. Want to crunch numbers, vectors and matrices?  NumPy  is your guy.  Want to do calculations for tech and engineering? Use  SciPy .  Want to go big in data manipulation and analysis? Give  Pandas  a go. Want to start out with Artificial Intelligence? Why not use  Scikit-Learn . Whichever computational task you’re trying to manage, chances are that there is a Python package for it out there. This makes Python stay on top of recent developments, can be seen from the surge in Machine Learning over the past few years. Based on the previous elaborations, you could imagine that Python will stay on top of sh*t for ages to come. But like every technology, Python has its weaknesses. I will go through the most important flaws, one by one, and assess whether these are fatal or not. Python is slow. Like, really slow. On average, you’ll need about 2–10 times longer to complete a task with Python than with any other language. There are  various reasons  for that. One of them is that it’s dynamically typed — remember that you don’t need to specify data types like in other languages. This means that a lot of memory needs to be used, because the program needs to reserve enough space for each variable that it works in any case. And lots of memory usage translates to lots of computing time. Another reason is that Python can only execute one task at a time. This is a consequence of flexible datatypes — Python needs to make sure each variable has only one datatype, and parallel processes could mess that up. In comparison, your average web browser can run a dozen different threads at once. And there are some other theories around, too. But at the end of the day, none of the speed issues matter. Computers and servers have gotten so cheap that we’re talking about fractions of seconds. And the end user doesn’t really care whether their app loads in 0.001 or 0.01 seconds. medium.com Originally, Python was  dynamically scoped . This basically means that, to evaluate an expression, a compiler first searches the current block and then successively all the calling functions. The problem with dynamic scoping is that every expression needs to be tested in every possible context — which is tedious. That’s why most modern programming languages use static scoping. Python tried to transition to static scoping, but  messed it up . Usually, inner scopes — for example functions within functions — would be able to see  and  change outer scopes. In Python, inner scopes can only see outer scopes, but not change them. This leads to a lot of confusion. Despite all of the flexibility within Python, the usage of Lambdas is rather restrictive. Lambdas can only be expressions in Python, and not be statements. On the other hand, variable declarations and statements are always statements. This means that Lambdas cannot be used for them. This distinction between expressions and statements is rather arbitrary, and doesn’t occur in other languages. In Python, you use whitespaces and indentations to indicate different levels of code. This makes it optically appealing and intuitive to understand. Other languages, for example C++, rely more on braces and semicolons. While this might not be visually appealing and beginner-friendly, it makes the code a lot more maintainable. For bigger projects, this is a lot more useful. Newer languages like Haskell solve this problem: They rely on whitespaces, but offer an alternative syntax for those who wish to go without. As we’re witnessing the shift from desktop to smartphone, it’s clear that we need robust languages to build mobile software. But not many mobile apps are being developed with Python. That doesn’t mean that it can’t be done — there is a Python package called Kivy for this purpose. But Python wasn’t made with mobile in mind. So even though it might produce passable results for basic tasks, your best bet is to use a language that was created for mobile app development. Some widely used programming frameworks for mobile include React Native, Flutter, Iconic, and Cordova. To be clear, laptops and desktop computers should be around for many years to come. But since mobile has long surpassed desktop traffic, it’s safe to say that learning Python is not enough to become a seasoned all-round developer. A Python script isn’t compiled first and then executed. Instead, it compiles every time you execute it, so any coding error manifests itself at runtime. This leads to poor performance, time consumption, and the need for a lot of tests. Like,  a lot  of tests. This is great for beginners since testing teaches them a lot. But for seasoned developers, having to debug a complex program in Python makes them go awry. This lack of performance is the biggest factor that sets a timestamp on Python. towardsdatascience.com There are a few new competitors on the market of programming languages: While there are other languages on the market, Rust, Go, and Julia are the ones that fix weak patches of Python. All of these languages excel in yet-to-come technologies, most notably in Artificial Intelligence. While their market share is still small, as reflected in the number of StackOverflow tags, the trend for all of them is clear: upwards. Given the ubiquitous popularity of Python at the moment, it will surely take half a decade, maybe even a whole, for any of these new languages to replace it. Which of the languages it will be — Rust, Go, Julia, or a new language of the future — is hard to say at this point. But given the performance issues that are fundamental in the architecture of Python, one will inevitably take its spot."
Building a Simple Chatbot from Scratch in Python (using NLTK),uilding a Simple Chatbot from Scratch in Python (using NLTK,"I am sure you’ve heard about  Duolingo : a popular language-learning app, which gamifies practicing a new language. It is pretty popular due to its innovative styles of teaching a foreign language. The concept is simple: five to ten minutes of interactive training a day is enough to learn a language."
The Next Level of Data Visualization in Python,he Next Level of Data Visualization in Pytho,"The sunk-cost fallacy is one of many  harmful cognitive biases  to which humans fall prey. It  refers to our tendency  to continue to devote time and resources to a lost cause because we have already spent — sunk — so much time in the pursuit. The sunk-cost fallacy applies to staying in bad jobs longer than we should, slaving away at a project even when it’s clear it won’t work, and yes, continuing to use a tedious, outdated plotting library — matplotlib — when more efficient, interactive, and better-looking alternatives exist. Over the past few months, I’ve realized the only reason I use  matplotlib  is the hundreds of hours I’ve sunk into learning the  convoluted syntax . This complication leads to hours of frustration on StackOverflow figuring out how to  format dates  or  add a second y-axis . Fortunately, this is a great time for Python plotting, and after exploring  the options , a clear winner — in terms of ease-of-use, documentation, and functionality — is the  plotly Python library.  In this article, we’ll dive right into  plotly , learning how to make better plots in less time — often with one line of code. All of the code for this article is  available on GitHub . The charts are all interactive and can be viewed on  NBViewer here . The  plotly   Python package is an open-source library built on  plotly.js  which  in turn is built on  d3.js .  We’ll be using a wrapper on plotly called  cufflinks  designed to work with Pandas dataframes. So, our entire stack is cufflinks > plotly > plotly.js > d3.js which means we get the efficiency of coding in Python with the incredible  interactive graphics capabilities of d3. ( Plotly itself  is a graphics company with several products and open-source tools. The Python library is free to use, and we can make unlimited charts in offline mode plus up to 25 charts in online mode to  share with the world .) All the work in this article was done in a Jupyter Notebook with plotly + cufflinks running in offline mode. After installing plotly and cufflinks with  pip install cufflinks plotly  import the following to run in Jupyter: Single variable — univariate — plots are a standard way to start an analysis and the histogram is a go-to plot ( although it has some issues ) for graphing a distribution. Here, using my Medium article statistics (you can see  how to get your own stats here  or use  mine here ) let’s make an interactive histogram of the number of claps for articles (  df  is a standard Pandas dataframe): For those used to  matplotlib , all we have to do is add one more letter (  iplot  instead of  plot ) and we get a much better-looking and interactive chart! We can click on the data to get more details, zoom into sections of the plot, and as we’ll see later, select different categories to highlight. If we want to plot overlaid histograms, that’s just as simple: With a little bit of  pandas  manipulation, we can do a barplot: s we saw, we can combine the  power of pandas  with plotly + cufflinks. For a boxplot of the fans per story by publication, we use a  pivot  and then plot: The benefits of interactivity are that we can explore and subset the data as we like. There’s a lot of information in a boxplot, and without the ability to see the numbers, we’ll miss most of it! The scatterplot is the heart of most analyses. It allows us to see the evolution of a variable over time or the relationship between two (or more) variables. A considerable portion of real-world data has a time element. Luckily, plotly + cufflinks was designed with time-series visualizations in mind. Let’s make a dataframe of my TDS articles and look at how the trends have changed. Here we are doing quite a few different things all in one line: For more information, we can also add in text annotations quite easily: For a two-variable scatter plot colored by a third categorical variable we use: Let’s get a little more sophisticated by using a log axis — specified as a plotly layout — (see the  Plotly documentation  for the layout specifics) and sizing the bubbles by a numeric variable: With a little more work ( see notebook for details ), we can even put four variables ( this is not advised ) on one graph! As before, we can combine pandas with plotly+cufflinks for useful plots See the notebook  or the documentation  for more examples of added functionality. We can add in text annotations, reference lines, and best-fit lines to our plots with a single line of code, and still with all the interaction. Now we’ll get into a few plots that you probably won’t use all that often, but which can be quite impressive. We’ll use the  plotly  figure_factory , to keep even these incredible plots to one line. When we want to explore relationships among many variables, a  scattermatrix  (also called a splom) is a great option: Even this plot is completely interactive allowing us to explore the data. To visualize the correlations between numeric variables, we calculate the correlations and then make an annotated heatmap: The list of plots goes on and on. Cufflinks also has several themes we can use to get completely different styling with no effort. For example, below we have a ratio plot in the “space” theme and a spread plot in “ggplot”: We also get 3D plots (surface and bubble): For those  who are so inclined , you can even make a pie chart: When you make these plots in the notebook, you’ll notice a small link on the lower right-hand side on the graph that says “Export to plot.ly”. If you click that link, you are then taken to the  chart studio  where you can touch up your plot for a final presentation. You can add annotations, specify the colors, and generally clean everything up for a great figure. Then, you can publish your figure online so anyone can find it with the link. Below are two charts I touched up in Chart Studio: With everything mentioned here, we are still not exploring the full capabilities of the library! I’d encourage you to check out both the plotly and the cufflinks documentation for more incredible graphics. The worst part about the sunk cost fallacy is you only realize how much time you’ve wasted after you’ve quit the endeavor. Fortunately, now that I’ve made the mistake of sticking with  matploblib  for too long, you don’t have to! When thinking about plotting libraries, there are a few things we want: As of right now, the best option for doing all of these in  Python is plotly . Plotly allows us to make visualizations quickly and helps us get better insight into our data through interactivity. Also, let’s admit it, plotting should be one of the most enjoyable parts of data science! With other libraries, plotting turned into a tedious task, but with plotly, there is again joy in making a great figure! Now that it’s 2019, it is time to upgrade your Python plotting library for better efficiency, functionality, and aesthetics in your data science visualizations. As always, I welcome feedback and constructive criticism. I can be reached on Twitter  @koehrsen_will."
Bye-bye Python. Hello Julia!,ye-bye Python. Hello Julia,"D on’t get me wrong. Python’s popularity is still backed by a rock-solid community of computer scientists, data scientists and AI specialists. But if you’ve ever been at a dinner table with these people, you also know how much they rant about the weaknesses of Python. From being slow to requiring excessive testing, to producing runtime errors despite prior testing — there’s enough to be pissed off about. Which is why more and more programmers are adopting other languages — the top players being Julia, Go, and Rust. Julia is great for mathematical and technical tasks, while Go is awesome for modular programs, and Rust is the top choice for systems programming. Since data scientists and AI specialists deal with lots of mathematical problems, Julia is the winner for them. And even upon critical scrutiny, Julia has upsides that Python can’t beat. towardsdatascience.com When people create a new programming language, they do so because they want to keep the good features of old languages and fix the bad ones. In this sense, Guido van Rossum created Python in the late 1980s to improve ABC. The latter was  too perfect  for a programming language — while its rigidity made it easy to teach, it was hard to use in real life. In contrast, Python is quite pragmatic. You can see this in the  Zen of Python , which reflects the intention that the creators have: Python still kept the good features of ABC: Readability, simplicity, and beginner-friendliness for example. But Python is far more robust and adapted to real life than ABC ever was. In the same sense, the creators of Julia want to keep the good parts of other languages and ditch the bad ones. But Julia is a lot more ambitious: instead of replacing one language, it wants to beat them all. This is how  Julia’s creators  say it: Julia wants to blend all upsides that currently exist, and not trade them off for the downsides in other languages. And even though Julia is a young language, it has already achieved a lot of the goals that the creators set. Julia can be used for everything from simple machine learning applications to enormous supercomputer simulations. To some extent, Python can do this, too — but Python somehow grew into the job. In contrast,  Julia was built  precisely for this stuff. From the bottom up. Julia’s creators wanted to make a language that is as fast as C — but what they created is  even faster . Even though Python has become easier to speed up in recent years, its performance is still a far cry from what Julia can do. In 2017, Julia even joined the  Petaflop Club  — the small club of languages who can exceed speeds of one petaflop per second at peak performance. Apart from Julia, only C, C++ and Fortran are  in the club  right now. towardsdatascience.com With its more than 30 years of age, Python has an enormous and supportive community. There is hardly a Python-related question that you can’t get answered within one Google search. In contrast, the Julia community is pretty tiny. While this means that you might need to dig a bit further to find an answer, you might link up with the same people again and again. And this can turn into programmer-relationships that are beyond value. You don’t even need to know a single Julia-command to code in Julia. Not only can you use Python and C code within Julia. You can even use  Julia within Python ! Needless to say, this makes it extremely easy to patch up the weaknesses of your Python code. Or to stay productive while you’re still getting to know Julia. This is one of the strongest points of Python — its zillion well-maintained libraries. Julia doesn’t have many libraries, and users have complained that they’re not amazingly maintained (yet). But when you consider that Julia is a very young language with a limited amount of resources, the number of libraries that they already have is pretty impressive. Apart from the fact that Julia’s amount of libraries is growing, it can also interface with libraries from C and Fortran to handle plots, for example. Python is 100% dynamically typed. This means that the program decides at runtime whether a variable is a float or an integer, for example. While this is extremely beginner-friendly, it also introduces a whole host of possible bugs. This means that you need to test Python code in all possible scenarios — which is quite a dumb task that takes a lot of time. Since the Julia-creators also wanted it to be easy to learn, Julia fully supports dynamical typing. But in contrast to Python, you can introduce static types if you like — in the way they are present in C or Fortran, for example. This can save you a ton of time: Instead of finding  excuses for not testing  your code, you can specify the type wherever it makes sense. towardsdatascience.com While all these things sound pretty great, it’s important to keep in mind that Julia is still tiny compared to Python. One pretty good metric is the number of questions on StackOverflow: At this point in time, Python is tagged about twenty more often than Julia! This doesn’t mean that Julia is unpopular — rather, it’s naturally taking some time to get adopted by programmers. Think about it — would you really want to write your whole code in a different language? No, you’d rather try a new language in some future project. This creates a time lag that every programming language faces between its release and its adoption. But if you adopt it now — which is easy because Julia allows an enormous amount of language conversion — you’re investing in the future. As more and more people adopt Julia, you’ll already have gained enough experience to answer their questions. Also, your code will be more durable as more and more Python code is replaced by Julia. Forty years ago, artificial intelligence was nothing but a niche phenomenon. The industry and investors didn’t believe in it, and many technologies were clunky and hard to use. But those who learned it back then are the giants of today — those that are so high in demand that  their salary  matches that of an NFL player. Similarly, Julia is still very niche now. But when it grows, the big winners will be those who adopted it early. I’m not saying that you’re guaranteed to make a shitload of money in ten years if you adopt Julia now. But you’re increasing your chances. Think about it: Most programmers out there have Python on their CV. And in the next few years, we’ll see even more Python programmers on the job market. But if the demand of enterprises for Python slows, the perspectives for Python programmers are going to go down. Slowly at first, but inevitably. On the other hand, you have a real edge if you can put Julia on your CV. Because let’s be honest, what distinguishes you from any other Pythonista out there? Not much. But there won’t be that many Julia-programmers out there, even in three years’ time. With Julia-skills, not only are you showing that you have interests beyond the job requirements. You’re also demonstrating that you’re eager to learn and that you have a broader sense of what it means to be a programmer. In other words, you’re fit for the job. You — and the other Julia programmers — are future rockstars, and you know it. Or, as  Julia’s creators  said it in 2012: Python is still insanely popular. But if you learn Julia now, that could be your golden ticket later on. In this sense: Bye-bye Python. Hello Julia! Edit: I’ve given a talk on Julia vs. Python! It was hosted by  Hatchpad , and the video is  here ."
Turn Python Scripts into Beautiful ML Tools,urn Python Scripts into Beautiful ML Tool,"In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools — often a patchwork of Jupyter Notebooks and Flask apps — are difficult to deploy, require reasoning about client-server architecture, and don’t integrate well with machine learning constructs like Tensorflow GPU sessions. I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on. As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares. When a tool became crucial, we  called in the tools team . They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a  design process : Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, “we’ll update your tool again in two months.” So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question:  What if we could make building tools as easy as writing Python scripts? We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should  feel  like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this: With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create  Streamlit , a  completely free and open source  app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are: #1: Embrace Python scripting.  Streamlit apps are really just scripts that run from top to bottom. There’s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen: #2: Treat widgets as variables.  There are  no callbacks in Streamlit ! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code: #3: Reuse data and computation.  What if you download lots of data or perform complex computation? The key is to  safely reuse  information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code  downloads data only once  from the  Udacity Self-driving car project , yielding a simple, fast app: In short, Streamlit works like this: Or in pictures: If this sounds intriguing, you can try it right now! Just run: This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link. Ok. Are you back from playing with fractals? Those can be mesmerizing. The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project I’ve seen eventually has had entire teams working on this tooling. Building such a tool in Streamlit is easy.  This Streamlit demo  lets you perform semantic search across the entire  Udacity self-driving car photo dataset , visualize human-annotated ground truth labels, and  run a complete neural net ( YOLO ) in real time  from within the app [1]. The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are  only 23 Streamlit calls in the whole app . You can run it yourself right now! As we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits: Streamlit apps are pure Python files.  So you can use your favorite editor and debugger with Streamlit. Pure Python scripts work seamlessly with Git  and other source control software, including commits, pull requests, issues, and comments. Because Streamlit’s underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free 🎉. Streamlit provides an immediate-mode live coding environment.  Just click  Always rerun  when Streamlit detects a source file change. Caching simplifies setting up computation pipelines.  Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider  this code  adapted from our  Udacity demo : Basically, the pipeline is load_metadata → create_summary. Every time the script is run  Streamlit only recomputes whatever subset of the pipeline is required to get the right answer . Cool! Streamlit is built for GPUs.  Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlit’s cache stores the entire  NVIDIA celebrity face GAN  [2]. This approach enables nearly instantaneous inference as the user updates sliders. Streamlit is a free and open-source library rather than a proprietary web app . You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally. This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. There’s a lot more we could say about how our architecture works and the features we have planned, but we’ll save that for future posts. We’re excited to finally share Streamlit with the community today and see what you all build with it. We hope that you’ll find it easy and delightful to turn your Python scripts into beautiful ML apps. Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Geneviève Wachtell, and Barney Pell for their helpful input on this article. References: [1] J. Redmon and A. Farhadi,  YOLOv3: An Incremental Improvement  (2018), arXiv. [2] T. Karras, T. Aila, S. Laine, and J. Lehtinen,  Progressive Growing of GANs for Improved Quality, Stability, and Variation  (2018), ICLR. [3] S. Guan,  Controlled image synthesis and editing using a novel TL-GAN model  (2018), Insight Data Science Blog."
A Complete Machine Learning Project Walk-Through in Python: Part One, Complete Machine Learning Project Walk-Through in Python: Part On,"Reading through a data science book or taking a course, it can feel like you have the individual pieces, but don’t quite know how to put them together. Taking the next step and solving a complete machine learning problem can be daunting, but preserving and completing a first project will give you the confidence to tackle any data science problem. This series of articles will walk through a complete machine learning solution with a real-world dataset to let you see how all the pieces come together. We’ll follow the general machine learning workflow step-by-step: Along the way, we’ll see how each step flows into the next and how to specifically implement each part in Python. The  complete project  is available on GitHub, with the  first notebook here.  This first article will cover steps 1–3 with the rest addressed in subsequent posts. (As a note, this problem was originally given to me as an “assignment” for a job screen at a start-up. After completing the work, I was offered the job, but then the CTO of the company quit and they weren’t able to bring on any new employees. I guess that’s how things go on the start-up scene!) The first step before we get coding is to understand the problem we are trying to solve and the available data. In this project, we will work with  publicly available building energy data  from New York City. The objective is to use the energy data to build a model that can predict the Energy Star Score of a building and interpret the results to find the factors which influence the score. The data includes the Energy Star Score, which makes this a supervised regression machine learning task: We want to develop a model that is both  accurate  — it can predict the Energy Star Score close to the true value — and  interpretable  — we can understand the model predictions. Once we know the goal, we can use it to guide our decisions as we dig into the data and build models. Contrary to what most data science courses would have you believe, not every dataset is a perfectly curated group of observations with no missing values or anomalies (looking at you  mtcars  and  iris  datasets). Real-world data is messy which means we need to  clean and wrangle  it into an acceptable format before we can even start the analysis. Data cleaning is an un-glamorous, but necessary part of most actual data science problems. First, we can load in the data as a Pandas  DataFrame  and take a look: This is a subset of the full data which contains 60 columns. Already, we can see a couple issues: first, we know that we want to predict the  ENERGY STAR Score  but we don’t know what any of the columns mean. While this isn’t necessarily an issue — we can often make an accurate model without any knowledge of the variables — we want to focus on interpretability, and it might be important to understand at least some of the columns. When I originally got the assignment from the start-up, I didn’t want to ask what all the column names meant, so I looked at the name of the file, and decided to search for “Local Law 84”. That led me to  this page  which explains this is an NYC law requiring all buildings of a certain size to report their energy use. More searching brought me to  all the definitions of the columns.  Maybe looking at a file name is an obvious place to start, but for me this was a reminder to go slow so you don’t miss anything important! We don’t need to study all of the columns, but we should at least understand the Energy Star Score, which is described as: A 1-to-100 percentile ranking based on self-reported energy usage for the reporting year. The  Energy Star score  is a relative measure used for comparing the energy efficiency of buildings. That clears up the first problem, but the second issue is that missing values are encoded as “Not Available”. This is a string in Python which means that even the columns with numbers will be stored as  object  datatypes because Pandas converts a column with any strings into a column of all strings. We can see the datatypes of the columns using the  dataframe.info() method: Sure enough, some of the columns that clearly contain numbers (such as ft²), are stored as objects. We can’t do numerical analysis on strings, so these will have to be converted to number (specifically  float ) data types! Here’s a little Python code that replaces all the “Not Available” entries with not a number (  np.nan ), which can be interpreted as numbers, and then converts the relevant columns to the  float  datatype: Once the correct columns are numbers, we can start to investigate the data. In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let’s get a sense of how many missing values are in each column (see the  notebook for code ). (To create this table, I used a function from this  Stack Overflow Forum ). While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem ( here is a discussion ), and for this project, we will remove any columns with more than 50% missing values. At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the  definition of extreme outliers : (For the code to remove the columns and the anomalies, see the notebook). At the end of the data cleaning and anomaly removal process, we are left with over 11,000 buildings and 49 features. Now that the tedious — but necessary — step of data cleaning is complete, we can move on to exploring our data!  Exploratory Data Analysis  (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. In short, the goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find interesting parts of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use. The goal is to predict the Energy Star Score (renamed to  score  in our data) so a reasonable place to start is examining the distribution of this variable. A histogram is a simple yet effective way to visualize the distribution of a single variable and is easy to make using  matplotlib . This looks quite suspicious! The Energy Star score is a percentile rank, which means we would expect to see a uniform distribution, with each score assigned to the same number of buildings. However, a disproportionate number of buildings have either the highest, 100, or the lowest, 1, score (higher is better for the Energy Star score). If we go back to the definition of the score, we see that it is based on “self-reported energy usage” which might explain the very high scores. Asking building owners to report their own energy usage is like asking students to report their own scores on a test! As a result, this probably is not the most objective measure of a building’s energy efficiency. If we had an unlimited amount of time, we might want to investigate why so many buildings have very high and very low scores which we could by selecting these buildings and seeing what they have in common. However, our objective is only to predict the score and not to devise a better method of scoring buildings! We can make a note in our report that the scores have a suspect distribution, but our main focus in on predicting the score. A major part of EDA is searching for relationships between the features and the target. Variables that are correlated with the target are useful to a model because they can be used to predict the target. One way to examine the effect of a categorical variable (which takes on only a limited set of values) on the target is through a density plot using the  seaborn  library. A  density plot can be thought of as a smoothed histogram  because it shows the distribution of a single variable. We can color a density plot by class to see how a categorical variable changes the distribution. The following code makes a density plot of the Energy Star Score colored by the the type of building (limited to building types with more than 100 data points): We can see that the building type has a significant impact on the Energy Star Score. Office buildings tend to have a higher score while Hotels have a lower score. This tells us that we should include the building type in our modeling because it does have an impact on the target. As a categorical variable, we will have to one-hot encode the building type. A similar plot can be used to show the Energy Star Score by borough: The borough does not seem to have as large of an impact on the score as the building type. Nonetheless, we might want to include it in our model because there are slight differences between the boroughs. To quantify relationships between variables, we can use the  Pearson Correlation Coefficient . This is a measure of the strength and direction of a linear relationship between two variables. A score of +1 is a perfectly linear positive relationship and a score of -1 is a perfectly negative linear relationship. Several values of the correlation coefficient are shown below: While the correlation coefficient cannot capture non-linear relationships, it is a good way to start figuring out how variables are related. In Pandas, we can easily calculate the correlations between any columns in a dataframe: The most negative (left) and positive (right) correlations with the target: There are several strong negative correlations between the features and the target with the most negative the different categories of EUI (these measures vary slightly in how they are calculated). The  EUI — Energy Use Intensity  — is the amount of energy used by a building divided by the square footage of the buildings. It is meant to be a measure of the efficiency of a building with a lower score being better. Intuitively, these correlations make sense: as the EUI increases, the Energy Star Score tends to decrease. To visualize relationships between two continuous variables, we use scatterplots. We can include additional information, such as a categorical variable, in the color of the points. For example, the following plot shows the Energy Star Score vs. Site EUI colored by the building type: This plot lets us visualize what a correlation coefficient of -0.7 looks like. As the Site EUI decreases, the Energy Star Score increases, a relationship that holds steady across the building types. The final exploratory plot we will make is known as the  Pairs Plot. This is a great exploration tool  because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the  PairGrid  function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle. To see interactions between variables, we look for where a row intersects with a column. For example, to see the correlation of  Weather Norm EUI  with  score , we look in the  Weather Norm EUI  row and the  score  column and see a correlation coefficient of -0.67. In addition to looking cool, plots such as these can help us decide which variables to include in modeling. Feature engineering and selection  often provide the greatest return on time invested in a machine learning problem. First of all, let’s define what these two tasks are: A machine learning model can only learn from the data we provide it, so ensuring that data includes all the relevant information for our task is crucial. If we don’t feed a model the correct data, then we are setting it up to fail and we should not expect it to learn! For this project, we will take the following feature engineering steps: One-hot encoding  is necessary to include categorical variables in a model. A machine learning algorithm cannot understand a building type of “office”, so we have to record it as a 1 if the building is an office and a 0 otherwise. Adding transformed features can help our model learn non-linear relationships within the data.  Taking the square root, natural log, or various powers of features  is common practice in data science and can be based on domain knowledge or what works best in practice. Here we will include the natural log of all numerical features. The following code selects the numeric features, takes log transformations of these features, selects the two categorical features, one-hot encodes these features, and joins the two sets together. This seems like a lot of work, but it is relatively straightforward in Pandas! After this process we have over 11,000 observations (buildings) with 110 columns (features). Not all of these features are likely to be useful for predicting the Energy Star Score, so now we will turn to feature selection to remove some of the variables. Many of the 110 features we have in our data are redundant because they are highly correlated with one another. For example, here is a plot of Site EUI vs Weather Normalized Site EUI which have a correlation coefficient of 0.997. Features that are strongly correlated with each other are known as  collinear  and removing one of the variables in these pairs of features can often help a  machine learning model generalize and be more interpretable . (I should point out we are talking about correlations of features with other features, not correlations with the target, which help our model!) There are a number of methods to calculate collinearity between features, with one of the most common the  variance inflation factor . In this project, we will use thebcorrelation coefficient to identify and remove collinear features. We will drop one of a pair of features if the correlation coefficient between them is greater than 0.6. For the implementation, take a look at the notebook (and  this Stack Overflow answer ) While this value may seem arbitrary, I tried several different thresholds, and this choice yielded the best model. Machine learning is an  empirical field  and is often about experimenting and finding what performs best! After feature selection, we are left with 64 total features and 1 target. We have now completed data cleaning, exploratory data analysis, and feature engineering. The final step to take before getting started with modeling is establishing a naive baseline. This is essentially a guess against which we can compare our results. If the machine learning models do not beat this guess, then we might have to conclude that machine learning is not acceptable for the task or we might need to try a different approach. For regression problems, a reasonable naive baseline is to guess the median value of the target on the training set for all the examples in the test set. This sets a relatively low bar for any model to surpass. The metric we will use is  mean absolute error  (mae)  which measures the average absolute error on the predictions. There are many metrics for regression, but I like  Andrew Ng’s advice  to pick a single metric and then stick to it when evaluating models. The mean absolute error is easy to calculate and is interpretable. Before calculating the baseline, we need to split our data into a training and a testing set: We will use 70% of the data for training and 30% for testing: Now we can calculate the naive baseline performance: The naive estimate is off by about 25 points on the test set. The score ranges from 1–100, so this represents an error of 25%, quite a low bar to surpass! In this article we walked through the first three steps of a machine learning problem. After defining the question, we: Finally, we also completed the crucial step of establishing a baseline against which we can judge our machine learning algorithms. The second post ( available here ) will show how to evaluate machine learning models using  Scikit-Learn , select the best model, and perform hyperparameter tuning to optimize the model. The third post, dealing with model interpretation and reporting results,  is here . As always, I welcome feedback and constructive criticism and can be reached on Twitter  @koehrsen_will ."
Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,achine Learning is Fun! Part 4: Modern Face Recognition with Deep Learnin,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  普通话 ,  Русский ,  한국어 ,  Português ,  Tiếng Việt ,  فارسی  or  Italiano . Giant update:   I’ve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you  like magic : This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing technology — Facebook can recognize faces with  98% accuracy  which is pretty much as good as humans can do! Let’s learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem — telling  Will Ferrell  (famous actor) apart from  Chad Smith  (famous rock musician)! So far in  Part 1 ,  2  and  3 , we’ve used machine learning to solve isolated problems that have only one step —  estimating the price of a house ,  generating new data based on existing data  and  telling if an image contains a certain object . All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result. But face recognition is really a series of several related problems: As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are  too good  at recognizing faces and end up seeing faces in everyday objects: Computers are not capable of this kind of high-level generalization ( at least not yet… ), so we have to teach them how to do each step in this process separately. We need to build a  pipeline  where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms: Let’s tackle this problem one step at a time. For each step, we’ll learn about a different machine learning algorithm. I’m not going to explain every single algorithm completely to keep this from turning into a book, but you’ll learn the main ideas behind each one and you’ll learn how you can build your own facial recognition system in Python using  OpenFace  and  dlib . The first step in our pipeline is  face detection . Obviously we need to locate the faces in a photograph before we can try to tell them apart! If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action: Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we’ll use it for a different purpose — finding the areas of the image we want to pass on to the next step in our pipeline. Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a  way to detect faces  that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We’re going to use  a method invented in 2005  called Histogram of Oriented Gradients — or just  HOG  for short. To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces: Then we’ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it: Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker: If you repeat that process for  every single pixel  in the image, you end up with every pixel being replaced by an arrow. These arrows are called  gradients  and they show the flow from light to dark across the entire image: This might seem like a random thing to do, but there’s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the  direction  that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve! But saving the gradient for every single pixel gives us way too much detail. We end up  missing the forest for the trees . It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image. To do this, we’ll break up the image into small squares of 16x16 pixels each. In each square, we’ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc…). Then we’ll replace that square in the image with the arrow directions that were the strongest. The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way: To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces: Using this technique, we can now easily find faces in any image: If you want to try this step out yourself using Python and dlib,  here’s code  showing how to generate and view HOG representations of images. Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer: To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps. To do this, we are going to use an algorithm called  face landmark estimation . There are lots of ways to do this, but we are going to use the approach  invented in 2014 by Vahid Kazemi and Josephine Sullivan. The basic idea is we will come up with 68 specific points (called  landmarks ) that exist on every face — the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face: Here’s the result of locating the 68 face landmarks on our test image: Now that we know were the eyes and mouth are, we’ll simply rotate, scale and  shear  the image so that the eyes and mouth are centered as best as possible. We won’t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called  affine transformations ): Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate. If you want to try this step out yourself using Python and dlib, here’s the  code for finding face landmarks  and here’s the  code for transforming the image  using those landmarks. Now we are to the meat of the problem — actually telling faces apart. This is where things get really interesting! The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right? There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours. What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you’ve ever watched a bad crime show like  CSI , you know what I am talking about: Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else? It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure. The solution is to train a Deep Convolutional Neural Network ( just like we did in Part 3 ). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart: After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements. Machine learning people call the 128 measurements of each face an  embedding . The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using  was invented in 2015 by researchers at Google  but many similar approaches exist. This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive  NVidia Telsa video card , it takes  about 24 hours  of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at  OpenFace  already did this and they  published several trained networks  which we can directly use. Thanks  Brandon Amos  and team! So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here’s the measurements for our test image: So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn’t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person. If you want to try this step yourself, OpenFace  provides a lua script  that will generate embeddings all images in a folder and write them to a csv file. You  run it like this . This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image. You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We’ll use a simple linear  SVM classifier , but lots of classification algorithms could work. All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person! So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon: Then I ran the classifier on every frame of the famous youtube video of  Will Ferrell and Chad Smith pretending to be each other  on the Jimmy Fallon show: It works! And look how well it works for faces in different poses — even sideways faces! Let’s review the steps we followed: Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer: UPDATE 4/9/2017:   You can still follow the steps below to use OpenFace. However, I’ve released a new Python-based face recognition library called  face_recognition  that is much easier to install and use. So I’d recommend trying out  face_recognition  first instead of continuing below! I even put together  a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed . You can download and run it on your computer very easily. Give the virtual machine a shot if you don’t want to install all these libraries yourself! Original OpenFace instructions: If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter: You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I’d love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 5 !"
"Every single Machine Learning course on the internet, ranked by your reviews","very single Machine Learning course on the internet, ranked by your review","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own  data science master’s program  using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role.   So I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few  coding classes  for the beginner data scientist. Then it was  statistics and probability classes . Then  introductions to data science . Also,  data visualization . For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings.  My end goal was to identify the three best courses available and present them to you, below. For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it’s David. I wrote this guide back in 2017. Since then, I’ve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros. datamaverickhq.com Okay, back to the guide. Each course must fit three criteria: We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on  Udemy , we chose to consider the most-reviewed and highest-rated ones only. There’s always a chance that we missed something, though. So please let us know in the comments section if we left a good course out. We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on three factors: A popular definition originates from  Arthur Samuel  in 1959: machine learning is a subfield of computer science that gives  “computers the ability to learn without being explicitly programmed.”  In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience. A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you’ll find helpful visualization of these core steps: The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves. First off, let’s define deep learning. Here is a succinct description: “Deep learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.” — Jason Brownlee from  Machine Learning Mastery As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we’ve got you covered with the following  article : medium.freecodecamp.com My top three recommendations from that list would be: Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline. Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles ( programming ,  statistics ) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar. Stanford University’s  Machine Learning  on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at  Baidu , this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews. Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available. Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning. Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice: In the past, I’ve tried to teach machine learning using a large variety of different programming languages including C++, Java, Python, NumPy, and also Octave … And what I’ve seen after having taught machine learning for almost a decade is that you learn much faster if you use Octave as your programming environment. Though Python and R are likely more compelling choices in 2017 with the  increased popularity of those languages , reviewers note that that shouldn’t stop you from taking the course. A few prominent reviewers noted the following: Of longstanding renown in the MOOC world, Stanford’s machine learning course really is the definitive introduction to this topic. The course broadly covers all of the major areas of machine learning … Prof. Ng precedes each segment with a motivating discussion and examples. Andrew Ng is a gifted teacher and able to explain complicated subjects in a very intuitive and clear way, including the math behind all concepts. Highly recommended. The only problem I see with this course if that it sets the expectation bar very high for other courses. Columbia University’s  Machine Learning  is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn’t have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews. The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia’s is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding). Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course’s total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase. Below are a few of the aforementioned sparkling  reviews : Over all my years of [being a] student I’ve come across professors who aren’t brilliant, professors who are brilliant but they don’t know how to explain the stuff clearly, and professors who are brilliant and know how explain the stuff clearly. Dr. Paisley belongs to the third group. This is a great course … The instructor’s language is precise and that is, to my mind, one of the strongest points of the course. The lectures are of high quality and the slides are great too. Dr. Paisley and his supervisor are … students of Michael Jordan, the father of machine learning. [Dr. Paisley] is the best ML professor at Columbia because of his ability to explain stuff clearly. Up to 240 students have selected his course this semester, the largest number among all professors [teaching] machine learning at Columbia. Machine Learning A-Z™  on Udemy is an impressively detailed offering that provides instruction in  both  Python and R, which is rare and can’t be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered. It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an “intuition” video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R. As a “bonus,” the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren’t the strong points of the course. Eremenko and the SuperDataScience team are revered for their ability to “make the complex simple.” Also, the prerequisites listed are “just some high school mathematics,” so this course might be a better option for those daunted by the Stanford and Columbia offerings. A few prominent reviewers  noted  the following: The course is professionally produced, the sound quality is excellent, and the explanations are clear and concise … It’s an incredible value for your financial and time investment. It was spectacular to be able to follow the course in two different programming languages simultaneously. Kirill is one of the absolute best instructors on Udemy (if not the Internet) and I recommend taking any class he teaches. … This course has a ton of content, like a ton! Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let’s look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide — you can find those  here . The Analytics Edge  (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews. Python for Data Science and Machine Learning Bootcamp  (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews. Data Science and Machine Learning Bootcamp with R  (Jose Portilla/Udemy): The comments for Portilla’s above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews. Machine Learning Series  (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course’s description. Uses Python. Cost varies depending on Udemy discounts, which are frequent. Machine Learning  (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Bite-sized videos, as is Udacity’s style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews. Implementing Predictive Analytics with Spark in Azure HDInsight  (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews. Data Science and Machine Learning with Python — Hands On!  (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews. Scala and Spark for Big Data and Machine Learning  (Jose Portilla/Udemy): “Big data” focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews. Machine Learning Engineer Nanodegree  (Udacity): Udacity’s flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews. Learning From Data (Introductory Machine Learning)  (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech’s independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews. Learning From Data (Introductory Machine Learning)  (Yaser Abu-Mostafa/California Institute of Technology): “A real Caltech course, not a watered-down version.” Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn’t as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews. Mining Massive Datasets  (Stanford University): Machine learning with a focus on “big data.” Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews. AWS Machine Learning: A Complete Guide With Python  (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews. Introduction to Machine Learning & Face Detection in Python  (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews. StatLearning: Statistical Learning  (Stanford University): Based on the excellent textbook, “ An Introduction to Statistical Learning, with Applications in R ” and taught by the professors who wrote it. Reviewers note that the MOOC isn’t as good as the book, citing “thin” exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews. Machine Learning Specialization  (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford’s or Caltech’s). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews. From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase  (Loony Corn/Udemy): “A down-to-earth, shy but confident take on machine learning techniques.” Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews. Principles of Machine Learning  (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews. Big Data: Statistical Inference and Machine Learning  (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews. Genomic Data Science and Clustering  (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD’s Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews. Intro to Machine Learning  (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity’s Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews. Machine Learning for Data Analysis  (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan’s Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews. Programming with Python for Data Science  (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews. Machine Learning for Trading  (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews. Practical Machine Learning  (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU’s Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews. Machine Learning for Data Science and Analytics  (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews. Recommender Systems Specialization  (University of Minnesota/Coursera): Strong focus one specific type of machine learning — recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews. Machine Learning With Big Data  (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD’s Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews. Practical Predictive Analytics: Models and Methods  (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW’s Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews. The following courses had one or no reviews as of May 2017. Machine Learning for Musicians and Artists  (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review. Applied Machine Learning in Python  (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available. Applied Machine Learning  (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. Machine Learning with Python  (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free. Machine Learning with Apache SystemML  (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free. Machine Learning for Data Science  (University of California, San Diego/edX): Doesn’t launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase. Introduction to Analytics Modeling  (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase. Predictive Analytics: Gaining Insights from Big Data  (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise’s Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase. Introducción al Machine Learning  (Universitas Telefónica/Miríada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks. Machine Learning Path Step  (Dataquest): Taught in Python using Dataquest’s interactive in-browser platform. Multiple guided projects and a “plus” project where you build your own machine learning system using your own data. Subscription required. The following six courses are offered by  DataCamp . DataCamp’s hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course. Introduction to Machine Learning  (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours. Supervised Learning with scikit-learn  (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours. Unsupervised Learning in R  (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours. Machine Learning Toolbox  (DataCamp): Teaches the “big ideas” in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours. Machine Learning with the Experts: School Budgets  (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school’s budget. DataCamp’s “Supervised Learning with scikit-learn” is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours. Unsupervised Learning in Python  (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours. Machine Learning  (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon’s graduate introductory machine learning course. A prerequisite to their second graduate level course, “Statistical Machine Learning.” Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A  2011 version  of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free. Statistical Machine Learning  (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon’s Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free. Undergraduate Machine Learning  (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below). Machine Learning  (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas’ undergraduate course (above) apply here as well. This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the  first article , statistics and probability in the  second article , intros to data science in the  third article , and data visualization in the  fourth . medium.freecodecamp.com The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering. If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ’s other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the 💚 so more people will see it here on Medium. This is a condensed version of my  original article  published on Class Central, where I’ve included detailed course syllabi."
Machine Learning is Fun!,achine Learning is Fun,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  日本語 ,  Português ,  Português (alternate) ,  Türkçe ,  Français ,  한국어  ,  العَرَبِيَّة‎‎ ,  Español (México) ,  Español (España) ,  Polski ,  Italiano ,  普通话 ,  Русский ,  한국어  ,  Tiếng Việt  or  فارسی . Giant update:   I’ve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let’s change that! This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading  the wikipedia article , got frustrated and gave up wishing someone would just give them a high-level explanation. That’s what this is. The goal is be accessible to anyone — which means that there’s a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished. Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data. For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic. “Machine learning” is an umbrella term covering lots of these kinds of generic algorithms. You can think of machine learning algorithms as falling into one of two main categories —  supervised learning  and  unsupervised learning . The difference is simple, but really important. Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses. To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for. So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price: Using that training data, we want to create a program that can estimate how much any other house in your area is worth: This is called  supervised learning . You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic. To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out. This kind of like having the answer key to a math test with all the arithmetic symbols erased: From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right. In  supervised learning , you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type! Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called  unsupervised  learning. This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!” So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts. Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions. Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer. Side note: There are lots of  other types  of machine learning algorithms. But this is a pretty good place to start. As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of  Strong AI  research is to be able to replicate this ability with computers. But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”. Unfortunately  “Machine Figuring out an equation to solve a specific problem based on some example data”  isn’t really a great name. So we ended up with “Machine Learning” instead. Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human. So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further. If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this: If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change. Wouldn’t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number: One way to think about this problem is that the  price  is a delicious stew and the ingredients are the  number of bedrooms , the  square footage  and the  neighborhood . If you could just figure out how much each ingredient impacts the final price, maybe there’s an exact ratio of ingredients to stir in to make the final price. That would reduce your original function (with all those crazy  if ’s and  else ’s) down to something really simple like this: Notice the magic numbers in bold —  .841231951398213 ,  1231.1231231 ,  2.3242341421 ,   and  201.23432095 . These are our  weights . If we could just figure out the perfect weights to use that work for every house, our function could predict house prices! A dumb way to figure out the best weights would be something like this: Start with each weight set to  1.0: Run every house you know about through your function and see how far off the function is at guessing the correct price for each house: For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house. Now add up the squared amount you are off for each house you have in your data set. Let’s say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That’s how “wrong” your function currently is. Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the  cost  of your function. If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that’s our goal — get this cost to be as low as possible by trying different weights. Repeat Step 2 over and over   with  every single possible combination of weights . Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you’ve solved the problem! That’s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow! But here’s a few more facts that will blow your mind: Pretty crazy, right? Ok, of course you can’t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you’d never run out of numbers to try. To avoid that, mathematicians have figured out lots of  clever ways  to quickly find good values for those weights without having to try very many. Here’s one way: First, write a simple equation that represents Step #2 above: Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now): This equation represents how wrong our price estimating function is for the weights we currently have set. If we graph this cost equation for all possible values of our weights for  number_of_bedrooms  and  sqft , we’d get a graph that might look something like this: In this graph, the lowest point in blue is where our cost is the lowest — thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we’ll have our answer! So we just need to adjust our weights so we are “walking down hill” on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we’ll eventually get there without having to try too many different weights. If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function’s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill. So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we’ll reach the bottom of the hill and have the best possible values for our weights. (If that didn’t make sense, don’t worry and keep reading). That’s a high level summary of one way to find the best weights for your function called  batch gradient descent . Don’t be afraid to  dig deeper  if you are interested on learning the details. When you use a machine learning library to solve a real problem, all of this will be done for you. But it’s still useful to have a good idea of what is happening. The three-step algorithm I described is called  multivariate linear regression . You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you’ve never seen before based where that house would appear on your line. It’s a really powerful idea and you can solve “real” problems with it. But while the approach I showed you might work in simple cases, it won’t work in all cases. One reason is because house prices aren’t always simple enough to follow a continuous line. But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like  neural networks  or  SVMs  with  kernels ). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies. Also, I ignored the idea of  overfitting . It’s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren’t in your original data set. But there are ways to deal with this (like  regularization  and using a  cross-validation  data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully. In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it’s a skill that any developer can learn! Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data! But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have. For example, if you build a model that predicts home prices based on the type of potted plants in each house, it’s never going to work. There just isn’t any kind of relationship between the potted plants in each house and the home’s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two. So remember, if a human expert couldn’t use the data to solve the problem manually, a computer probably won’t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly. In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn’t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it’s getting a little better every day. If you want to try out what you’ve learned in this article, I made  a course that walks you through every step of this article, including writing all the code . Give it a try! If you want to go deeper, Andrew Ng’s free  Machine Learning class on Coursera  is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math. Also, you can play around with tons of machine learning algorithms by downloading and installing  SciKit-Learn . It’s a python framework that has “black box” versions of all the standard algorithms. If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter: Also, please check out the  full-length course version of this article .  It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course  if you sign up with this link . You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I’d love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 2 !"
Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,achine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Network,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  普通话 ,  Русский ,  한국어 ,  Português ,  Tiếng Việt ,  فارسی   or  Italiano . Giant update:   I’ve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Are you tired of reading endless news stories about  deep learning  and not really knowing what that means? Let’s change that! This time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we’re going to explain the black magic that allows Google Photos to search your photos based on what is in the picture: Just like  Part 1  and  Part 2 , this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished! (If you haven’t already read  part 1  and  part 2 , read them now!) You might have seen  this famous xkcd comic  before. The goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years. In the last few years, we’ve finally found a good approach to object recognition using  deep convolutional neural networks . That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one. So let’s do it — let’s write a program that can recognize birds! Before we learn how to recognize pictures of birds, let’s learn how to recognize something much simpler — the handwritten number “8”. In  Part 2 , we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in: We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let’s modify this same neural network to recognize handwritten text. But to make the job really simple, we’ll only try to recognize one letter — the numeral “8”. Machine learning only works when you have data — preferably a lot of data. So we need lots and lots of handwritten “8”s to get started. Luckily, researchers created the  MNIST data set of handwritten numbers  for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some “8”s from the data set: The neural network we made in  Part 2  only took in a three numbers as the input (“3” bedrooms, “2000” sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers? The answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is: To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers: The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes: Notice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an “8” and thee second output will predict the likelihood it isn’t an “8”. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups. Our neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone. All that’s left is to train the neural network with images of “8”s and not-“8""s so it learns to tell them apart. When we feed in an “8”, we’ll tell it the probability the image is an “8” is 100% and the probability it’s not an “8” is 0%. Vice versa for the counter-example images. Here’s some of our training data: We can train this kind of neural network in a few minutes on a modern laptop. When it’s done, we’ll have a neural network that can recognize pictures of “8”s with a pretty high accuracy. Welcome to the world of (late 1980’s-era) image recognition! It’s really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic!  …right? Well, of course it’s not that simple. First, the good news is that our “8” recognizer really does work well on simple images where the letter is right in the middle of the image: But now the really bad news: Our “8” recognizer  totally fails  to work when the letter isn’t perfectly centered in the image. Just the slightest position change ruins everything: This is because our network only learned the pattern of a perfectly-centered “8”. It has absolutely no idea what an off-center “8” is. It knows exactly one pattern and one pattern only. That’s not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the “8” isn’t perfectly centered. We already created a really good program for finding an “8” centered in an image. What if we just scan all around the image for possible “8”s in smaller sections, one section at a time, until we find one? This approach called a sliding window. It’s the brute force solution. It works well in some limited cases, but it’s really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this! When we trained our network, we only showed it “8”s that were perfectly centered. What if we train it with more data, including “8”s in all different positions and sizes all around the image? We don’t even need to collect new training data. We can just write a script to generate new images with the “8”s in all kinds of different positions in the image: Using this technique, we can easily create an endless supply of training data. More data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns. To make the network bigger, we just stack up layer upon layer of nodes: We call this a “deep neural network” because it has more layers than a traditional neural network. This idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play  Overwatch  can be used to train neural networks incredibly quickly. But even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn’t going to get us all the way to a solution. We need to be smarter about how we process images into our neural network. Think about it. It doesn’t make sense to train a network to recognize an “8” at the top of a picture separately from training it to recognize an “8” at the bottom of a picture as if those were two totally different objects. There should be some way to make the neural network smart enough to know that an “8” anywhere in the picture is the same thing without all that extra training. Luckily… there is! As a human, you intuitively know that pictures have a  hierarchy  or  conceptual structure . Consider this picture: As a human, you instantly recognize the hierarchy in this picture: Most importantly, we recognize the idea of a  child  no matter what surface the child is on. We don’t have to re-learn the idea of  child  for every possible surface it could appear on. But right now, our neural network can’t do this. It thinks that an “8” in a different part of the image is an entirely different thing. It doesn’t understand that moving an object around in the picture doesn’t make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks. We need to give our neural network understanding of  translation invariance  — an “8” is an “8” no matter where in the picture it shows up. We’ll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images). Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture. Here’s how it’s going to work, step by step — Similar to our sliding window search above, let’s pass a sliding window over the entire original image and save each result as a separate, tiny picture tile: By doing this, we turned our original image into 77 equally-sized tiny image tiles. Earlier, we fed a single image into a neural network to see if it was an “8”. We’ll do the exact same thing here, but we’ll do it for each individual image tile: However,  there’s one big twist : We’ll keep the  same neural network weights  for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we’ll mark that tile as interesting. We don’t want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this: In other words, we’ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting. The result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big: To reduce the size of the array, we  downsample  it using an algorithm called  max pooling . It sounds fancy, but it isn’t at all! We’ll just look at each 2x2 square of the array and keep the biggest number: The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we’ll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits. So far, we’ve reduced a giant image down into a fairly small array. Guess what? That array is just a bunch of numbers, so we can use that small array as input into  another neural network . This final neural network will decide if the image is or isn’t a match. To differentiate it from the convolution step, we call it a “fully connected” network. So from start to finish, our whole five-step pipeline looks like this: Our image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network. When solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data. The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize. For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it’s knowledge of sharp edges, the third step might recognize entire birds using it’s knowledge of beaks, etc. Here’s what a more realistic deep convolutional network (like you would find in a research paper) looks like: In this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories! So how do you know which steps you need to combine to make your image classifier work? Honestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error! Now finally we know enough to write a program that can decide if a picture is a bird or not. As always, we need some data to get started. The free  CIFAR10 data set  contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we’ll also add in the  Caltech-UCSD Birds-200–2011 data set  that has another 12,000 bird pics. Here’s a few of the birds from our combined data set: And here’s some of the 52,000 non-bird images: This data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need  millions  of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data! To build our classifier, we’ll use  TFLearn . TFlearn is a wrapper around Google’s  TensorFlow  deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network. Here’s the code to define and train the network: If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer. As it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn’t help, so I stopped it there. Congrats! Our program can now recognize birds in images! Now that we have a trained neural network, we can use it!  Here’s a simple script  that takes in a single image file and predicts if it is a bird or not. But to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time. That seems pretty good, right? Well… it depends! Our network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things. For example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed “not a bird” every single time would be 95% accurate! But it would also be 100% useless. We need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at  how  it failed, not just the percentage of the time that it failed. Instead of thinking about our predictions as “right” and “wrong”, let’s break them down into four separate categories — Using our validation set of 15,000 images, here’s how many times our predictions fell into each category: Why do we break our results down like this? Because not all mistakes are created equal. Imagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we’d rather have false positives than false negatives. False negatives would be the worse possible case — that’s when the program told someone they definitely didn’t have cancer but they actually did. Instead of just looking at overall accuracy, we calculate  Precision and Recall  metrics. Precision and Recall metrics give us a clearer picture of how well we did: This tells us that 97% of the time we guessed “Bird”, we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one! Now that you know the basics of deep convolutional networks, you can try out some of the  examples that come with tflearn  to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don’t even have to find your own images. You also know enough now to start branching and learning about other areas of machine learning. Why not learn  how to use algorithms to train computers how to play Atari games  next? If you liked this article, please consider  signing up for my Machine Learning is Fun! email list . I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this. You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I’d love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 4 ,  Part 5  and  Part 6 !"
Essential Cheat Sheets for Machine Learning and Deep Learning Engineers,ssential Cheat Sheets for Machine Learning and Deep Learning Engineer,"Machine learning is complex. For newbies, starting to learn machine learning can be painful if they don’t have right resources to learn from. Most of the machine learning libraries are difficult to understand and learning curve can be a bit frustrating. I am creating a repository on Github( cheatsheets-ai ) containing cheatsheets for different machine learning frameworks, gathered from different sources. Do visit the Github repository, also, contribute cheat sheets if you have any. Thanks. List of Cheatsheets:  1. Keras 2. Numpy 3. Pandas 4. Scipy 5. Matplotlib 6. Scikit-learn 7. Neural Networks Zoo 8. ggplot2 9. PySpark 10. R Studio 11. Jupyter Notebook 12. Dask 2. Numpy 3. Pandas 4. Scipy 5. Matplotlib 6. Scikit-learn 7. Neural Networks Zoo 8. ggplot2 9. PySpark 10. R Studio (dplyr and tidyr) 11. Jupyter Notebook 12. Dask Thank you for reading. If you want to get into contact, you can reach out to me at  ahikailash1@gmail.com About Me: I am a Co-Founder of  MateLabs , where we have built  Mateverse , an ML Platform which enables everyone to easily build and train Machine Learning Models, without writing a single line of code. Note : Recently, I published a book on GANs titled “Generative Adversarial Networks Projects”, in which I covered most of the widely popular GAN architectures and their implementations. DCGAN, StackGAN, CycleGAN, Pix2pix, Age-cGAN, and 3D-GAN have been covered in details at the implementation level. Each architecture has a chapter dedicated to it. I have explained these networks in a very simple and descriptive language using Keras framework with Tensorflow backend. If you are working on GANs or planning to use GANs, give it a read and share your valuable feedback with me at  ahikailash1@gmail.com www.amazon.com You can grab a copy of the book from  http://www.amazon.com/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679 https://www.amazon.in/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679?fbclid=IwAR0X2pDk4CTxn5GqWmBbKIgiB38WmFX-sqCpBNI8k9Z8I-KCQ7VWRpJXm7I   https://www.packtpub.com/big-data-and-business-intelligence/generative-adversarial-networks-projects?fbclid=IwAR2OtU21faMFPM4suH_HJmy_DRQxOVwJZB0kz3ZiSbFb_MW7INYCqqV7U0c Triplebyte helps programmers find great companies to work at. They’ll go through a technical interview with you, match you with companies that are looking for people with your specific skill sets, and then fast track you through their hiring processes. Looking for a new job? Take Triplebyte’s quiz and get a job at top companies!"
"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","heat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Dat","Over the past few months, I have been collecting AI cheat sheets. From time to time I share them with friends and colleagues and recently I have been getting asked a lot, so I decided to organize and share the entire collection. To make things more interesting and give context, I added descriptions and/or excerpts for each major topic. This is the most complete list and the Big-O is at the very end, enjoy… >>> Update: We have recently redesigned these cheat sheets into a Super High Definition PDF. Check them out below: becominghuman.ai chatbotslife.com aijobsboard.com This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. The flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it. >>>  See Latest Jobs in AI, ML & BIG DATA  <<< Scikit-learn  (formerly  scikits.learn ) is a  free software   machine learning   library  for the  Python  programming language. It features various  classification ,  regression  and  clustering  algorithms including  support vector machines ,  random forests ,  gradient boosting ,  k -means  and  DBSCAN , and is designed to interoperate with the Python numerical and scientific libraries  NumPy  and  SciPy . This machine learning cheat sheet from Microsoft Azure will help you choose the appropriate machine learning algorithms for your predictive analytics solution. First, the cheat sheet will asks you about the data nature and then suggests the best algorithm for the job. becominghuman.ai aijobsboard.com In May 2017 Google announced the second-generation of the TPU, as well as the availability of the TPUs in  Google Compute Engine . [12]  The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs provide up to 11.5 petaflops. becominghuman.ai In 2017, Google’s TensorFlow team decided to support Keras in TensorFlow’s core library. Chollet explained that Keras was conceived to be an interface rather than an end-to-end machine-learning framework. It presents a higher-level, more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library. NumPy targets the  CPython  reference  implementation  of Python, which is a non-optimizing  bytecode  interpreter. Mathematical algorithms written for this version of Python often run much slower than  compiled  equivalents. NumPy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy. The name ‘Pandas’ is derived from the term “ panel data ”, an  econometrics  term for multidimensional structured data sets. The term “data wrangler” is starting to infiltrate pop culture. In the 2017 movie  Kong: Skull Island , one of the characters, played by actor  Marc Evan Jackson  is introduced as “Steve Woodward, our data wrangler”. becominghuman.ai chatbotslife.com aijobsboard.com SciPy builds on the  NumPy  array object and is part of the NumPy stack which includes tools like  Matplotlib ,  pandas  and  SymPy , and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as  MATLAB ,  GNU Octave , and  Scilab . The NumPy stack is also sometimes referred to as the SciPy stack. [3] matplotlib  is a  plotting   library  for the  Python  programming language and its numerical mathematics extension  NumPy . It provides an  object-oriented   API  for embedding plots into applications using general-purpose  GUI toolkits  like  Tkinter ,  wxPython ,  Qt , or  GTK+ . There is also a  procedural  “pylab” interface based on a  state machine  (like  OpenGL ), designed to closely resemble that of  MATLAB , though its use is discouraged. [2]   SciPy  makes use of matplotlib. pyplot is a matplotlib module which provides a MATLAB-like interface. [6]  matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. >>> If you like this list, you can let me know  here . <<< aijobsboard.com becominghuman.ai becominghuman.ai Stefan is the founder of  Chatbot’s Life , a Chatbot media and consulting firm. Chatbot’s Life has grown to over 150k views per month and has become the premium place to learn about Bots & AI online. Chatbot’s Life has also consulted many of the top Bot companies like Swelly, Instavest, OutBrain, NearGroup and a number of Enterprises. Big-O Algorithm Cheat Sheet:  http://bigocheatsheet.com/ Bokeh Cheat Sheet:  https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf Data Science Cheat Sheet:  https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics Data Wrangling Cheat Sheet:  https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf Data Wrangling:  https://en.wikipedia.org/wiki/Data_wrangling Ggplot Cheat Sheet:  https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Keras Cheat Sheet:  https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs Keras:  https://en.wikipedia.org/wiki/Keras Machine Learning Cheat Sheet:  https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/ Machine Learning Cheat Sheet:  https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet ML Cheat Sheet::  http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html Matplotlib Cheat Sheet:  https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY Matpotlib:  https://en.wikipedia.org/wiki/Matplotlib Neural Networks Cheat Sheet:  http://www.asimovinstitute.org/neural-network-zoo/ Neural Networks Graph Cheat Sheet:  http://www.asimovinstitute.org/blog/ Neural Networks:  https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network Numpy Cheat Sheet:  https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE NumPy:  https://en.wikipedia.org/wiki/NumPy Pandas Cheat Sheet:  https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM Pandas:  https://en.wikipedia.org/wiki/Pandas_(software) Pandas Cheat Sheet:  https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc Pyspark Cheat Sheet:  https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ Scikit Cheat Sheet:  https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet Scikit-learn:  https://en.wikipedia.org/wiki/Scikit-learn Scikit-learn Cheat Sheet:  http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html Scipy Cheat Sheet:  https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI SciPy:  https://en.wikipedia.org/wiki/SciPy TesorFlow Cheat Sheet:  https://www.altoros.com/tensorflow-cheat-sheet.html Tensor Flow:  https://en.wikipedia.org/wiki/TensorFlow"
30 Amazing Machine Learning Projects for the Past Year (v.2018),0 Amazing Machine Learning Projects for the Past Year (v.2018,"For the past year , we’ve compared nearly 8,800 open source Machine Learning projects to pick Top 30 (0.3% chance). This is an extremely competitive list and it carefully picks the best open source Machine Learning libraries, datasets and apps published between January and December 2017.  Mybridge AI  evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of  Github stars is 3,558. Open source projects can be useful for data scientists. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Machine Learning projects you may have missed for the past year. <Recommended Learning> A) Neural Networks Deep Learning A-Z™: Hands-On Artificial Neural Networks [68,745 recommends, 4.5/5 stars] B) TensorFlow Complete Guide to TensorFlow for Deep Learning with Python [17,834 recommends, 4.6/5 stars] <Others> A) Web hosting : Get free domain name for a year. For your ‘simple’ personal website or project site. (Click the numbers below. Credit given to the biggest contributor.) FastText: Library for fast text representation and classification.  [11786 stars on Github] . Courtesy of  Facebook Research ……….. [  Muse : Multilingual Unsupervised or Supervised word Embeddings, based on Fast Text. 695 stars on Github] Deep-photo-styletransfer: Code and data for paper “Deep Photo Style Transfer”  [9747 stars on Github] . Courtesy of Fujun Luan, Ph.D. at Cornell University The world’s simplest facial recognition api for Python and the command line  [8672 stars on Github] . Courtesy of  Adam Geitgey Magenta: Music and Art Generation with Machine Intelligence  [8113 stars on Github] . Sonnet: TensorFlow-based neural network library  [5731 stars on Github] . Courtesy of  Malcolm Reynolds  at Deepmind deeplearn.js: A hardware-accelerated machine intelligence library for the web  [5462 stars on Github] . Courtesy of Nikhil Thorat at Google Brain Fast Style Transfer in TensorFlow  [4843 stars on Github] . Courtesy of  Logan Engstrom  at MIT Pysc2: StarCraft II Learning Environment  [3683 stars on Github] . Courtesy of Timo Ewalds at DeepMind AirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research  [3861 stars on Github] . Courtesy of  Shital Shah  at Microsoft Facets: Visualizations for machine learning datasets  [3371 stars on Github] . Courtesy of Google Brain Style2Paints: AI colorization of images  [3310 stars on Github] . Tensor2Tensor: A library for generalized sequence to sequence models — Google Research  [3087 stars on Github] . Courtesy of  Ryan Sepassi  at Google Brain Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more)  [2847 stars on Github] . Courtesy of Jun-Yan Zhu, Ph.D at Berkeley Faiss: A library for efficient similarity search and clustering of dense vectors.  [2629 stars on Github] . Courtesy of Facebook Research Fashion-mnist: A MNIST-like fashion product database  [2780 stars on Github] . Courtesy of Han Xiao, Research Scientist  Zalando Tech ParlAI: A framework for training and evaluating AI models on a variety of openly available dialog datasets  [2578 stars on Github] . Courtesy of Alexander Miller at  Facebook Research Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit  [2571 stars on Github] . Pyro: Deep universal probabilistic programming with Python and PyTorch  [2387 stars on Github] . Courtesy of Uber AI Labs iGAN: Interactive Image Generation powered by GAN  [2369 stars on Github] . Deep-image-prior: Image restoration with neural networks but without learning  [2188 stars on Github] . Courtesy of Dmitry Ulyanov, Ph.D at Skoltech Face_classification: Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV.  [1967 stars on Github] . Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind’s WaveNet and tensorflow  [1961 stars on Github] . Courtesy of Namju Kim at Kakao Brain StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation  [1954 stars on Github] . Courtesy of  Yunjey Choi  at Korea University Ml-agents: Unity Machine Learning Agents  [1658 stars on Github] . Courtesy of  Arthur Juliani , Deep Learning at Unity3D DeepVideoAnalytics: A distributed visual search and visual data analytics platform  [1494 stars on Github] . Courtesy of  Akshay Bhat , Ph.D at Cornell University OpenNMT: Open-Source Neural Machine Translation in Torch  [1490 stars on Github] . Pix2pixHD: Synthesizing and manipulating 2048x1024 images with conditional GANs  [1283 stars on Github] . Courtesy of  Ming-Yu Liu  at AI Research Scientist at Nvidia Horovod: Distributed training framework for TensorFlow.  [1188 stars on Github] . Courtesy of Uber Engineering AI-Blocks: A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models  [899 stars on Github] . Deep neural networks for voice conversion (voice style transfer) in Tensorflow  [845 stars on Github] . Courtesy of  Dabi Ahn , AI Research at Kakao Brain That’s it for Machine Learning Open Source of the Year. Visit  our publication   to find top posts for more programming skills."
‘I want to learn Artificial Intelligence and Machine Learning. Where can I start?’,I want to learn Artificial Intelligence and Machine Learning. Where can I start?,"I was working at the Apple Store and I wanted a change. To start building the tech I was servicing. I began looking into Machine Learning (ML) and Artificial Intelligence (AI). There’s so much going on. Too much. Every week it seems like Google or Facebook are releasing a new kind of AI to make things faster or improve our experience. And don’t get me started on the number of self-driving car companies. This is a good thing though. I’m not a fan of driving and roads are dangerous. Even with all this happening, there’s still yet to be an agreed definition of what exactly artificial intelligence is. Some argue deep learning can be considered AI, others will say it’s not AI unless it passes the Turing Test. This lack of definition really stunted my progress in the beginning. It was hard to learn something which had so many different definitions. Enough with the definitions. My friends and I were building a web startup. It failed. We gave up due to a lack of meaning. But along the way, I was starting to hearing more and more about ML and AI. “The computer learns things for you?” I couldn’t believe it. I stumbled across Udacity’s Deep Learning Nanodegree. A fun character called Siraj Raval was in one of the promo videos. His energy was contagious. Despite not meeting the basic requirements (I had never written a line of Python before), I signed up. 3 weeks before the course start date I emailed Udacity support asking what the refund policy was. I was scared I wouldn’t be able to complete the course. I didn’t get a refund. I completed the course within the designated timeline. It was hard. Really hard at times. My first two projects were handed in four days late. But the excitement of being involved in one of the most important technologies in the world drove me forward. Finishing the Deep Learning Nanodegree, I had guaranteed acceptance into either Udacity’s AI Nanodegree, Self-Driving Car Nanodegree or Robotics Nanodegree. All great options. I was lost again. The classic. “Where do I go next?” I needed a curriculum. I’d built a foundation with the Deep Learning Nanodegree, now it was time to figure out what was next. I didn’t plan on going back to university anytime soon. I didn’t have $100,000 for a proper Masters Degree anyway. So I did what I did in the beginning. Asked my mentor, Google, for help. I’d jumped into deep learning without any prior knowledge of the field. Instead of climbing to the tip of the AI iceberg, a helicopter had dropped me off on the top. After researching a bunch of courses, I put a list of which ones interested me the most in Trello. I knew online courses had a high drop out rate. I wasn’t going to let myself be a part of this number. I had a mission. To make myself accountable, I started sharing my learning journey online. I figured I could practice communicating what I learned plus find other people who were interested in the same things I was. My friends still think I’m an alien when I go on one of my AI escapades. I made the  Trello board public  and wrote a blog post about my endeavours. The curriculum has changed slightly since I first wrote it but it’s still relevant. I’d visit the Trello board multiple times per week to track my progress. I’m Australian. And all the commotion seemed to be happening in the US. So I did the most logical thing and bought a one-way ticket. I’d been studying for a year and I figured it was about time I started putting my skills into practice. My plan was to rock up to the US and get hired. Then Ashlee messaged me on LinkedIn, “Hey I’ve seen your posts and they’re really cool, I think you should meet Mike.” I met Mike. I told him my story of learning online, how I loved healthtech and my plans to go to the US. “You may be better off staying here a year or so and seeing what you can find, I’ think you’d love to meet Cameron.” I met Cameron. We had a similar chat what Mike and I talked about. Health, tech, online learning, US. “We’re working on some health problems, why don’t you come in on Thursday?” Thursday came. I was nervous. But someone once told me being nervous is the same as being excited. I flipped to being excited. I spent the day meeting the  Max Kelsen  team and the problems they were working on. Two Thursday’s later, Nick, the CEO, Athon, lead machine learning engineer, and I went for coffee. “How would you like to join the team?” Asked Nick. “Sure,” I said. My US flight got pushed back a couple of months and I purchased a return ticket. Learning online, I knew it was unconventional. All the roles I’d gone to apply for had Masters Degree requirements or at least some kind of technical degree. I didn’t have either of these. But I did have the skills I’d gathered from a plethora of online courses. Along the way, I was sharing my work online. My GitHub contained all the projects I’d done, my LinkedIn was stacked out and I’d practised communicating what I learned through YouTube and articles on Medium. I never handed in a resume for Max Kelsen. “We saw your LinkedIn profile.” My body of work was my resume. Regardless if you’re learning online or through a Masters Degree, having a portfolio of what you’ve worked on is a great way to build skin in the game. ML and AI skills are in demand but that doesn’t mean you don’t have to showcase them. Even the best product won’t sell without any shelf space. Whether it be GitHub, Kaggle, LinkedIn or a blog, have somewhere where people can find you. Plus, having your own corner of the internet is great fun. Where do you go to learn these skills? What courses are the best? There’s no best answer. Everyone’s path will be different. Some people learn better with books, others learn better through videos. What’s more important than how you start is why you start. Start with why. Why do you want to learn these skills? Do you want to make money? Do you want to build things? Do you want to make a difference? There’s no right reason. All are valid in their own way. Start with why because having a why is more important than how. Having a why means when it gets hard and it  will  get hard, you’ve got something to turn to. Something to remind you why you started. Got a why? Good. Time for some hard skills. I can only recommend what I’ve tried. I’ve completed courses from (in order): They’re all world-class. I’m a visual learner. I learn better seeing things being done. All of these courses do that. If you’re an absolute beginner, start with some introductory Python courses and when you’re a bit more confident, move into data science, machine learning and AI. DataCamp is great for beginners learning Python but wanting to learn it with a data science and machine learning focus. The highest level of math education I’ve had was in high school. The rest I’ve learned through Khan Academy as I’ve needed it. There are many different opinions on how much math you need to know to get into machine learning and AI. I’ll share mine. If you want to apply machine learning and AI techniques to a problem, you don’t necessarily need an in-depth understanding of the math to get a good result. Libraries such as TensorFlow and PyTorch allow someone with a bit of Python experience to build state of the art models whilst the math is taken care of behind the scenes. If you’re looking to get deep into machine learning and AI research, through means of a PhD program or something similar, having an in-depth knowledge of the math is paramount. In my case, I’m not looking to dive deep into the math and improve an algorithm’s performance by 10%. I’ll leave that to people smarter than me. Instead, I’m more than happy to use the libraries available and manipulate them to help solve problems as I see fit. What a machine engineer does in practice might not be what you think. Despite the cover photos of many online articles, it doesn’t always involve working with robots that have red eyes. Here are a few questions a machine learning engineer has to ask themselves daily. I borrowed these from a  great article  by Rachel Thomas, one of the co-founders of  fast.ai , she goes into more depth in the full text. For more, I made a video of what we usually get up to on Monday’s at Max Kelsen. There’s no right or wrong way to get into ML or AI (or anything else). The beautiful thing about this field is we have access to some of the best technologies in the world, all we’ve got to do is learn how to use them. You could begin by learning Python code (my favourite). You could begin by studying calculus and statistics. You could begin by learning about the philosophy of decision making. Machine learning and AI fascinate me because they meet at the intersection of all of these. The more I learn about it, the more I realise there’s plenty more to learn. And it gets me excited. Sometimes I get frustrated when my code doesn’t run. Or I don’t understand a concept. So I give up temporarily. I give up by letting myself walk away from the problem and take a nap. Or go for a walk. When I come back it feels like I’m looking at it with different eyes. The excitement comes back. I keep learning. I tell myself. I’m a learning machine. There’s so much happening in the field it can be daunting to get started. Too many options lead to no options. Ignore this. Start wherever interests you most and follow it. If it leads to a dead end, great, you’ve figured out what you’re not interested in. Retrace your steps and take the other fork in the road instead. Computers are smart but they still can’t learn on their own. They need your help."
Machine Learning is Fun! Part 2,achine Learning is Fun! Part ,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 !  You can also read this article in  Italiano ,  Español ,  Français ,  Türkçe ,  Русский ,  한국어  Português ,  فارسی ,  Tiếng Việt   or  普通话 . Giant update:   I’ve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! In  Part 1 , we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven’t already read  part 1 , read it now!). This time, we are going to see one of these generic algorithms do something really cool  —  create video game levels that look like they were made by humans. We’ll build a neural network, feed it existing Super Mario levels and watch new ones pop out! Just like  Part 1 , this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished. Back in  Part 1 , we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this: We ended up with this simple estimation function: In other words, we estimated the value of the house by multiplying each of its attributes by a  weight . Then we just added those numbers up to get the house’s value. Instead of using code, let’s represent that same function as a simple diagram: However this algorithm only works for simple problems where the result has a  linear  relationship with the input. What if the truth behind house prices isn’t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn’t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model? To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases: Now we have four different price estimates. Let’s combine those four price estimates into one final estimate. We’ll run them through the same algorithm again (but using another set of weights)! Our new  Super Answer  combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model. Let’s combine our four attempts to guess into one big diagram: This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions. There’s a lot that I’m skipping over to keep this brief (including  feature scaling  and the  activation function ), but the most important part is that these basic ideas  click: It’s just like LEGO! We can’t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together: The neural network we’ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it’s a  stateless algorithm . In many cases (like estimating the price of house), that’s exactly what you want. But the one thing this kind of model can’t do is respond to patterns in data over time. Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess? I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. Our model might look like this: But let’s make the problem harder. Let’s say I need to guess the  next  letter you are going to type at any point in your story. This is a much more interesting problem. Let’s use the first few words of Ernest Hemingway’s  The Sun Also Rises  as an example: Robert Cohn was once middleweight boxi What letter is going to come next? You probably guessed ’n’ — the word is probably going to be  boxing . We know this based on the letters we’ve already seen in the sentence and our knowledge of common words in English. Also, the word ‘middleweight’ gives us an extra clue that we are talking about boxing. In other words, it’s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English. To solve this problem with a neural network, we need to add  state  to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently. Keeping track of state in our model makes it possible to not just predict the most likely  first  letter in the story, but to predict the most likely  next  letter given all previous letters. This is the basic idea of a  Recurrent  Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory. Predicting the next letter in a story might seem pretty useless. What’s the point? One cool use might be auto-predict for a mobile phone keyboard: But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over — forever? We’d be asking it to write a complete story for us! We saw how we could guess the next letter in Hemingway’s sentence. Let’s try generating a whole story in the style of Hemingway. To do this, we are going to use the  Recurrent Neural Network implementation  that  Andrej Karpathy  wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote  an excellent introduction to generating text with RNNs , You can view all the  code for the model on github . We’ll create our model from the complete text of  The Sun Also Rises —  362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway’s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example. As we just start to train the RNN, it’s not very good at predicting letters. Here’s what it generates after a 100 loops of training: hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h’e btentmuhgehi bcgdltt. gey heho grpiahe. Ddelnss.eelaishaner” cot AAfhB ht ltny ehbih a”on bhnte ectrsnae abeahngy amo k ns aeo?cdse nh a taei.rairrhelardr er deffijha You can see that it has figured out that sometimes words have spaces between them, but that’s about it. After about 1000 iterations, things are looking more promising: hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been soms inick stalling that aid.  “Hon’t me and acrained on .Hw’s don’t you for the roed,” In’s pair.”  “Alough marith him.” The model has started to identify the patterns in basic sentence structure. It’s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there’s also still a lot of nonsense. But after several thousand more training iterations, it looks pretty good: He went over to the gate of the café. It was like a country bed.  “Do you know it’s been me.”  “Damned us,” Bill said.  “I was dangerous,” I said. “You were she did it and think I would a fine cape you,” I said.  “I can’t look strange in the cab.”  “You know I was this is though,” Brett said.  “It’s a fights no matter?”  “It makes to do it.”  “You make it?”  “Sit down,” I said. “I wish I wasn’t do a little with the man.”  “You found it.”  “I don’t know.”  “You see, I’m sorry of chatches,” Bill said. “You think it’s a friend off back and make you really drunk.” At this point, the algorithm has captured the basic pattern of Hemingway’s short, direct dialog. A few sentences even sort of make sense. Compare that with some real text from the book: There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave.  “Sit down,” said Harvey, “I’ve been looking for you.”  “What’s the matter?”  “Nothing. Just looking for you.”  “Been out to the races?”  “No. Not since Sunday.”  “What do you hear from the States?”  “Nothing. Absolutely nothing.”  “What’s the matter?” Even by only looking for patterns  one character at a time , our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing! We don’t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters. For fun, let’s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of “Er”, “He”, and “The S”: Not bad! But the  really mind-blowing part  is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking  recipes  or  fake Obama speeches . But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern. In 2015, Nintendo released  Super Mario Maker™  for the Wii U gaming system. This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It’s like a virtual LEGO set for people who grew up playing Super Mario Brothers. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? First, we need a data set for training our model. Let’s take all the outdoor levels from the original Super Mario Brothers game released in 1985: This game has 32 levels and about 70% of them have the same outdoor style. So we’ll stick to those. To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game’s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game’s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime. Here’s the first level from the game (which you probably remember if you ever played it): If we look closely, we can see the level is made of a simple grid of objects: We could just as easily represent this grid as a sequence of characters with one character representing each object: We’ve replaced each object in the level with a letter: …and so on, using a different letter for each different kind of object in the level. I ended up with text files that looked like this: Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line: The patterns in a level really emerge when you think of the level as a series of columns: So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called  feature selection ) is one of the keys of using machine learning algorithms well. To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up: Just like we saw when creating the model of Hemingway’s prose, a model improves as we train it. After a little training, our model is generating junk: It sort of has an idea that ‘-’s and ‘=’s should show up a lot, but that’s about it. It hasn’t figured out the pattern yet. After several thousand iterations, it’s starting to look like something: The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the “P”s in the data should appear in 2x2 clusters. That’s pretty cool! With a lot more training, the model gets to the point where it generates perfectly valid data: Let’s sample an entire level’s worth of data from our model and rotate it back horizontal: This data looks great! There are several awesome things to notice: Finally, let’s take this level and recreate it in Super Mario Maker: Play it yourself! If you have Super Mario Maker, you can play this level by  bookmarking it online  or by looking it up using level code  4AC9–0000–0157-F3C3 . The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‘toy’ instead of cutting-edge is that our model is generated from very little data. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can’t — because Nintendo won’t let us have them. Big companies don’t give away their data for free. As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That’s why companies like Google and Facebook need your data so badly! For example, Google recently open sourced  TensorFlow , its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate. But without Google’s massive trove of data in every language, you can’t create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your  Google Maps Location History  or  Facebook Location History  and notice that it stores every place you’ve ever been. In machine learning, there’s never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often  combining multiple approaches  will give you better results than any single approach. Readers have sent me links to other interesting approaches to generating Super Mario levels: If you liked this article, please consider  signing up for my Machine Learning is Fun! email list . I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this. You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I’d love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 3 !"
The best Mario Kart character according to data science,he best Mario Kart character according to data scienc,"Mario Kart was a staple of my childhood — my friends and I would spend hours after school as Mario, Luigi, and other characters from the Nintendo universe racing around cartoonish tracks and lobbing pixelated bananas at each other. One thing that always vexed our little group of would-be speedsters was the question of which character was best. Some people swore by zippy Yoshi, others argued that big, heavy Bowser was the best option. Back then there were only eight options to choose from; fast forward to the current iteration of the Mario Kart franchise and the question is even more complicated because you can select different karts and tires to go with your character. My Mario Kart reflexes aren’t what they used to be, but I am better at data science than I was as a fourth grader, so in this post I’ll use data to finally answer the question “Who is the best character in Mario Kart?” This is a tricky question because there are tons of potential character / kart / tire configurations now and they all have widely varying stats across a number of attributes. In general, it isn’t possible to optimize across multiple dimensions simultaneously, however some setups are undeniably worse than others. The question for an aspiring Mario Kart champion nowadays is “How can I pick a character / kart / tire combination that is in some sense optimal, even if there isn’t one ‘best’ option?” To answer this question we turn to one of Mario’s compatriots, the nineteenth century Italian economist Vilfredo Pareto who introduced the concept of  Pareto efficiency  and the related  Pareto frontier . The concept of Pareto efficiency applies to situations where there is a finite pool of resources and multiple competing outcomes that depend on how those resources are allocated. The “Pareto efficient” allocations are those in which it’s impossible to improve one outcome without worsening another outcome. This is more easily explained with a picture (courtesy of  Wikipedia ). Each circle is a potential resource allocation, which in our case means a distribution of stat points across the different attributes like weight, handling, and traction (characters in Mario Kart have about the same number of total stat points, and differ only in their distribution). The position of each circle represents the outcome of that allocation on two competing dimensions, for example speed and acceleration. The allocations in red lie on the Pareto frontier: for each of these allocations, an improvement in one outcome requires a decrease in the other. Allocations in grey are not Pareto efficient because you can improve both outcomes with a different allocation of resources. Speed and acceleration are generally the two most important attributes in Mario Kart, so the goal of this analysis is to identify the character / kart / tire configurations that lie on the Pareto frontier for speed and acceleration. We’ll start by examining the stats of each character, kart, and tire independently using some fan-compiled  data . One particular quirk of Mario Kart is that while there are a couple dozen characters, many of them have identical stats. From here on out, I’ll refer to the character (or kart, or tire) class by the name of one of its members. For example, in the heatmap below the row labelled ‘Peach’ also describes the stats for Daisy and Yoshi. The complete class memberships are listed at the end of the post in case you want to see where your favorite character lands. There are seven classes of characters. Let’s have a look at how their stats compare. The most obvious trend is the trade-off between speed and acceleration: heavy characters have good speed but poor acceleration, while light characters have snappy acceleration but a low top speed. There are variations in the other stats as well, but to a large extent, the speed and acceleration dominate the performance of a particular set up, so we’ll be ignoring the rest of the stats. Karts and tires modify the base stats of the characters: the attributes of the final configuration are a sum of the character’s stats and the kart / tire modifiers. As with characters, there are dozens of karts and tires, but only a few categories with different stats. The trends here are less obvious, but they generally agree with what we saw in the character stats: improvements in speed come at the expense of acceleration, and vice versa. Our goal is to find all the configurations that have an optimal combination of speed and acceleration, so the next step is to compute the stats for each unique (character, kart, tire) combination. With a little bit of Python we can enumerate all character / kart / tire combinations and calculate their attributes by adding up the values in the figures above. Equipped with the statistics for each possible combination, we can can plot the speed vs. the acceleration of each possible setup, and identify those that lie on the Pareto frontier. According to the above chart, the optimal configurations make up a fairly small subset of the total possible setups. We can quantify this by counting all the different combinations (note that some combinations overlap in the figure). Just for fun, let’s also count up the possible combinations including all the characters, karts, and tires with identical stats. Possible combinations: 149760 Unique stat combinations: 294 Optimal combinations: 15 The optimal configurations make up just 5% of the potential unique stat configurations! Let’s have a look at what these optimal configurations look like. Unless you’re going all-in on acceleration, it looks like a heavy character is the way to go; the two heaviest character classes (Wario and Donkey Kong) account for 11/15 of the Pareto-optimal configurations. We can also look at the other main stats for each of these configurations. So there it is, if speed and acceleration are your main concerns, then one of these 15 configurations is your best bet. Sometimes an optimal configuration isn’t what you’re looking for though (say, because your roommate threatened to stop playing if there wasn’t some sort of handicap, to choose a random example). In that case, we can explore all the possible configurations with a quick  bokeh  interactive graphic. A few observations: If you’d like to see the code behind this analysis you can find it  here . And finally, in case you have a particular attachment to one of the characters (or karts / tires) you can look up which class he / she / it belongs to below. Character Classes ***************** - Baby Mario, Baby Luigi, Baby Peach, Baby Daisy, Baby Rosalina, Lemmy Koopa, Mii Light - Toad, Shy Guy, Koopa Troopa, Lakitu, Wendy Koopa, Larry Koopa, Toadette - Peach, Daisy, Yoshi - Mario, Luigi, Iggy Koopa, Ludwig Koopa, Mii Medium - Donkey Kong, Waluigi, Rosalina, Roy Koopa - Metal Mario, Pink Gold Peach - Wario, Bowser, Morton Koopa, Mii Heavy Body Classes ***************** - Standard Kart, Prancer, Cat Cruiser, Sneeker, The Duke, Teddy Buggy - Gold Standard, Mach 8, Circuit Special, Sports Coupe - Badwagon, TriSpeeder, Steel Driver, Standard ATV - Biddybuggy, Landship, Mr. Scooty - Pipe Frame, Standard Bike, Flame Ride, Varmit, Wild Wiggler - Sports Bike, Jet Bike, Comet, Yoshi Bike Tire Classes ***************** - Standard, Blue Standard, Offroad, Retro Offroad - Monster, Hot Monster - Slick, Cyber Slick - Roller, Azure Roller, Button - Slim, Crimson Slim - Metal, Gold - Wood, Sponge, Cushion"
Essential Math for Data Science,ssential Math for Data Scienc,"M athematics is the bedrock of any contemporary discipline of science. Almost all the techniques of modern data science, including machine learning, have a deep mathematical underpinning. It goes without saying that you will absolutely need all the other pearls of knowledge—programming…"
"If you want to learn Data Science, start with one of these programming classes","f you want to learn Data Science, start with one of these programming classe","A year ago, I was a numbers geek with no coding background. After trying an online programming course, I was so inspired that I enrolled in one of the best computer science programs in Canada. Two weeks later, I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. So I dropped out. The decision was not difficult. I could learn the content I wanted to faster, more efficiently, and for a fraction of the cost. I already had a university degree and, perhaps more importantly, I already had the university experience. Paying $30K+ to go back to school seemed irresponsible. I started creating my own  data science master’s degree  using online courses shortly afterwards, after realizing it was a better fit for me than computer science. I scoured the introduction to programming landscape. I’ve already taken several courses and audited portions of many others. I know the options, and what skills are needed if you’re targeting a data analyst or data scientist role. For this guide, I spent 20+ hours trying to find every single online introduction to programming course offered as of August 2016, extracting key bits of information from their syllabi and reviews, and compiling their ratings. For this task, I turned to none other than the open source Class Central community and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it’s David. I wrote this guide back in 2016. Since then, I’ve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros.  My top programming course recommendation for 2023 is in there, too. datamaverickhq.com Okay, back to the guide. Each course had to fit four criteria: We believe we covered every notable course that exists and which fits the above criteria. Since there are seemingly hundreds of courses on Udemy in Python and R, we chose to consider the most reviewed and highest rated ones only. There is a chance we missed something, however. Please let us know if you think that is the case. We compiled average rating and number of reviews from Class Central and other review sites. We calculated a weighted average rating for each course. If a series had multiple courses (like Rice University’s  Part 1  and  Part 2 ), we calculated the weighted average rating across all courses. We also read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on three factors: Programming is not computer science and vice versa. There is a difference of which beginners may not be acutely aware. Borrowing  this answer  from Programmers Stack Exchange: Computer science is the study of what computers [can] do; programming is the practice of making computers do things. The course we are looking for introduces  programming  and optionally touches on relevant aspects of computer science that would benefit a new programmer in terms of awareness. Many of the courses considered, you’ll notice, do indeed have a computer science portion. None of the courses, however, are strictly computer science courses, which is why something like  Harvard’s CS50x  on edX is excluded. University of Toronto’s “Learn to Program” series on Coursera.  LTP1: The Fundamentals  and  LTP2: Crafting Quality Code  have a near-perfect weighted average rating of 4.71 out of 5 stars over 284 reviews. They also have a great mix of content difficulty and scope for the beginner data scientist. This free, Python-based introduction to programming sets itself apart from the other 20+ courses we considered. Jennifer Campbell and Paul Gries, two associate professors in the University of Toronto’s department of computer science (which is regarded as  one of the best in the world ) teach the series. The self-paced, self-contained Coursera courses match the material in their book, “ Practical Programming: An Introduction to Computer Science Using Python 3 .” LTP1 covers 40–50% of the book and LTP2 covers another 40%. The 10–20% not covered is not particularly useful for data science, which helped their case for being our pick. The professors kindly and promptly sent me detailed course syllabi upon request, which were difficult to find online prior to the course’s official restart in September 2016. Learn to Program: The Fundamentals (LTP1) Timeline: 7 weeks Estimated time commitment: 6–8 hours per week This course provides an introduction to computer programming intended for people with no programming experience. It covers the basics of programming in Python including elementary data types (numeric types, strings, lists, dictionaries, and files), control flow, functions, objects, methods, fields, and mutability. Modules Learn to Program: Crafting Quality Code (LTP2) Timeline: 5 weeks Estimated time commitment: 6–8 hours per week You know the basics of programming in Python: elementary data types (numeric types, strings, lists, dictionaries, and files), control flow, functions, objects, methods, fields, and mutability.  You need to be good at these in order to succeed in this course. LTP: Crafting Quality Code  covers the next steps: designing larger programs, testing your code so that you know it works, reading code in order to understand how efficient it is, and creating your own types. Modules Associate professor Gries also provided the following commentary on the course structure: “Each module has between about 45 minutes to a bit more than an hour of video. There are in-video quiz questions, which will bring the total time spent studying the videos to perhaps 2 hours.” These videos are generally shorter than ten minutes each. He continued: “In addition, we have one exercise (a dozen or two or so multiple choice and short-answer questions) per module, which should take an hour or two. There are three programming assignments in LTP1, each of which might take four to eight hours of work. There are two programming assignments in LTP2 of similar size.” He emphasized that the estimate of 6–8 hours per week is a rough guess: “Estimating time spent is incredibly student-dependent, so please take my estimates in that context. For example, someone who knows a bit of programming, perhaps in another programming language, might take half the time of someone completely new to programming. Sometimes someone will get stuck on a concept for a couple of hours, while they might breeze through on other concepts … That’s one of the reasons the self-paced format is so appealing to us.” In total, the University of Toronto’s  Learn to Program  series runs an estimated 12 weeks at 6–8 hours per week, which is about standard for most online courses created by universities. If you prefer to binge-study your MOOCs, that’s 72–96 hours, which could feasibly be completed in two to three weeks, especially if you have a bit of programming experience. If you already have some familiarity with programming, and don’t mind a syllabus that has a notable skew towards games and interactive applications, I would also recommend Rice University’s An Introduction to Interactive Programming in Python ( Part 1  and  Part 2 ) on Coursera. With 6,000+ reviews and the highest weighted average rating of 4.93/5 stars, this popular course is noted for its engaging videos, challenging quizzes, and enjoyable mini projects. It’s slightly more difficult, and focuses less on the fundamentals and more on topics that aren’t applicable in data science than our #1 pick. These courses are also part of the 7 course  Principles in Computing Specialization  on Coursera. The materials are self-paced and free, and a paid certificate is available. The course must be purchased for $79 (USD) for access to graded materials. The condensed course description and full syllabus are as follows: “This two-part course is designed to help students with very little or no computing background learn the basics of building simple interactive applications … To make learning Python easy, we have developed a new browser-based programming environment that makes developing interactive applications in Python simple. These applications will involve windows whose contents are graphical and respond to buttons, the keyboard, and the mouse. Recommended background: A knowledge of high school mathematics is required.  While the class is designed for students with no prior programming experience, some beginning programmers have viewed the class as being fast-paced . For students interested in some light preparation prior to the start of class, we recommend a self-paced Python learning site such as codecademy.com.” Timeline: 5 weeks Estimated time commitment: 7–10 hours per week Week 0 — statements, expressions, variables   Understand the structure of this class, and explore Python as a calculator. Week 1 — functions, logic, conditionals  Learn the basic constructs of Python programming, and create a program that plays a variant of Rock-Paper-Scissors. Week 2 — event-driven programming, local/global variables  Learn the basics of event-driven programming, understand the difference between local and global variables, and create an interactive program that plays a simple guessing game. Week 3 — canvas, drawing, timers  Create a canvas in Python, learn how to draw on the canvas, and create a digital stopwatch. Week 4 — lists, keyboard input, the basics of modeling motion  Learn the basics of lists in Python, model moving objects in Python, and recreate the classic arcade game “Pong.” Week 5 — mouse input, list methods, dictionaries   Read mouse input, learn about list methods and dictionaries, and draw images.   Week 6 — classes and object-oriented programming   Learn the basics of object-oriented programming in Python using classes, and work with tiled images. Week 7 — basic game physics, sprites   Understand the math of acceleration and friction, work with sprites, and add sound to your game. Week 8 — sets and animation   Learn about sets in Python, compute collisions between sprites, and animate sprites. If you are set on an introduction to programming course in R, we recommend DataCamp’s series of R courses:  Introduction to R ,  Intermediate R ,  Intermediate R — Practice , and  Writing Functions in R . Though the latter three come at a price point of $25/month, DataCamp is best in category for covering the programming fundamentals and R-specific topics, which is reflected in its average rating of 4.29/5 stars. We believe the best approach to learning programming for data science using  online courses   is to do it first through Python. Why? There is a lack of MOOC options that teach core programming principles and use R as the language of instruction. We found six such R courses that fit our testing criteria, compared to twenty-two Python-based courses. Most of the R courses didn’t receive great ratings and failed to meet most of our subjective testing criteria. The series breakdown is as follows: Estimated time commitment: 4 hours Chapters: Estimated time commitment: 6 hours Chapters: Estimated time commitment: 4 hours This follow-up course on intermediate R does not cover new programming concepts. Instead, you will strengthen your knowledge of the topics in intermediate R with a bunch of new and fun exercises. Estimated time commitment: 4 hours Chapters: Another option for R would be to take a Python-based introduction to programming course to cover the fundamentals of programming, and then pick up R syntax with an R basics course. This is what I did, but I did it with Udacity’s  Data Analysis with R . It worked well for me. You can also pick up R with our  top recommendation for a statistics class , which teaches the basics of R through coding up stats problems. Our #1 and #2 picks had a 4.71 and 4.93 star weighted average rating over 284 and 6,069 reviews, respectively. Let’s look at the other alternatives. This is the first of a six-piece series that covers the best MOOCs for launching yourself into the data science field. It will cover several other data science core competencies:  statistics , the  data science process , data visualization, and machine learning. medium.freecodecamp.com medium.freecodecamp.com The final piece will be a summary of those courses, and the best MOOCs for other key topics such as data wrangling, databases, and even software engineering. If you’re looking for a complete list of Data Science MOOCs, you can find them on Class Central’s  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ’s other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the 💚 so more people will see it here on Medium. This is a condensed version of the  original article published on Class Central , where course descriptions, syllabi, and multiple reviews are included."
How To Learn Data Science If You’re Broke,ow To Learn Data Science If You’re Brok,"Over the last year, I taught myself data science. I learned from hundreds of online resources and studied 6–8 hours every day. All while working for minimum wage at a day-care. My goal was to start a career I was passionate about, despite my lack of funds. Because of this choice I have accomplished a lot over the last few months. I published my own  website , was posted in a major online data science  publication , and was given scholarships to a competitive computer science graduate  program . In the following article, I give guidelines and advice so you can make your own data science curriculum. I hope to give others the tools to begin their own educational journey. So they can begin to work towards a more passionate career in data science. When I say “data science”, I am referring to the collection of tools that turn data into real-world actions. These include machine learning, database technologies, statistics, programming, and domain-specific technologies. The internet is a chaotic mess. Learning from it can often feel like drinking from the fun end of a fire-hose. There are simpler alternatives that offer to sort the mess for you. Sites like  Dataquest ,  DataCamp , and  Udacity  all offer to teach you data science skills. Each creating an education program that shepherds you from topic to topic. Each requires little course-planning on your part. The problem? They cost too much, they don’t teach you how to apply concepts in a job setting, and they prevent you from exploring your own interests and passions. There are free alternatives like  edX  and  coursera  which offer one-off courses diving into specific topics. If you learn well from videos or a classroom setting, these are excellent ways to learn data science. Check out this  website  for a listing of available data science courses. There are also a few free course curricula you can use. Check out  David Venturi ’s post, or the  Open Source DS Masters  (a more traditional education plan). If you learn well from reading, look at the  Data Science From Scratch  book. This textbook is a full learning plan that can be supplemented with online resources. You can find the full book online or get a physical copy from  Amazon  ($27). These are just a few of the free resources that provide a detailed learning path for data science. There are many more. To better understand the skills you need to acquire on your educational journey, in the next section I detail a broader curriculum guideline. This is intended to be high-level, and not just a list of courses to take or books to read. Programming is a fundamental skill of data scientists. Get comfortable with the syntax of Python. Understand how to run a python program in many different ways. (Jupyter notebook vs. command line vs IDE) I took about a month to review the  Python docs , the  Hitchhiker’s Guide to Python , and coding challenges on  CodeSignal . Hint: Keep an ear out for common problem-solving techniques used by programmers. (pronounced “algorithms”) A prerequisite for machine learning and data analysis. If you already have a solid understanding spend a week or two brushing up on key concepts. Focus especially hard on descriptive statistics . Being able to understand a data set is a skill worth its weight in gold. Learn how to load, manipulate, and visualize data. Mastery of these libraries will be crucial to your personal projects. Quick hint: Don’t feel like you have to memorize every method or function name, that comes with practice. If you forget, Google it. Check out the  Pandas Docs ,  Numpy Docs , and  Matplotlib Tutorials . There are better resources out there, but these are what I used. Remember, the only way you will learn these libraries is by using them! Learn the theory and application of machine learning algorithms. Then apply the concepts you learn to real-world data that you care about. Most beginners start by working with toy data-sets from the  UCI ML Repository . Play around with the data and go through guided ML tutorials. The  Scikit-learn  documentation has excellent tutorials on the application of common algorithms. I also found this  podcast  to be a great (and free) educational resource behind the theory of ML. You can listen to it on your commute or while working out. Getting a job means being able to take real-world data and turn it into action. To do this you will need to learn how to use a business’ computational resources to get, transform, and process data. This is the most under-taught part of the data science curriculum. Mainly because the specific tools you use depend on the industry you are going in to. However, database manipulation is a required skill set.  You can learn how to manipulate databases with code on  ModeAnalytics  or  Codecademy . You can also implement your own database (cheaply) on  DigitalOcean . Another (often) required skill is  version control .  You can acquire this skill easily by creating a  GitHub  account and using the command line to commit your code daily. When considering what other technologies to learn, it is important to think about your interests and passions. For example, if you are interested in web development, then look into the tools used by companies in that industry. There are literally thousands of web pages and forums explaining the use of common data science tools. Because of this, it is very easy to get side-tracked while learning online. When you start researching a topic you need to hold your goal in mind. If you don’t, you risk getting caught up in whatever catchy link draws your eye. The solution,  get a good storage system to save interesting web-resources . This way you can save material for later, and focus on the topic that is relevant to you at the moment. If you do this right, you can make an ordered learning path that shows you what you should be focused on. You will also learn faster and avoid being distracted. Warning,  your reading list will quickly grow into the hundreds  as you explore new topics that interest you. Don’t worry, this leads us to my second piece of advice. Having a self-driven education can often feel like trying to read a never-ending library of knowledge. If you’re going to be successful in data science you need to think of your education as a lifelong process. Just remember, the process of learning is its own reward. Throughout your educational journey, you will explore your interests and discover more about what drives you . The more you learn about yourself, the more enjoyment you will get out of learning. Don’t settle for just learning a concept and then moving to the next thing. The process of learning doesn’t stop until you can apply a concept to the real world. Not every concept needs to have a dedicated project in your portfolio. But it is important to stay grounded and  remember that you are learning so you can make an impact in the world. When it comes down to it,  skepticism is one of the biggest adversities you will face when learning data science. This may come from others, or it may come from  yourself . Your portfolio is your way of showing the world that you are capable and confident in your own skills. Because of this, building a portfolio is the single most important thing you can do while studying data science. A good portfolio can land you a job and make you a more confident data scientist. Fill your portfolio with projects that you are proud of. Did you build your own web app from scratch? Did you make your own IMDB database? Have you written an interesting data analysis of healthcare data? Put it in your portfolio. Just make sure write-ups are readable, the code is well documented, and the portfolio itself looks good. This is my portfolio. A simpler method to publish your portfolio is to create a GitHub repository that includes a great ReadMe (summary page) as well as relevant project files. Here is an aesthetically pleasing, yet simple,  GitHub portfolio . For a more advanced portfolio, look into GitHub-IO to host your own free website. ( example ) Data science is a set of tools intended to make a change in the world. Some data scientists build computer vision systems to diagnose medical images, others traverse billions of data entries to find patterns in website user preferences. The applications of data science are endless, that’s why it is important to find what applications excite you. If you find topics that you are passionate about, you will be more willing to put in the work to make a great project. This leads to my favorite piece of advice in this article. When you are learning, keep your eyes open for projects or ideas that excite you. Once you have spent time learning, try to connect the dots. Find similarities between projects that fascinate you. Then spend some time researching industries that work on those types of projects. Once you find an industry that you are passionate about, make it your goal to acquire the skills and technical expertise needed in that business. If you can do this, you will be primed to turn your hard work and dedication for learning into a passionate and successful career. If you love making discoveries about the world. If you are fascinated by artificial intelligence. Then you can break into the data science industry no matter what your situation is. It won’t be easy. To motivate your own education you will need perseverance and discipline. But if you are the type of person who can push yourself to improve, you are more than capable of mastering these skills on your own. After all, that’s what being a data scientist is all about. Being curious, self-driven, and passionate about finding answers."
How to Build a Data Science Portfolio,ow to Build a Data Science Portfoli,"How do you get a job in data science?  Knowing enough statistics, machine learning, programming, etc to be able to get a job is difficult. One thing I have found lately is quite a few people  may   have the required skills to get a job, but no portfolio . While a resume matters, having a portfolio of public evidence of your data science skills can do wonders for your job prospects. Even if you have a  referral ,  the ability to show potential employers what you can do instead of just telling them you can do something is important . This post will include links to where various data science professionals (data science managers, data scientists, social media icons, or some combination thereof) and others talk about what to have in a portfolio and how to get noticed. With that, let’s get started! Besides the benefit of learning by making a portfolio, a portfolio is important as it can help get you employment. For the purpose of this article, let’s define a portfolio as public evidence of your data science skills. I got this definition from  David Robinson  Chief Data Scientist at DataCamp when he was interviewed by  Marissa Gemma  on  Mode Analytics blog . He was asked about landing his first job in industry and said, The most effective strategy for me was doing public work. I blogged and did a lot of open source development late in my PhD, and these helped give public evidence of my data science skills. But the way I landed my first industry job was a particularly noteworthy example of the public work. During my PhD I was an active answerer on the programming site Stack Overflow, and an engineer at the company came across one of my answers (one explaining the intuition behind the beta distribution). He was so impressed with the answer that he got in touch with me [through Twitter], and a few interviews later  I was hired . You may think of this as a freak occurrence, but you will often find that the more active you are, the greater chance you have of something like this occuring. From  David’s blog post , The more public work you do, the higher the chance of a freak accident like that: of someone noticing your work and pointing you towards a job opportunity, or of someone who’s interviewing you having heard of work you’ve done. People often forget that software engineers and data scientists also Google their issues. If these same people have their problems solved by reading your public work, they might think better of you and reach out to you. Even for an entry level role, most companies want to have people with at least a little bit of real life experience. You may have seen memes like the one below. The question is how do you get experience if you need experience to get your first job? If there is an answer, the answer is  projects . Projects are perhaps the best substitutes for work experience or as  Will Stanton  said, If you don’t have any experience as a data scientist, then you absolutely  have to  do independent projects. In fact, when  Jacqueline Nolis   interviews candidates , she wants to hear about a description of a recent problem/project that you have faced. I want to hear about a project they’ve worked on recently. I ask them about how the project started, how they determined it was worth time and effort, their process, and their results. I also ask them about what they learned from the project. I gain a lot from answers to this question: if they can tell a narrative, how the problem related to the bigger picture, and how they tackled the hard work of doing something. If you don’t have some data science related work experience, the best option here is to talk about a data science project that you have worked on. Data science is such a broad field that it is hard to know what kind of projects hiring managers want to see.  William Chen , a Data Science Manager at Quora, shared his thoughts on the subject at Kaggle’s CareerCon 2018 ( video ). I love projects where people show that they are interested in data in a way that goes beyond homework assignments. Any sort of class final project where you explore an interesting dataset and find interesting results… Put effort into the writeup… I really like seeing really good writeups where people find interesting and novel things…have some visualizations and share their work. A lot of people recognize the value of creating projects, but one issue a lot of people wonder is where do you get that interesting dataset and what do you do with it.  Jason Goodman , Data Scientist at Airbnb, has a post  Advice on Building Data Portfolio Projects  where he talks about many different project ideas and has good advice on what kind of datasets you should use. He also echos one of William’s points about working with interesting data. I find that the best portfolio projects are less about doing fancy modeling and more about working with interesting data. A lot of people do things with financial information or Twitter data; those can work, but the data isn’t inherently that interesting, so you’re working uphill. One of his other points in the article is that webscraping is a great way to get interesting data. If you are interested in learning how to build your own dataset by webscraping in Python, you can see my post  here . If you are coming from academia, it is important to note that your thesis can count as a project (a very large project). You can hear  William Chen  talk about it  here . One thing I have found very common (to the point of it appearing multiple times in this blog post) in a lot of portfolio/resume advice is not to have common projects in your portfolio. Jeremie Harris  in  The 4 fastest ways not to get hired as a data scientist  said, It’s hard to think of a faster way to have your resume thrown into the ‘definite no’ pile than featuring work you did on trivial proof-of-concept datasets among your highlighted personal projects. When in doubt, here are some projects that hurt you more than they help you: * Survival classification on the  Titanic dataset . * Hand-written digit classification on the  MNIST dataset . * Flower species classification using the  iris dataset . The image below shows partial examples of classification of Titanic (A), MNIST (B), and iris (C) datasets. There aren’t a lot of ways to use these datasets to distinguish yourself from other applicants. Make sure to list novel projects. Favio Vazquez  has an  excellent article  where he talked about how he got his job as a data scientist. Of course, one of his tips is to have a portfolio. Have a portfolio. If you are looking for a serious paid job in data science do some projects with real data. If you can post them on GitHub. Apart from Kaggle competitions, find something that you love or a problem you want to solve and use your knowledge to do it. One of the other interesting findings was that you always have to keep on improving as you go through the job hunt. I applied to almost 125 jobs (for real, maybe you applied for much more), I got only like 25–30 replies. Some of them were just: Thanks but nope. And I got almost 15 interviews. I learned from each one. Got better. I had to deal with a lot of rejection. Something I was actually not prepared to. But I loved the process of getting interviewed (not all of them to be honest). I studied a lot, programmed everyday, read a lot of articles and posts. They helped a lot. As you learn more and improve yourself, your portfolio should also be updated. This same sentiment is echoed in many other advice articles. As  Jason Goodman  said, The project isn’t done when you post it publicly. Don’t be afraid to keep adding on to or editing your projects after they’re published! This advice is especially true when you are looking for a job. There are many stories of successful people like  Kelly Peng , Data Scientist at Airbnb, who really persevered and kept on working and improving. In  one of her blog posts , she went over how many places she applied for and interviewed with. Applications: 475 Phone interviews: 50 Finished data science take-home challenges: 9 Onsite interviews: 8 Offers: 2 Time spent: 6 months She clearly applied to a lot of jobs and kept on persisting. In her article, she even mentions how you need to keep on learning from your interviewing experiences. Take note of all the interview questions you got asked, especially those questions you failed to answer. You can fail again, but don’t fail at the same spot. You should always be learning and improving. One of the ways someone finds your portfolio is often through your resume so it is worth a mention. A data science resume is a place to focus on your technical skills. Your resume is a chance to succinctly represent your qualifications and fit for that particular role. Recruiters and hiring managers skim resumes very quickly, and you only have a short time to make an impression. Improving your resume can increase your chance of getting an interview. You have to make sure every single line and every single section of your resume counts. William Chen , a Data Science Manager from Quora has  9 Tips for making your data science resume .  Notice in the brief summary of his points below, that projects and portfolio are points 6, 7, 8, and arguably 9 . 2. Objective : Don’t include one. They don’t help you distinguish yourself from other people. They take away space from the more important things (skills, projects, experience etc). Cover letters are extremely optional unless you really personalize it. 3. Coursework : Do list  relevant coursework  that is applicable for the job description. 4. Skills : Don’t give numerical ratings for your skills. If you want to rate yourself on your skills, use words like proficient or familiar or things like that. You can even exclude assessments altogether. 5. Skills : Do list technical skills that the job description mentions. The order you list your skills in can suggest what you are best at. 6. Projects : Don’t list common projects or homework. They aren’t that helpful in distinguishing you from other applicants. List projects that are novel. 7. Projects :   Show results and include links. If you participated in Kaggle competition, put percentile rank as it helps the person reading your resume understand where you are in the competition. In projects sections, there is always room for links to writeups and papers as they let the hiring manager or recruiter dig in deeper (bias to real world messy problems where you learn something new). Notice that in one of the projects sections above, a person has an additional link to a blog that lets the recruiter or hiring manager find out more. This is one way to link to various parts of your portfolio from your resume. 8. Portfolio:  Fill our your online presence. The most basic is a LinkedIn profile. It is kind of like an extended resume. Github and Kaggle profiles can help show off your work. Fill out each profile and include links to other sites. Fill out descriptions for your GitHub respositories. Include links to your knowledge sharing profiles/blog (medium, quora). Data science specifically is about knowledge sharing and communicating what the data means to other people. You don’t have to do all of them, but pick a few and do it (More on this later). 9. Experience : Tailor your experience towards the job. Experience is the core of your resume, but if you don’t have work experience what do you do? Focus your resume on independent projects, like capstone projects, independent research, thesis work, or Kaggle competitions. These are substitutes for work experience if you don’t have work experience to put on your resume. Avoid putting irrelevant experience on your resume. If you want to know hear data science managers go over portfolios and resumes, here are links to Kaggle’s CareerCon 2018 ( video ,  resumes reviewed ). This is very similar to the Importance of a Portfolio section, just divided into subsections. Having a Github page, a Kaggle profile, a Stack Overflow, etc can provide support for your resume. Having online profiles filled out can be a good signal for hiring managers. As  David Robinson  phrases it, Generally, when I’m evaluating a candidate, I’m excited to see what they’ve shared publicly, even if it’s not polished or finished. And sharing  anything  is almost always better than sharing nothing. The reason why data scientists like seeing public work is as  Will Stanton  said, Data scientists use these tools to share their own work and find answers to questions. If you use these tools, then you are signaling to data scientists that you are one of  them , even if you haven’t ever worked as a data scientist. A lot of Data science is about communication and presenting data so it is good to have these online profiles. Besides from the fact that these platforms help provide valuable experience, they can also help you get noticed and lead people to your resume. People can and do find your resume online through various sources (LinkedIn, GitHub, Twitter, Kaggle, Medium, Stack Overflow, Tableau Public, Quora, Youtube, etc). You will even find that different types of social media feed into eachother. A Github profile is a powerful signal that you are a competent data scientist. In the projects section of a resume, people often leave links to their GitHub where the code is stored for their projects. You can also have writeups and markdown there. GitHub lets people see what you have built and how you have built it. At some companies, hiring managers look at an applicants GitHub. It is another way to show employers you aren’t a false positive. If you take the time to develop your GitHub profile, you can be better evaluated than others. It is worth mentioning that you need to have some sort of README.md with a description of your project as a lot of  data science is about communicating results . Make sure the README.md file clearly describes what your project is, what it does, and how to run your code. Participating in Kaggle competitions, creating a kernel, and contributing to discussions are ways to show some competency as a data scientist. It is important to emphasize that Kaggle is not like an industry project as  Colleen Farrelly , mentions in this  quora question . Kaggle competitions take care of coming up with a task, acquire data for you, and clean it into some usable form. What it does is give you practice analyzing data and coming up with a model. Note that there is a good reason why  Kaggle Grandmasters continue to participate in Kaggle competitions .  Reshama Shaikh  has a post  To Kaggle Or Not  where she talked about the value of Kaggle competitions. From her post, It is true, doing one Kaggle competition does not qualify someone to be a data scientist. Neither does taking one class or attending one conference tutorial or analyzing one dataset or reading one book in data science. Working on competition(s) adds to your experience and augments your portfolio. It is a complement to your other projects, not the sole litmus test of one’s data science skillset. I completely agree with Reshama’s view on this. In particular, the point about how taking a class in something doesn’t make you an expert in something nor does it give you a job. I literally have made a course called  Python for Data Visualization  and I go into extensive depth about Pandas, Matplotlib, and Seaborn. It wont immediately give you a job or make you an immediate expert in Matplotlib or Seaborn, but it will make your knowledge greater, teach you how the libraries work, and aid in building your portfolio. Everything you do can make you more employable. Unlike a resume, which is confined by length, a LinkedIn profile allows you to describe your projects and work experience in more depth. Udacity has a  guide on making a good LinkedIn profile . An important part of LinkedIn is their search tool and for you to show up, you must have  relevant keywords   in   your profile. Recruiters often search for people on LinkedIn. LinkedIn allows you to see which companies have searched for you and who has viewed your profile. Besides companies finding you and sending you messages on your availability, LinkedIn also has many features like  Ask for a Referral .  Jason Goodman  in his article  Advice on Applying to Data Science Jobs  uses LinkedIn to  indirectly  ask for referrals. I never, never, never applied to any companies without an introduction to someone who worked at the company…once I was interested in a company, I would use LinkedIn to find a first- or second- degree connection at the company. I would write to that connection, asking to talk to them about their experience at the company and, if possible, whether they’d be able to connect me to someone on the Data Science team. Whenever I could, I did in-person meetings (coffee or lunch) instead of phone calls. As an aside, Trey Causey recently wrote  a great post  on how to ask for just these kinds of meetings. I would never ask for a job directly, but they would usually ask for my resume and offer to submit me as an internal referral, or put me in touch with a hiring manager. If they didn’t seem comfortable doing so...I’d just thank them for their time and move on. Notice that he doesn’t right away ask for a referral. While common job advice when applying to a company is to get a referral, it is VERY IMPORTANT to note that you still need a portfolio, experience, or some sort of proof you can do a job. Jason even mentions the importance of a portfolio in that and  other articles he has written . Aman Dalmia  learned something similar by  Interviewing at Multiple AI Companies and Startups . Networking is  NOT  messaging people to place a referral for you .  When I was starting off, I did this mistake way too often until I stumbled upon an article that talked about the importance of building a  real  connection with people by offering our help first. One other point he had is that LinkedIn is great for getting your content/portfolio out. Another important step in networking is to get your content out. For example, if you’re good at something, blog about it and share that blog on Facebook and LinkedIn .  Not only does this help others,  it helps you as well. Having some form of blog can be highly beneficial. A lot of data science is about communication and presenting data. Blogging is a way of practicing this and showing you can do this. Writing about a project or a data science topic allows you to share with the community as well as encourages you to write out your work process and thoughts. This is a useful skill when interviewing. As  David Robinson  said, A blog is your chance to practice the relevant skills. By writing a blog, you can practice communicate findings to others. It also is another form of advertising yourself. Blogs about  Using Scrapy to Build your Own Dataset , and ironically  Python Environment Management with Conda  have taught me a lot and have gotten me a lot of opportunities I would normally not have gotten. Recently, my  boxplot blog  brought me the opportunity to create my own  Python for Data Visualization course . One of the major benefits I have found is that throughout the process of people critiquing my projects and suggesting improvements (though the comments section of the blog) makes it so interviewers aren’t the first ones pointing out these same flaws. The more obvious benefit is that by making a blog you tend to read a lot more data science/machine learning blog posts and hence learn more. As for what platform to blog on, I recommend using Medium.  Manali Shinde  in her blog post  How to Construct a Data Science Portfolio from Scratch  had a really good point on why she choose Medium for her blog. I thought of creating my own website on a platform such as WordPress or Squarespace. While those platforms are amazing to host your own portfolio, I wanted a place where I would get some visibility, and a pretty good tagging system to reach greater audiences. Luckily Medium, as we know, has those options (and it’s also free). If you don’t know what to write about, I suggest you look at  David Robinson’s advice . Being active on Twitter is a great way to identify and interact with people in your field. You can also promote your blog on Twitter so that your portfolio can be that much more visible. There are so many opportunities to interact with people on twitter. One of them as  Reshama Shaikh  said in her famous blog post “ How Do I Get My First Data Science Job? ” was, David Robinson  generously offers to retweet your first data science post. With 20K+ followers, that’s an offer that can’t be refused. Twitter can be used for other things than self promotion.  Data Science Renee  has a post “ How to use Twitter to Learn Data Science (or Anything) ” that is quite insightful about taking Twitter to learn skills. One other takeaway from her article was how much her Twitter presence helped her network and get opportunities. I have been asked to be interviewed on podcasts and blogs (some of those should be coming up soon), offered contract work, and offered free admission to a conference I unfortunately couldn’t go to, but was excited to be considered for. “Famous” people in the industry are now coming to me to work with them in some way. Not every data science job uses Tableau or other BI tools. However, if you are applying to jobs where these tools are used, it is important to note that there are websites where you can put dashboards for public consumption. For example, if you say you are learning or know Tableau, put a couple dashboards on  Tableau Public . While a lot of companies might be okay with you learning Tableau on the job, having public evidence of your Tableau skill can help. If you want to see good examples of Tableau Public profiles, please see  Orysya Stus’  and  Brit Cava’s  profiles. Having a strong resume has long been the primary tool for job seekers to relay their skills to potential employers. These days, there is more than one way to showoff your skills and get a job. A portfolio of public evidence is a way to get opportunities that you normally wouldn’t get. It is important to emphasize that a portfolio is an iterative process. As your knowledge grows, your portfolio should be updated over time. Never stop learning or growing. Even this blog post will be updated with feedback and with increasing knowledge. If you want interview advice/guides/courses, time to check out  Brandon Rohrer’s advice on how to survive a data science interview ,  Sadat’s   interview guide , or my  15 Tips for Landing a Data Science Job course . If you want some general data science career advice, I wrote an article on it  here . If you have any questions or thoughts on the tutorial, feel free to reach out in the comments below or through  Twitter ."
Bringing the best out of Jupyter Notebooks for Data Science,ringing the best out of Jupyter Notebooks for Data Scienc,"Reimagining what a Jupyter notebook can be and what can be done with it. Netflix  aims to provide personalized content to their 130 million viewers. One of the significant ways by which data scientists and engineers at Netflix interact with their data is through  Jupyter notebooks . Notebooks leverage the use of collaborative, extensible, scalable, and reproducible data science. For many of us, Jupyter Notebooks is the  de facto  platform when it comes to quick prototyping and exploratory analysis. However, there’s more to this than meets the eye. A lot of Jupyter functionalities sometimes lies under the hood and is not adequately explored. Let us try and explore Jupyter Notebooks’ features which can enhance our productivity while working with them. The notebook is the new shell The shell is a way to interact textually with the computer. The  most popular  Unix shell is Bash( Bourne Again SHell  ). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows. Now, when we work with any Python interpreter, we need to regularly switch between the shell and the IDLE, in case we need to use the command line tools. However, the Jupyter Notebook gives us the ease to execute shell commands from within the notebook by placing an extra  ! before the commands.  Any  command that works at the command-line can be used in IPython by prefixing it with the  !  character. We can even pass values to and from the shell as follows: Notice, the data type of the returned results is not a list. Theme-ify your Jupyter Notebooks! If you are a person who gets bored while staring at the white background of the Jupyter notebook, themes are just for you. The themes also enhance the presentation of the code. You can find more about Jupyter themes  here . Let’s get to the working part. Installation List of available themes Currently, the available themes are  chesterish, grade3, gruvboxd, gruvboxl monokai, oceans16, onedork, solarizedd ,solarizedl. Extend the possibilities Notebook extensions let you move beyond the general vanilla way of using the Jupyter Notebooks. Notebook extensions (or nbextensions) are JavaScript modules that you can load on most of the views in your Notebook’s frontend. These extensions modify the user experience and interface. Installation with conda: Or with pip: Start a Jupyter notebook now, and you should be able to see an  NBextensions Tab  with a lot of options. Click the ones you want and see the magic happen. In case you couldn’t find the tab, a second small nbextension, can be located under the menu Edit . Let us discuss some of the useful extensions. Hinterland  enables code autocompletion menu for every keypress in a code cell, instead of only calling it with the tab. This makes Jupyter notebook’s autocompletion behave like other popular IDEs such as PyCharm. This extension adds a drop-down menu to the Notebook toolbar that allows easy insertion of code snippet cells into the current notebook. This extension splits the cells of the notebook and places then adjacent to each other. This extension enables to collect all running headers and display them in a floating window, as a sidebar or with a navigation menu. The extension is also draggable, resizable, collapsible and dockable. Collapsible Headings a llows the notebook to have collapsible sections, separated by headings. So in case you have a lot of dirty code in your notebook, you can simply collapse it to avoid scrolling it again and again. Autopep8 helps to reformat/prettify the contents of code cells with just a click. If you are tired of hitting the spacebar again and again to format the code, autopep8 is your savior. Make notebooks interactive Widgets  are eventful python objects that have a representation in the browser, often as a control like a slider, textbox, etc. Widgets can be used to build  interactive GUIs  for the notebooks. Let us have a look at some of the widgets. For complete details, you can visit their  Github repository . Interact The  interact  function ( ipywidgets.interact ) automatically creates a user interface (UI) controls for exploring code and data interactively. It is the easiest way to get started using IPython's widgets. Here is a list of some of the useful advanced widgets. The Play widget is useful to perform animations by iterating on a sequence of integers at a certain speed. The value of the slider below is linked to the player. The date picker widget works in Chrome and IE Edge but does not currently work in Firefox or Safari because they do not support the HTML date input field. Make Data frames intuitive Qgrid is also a Jupyter notebook widget but mainly focussed at dataframes. It uses  SlickGrid  to render pandas DataFrames within a Jupyter notebook. This allows you to explore your DataFrames with intuitive scrolling, sorting and filtering controls, as well as edit your DataFrames by double-clicking cells. The  Github Repository  contains more details and examples. Installing with pip: Installing with conda: Code is great when communicated. Notebooks are an effective tool for teaching and writing explainable codes. However, when we want to present our work either we display our entire notebook(with all the codes) or we take the help of powerpoint. Not any more. Jupyter Notebooks can be easily converted to slides and we can easily choose what to show and what to hide from the notebooks. There are two ways to convert the notebooks into slides: Open a new notebook and navigate to  View → Cell Toolbar → Slideshow.  A light grey bar appears on top of each cell, and you can customize the slides. Now go to the directory where the notebook is present and enter the following code: The slides get displayed at port 8000. Also, a  .html  file will be generated in the directory, and you can also access the slides from there. This would look even more classy with a themed background. Let us apply the theme ’ onedork ’ to the notebook and then convert it into a slideshow. These slides have a drawback i.e. you can see the code but cannot edit it. RISE plugin offers a solution. RISE is an acronym for  Reveal.js — Jupyter/IPython Slideshow Extension . It utilized the  reveal.js  to run the slideshow. This is super useful since it also gives the ability to run the code without having to exit the slideshow. Installation 1 — Using conda (recommended): 2 — Using pip (less recommended): and then two more steps to install the JS and CSS in the proper places: Let us now use RISE for the interactive slideshow. We shall re-open the Jupyter Notebook we created earlier. Now we notice a new extension that says “Enter/Exit RISE Slideshow.” Click on it, and you are good to go. Welcome to the world of interactive slides. Refer to the  documentation  for more information. Display it right there! Why go with mere links when you can easily embed an URL, pdf, and videos into your Jupyter Notebooks using IPython’s  display  module. These were some of the features of the Jupyter Notebooks that I found useful and worth sharing. Some of them would be obvious to you while some may be new. So, go ahead and experiment with them. Hopefully, they will be able to save you some time and give you a better UI experience. Also feel free to suggest other useful features in the comments."
"I ranked every Intro to Data Science course on the internet, based on thousands of data points"," ranked every Intro to Data Science course on the internet, based on thousands of data point","A year ago, I dropped out of one of the best computer science programs in Canada. I started creating my own  data science master’s program  using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role.  A few months ago, I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few  coding classes  for the beginner data scientist. Then it was  statistics and probability classes . (Don’t worry if you’re unsure of what an intro to data science course entails. I’ll explain shortly.) For this guide, I spent 10+ hours trying to identify every online intro to data science course offered as of January 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. For this task, I turned to none other than the open source Class Central community and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it’s David. I wrote this guide back in 2017. Since then, I’ve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros.  My top intro to data science course recommendation for 2023 is in there, too. datamaverickhq.com Okay, back to the guide. Each course must fit three criteria: We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on  Udemy , we chose to consider the most-reviewed and highest-rated ones only. There’s always a chance that we missed something, though. So please let us know in the comments section if we left a good course out. We compiled average rating and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on two factors: 1.  Coverage of the data science process.  Does the course brush over or skip certain subjects? Does it cover certain subjects in too much detail? See the next section for what this process entails. 2.  Usage of common data science tools.  Is the course taught using popular programming languages like Python and/or R? These aren’t necessary, but helpful in most cases so slight preference is given to these courses. What is data science? What does a data scientist do?  These are the types of fundamental questions that an intro to data science course should answer. The following infographic from Harvard professors Joe Blitzstein and Hanspeter Pfister outlines a typical  data science process , which will help us answer these questions. Our goal with this introduction to data science course is to become familiar with the data science process. We don’t want too in-depth coverage of specific aspects of the process, hence the “intro to” portion of the title. For each aspect, the ideal course explains key concepts within the framework of the process, introduces common tools, and provides a few examples (preferably hands-on). We’re only looking for an introduction. This guide therefore won’t include full specializations or programs like Johns Hopkins University’s  Data Science Specialization  on Coursera or Udacity’s  Data Analyst Nanodegree . These compilations of courses elude the purpose of this series: to find the best  individual  courses for each subject to comprise a data science education. The final three guides in this series of articles will cover each aspect of the data science process in detail. Several courses listed below require basic programming, statistics, and probability experience. This requirement is understandable given that the new content is reasonably advanced, and that these subjects often have several courses dedicated to them. This experience can be acquired through our recommendations in the first two articles ( programming ,  statistics ) in this Data Science Career Guide. Kirill Eremenko’s  Data Science A-Z™  on Udemy is the clear winner in terms of breadth and depth of coverage of the data science process of the 20+ courses that qualified. It has a 4.5-star weighted average rating over 3,071 reviews, which places it among the highest rated and most reviewed courses of the ones considered. It outlines the full process and provides real-life examples. At 21 hours of content, it is a good length. Reviewers love the instructor’s delivery and the organization of the content. The price varies depending on Udemy discounts, which are frequent, so you may be able to purchase access for as little as $10. Though it doesn’t check our “usage of common data science tools” box ,  the non-Python/R tool choices (gretl, Tableau, Excel) are used effectively in context. Eremenko mentions the following when explaining the gretl choice (gretl is a statistical software package), though it applies to all of the tools he uses (emphasis mine): In gretl, we will be able to do the same modeling just like in R and Python but we won’t have to code. That’s the big deal here. Some of you may already know R very well, but some may not know it at all. My goal is to show you how to build a robust model and  give you a framework that you can apply in any tool you choose . gretl will help us avoid getting bogged down in our coding. One prominent reviewer noted the following: Kirill is the best teacher I’ve found online. He uses real life examples and explains common problems so that you get a deeper understanding of the coursework. He also provides a lot of insight as to what it means to be a data scientist from working with insufficient data all the way to presenting your work to C-class management. I highly recommend this course for beginner students to intermediate data analysts! Udacity’s  Intro to Data Analysis  is a relatively new offering that is part of Udacity’s popular  Data Analyst Nanodegree . It covers the data science process clearly and cohesively using Python, though it lacks a bit in the modeling aspect. The estimated timeline is 36 hours (six hours per week over six weeks), though it is shorter in my experience. It has a 5-star weighted average rating over two reviews. It is free. The videos are well-produced and the instructor (Caroline Buckey) is clear and personable. Lots of programming quizzes enforce the concepts learned in the videos. Students will leave the course confident in their new and/or improved NumPy and Pandas skills (these are popular Python libraries). The final project — which is graded and reviewed in the Nanodegree but not in the free individual course — can be a nice add to a portfolio. Data Science Fundamentals is a four-course series provided by IBM’s Big Data University. It includes courses titled  Data Science 101 ,  Data Science Methodology ,  Data Science Hands-on with Open Source Tools , and  R 101 . It covers the full data science process and introduces Python, R, and several other open-source tools. The courses have tremendous production value. 13–18 hours of effort is estimated, depending on if you take the “R 101” course at the end, which isn’t necessary for the purpose of this guide. Unfortunately, it has no review data on the major review sites that we used for this analysis, so we can’t recommend it over the above two options yet. It is free. Our #1 pick had a weighted average rating of 4.5 out of 5 stars over 3,068 reviews. Let’s look at the other alternatives, sorted by descending rating. Below you’ll find several R-focused courses, if you are set on an introduction in that language. The following courses had no reviews as of January 2017. This is the third of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the  first article  and statistics and probability in the  second article . The remainder of the series will cover other data science core competencies: data visualization and machine learning. medium.freecodecamp.com medium.freecodecamp.com The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering. If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ’s other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the 💚 so more people will see it here on Medium. This is a condensed version of my  original article published on Class Central , where I’ve included further course descriptions, syllabi, and multiple reviews."
"The best Data Science courses on the internet, ranked by your reviews","he best Data Science courses on the internet, ranked by your review","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own  data science master’s program  using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role.   So I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few  coding classes  for the beginner data scientist. Then it was  statistics and probability classes . Then  introductions to data science . Then  data visualization .  Machine learning  was the fifth and latest guide. And now I’m back to conclude this series with even more resources. For each of the five major guides in this series, I spent several hours trying to identify every online course for the subject in question, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My goal was to identify the three best courses available for each subject and present them to you. The 13 supplemental topics — like databases, big data, and general software engineering — didn’t have enough courses to justify full guides. But over the past eight months, I kept track of them as I came across them. I also scoured the internet for courses I may have missed. For these tasks, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it’s David. I wrote this guide back in 2017. Since then, I’ve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros. datamaverickhq.com Okay, back to the guide. Each course within each guide must fit certain criteria. There were subject-specific criteria, then two common ones that each guide shared: We believe we covered every notable course that fit the criteria in each guide. There is always a chance that we missed something, though. Please let us know in each guide’s comments section if we left a good course out. We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on a variety of factors specific to each subject. The criteria in our intro to programming guide, for example: Learn to Program: The Fundamentals (LPT1)  and  Crafting Quality Code (LPT2)  by the University of Toronto via Coursera The University of Toronto’s Learn to Program series has an excellent mix of content difficulty and scope for the beginner data scientist. Taught in Python, the series has a 4.71-star weighted average rating over 284 reviews. An Introduction to Interactive Programming in Python (Part 1)  and  (Part 2)  by Rice University via Coursera Rice University’s Interactive Programming in Python series contains two of the best online courses ever. They skew towards games and interactive applications, which are less applicable topics in data science. The series has a 4.93-star weighted average rating over 6,069 reviews. R Programming Track  by DataCamp If you are set on learning R, DataCamp’s R Programming Track effectively combines programming fundamentals and R syntax instruction. It has a 4.29-star weighted average rating over 14 reviews. Foundations of Data Analysis — Part 1: Statistics Using R  and  Part 2: Inferential Statistics  by the University of Texas at Austin via edX The courses in the UT Austin’s Foundations of Data Analysis series are two of the few with great reviews that also teach statistics and probability with a focus on coding up examples. The series has a 4.61-star weighted average rating over 28 reviews. Statistics with R Specialization  by Duke University via Coursera Duke’s Statistics with R Specialization, which is split into five courses, has a comprehensive syllabus with full sections dedicated to probability. It has a 3.6-star weighted average rating over 5 reviews, but the course it was based upon has a 4.77-star weighted average rating over 60 reviews. Introduction to Probability — The Science of Uncertainty  by the Massachusetts Institute of Technology (MIT) via edX MIT’s Intro to Probability course by far has the highest ratings of the courses considered in the statistics and probability guide. It exclusively probability in great detail, plus it is longer (15 weeks) and more challenging than most MOOCs. It has a 4.82-star weighted average rating over 38 reviews. Data Science A-Z™: Real-Life Data Science Exercises Included  by Kirill Eremenko and the SuperDataScience Team via Udemy Kirill Eremenko’s Data Science A-Z excels in breadth and depth of coverage of the data science process. The instructor’s natural teaching ability is frequently praised by reviewers. It has a 4.5-star weighted average rating over 5,078 reviews. Intro to Data Analysis  by Udacity Udacity’s Intro to Data Analysis covers the data science process cohesively using Python. It has a 5-star weighted average rating over 2 reviews. Data Science Fundamentals  by Big Data University Big Data University’s Data Science Fundamentals covers the full data science process and introduces Python, R, and several other open-source tools. There are no reviews for this course on the review sites used for this analysis. Data Visualization with Tableau Specialization  by the University of California, Davis via Coursera A five-course series, UC Davis’ Data Visualization with Tableau Specialization dives deep into visualization theory. Opportunities to practice Tableau are provided through walkthroughs and a final project. It has a 4-star weighted average rating over 2 reviews. Data Visualization with ggplot2 Series  by DataCamp Endorsed by ggplot2 creator Hadley Wickham, a substantial amount of theory is covered in DataCamp’s Data Visualization with ggplot2 series. You will know R and its quirky syntax quite well leaving these courses. There are no reviews for these courses on the review sites used for this analysis. Tableau 10 Series ( Tableau 10 A-Z  and  Tableau 10 Advanced Training ) by Kirill Eremenko and the SuperDataScience Team on Udemy An effective practical introduction, Kirill Eremenko’s Tableau 10 series focuses mostly on tool coverage (Tableau) rather than data visualization theory. Together, the two courses have a 4.6-star weighted average rating over 3,724 reviews. Machine Learning  by Stanford University via Coursera Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, Stanford University’s Machine Learning covers all aspects of the machine learning workflow and several algorithms. Taught in MATLAB or Octave, It has a 4.7-star weighted average rating over 422 reviews. Machine Learning  by Columbia University via edX A more advanced introduction than Stanford’s, CoIumbia University’s Machine Learning is a newer course with exceptional reviews and a revered instructor. The course’s assignments can be completed using Python, MATLAB, or Octave. It has a 4.8-star weighted average rating over 10 reviews. Machine Learning A-Z™: Hands-On Python & R In Data Science  by Kirill Eremenko and Hadelin de Ponteves via Udemy Kirill Eremenko and Hadelin de Ponteves’ Machine Learning A-Z is an impressively detailed offering that provides instruction in both Python and R, which is rare and can’t be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews. Creative Applications of Deep Learning with TensorFlow  by Kadenze Parag Mital’s Creative Applications of Deep Learning with Tensorflow adds a unique twist to a technical subject. The “creative applications” are inspiring, the course is professionally produced, and the instructor knows his stuff. Taught in Python, It has a 4.75-star weighted average rating over 16 reviews. Neural Networks for Machine Learning  by the University of Toronto via Coursera Learn from a legend.  Geoffrey Hinton  is known as the “godfather of deep learning” is internationally distinguished for his work on artificial neural nets. His Neural Networks for Machine Learning is an advanced class. Taught in Octave with exercises also in Python, it has a 4.11-star weighted average rating over 35 reviews. Deep Learning A-Z™: Hands-On Artificial Neural Networks  by Kirill Eremenko and Hadelin de Ponteves via Udemy Deep Learning A-Z is an accessible introduction to deep learning, with intuitive explanations from Kirill Eremenko and helpful code demos from Hadelin de Ponteves. Taught in Python, it has a 4.6-star weighted average rating over 1,314 reviews. Python Programming Track  by DataCamp, plus their individual pandas courses: DataCamp’s code-heavy instruction style and in-browser programming environment are great for learning syntax. Their Python courses have a 4.64-star weighted average rating over 14 reviews. Udacity’s Intro to Data Analysis, one of our recommendations for intro to data science courses, covers NumPy and pandas as well. R Programming Track  by DataCamp, plus their individual dplyr and data.table courses: Again, DataCamp’s code-heavy instruction style and in-browser programming environment are great for learning syntax. Their R Programming Track, which is also one of our recommendations for programming courses in general, effectively combines programming fundamentals and R syntax instruction. The series has a 4.29-star weighted average rating over 14 reviews. Introduction to Databases  by Stanford University via Stanford OpenEdx (note:  reviews  from the deprecated version on Coursera) Stanford University’s Introduction to Databases covers database theory comprehensively while introducing several open source tools. Programming exercises are challenging. Jennifer Widom,  now the Dean of Stanford’s School of Engineering , is clear and precise. It has a 4.61-star weighted average rating over 59 reviews. Importing & Cleaning Data Tracks by DataCamp: DataCamp’s Importing & Cleaning Data Tracks (one in Python and one in R) excel at teaching the mechanics of preparing your data for analysis and/or visualization. There are no reviews for these courses on the review sites used for this analysis. Data Analysis with R  by Udacity and Facebook Udacity’s Data Analysis with R is an enjoyable introduction to exploratory data analysis. The expert interviews with Facebook’s data scientists are insightful and inspiring. The course has a 4.58-star weighted average rating over 19 reviews. It also serves as a light introduction to R. The Ultimate Hands-On Hadoop — Tame your Big Data!  by Frank Kane via Udemy, then if you want more on specific tools (all by Frank Kane via Udemy): Frank Kane’s Big Data series teaches all of the most popular big data technologies, including over 25 in the “Ultimate” course alone. Kane shares his knowledge from a decade of industry experience working with distributed systems at Amazon and IMDb. Together, the courses have a 4.52-star weighted average rating over 6,932 reviews. Software Testing  by Udacity Software Debugging  by Udacity Version Control with Git  and  GitHub & Collaboration  by Udacity (updates to Udacity’s popular  How to Use Git & GitHub  course) Software skills are an  oft-overlooked  part of a data science education. Udacity’s testing, debugging, and version control courses introduce three core topics relevant to anyone who deals with code, especially those in team-based environments. Together, the courses have a 4.34-star weighted average rating over 68 reviews. Georgia Tech and Udacity have a  new course  that covers software testing and debugging together, though it is more advanced and not all relevant for data scientists. Building a Data Science Team  by Johns Hopkins University via Coursera Learning How to Learn: Powerful mental tools to help you master tough subjects  by Dr. Barbara Oakley and the University of California, San Diego via Coursera Mindshift: Break Through Obstacles to Learning and Discover Your Hidden Potential  by Dr. Barbara Oakley and McMaster University via Coursera Johns Hopkins University’s Building a Data Science Team provides a useful peek into data science in practice. It is an extremely short course that can be completed in a handful of hours and audited for free. Ignore its 3.41-star weighted average rating over 12 reviews, some of which were likely from paying customers. Dr. Barbara Oakley’s Learning How to Learn and Mindshift aren’t data science courses per se. Learning How to Learn, the  most popular online course ever , covers best practices shown by research to be most effective for mastering tough subjects, including memory techniques and dealing with procrastination. In Mindshift, she demonstrates how to get the most out of online learning and MOOCs, how to seek out and work with mentors, and the secrets to avoiding career ruts and general ruts in life. These are two courses that  everyone  should take. They have a 4.74-star and a 4.87-star weighted average rating over 959 and 407 reviews, respectively. Both courses are four weeks in duration. This Data Science Career Guide will continue to be updated as new courses are released and ratings and reviews for them are generated. Are you passionate about another discipline (e.g. Computer Science)? Would you like to help educate the world? If you are interested in creating a Career Guide similar in structure to this one, drop us a note at  guides@class-central.com . As for my future, I’m excited to share that I have taken a position with Udacity as a  Content Developer . That means I’ll be creating and teaching courses. That also means that this guide will be updated by somebody else. I’m joining Udacity because I believe they are best positioned to create the best education product on the planet. Of all of the courses I have taken, online or at university, I learned best while enrolled a Nanodegree. They are incorporating the latest in pedagogy and production and feature an excellent project review system, upbeat instructors, and healthy student and career support teams. Though a piecewise approach like the one we took in this guide can work, a cohesive program with projects and reviews throughout is much more student-friendly. Updating the  Data Analyst Nanodegree  is my first task, which is a part of a larger effort to create a clear path of Nanodegrees for all things data. Students will soon be able to start from scratch with data basics at Udacity and progress all the way through  machine learning ,  artificial intelligence , and even  self-driving cars  if they wish. This is the final piece of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the  first article , statistics and probability in the  second article , intros to data science in the  third article , data visualization in the  fourth , and machine learning in the  fifth . medium.freecodecamp.com Here, we summarized the above five articles, and recommended the best online courses for other key topics such as databases, big data, and even software engineering. If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ’s other pieces: medium.freecodecamp.com medium.freecodecamp.com If you found this helpful, click the 💚 so more people will see it here on Medium. This is a modified version of my  original article  published on Class Central, where a simple list of the courses mentioned here is also provided."
How I Got 4 Data Science Offers and Doubled my Income 2 Months after being Laid Off,ow I Got 4 Data Science Offers and Doubled my Income 2 Months after being Laid Of,"During this unprecedented time with the pandemic, many are finding their careers affected. This includes some of the most talented data scientists with which I have ever worked. Having shared my personal experience with some close friends to help them find a new job after being laid off, I thought it worth sharing publicly. After all, this touches more than me and my friends. Any data scientist who was laid off due to the pandemic or who is actively looking for a data science position can find something here to which they can relate, and which I hope will ultimately offer hope in your job search. So if you’ve ever been stuck - in getting interviews, in interview preparation, in negotiation, anything - I’ve been there, and I want to help. You can reach out to me  here  if you think I might be able to make your journey easier in any way! Here’s my story. I hope you find some useful tips and encouragement within it. In December of 2018, I was informed by my manager that I was to be laid off in January 2019. Three months before, the VP of Engineering of my then startup company had written a letter to our head of People Success. This letter explained why I was one of the top performers in the company and advocated for an increase in my salary. This helped me get a 33% increase in my salary. I was naturally feeling motivated and eager to crack the next milestone on an important project. The company’s future and my own looked bright. It was during this moment of success that I was told that I was impacted by the company-wise cost-cutting initiative. I was let go on January 15th. To be forced to start looking for a new job was daunting, to say the least. After browsing the data science job openings on the market, I soon realized my knowledge gap. What I was doing at the B2B startup (a mix of entry-level data engineering and machine learning) was simply irrelevant to many of the job requirements out there, such as product sense, SQL, stats, and more. I knew the basics but was unsure how to fill the gap towards more advanced skills. However, even that issue seemed secondary to more pressing questions, such as  how do I even get an interview ? I had a mere 1.5 years of work experience with a startup, and I lacked any statistics or computer science-related degree. More questions soon followed. What if I cannot find a job before I lose my visa status? What if the economy takes a downturn before I can find a new job? Despite my fears, there was little choice.  I had to find a new job . In the face of what felt like an overwhelming task, I needed some information to decide my next steps. After doing some research, I realized that more than half of the data science positions on the market were  product-driven positions  (‘product analytics’), and the rest were either modeling or data engineering oriented positions. I also noted that positions other than product analytics tended to have higher requirements. For example, most modeling positions required a PhD degree, and engineering positions required a computer science background. Clearly, the requirements for different tracks varied widely, so it followed that preparation for each would differ as well. With this knowledge in hand, I made an important decision: preparing for all tracks would be both overwhelming and most likely less effective. I would need to  focus on one . I choose product analytics because, based on my background and experience, there was a higher chance that I could get interviews on this track. Of course, not everyone in data science has my exact background and experience, so below I have summarized the general requirements for three categories of data science positions at big companies. Understanding this basic breakdown saved me a lot of time, and I trust it will prove useful for others looking for a job in data science. I will add, however, that for small startups it’s possible that the interview will be less structured and require more of a mixture of all three. Product Analytics (~70% on the market) Modeling (~20% on the market) Data Engineering (~10% on the market) In light of my own experience, the rest of this post is strongly tailored towards those preparing for positions in  product analytics . Come back later to check out my post on preparation for a data engineering position. The very first thing I did once I knew I was going to be laid off was to apply widely and aggressively to other jobs. I used all the job boards I knew including  GlassDoor ,  Indeed  and  LinkedIn . I also asked everyone I knew for referrals. However, since it was almost at the end of the year, I did not receive any responses until January 2019. Asking for referrals proved to be much more effective than applying by myself.  Out of about 50 raw applications, I only got 3 interviews, but out of 18 referrals, I got 7 interviews . Overall, it was becoming obvious that I was not considered a strong candidate in this market. While the structure of interviews was different for each company, there was a general outline that most companies followed: Around half of the companies (4/10) that I’ve interviewed with had a take-home assignment before or instead of a TPS. Take-home assignments consumed a lot of energy. Typically, an 8-hour take-home assignment caused me to need at least half a day to rest after submission. Because of this, I did my best to schedule the interview accordingly. There were no interviews the morning after my take-home assignment. Simply being aware of the basic structure can go a long way in making you feel more at ease and able to cope with the process of finding a new job. Going into my interviews,  every opportunity was critical   to me . Although I was aware that some people learn by interviewing, becoming better after many interviews, and typically obtaining offers for the last few companies with which they interview, I did not feel I could take this approach. When I graduated in 2017, I only received 4 interviews out of 500 raw applications. I was not expecting to get many more in 2019. Thus, my plan was to be fully prepared for each interview I got. I would let  no opportunity go to waste . One benefit of being laid off was that I could study full time for the interview. Each day I structured what I studied, focusing on two or three things per day. No more. From previous interviews, I had learned that a deep understanding allows you to give more thorough answers during interviews. It especially helps to have a depth of knowledge in an interview situation when you tend to be more nervous and anxious than usual. That is not the time when you want to try faking things. As I describe my own experience, I can’t help thinking of a  common misconception  I often hear: it’s not possible to gain the knowledge on product/experimentation without real experience. I firmly disagree. I did not have any prior experience in product or A/B testing, but I believed that those skills could be gained by reading, listening, thinking, and summarizing. After all, this is the same way we were taught things in school. Actually, as I get to know more senior data scientists I continue to learn that this method is common, even for people with years of experience. What you will be interviewed on may not be related to what you were doing at all, but you can gain the knowledge you need in ways other than job experience. Here are the basics of what you can expect. Typically, product and SQL questions were asked during a TPS. Onsite interviews included a few rounds of questions, including product sense, SQL, stats, modeling, behavior, and maybe a presentation. The next few subsections summarize the most useful resources (all freely available) I used when preparing for interviews. In general,  GlassDoor  was a good source to get a sense of company-specific problems. Once I saw those problems, I understood both what the company needed and where my gaps were in fulfilling those needs. I was then able to develop a plan to fill those gaps. The following six subsections are how I prepared for the specific content that comes up in interviews for the product analytics track. In explaining my own preparation, I hope to make the path smoother for those who come after me. Working as a data scientist at a startup, I was mainly responsible for developing and deploying machine learning models and writing spark jobs. Thus, I barely gained any product knowledge. When I saw some real interview questions on  GlassDoor , such as “how to measure success?” or “how to validate the new feature by current users’ behaviors?”, I had utterly no idea how to approach such questions. At the time, they seemed far too abstract and open-ended. To learn product sense I resorted to the basic  read and summarize  strategy, using the resources listed below. All this reading helped me build up my product knowledge. As a result, I came up with a structured way (my own ‘ framework ’) to answer any type of product questions. I then put my knowledge and framework to the test with that all essential to learning any skill:  practice . I wrote out answers to questions involving product sense. I said my answers out loud (even recording myself with my phone), and used the recordings to finetune my answers. Soon I could not only fake it for an interview, I actually knew my stuff. Resources: The first time I took a SQL TPS I failed, and it was with a company in which I was very interested. Clearly, something needed to change. I needed to, once again, practice, and so I spent time grinding SQL questions. Eventually, I was able to complete in a day, questions that had previously taken me an entire week. Practice makes perfect! Resources: To prepare for these kinds of questions, I brushed up on elementary statistics and probability and did some coding exercises. While this may seem overwhelming (there is a lot of content for both topics), the interview questions for a product data scientist were never hard. The resources below are a great way to review. Resources: Without a CS degree, I went into the job search with limited machine knowledge. I had taken some courses during my previous job, and I reviewed my notes from these to prep for interviews. However, even though modeling questions are getting more and more frequent nowadays, the interview questions for a product data scientist mainly geared toward how to apply those models rather than the underlying math and theories. Still here are some helpful resources to bump up your machine learning skills before the interview time. Resources: Some companies required candidates to either present the take-home assignment or a project of which they are most proud. Still, other companies asked about the most impactful project during behavioral interviews. However, no matter what the form the key is to make your presentation interesting and challenging. That sounds great, but how do you do that? My main recommendation is to  think through all the details , such as high-level goals and success metrics to ETL to modeling implementation details, to deployment, monitoring, and improvement. The little things add up to make a great presentation rather than one big idea. Here are a few questions worth rethinking to help reach your ideal presentation: When presenting a project, you want to engage the audience. To make my presentations interesting, I often share  interesting findings and the biggest challenges  of the project. But the best way to make sure you are engaging is practice. Practice and practice out loud. I practiced presenting to my family to ensure my grasp of the material and ease of communication. If you can engage the people you know, an interviewer, who is required to listen, doesn’t stand a chance. While it is easy to get caught up in preparing for the technical interview questions, don’t forget that the behavioral questions are equally important. All companies I’ve interviewed with had at least 1 round of behavior interviews during the onsite portions. These questions typically fall into these three categories: Behavioral questions are very important for data scientists. So be prepared! Understanding a company’s mission and core values helps answer questions in the first group. Questions like 2 and 3 can be answered by telling a story — 3 stories were enough to answer all behavioral questions. Make sure you’ve got a few good stories on hand when you walk in for an interview. Similar to product questions, I practiced a lot by saying it out loud, recording, and listening to then fine-tune my answers. Hearing a story is the best way to make sure it works. The night before an onsite interview was typically a stressful, hectic night. I always tried to cram in more technical knowledge while simultaneously reviewing my statistics notes and thinking of my framework to answer a product question. Of course, as we all learned in school, none of that was incredibly useful. The results were largely determined due to the amount of preparation before not a single night of cramming. So preparation is important, but there are some rules you can follow the day of to make sure your interview is a success. Using these rules, this was the feedback I got from onsite interviews: After receiving verbal offers, the next step was to work with recruiters to finalize the numbers. There’s only one rule here that I stick with -  ALWAYS negotiate . But how? Haseeb Qureshi has a very helpful guide on  negotiating a job offer  (with scripts!) which I followed religiously during my offer negotiation phase. Every single rule was so true. I negotiated with all companies that gave me an offer. The average increase for offers was  15% , and the highest offer was, in total value, increased by  25% . Negotiating works, so don’t be afraid to try it! After losing 11 pounds and lots of cries and screaming (job hunting is stressful and it is okay to admit that), I finally got 4 offers within 2 months of being laid off. 3 of those offers were from companies that I have never dreamed of joining:  Twitter, Lyft , and  Airbnb  (where I ultimately joined) and another offer from a healthcare startup. By the end of two frenzied months, I had received a total of 10 interviews, 4 onsite interviews, and 4 job offers, giving me a  40% TPS-to-onsite rate  and  100% onsite-to-offer rate . I was so lucky that I got lots of support and help from family and friends after being laid off, which was critical to landing a job at my dream company. It was difficult. Ironically looking for a job is also a lot of work, but everything was worth it. I wrote this blog because I know how overwhelmed I was. There is so much to prepare for interviews. I hope this post has made things clearer for other data specialists out there in need of work, and if you want more advice feel free to contact me  here . I am grateful to now be working in a great job, and I would be happy to help you get there too! Since I published this post three weeks ago, I got hundreds of questions on data science interviews. So I decided to make a series of videos to help you land your dream data science job. Check my YouTube channel if you are interested! If you like this post and want to support me…"
Python for Data Science: 8 Concepts You May Have Forgotten,ython for Data Science: 8 Concepts You May Have Forgotte,"If you’ve ever found yourself looking up the same question, concept, or syntax over and over again when programming, you’re not alone. I find myself doing this constantly. While it’s not unnatural to look things up on StackOverflow or other resources, it does slow you down a good bit and raise questions as to your complete understanding of the language. We live in a world where there is a seemingly infinite amount of accessible, free resources looming just one search away at all times. However, this can be both a blessing and a curse. When not managed effectively, an over-reliance on these resources can build poor habits that will set you back long-term. Personally, I find myself pulling code from similar discussion threads several times, rather than taking the time to learn and solidify the concept so that I can reproduce the code myself the next time. This approach is lazy and while it may be the path of least resistance in the short-term, it will ultimately hurt your growth, productivity, and ability to recall syntax (cough,  interviews ) down the line. Recently, I’ve been working through an online data science course titled  Python for Data Science and Machine Learning on Udemy  (Oh God, I sound like  that guy on Youtube ). Over the early lectures in the series, I was reminded of some concepts and syntax that I consistently overlook when performing data analysis in Python. In the interest of solidifying my understanding of these concepts once and for all and saving you guys a couple of StackOverflow searches, here’s the stuff that I’m always forgetting when working with Python, NumPy, and Pandas. I’ve included a short description and example for each, however for your benefit, I will also include links to videos and other resources that explore each concept more in-depth as well. Writing out a for loop every time you need to define some sort of list is tedious, luckily Python has a built-in way to address this problem in just one line of code. The syntax can be a little hard to wrap your head around but once you get familiar with this technique you’ll use it fairly often. See the example above and below for how you would normally go about list comprehension with a for loop vs. creating your list with in one simple line with no loops necessary. Ever get tired of creating function after function for limited use cases? Lambda functions to the rescue! Lambda functions are used for creating small, one-time and anonymous function objects in Python. Basically, they let you create a function,  without creating a function . The basic syntax of lambda functions is: Note that lambda functions can do everything that regular functions can do, as long as there’s just one expression. Check out the simple example below and the upcoming video to get a better feel for the power of lambda functions: Once you have a grasp on lambda functions, learning to pair them with the map and filter functions can be a powerful tool. Specifically, map takes in a list and transforms it into a new list by performing some sort of operation on each element. In this example, it goes through each element and maps the result of itself times 2 to a new list. Note that the list function simply converts the output to list type. The filter function takes in a list and a rule, much like map, however it returns a subset of the original list by comparing each element against the boolean filtering rule. For creating quick and easy Numpy arrays, look no further than the arange and linspace functions. Each one has their specific purpose, but the appeal here (instead of using range), is that they output NumPy arrays, which are typically easier to work with for data science. Arange returns evenly spaced values within a given interval. Along with a starting and stopping point, you can also define a step size or data type if necessary. Note that the stopping point is a ‘cut-off’ value, so it will not be included in the array output. Linspace is very similar, but with a slight twist. Linspace returns evenly spaced numbers over a specified interval. So given a starting and stopping point, as well as a number of values, linspace will evenly space them out for you in a NumPy array. This is especially helpful for data visualizations and declaring axes when plotting. You may have ran into this when dropping a column in Pandas or summing values in NumPy matrix. If not, then you surely will at some point. Let’s use the example of dropping a column for now: I don’t know how many times I wrote this line of code before I actually knew why I was declaring axis what I was. As you can probably deduce from above, set axis to 1 if you want to deal with columns and set it to 0 if you want rows. But why is this? My favorite reasoning, or atleast how I remember this: Calling the shape attribute from a Pandas dataframe gives us back a tuple with the first value representing the number of rows and the second value representing the number of columns. If you think about how this is indexed in Python, rows are at 0 and columns are at 1, much like how we declare our axis value. Crazy, right? If you’re familiar with SQL, then these concepts will probably come a lot easier for you. Anyhow, these functions are essentially just ways to combine dataframes in specific ways. It can be difficult to keep track of which is best to use at which time, so let’s review it. Concat allows the user to append one or more dataframes to each other either below or next to it (depending on how you define the axis). Merge combines multiple dataframes on specific, common columns that serve as the primary key. Join, much like merge, combines two dataframes. However, it joins them based on their indices, rather than some specified column. Check out the excellent  Pandas documentation  for specific syntax and more concrete examples, as well as some special cases that you may run into. Think of apply as a map function, but made for Pandas DataFrames or more specifically, for Series. If you’re not as familiar, Series are pretty similar to NumPy arrays for the most part. Apply sends a function to every element along a column or row depending on what you specify. You might imagine how useful this can be, especially for formatting and manipulating values across a whole DataFrame column, without having to loop at all. Last but certainly not least is pivot tables. If you’re familiar with Microsoft Excel, then you’ve probably heard of pivot tables in some respect. The Pandas built-in pivot_table function creates a spreadsheet-style pivot table as a DataFrame. Note that the levels in the pivot table are stored in MultiIndex objects on the index and columns of the resulting DataFrame. That’s it for now. I hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat tricky methods, functions, and concepts you frequently encounter when using Python for data science. Personally, I know that even the act of writing these out and trying to explain them in  simple terms  has helped me out a ton. Thanks for reading! Feel free to check out some of my similar essays below and  subscribe  to my newsletter for interesting links and new content. You can follow me on Medium for more posts like this and find me on  Twitter  as well. For more on me and what I’m up to, check out  my website ."
Want to know how Deep Learning works? Here’s a quick guide for everyone.,ant to know how Deep Learning works? Here’s a quick guide for everyone,"Artificial Intelligence  (AI) and  Machine Learning  (ML) are some of the hottest topics right now. The term  “AI”  is thrown around casually every day. You hear aspiring developers saying they want to learn AI. You also hear executives saying they want to implement AI in their services. But quite often, many of these people don’t understand what…"
Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,eep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machine,"(An alternate version of this article was originally published in  the Boston Globe) On December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as  Chicago Pile-1 , it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words. Now, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it’s vital we understand this new language, and what it’s increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it. The language is a new class of machine learning known as  deep learning , and the “whispered word” was a computer’s use of it to seemingly out of nowhere  defeat three-time European Go champion Fan Hui , not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world’s best living Go players, if not  the  best. Imagining such a grand duel of man versus machine,  China’s top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most . What actually ended up happening when they faced off? Lee went on to lose  all but one  of their match’s five games. An AI named AlphaGo is now a better Go player than any human and has been  granted the “divine” rank of 9 dan . In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue. “AlphaGo’s historic victory is a clear signal that we’ve gone from linear to parabolic.” So, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what’s happening is no game. AlphaGo’s historic victory is a clear signal that we’ve gone  from linear to parabolic . Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income. This may all sound like exaggeration, so let’s take a few decade steps back, and look at what computer technology has been actively doing to human employment so far: Let the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It’s already here.  Computer technology is already eating jobs and has been since 1990. All work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn’t change, and that work can be better handled by machines. Distressingly, it’s exactly routine work that once formed the basis of the American middle class. It’s routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it’s routine cognitive work that once filled US office spaces.  Such jobs are now increasingly unavailable , leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them. If we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That’s what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to  learn . I’m a writer at heart, but my educational background happens to be in psychology and physics. I’m fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as  cognitive neuroscience . I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me. As a quick primer in the way our brains function, they’re a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It’s all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we’ve been learning about how we work, we’ve now begun applying to the way machines work. One of these applications is the creation of  deep neural networks  - kind of like pared-down virtual brains. They provide an avenue to machine learning that’s made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It’s not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka  big data . Big data isn’t just some buzzword. It’s information, and when it comes to information, we’re creating more and more of it every day. In fact we’re creating so much that a 2013 report by SINTEF estimated that  90% of all information in the world had been created in the prior two years . This incredible rate of data creation is even  doubling every 1.5 years  thanks to the Internet, where in 2015  every minute  we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets . Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn  to learn . Why? Imagine programming a computer to recognize a chair. You’d need to enter a ton of instructions, and the result would still be a program detecting chairs that aren’t, and  not  detecting chairs that are. So how did  we  learn to detect chairs? Our parents pointed at a chair and said, “chair.” Then we thought we had that whole chair thing all figured out, so we pointed at a table and said “chair”, which is when our parents told us that was “table.” This is called reinforcement learning. The label “chair” gets connected to every chair we see, such that certain neural pathways are weighted and others aren’t. For “chair” to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains. The power of deep learning is that it’s a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing “chairness” to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of “chairness.” Next we test it with even more images. Where it’s wrong, we correct it, which further improves its “chairness” detection. Repetition of this process results in a computer that knows what a chair is when it sees it,  for the most part as well as we can . The important difference though is that  unlike us, it can then sort through  millions  of images  within a matter of  seconds . This combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo,  Google’s DeepMind AI learned how to read and comprehend what it read  through hundreds of thousands of annotated news articles.  DeepMind also  taught itself  to play dozens of Atari 2600 video games better than humans , just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions,  attaining International Master level status in just 72 hours by repeatedly playing itself . In 2015,  an AI even passed a visual Turing test by learning to learn  in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all  major  milestones in AI. However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to  the announcement by Google of AlphaGo’s  victory, was by experts essentially, “ Maybe in another ten years .” A decade was considered a fair guess because Go is a game so complex I’ll just let Ken Jennings of Jeopardy fame,  another former champion human defeated by AI , describe it: Go is famously a more complex game than chess, with its larger board, longer games, and many more pieces. Google’s DeepMind artificial intelligence team likes to say that there are more possible Go boards than atoms in the known universe, but that vastly  understates  the computational problem. There are about 10¹⁷⁰ board positions in Go, and  only  10⁸⁰ atoms in the universe. That means that if there were as many parallel universes as there are atoms in our universe (!), then the  total  number of atoms in  all  those universes combined would be close to the possibilities on a single Go board. Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what  feels  like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and  playing itself millions of times . So the answer to when the game of Go would fall to machines wasn’t even close to ten years. The correct answer ended up being, “ Any time now. ” Any time now. That’s the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it. We need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean  nothing  humans do as a job is uniquely safe anymore. From  hamburgers  to  healthcare , machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans. Amelia  is just one AI out there currently being beta-tested in companies right  now . Created by IPsoft over the past 16 years, she’s learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she’s able to learn, she’s able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it’s been estimated that she can put 250 million people out of a job,  worldwide . Viv  is an AI coming soon from the creators of Siri who’ll be our own personal assistant. She’ll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she’ll know we’ll like best. In doing all of this for us, we’ll see far fewer ads, and that means the entire advertising industry — that industry the entire Internet is built upon — stands to be hugely disrupted. A world with Amelia and Viv — and the countless other AI counterparts coming online soon — in combination with robots like  Boston Dynamics’ next generation Atlas  portends, is a world where machines can do  all four   types of jobs  and that means serious societal reconsiderations. If a machine can do a job instead of a human,  should any human be forced at the threat of destitution to perform that job ? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when  jobs for many are entirely unobtainable ? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them,  where does that money go instead ? And  what does it no longer buy ?  Is it even possible that many of the jobs we’re creating don’t need to exist at all , and only do because of the incomes they provide? These are questions we need to start asking, and fast. Fortunately, people  are  beginning  to   ask   these   questions , and there’s an answer that’s building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is  universal basic income . By adopting UBI, aside from  immunizing  against the negative effects of automation, we’d also be decreasing  the risks inherent in entrepreneurship , and  the sizes of bureaucracies  necessary to boost incomes. It’s for these reasons, it has  cross-partisan support , and is even now in the beginning stages of possible implementation in countries like  Switzerland ,  Finland , the  Netherlands , and  Canada . The future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will.  The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million . That’s a net loss, not a net gain of 5 million jobs. In a frequently cited paper,  an Oxford study estimated the automation of about half of all existing jobs by 2033 . Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies —  especially the US economy as I wrote last year about automating truck driving  — by eliminating millions of jobs within a short span of time. And now even the White House,  in a stunning report to Congress , has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable “ duck and cover ” strategies for avoiding nuclear blasts during the Cold War. All of this is why it’s those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist  Jeremy Howard  asked “Do you want half of people to starve because they literally can’t add economic value, or not?” before going on to suggest, ”If the answer is  not , then the smartest way to distribute the wealth is by implementing a  universal basic income .” AI pioneer  Chris Eliasmith , director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, “AI is already having a big impact on our economies… My suspicion is that more countries will have to follow Finland’s lead in exploring  basic income guarantees  for people.” Moshe Vardi  expressed the same sentiment  after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, “we need to rethink the very basic structure of our economic system… we may have to consider instituting a  basic income guarantee .” Even Baidu’s chief scientist and founder of Google’s “Google Brain” deep learning project,  Andrew Ng , during an onstage interview at this year’s Deep Learning Summit, expressed the shared notion that  basic income  must be “seriously considered” by governments, citing “a high chance that AI will create massive labor displacement.” When those building the tools begin warning about the implications of their use, shouldn’t those wishing to use those tools listen with the utmost attention, especially when it’s the very livelihoods of millions of people at stake? If not then, what about when  Nobel prize winning economists  begin agreeing with them in increasing numbers? No nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let’s ask ourselves, what’s the purpose of the technologies we’re creating? What’s the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose  how  we work, and to decline any pay/hours we deem insufficient because we’re already earning the incomes that machines aren’t? What’s the big lesson to learn, in a century when machines can learn? I offer it’s that jobs are for machines, and life is for people. This article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with  a monthly patron pledge  of $1+. Are you a creative? Become a creator on Patreon . Join me in taking  the BIG Patreon Creator Pledge for basic income Special thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O’Sullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli Niinimäki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith. Would you like to see your name here too? Scott Santens writes about basic income on  his blog . You can also follow him here on  Medium , on  Twitter , on  Facebook , or on  Reddit  where he is a moderator for the  /r/BasicIncome  community of over 30,000 subscribers. If you feel others would appreciate this article, please click the green heart."
Thoughts after taking the Deeplearning.ai courses,houghts after taking the Deeplearning.ai course,"[ Update — Feb 2nd 2018: When this blog post was written, only 3 courses had been released.  All 5 courses in this specialization are now out . I will have a follow-up blog post soon. ] Between a full time job and a toddler at home, I spend my spare time learning about the ideas in cognitive science & AI. Once in a while a great paper/video/course comes out and you’re instantly hooked. Andrew Ng’s new deeplearning.ai course is like that  Shane Carruth  or  Rajnikanth movie  that one yearns for! Naturally, as soon as the course was released on coursera, I registered and spent the past 4 evenings binge watching the lectures, working through quizzes and programming assignments. DL practitioners and ML engineers typically spend most days working at an abstract Keras or TensorFlow level. But it’s nice to take a break once in a while to get down to the nuts and bolts of learning algorithms and actually do back-propagation by hand. It is both fun and  incredibly useful ! Andrew Ng’s new  ad venture is a  bottom-up approach  to teaching neural networks — powerful non-linearity learning algorithms, at a beginner-mid level. In classic Ng style, the course is delivered through a carefully chosen curriculum, neatly timed videos and precisely positioned information nuggets. Andrew picks up from where his classic ML course left off and introduces the idea of neural networks using a single neuron(logistic regression) and slowly adding complexity — more neurons and layers. By the end of the 4 weeks(course 1), a student is introduced to all the core ideas required to build a dense neural network such as cost/loss functions, learning iteratively using gradient descent and vectorized parallel python(numpy) implementations. Andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math/coding. Lectures are delivered using presentation slides on which Andrew writes using digital pens. It felt like an effective way to get the listener to focus. I felt comfortable watching videos at 1.25x or 1.5x speed. Quizzes are placed at the end of each lecture sections and are in the multiple choice question format. If you watch the videos once, you should be able to quickly answer all the quiz questions. You can attempt quizzes multiple times and the system is designed to keep your highest score. Programming assignments are done via J upyter notebooks  — powerful browser based applications. Assignments have a nice guided sequential structure and you are not required to write more than 2–3 lines of code in each section. If you understand the concepts like vectorization intuitively, you can complete most programming sections with just 1 line of code! After the assignment is coded, it takes 1 button click to submit your code to the automated grading system which returns your score in a few minutes. Some assignments have time restrictions — say, three attempts in 8 hours etc. Jupyter notebooks are well designed and work without any issues. Instructions are precise and it feels like a polished product. Anyone interested in understanding what neural networks are, how they work, how to build them and the tools available to bring your ideas to life. If your math is rusty, there is no need to worry — Andrew explains all the required calculus and provides derivatives at every occasion so that you can focus on building the network and concentrate on implementing your ideas in code. If your programming is rusty, there is a nice coding assignment to teach you numpy. But I recommend learning python first on  codecademy . Let me explain this with an analogy:  Assume you are trying to learn how to drive a car. Jeremy’s FAST.AI course  puts you in the drivers seat from the get-go. He teaches you to move the steering wheel, press the brake, accelerator etc. Then he slowly explains more details about how the car works — why rotating the wheel makes the car turn, why pressing the brake pedal makes you slow down and stop etc. He keeps getting deeper into the inner workings of the car and by the end of the course, you know how the internal combustion engine works, how the fuel tank is designed etc.  The goal of the course is to get you driving. You can choose to stop at any point after you can drive reasonably well — there is no need to learn how to build/repair the car. Andrew’s DL course  does all of this, but in  the complete opposite order . He teaches you about internal combustion engine first! He keeps adding  layers  of abstraction and by the end of the course you are driving like an F1 racer! The fast AI course mainly teaches you the  art of driving  while Andrew’s course primarily teaches you the  engineering behind the car . If you have not done any machine learning before this, don’t take this course first. The best starting point is Andrew’s original  ML course on coursera . After you complete that course, please try to complete part-1 of Jeremy Howard’s excellent  deep learning course . Jeremy teaches deep learning  Top-Down which is essential for absolute beginners . Once you are comfortable creating deep neural networks, it makes sense to take this  new deeplearning.ai course specialization  which fills up any gaps in your understanding of the underlying details and concepts. 2. Andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money — the third course in the DL specialization felt incredibly useful for my role as an  architect leading engineering teams . 3. Jargon is handled well. Andrew explains that an  empirical process = trial & error  — He is brutally honest about the reality of designing and training deep nets. At some point I felt  he might have as well just called Deep Learning as glorified curve-fitting 4. Squashes all hype around DL and AI — Andrew makes restrained, careful comments about proliferation of AI hype in the mainstream media and by the end of the course it is pretty clear that DL is nothing like the terminator. 5.Wonderful boilerplate code that just works out of the box! 6. Excellent course structure. 7. Nice, consistent and useful notation. Andrew strives to establish a fresh nomenclature for neural nets and I feel he could be quite successful in this endeavor. 8. Style of teaching that is unique to Andrew and carries over from ML — I could feel the same excitement I felt in 2013 when I took his original ML course. 9.The interviews with deep learning heroes are refreshing — It is motivating and fun to hear personal stories and anecdotes. I wish that he’d said ‘ concretely ’  more often ! 2. Good tools are important and will help you accelerate your learning pace. I bought a digital pen after seeing Andrew teach with one. It helped me work more efficiently. 3. There is  a psychological reason  why I recommend the Fast.ai course before this one. Once you find your passion, you can learn uninhibited. 4. You just get that dopamine rush each time you score full points: 5. Don’t be scared by DL jargon ( hyperparameters = settings, architecture/topology=style etc. ) or the math symbols. If you take a leap of faith and pay attention to the lectures, Andrew shows why the symbols and notation are actually quite useful.  They will soon become your tools of choice and you will wield them with style ! Thanks for reading and best wishes! Update: Thanks for the overwhelmingly positive response! Many people are asking me to explain gradient descent and the differential calculus.  I hope this helps !"
Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,achine Learning is Fun Part 6: How to do Speech Recognition with Deep Learnin,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  普通话  ,  한국어 ,  Tiếng Việt ,  فارسی  or  Русский . Giant update:   I’ve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Speech recognition is invading our lives. It’s built into our phones, our game consoles and our smart watches. It’s even automating our homes. For just $50, you can get an Amazon Echo Dot — a magic box that allows you to order pizza, get a weather report or even buy trash bags — just by speaking out loud: The Echo Dot has been so popular this holiday season that Amazon  can’t seem to keep them in stock ! But speech recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments. Andrew Ng  has long predicted that as speech recognition goes from 95% accurate to 99% accurate, it will become a primary way that we interact with computers. The idea is that this 4% accuracy gap is the difference between  annoyingly unreliable  and  incredibly useful . Thanks to Deep Learning, we’re finally cresting that peak. Let’s learn how to do speech recognition with deep learning! If you know  how neural machine translation works , you might guess that we could simply feed sound recordings into a neural network and train it to produce text: That’s the holy grail of speech recognition with deep learning, but we aren’t quite there yet (at least at the time that I wrote this — I bet that we will be in a couple of years). The big problem is that speech varies in speed. One person might say “hello!” very quickly and another person might say “heeeelllllllllllllooooo!” very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text — “hello!” Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard. To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let’s see how it works! The first step in speech recognition is obvious — we need to feed sound waves into a computer. In  Part 3 , we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition: But sound is transmitted as  waves . How do we turn sound waves into numbers? Let’s use this sound clip of me saying “Hello”: Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let’s zoom in on one tiny part of the sound wave and take a look: To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points: This is called  sampling . We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That’s basically all an uncompressed .wav audio file is. “CD Quality” audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech. Lets sample our “Hello” sound wave 16,000 times per second. Here’s the first 100 samples: You might be thinking that sampling is only creating a rough approximation of the original sound wave because it’s only taking occasional readings. There’s gaps in between our readings so we must be losing data, right? But thanks to the  Nyquist theorem , we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples — as long as we sample at least twice as fast as the highest frequency we want to record. I mention this only because  nearly everyone gets this wrong  and assumes that using higher sampling rates always leads to better audio quality. It doesn’t. </end rant> We now have an array of numbers with each number representing the sound wave’s amplitude at 1/16,000th of a second intervals. We  could  feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data. Let’s start by grouping our sampled audio into 20-millisecond-long chunks. Here’s our first 20 milliseconds of audio (i.e., our first 320 samples): Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time: This recording is only  1/50th of a second   long . But even this short recording is a complex mish-mash of different frequencies of sound. There’s some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech. To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it’s component parts. We’ll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a  fingerprint  of sorts for this audio snippet. Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes— C, E and G — all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea. We do this using a mathematic operation called a  Fourier transform . It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one. The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip: But this is a lot easier to see when you draw this as a chart: If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk): A spectrogram is cool because you can actually  see  musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we’ll actually feed into our neural network. Now that we have our audio in a format that’s easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the  letter  that corresponds the sound currently being spoken. We’ll use a  recurrent neural network  — that is, a neural network that has a memory that influences future predictions. That’s because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said “HEL” so far, it’s very likely we will say “LO” next to finish out the word “Hello”. It’s much less likely that we will say something unpronounceable next like “XYZ”. So having that memory of previous predictions helps the neural network make more accurate predictions going forward. After we run our entire audio clip through the neural network (one chunk at a time), we’ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here’s what that mapping looks like for me saying “Hello”: Our neural net is predicting that one likely thing I said was “HHHEE_LL_LLLOOO”. But it also thinks that it was possible that I said “HHHUU_LL_LLLOOO” or even “AAAUU_LL_LLLOOO”. We have some steps we follow to clean up this output. First, we’ll replace any repeated characters a single character: Then we’ll remove any blanks: That leaves us with three possible transcriptions — “Hello”, “Hullo” and “Aullo”. If you say them out loud, all of these sound similar to “Hello”. Because it’s predicting one character at a time, the neural network will come up with these very  sounded-out  transcriptions. For example if you say “He would not go”, it might give one possible transcription as “He wud net go”. The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic. Of our possible transcriptions “Hello”, “Hullo” and “Aullo”, obviously “Hello” will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we’ll pick “Hello” as our final transcription instead of the others. Done! You might be thinking  “But what if someone says ‘ Hullo ’? It’s a valid word. Maybe ‘Hello’ is the wrong transcription!” Of course it is possible that someone actually said “Hullo” instead of “Hello”. But a speech recognition system like this (trained on American English) will basically never produce “Hullo” as the transcription. It’s just such an unlikely thing for a user to say compared to “Hello” that it will always think you are saying “Hello” no matter how much you emphasize the ‘U’ sound. Try it out! If your phone is set to American English, try to get your phone’s digital assistant to recognize the world “Hullo.” You can’t! It refuses! It will always understand it as “Hello.” Not recognizing “Hullo” is a reasonable behavior, but sometimes you’ll find annoying cases where your phone just refuses to understand something valid you are saying. That’s why these speech recognition models are always being retrained with more data to fix these edge cases. One of the coolest things about machine learning is how simple it sometimes seems. You get a bunch of data, feed it into a machine learning algorithm, and then magically you have a world-class AI system running on your gaming laptop’s video card…  Right ? That sort of true in some cases, but not for speech. Recognizing speech is a hard problem. You have to overcome almost limitless challenges: bad quality microphones, background noise, reverb and echo, accent variations, and on and on. All of these issues need to be present in your training data to make sure the neural network can deal with them. Here’s another example: Did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise? Humans have no problem understanding you either way, but neural networks need to be trained to handle this special case. So you need training data with people yelling over noise! To build a voice recognition system that performs on the level of Siri, Google Now!, or Alexa, you will need a  lot  of training data — far more data than you can likely get without hiring hundreds of people to record it for you. And since users have low tolerance for poor quality voice recognition systems, you can’t skimp on this. No one wants a voice recognition system that works 80% of the time. For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is  gold . That’s the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting  Google Now!  and  Siri  on every cell phone for free or selling $50  Alexa  units that have no subscription fee is to get you to  use them as much as possible . Every single thing you say into one of these systems is  recorded forever  and used as training data for future versions of speech recognition algorithms. That’s the whole game! Don’t believe me? If you have an Android phone with  Google Now! ,  click here to listen to actual recordings of yourself saying every dumb thing you’ve ever said into it : So if you are looking for a start-up idea, I wouldn’t recommend trying to build your own speech recognition system to compete with Google. Instead, figure out a way to get people to give you recordings of themselves talking for hours. The data can be your product instead. If you liked this article, please consider  signing up for my Machine Learning is Fun! email list . I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this. You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I’d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun! Part 7 !"
How to learn Deep Learning in 6 months,ow to learn Deep Learning in 6 month,"It is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months’ time. This article details out the steps to achieve that. Pre-requisites - You are willing to spend 10–20 hours per week for the next 6 months - You have some programming skills. You should be comfortable to pick up Python along the way. And cloud. (No background in Python and cloud assumed). - Some math education in the past (algebra, geometry etc).  - Access to internet and computer. Step 1 We learn driving a car — by driving. Not by learning how the clutch and the internal combustion engine work. Atleast not initially. When learning deep learning, we will follow the same top-down approach. Do the  fast.ai  course —  Practical Deep Learning for Coders — Part 1 . This takes about 4–6 weeks of effort. This course has a session on running the code on cloud.  Google Colaboratory  has free GPU access. Start with that. Other options include  Paperspace ,  AWS ,  GCP ,  Crestle  and  Floydhub . All of these are great. Do not start to build your own machine. Atleast not yet. Step 2 This is the time to know some of the basics. Learn about calculus and linear algebra. For calculus,  Big Picture of Calculus  provides a good overview. For Linear Algebra, Gilbert Strang’s MIT course on  OpenCourseWare  is amazing. Once you finish the above two, read the  Matrix Calculus for Deep Learning . Step 3 Now is the time to understand the bottom-up approach to deep learning. Do all the 5 courses in the  deep learning specialisation  in Coursera. You need to pay to get the assignments graded. But the effort is truly worth it. Ideally, given the background you have gained so far, you should be able to complete one course every week. Step 4 “All work and no play makes Jack a dull boy” Do a capstone project. This is the time where you delve deep into a deep learning library(eg: Tensorflow, PyTorch, MXNet) and implement an architecture from scratch for a problem of your liking. The first three steps are about understanding how and where to use deep learning and gaining a solid foundation. This step is all about implementing a project from scratch and developing a strong foundation on the tools. Step 5 Now go and do  fast.ai’s  part II course —  Cutting Edge Deep Learning for Coders . This covers more advanced topics and you will learn to read the latest research papers and make sense out of them. Each of the steps should take about 4–6 weeks’ time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning. Where to go next? Do the Stanford’s  CS231n  and  CS224d  courses. These two are amazing courses with great depth for vision and NLP respectively. They cover the latest state-of-art. And read the  deep learning book . This will solidify your understanding. Happy deep learning. Create every single day."
"The Difference Between Artificial Intelligence, Machine Learning, and Deep Learning","he Difference Between Artificial Intelligence, Machine Learning, and Deep Learnin","We’re all familiar with the term “Artificial Intelligence.” After all, it’s been a popular focus in movies such as The Terminator, The Matrix, and Ex Machina (a personal favorite of mine). But you may have recently been hearing about other terms like “Machine Learning” and “Deep Learning,” sometimes used interchangeably with artificial intelligence. As a result, the difference between artificial intelligence, machine learning, and deep learning can be very unclear. I’ll begin by giving a quick explanation of what Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) actually mean and how they’re different. Then, I’ll share how AI and the Internet of Things are inextricably intertwined, with several technological advances all converging at once to set the foundation for an AI and IoT explosion. First coined in 1956 by John McCarthy,  AI involves machines that can perform tasks that are characteristic of human intelligence . While this is rather general, it includes things like planning, understanding language, recognizing objects and sounds, learning, and problem-solving. We can put AI in two categories, general and narrow. General AI would have all of the characteristics of human intelligence, including the capacities mentioned above. Narrow AI exhibits some facet(s) of human intelligence, and can do that facet extremely well, but is lacking in other areas. A machine that’s great at recognizing images, but nothing else, would be an example of narrow AI. At its core,  machine learning is simply a way of achieving AI. Arthur Samuel coined the phrase not too long after AI, in 1959, defining it as, “the ability to learn without being explicitly programmed.” You see, you can get AI  without  using machine learning, but this would require building millions of lines of codes with complex rules and decision-trees. So instead of hard-coding software routines with specific instructions to accomplish a particular task, machine learning is a way of “training” an algorithm so that it can  learn how . “Training” involves feeding huge amounts of data to the algorithm and allowing the algorithm to adjust itself and improve. To give an example, machine learning has been used to make drastic improvements to computer vision (the ability of a machine to recognize an object in an image or video). You gather hundreds of thousands or even millions of pictures and then have humans tag them. For example, the humans might tag pictures that have a cat in them versus those that do not. Then, the algorithm tries to build a model that can accurately tag a picture as containing a cat or not as well as a human. Once the accuracy level is high enough, the machine has now “learned” what a cat looks like. Deep learning is one of many approaches to machine learning . Other approaches include decision tree learning, inductive logic programming, clustering, reinforcement learning, and Bayesian networks, among others. Deep learning was inspired by the structure and function of the brain, namely the interconnecting of many neurons. Artificial Neural Networks (ANNs) are algorithms that mimic the biological structure of the brain. In ANNs, there are “neurons” which have discrete layers and connections to other “neurons”. Each layer picks out a specific feature to learn, such as curves/edges in image recognition. It’s this layering that gives deep learning its name, depth is created by using multiple layers as opposed to a single layer. I think of the relationship between AI and IoT much like the relationship between the human brain and body. Our bodies collect sensory input such as sight, sound, and touch. Our brains take that data and make sense of it, turning light into recognizable objects and turning sounds into understandable speech. Our brains then make decisions, sending signals back out to the body to command movements like picking up an object or speaking. All of the connected sensors that make up the Internet of Things are like our bodies, they provide the raw data of what’s going on in the world. Artificial intelligence is like our brain, making sense of that data and deciding what actions to perform. And the connected devices of IoT are again like our bodies, carrying out physical actions or communicating to others. The value and the promises of both AI and IoT are being realized because of the other. Machine learning and deep learning have led to huge leaps for AI in recent years. As mentioned above, machine learning and deep learning require massive amounts of data to work, and this data is being collected by the billions of sensors that are continuing to come online in the Internet of Things.  IoT makes better AI. Improving AI will also drive the adoption of the Internet of Things, creating a virtuous cycle in which both areas will accelerate drastically. That’s because  AI makes IoT useful. On the industrial side, AI can be applied to predict when machines will need maintenance or analyze manufacturing processes to make big efficiency gains, saving millions of dollars. On the consumer side, rather than having to adapt to technology, technology can adapt to us. Instead of clicking, typing, and searching, we can simply ask a machine for what we need. We might ask for information like the weather or for an action like preparing the house for bedtime (turning down the thermostat, locking the doors, turning off the lights, etc.). Shrinking computer chips and improved manufacturing techniques means cheaper, more powerful sensors. Quickly improving battery technology means those sensors can last for years without needing to be connected to a power source. Wireless connectivity, driven by the advent of smartphones, means that data can be sent in high volume at cheap rates, allowing all those sensors to send data to the cloud. And the birth of the cloud has allowed for virtually unlimited storage of that data and virtually infinite computational ability to process it. Of course, there are  one  or  two  concerns about the impact of AI on our society and our future. But as advancements and adoption of both AI and IoT continue to accelerate, one thing is certain; the impact is going to be profound. www.leverege.com 🗓  This article was originally posted on  iotforall.com ."
A list of artificial intelligence tools you can use today — for personal use (1/3), list of artificial intelligence tools you can use today — for personal use (1/3,"A rtificial Intelligence and  the fourth industrial revolution  has made some considerable progress over the last couple of years. Most of this current progress that is usable has been developed for industry and business purposes, as you’ll see in coming posts. Research institutes and dedicated, specialised companies are working toward the ultimate goal…"
Artificial Intelligence — The Revolution Hasn’t Happened Yet,rtificial Intelligence — The Revolution Hasn’t Happened Ye,"Artificial Intelligence (AI) is the mantra of the current era. The phrase is intoned by technologists, academicians, journalists and venture capitalists alike. As with many phrases that cross over from technical academic fields into general circulation, there is significant misunderstanding accompanying the use of the phrase. But this is not the classical case of the public not understanding the scientists — here the scientists are often as befuddled as the public. The idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us — enthralling us and frightening us in equal measure. And, unfortunately, it distracts us. There is a different narrative that one can tell about the current era. Consider the following story, which involves humans, computers, data and life-or-death decisions, but where the focus is something other than intelligence-in-silicon fantasies. When my spouse was pregnant 14 years ago, we had an ultrasound. There was a geneticist in the room, and she pointed out some white spots around the heart of the fetus. “Those are markers for Down syndrome,” she noted, “and your risk has now gone up to 1 in 20.” She further let us know that we could learn whether the fetus in fact had the genetic modification underlying Down syndrome via an amniocentesis. But amniocentesis was risky — the risk of killing the fetus during the procedure was roughly 1 in 300. Being a statistician, I determined to find out where these numbers were coming from. To cut a long story short, I discovered that a statistical analysis had been done a decade previously in the UK, where these white spots, which reflect calcium buildup, were indeed established as a predictor of Down syndrome. But I also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the UK study. I went back to tell the geneticist that I believed that the white spots were likely false positives — that they were literally “white noise.” She said “Ah, that explains why we started seeing an uptick in Down syndrome diagnoses a few years ago; it’s when the new machine arrived.” We didn’t do the amniocentesis, and a healthy girl was born a few months later. But the episode troubled me, particularly after a back-of-the-envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide, that many of them had opted for amniocentesis, and that a number of babies had died needlessly. And this happened day after day until it somehow got fixed. The problem that this episode revealed wasn’t about my individual medical care; it was about a medical system that measured variables and outcomes in various places and times, conducted statistical analyses, and made use of the results in other places and times. The problem had to do not just with data analysis per se, but with what database researchers call “provenance” — broadly, where did data arise, what inferences were drawn from the data, and how relevant are those inferences to the present situation? While a trained human might be able to work all of this out on a case-by-case basis, the issue was that of designing a planetary-scale medical system that could do this without the need for such detailed human oversight. I’m also a computer scientist, and it occurred to me that the principles needed to build planetary-scale inference-and-decision-making systems of this kind, blending computer science with statistics, and taking into account human utilities, were nowhere to be found in my education. And it occurred to me that the development of such principles — which will be needed not only in the medical domain but also in domains such as commerce, transportation and education — were at least as important as those of building AI systems that can dazzle us with their game-playing or sensorimotor skills. Whether or not we come to understand “intelligence” any time soon, we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life. While this challenge is viewed by some as subservient to the creation of “artificial intelligence,” it can also be viewed more prosaically — but with no less reverence — as the creation of a new branch of engineering. Much like civil engineering and chemical engineering in decades past, this new discipline aims to corral the power of a few key ideas, bringing new resources and capabilities to people, and doing so safely. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to — ideas such as “information,” “algorithm,” “data,” “uncertainty,” “computing,” “inference,” and “optimization.” Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities. While the building blocks have begun to emerge, the principles for putting these blocks together have not yet emerged, and so the blocks are currently being put together in ad-hoc ways. Thus, just as humans built buildings and bridges before there was civil engineering, humans are proceeding with the building of societal-scale, inference-and-decision-making systems that involve machines, humans and the environment. Just as early buildings and bridges sometimes fell to the ground — in unforeseen ways and with tragic consequences — many of our early societal-scale inference-and-decision-making systems are already exposing serious conceptual flaws. And, unfortunately, we are not very good at anticipating what the next emerging serious flaw will be. What we’re missing is an engineering discipline with its principles of analysis and design. The current public dialog about these issues too often uses “AI” as an intellectual wildcard, one that makes it difficult to reason about the scope and consequences of emerging technology. Let us begin by considering more carefully what “AI” has been used to refer to, both recently and historically. Most of what is being called “AI” today, particularly in the public sphere, is what has been called “Machine Learning” (ML) for the past several decades. ML is an algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions and help make decisions. In terms of impact on the real world, ML is the real thing, and not just recently. Indeed, that ML would grow into massive industrial relevance was already clear in the early 1990s, and by the turn of the century forward-looking companies such as Amazon were already using ML throughout their business, solving mission-critical back-end problems in fraud detection and supply-chain prediction, and building innovative consumer-facing services such as recommendation systems. As datasets and computing resources grew rapidly over the ensuing two decades, it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large-scale data. New business models would emerge. The phrase “Data Science” began to be used to refer to this phenomenon, reflecting the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, and reflecting the larger social and environmental scope of the resulting systems. This confluence of ideas and technology trends has been rebranded as “AI” over the past few years. This rebranding is worthy of some scrutiny. Historically, the phrase “AI” was coined in the late 1950’s to refer to the heady aspiration of realizing in software and hardware an entity possessing human-level intelligence. We will use the phrase “human-imitative AI” to refer to this aspiration, emphasizing the notion that the artificially intelligent entity should seem to be one of us, if not physically at least mentally (whatever that might mean). This was largely an academic enterprise. While related academic fields such as operations research, statistics, pattern recognition, information theory and control theory already existed, and were often inspired by human intelligence (and animal intelligence), these fields were arguably focused on “low-level” signals and decisions. The ability of, say, a squirrel to perceive the three-dimensional structure of the forest it lives in, and to leap among its branches, was inspirational to these fields. “AI” was meant to focus on something different — the “high-level” or “cognitive” capability of humans to “reason” and to “think.” Sixty years later, however, high-level reasoning and thought remain elusive. The developments which are now being called “AI” arose mostly in the engineering fields associated with low-level pattern recognition and movement control, and in the field of statistics — the discipline focused on finding patterns in data and on making well-founded predictions, tests of hypotheses and decisions. Indeed, the famous “backpropagation” algorithm that was rediscovered by David Rumelhart in the early 1980s, and which is now viewed as being at the core of the so-called “AI revolution,” first arose in the field of control theory in the 1950s and 1960s. One of its early applications was to optimize the thrusts of the Apollo spaceships as they headed towards the moon. Since the 1960s much progress has been made, but it has arguably not come about from the pursuit of human-imitative AI. Rather, as in the case of the Apollo spaceships, these ideas have often been hidden behind the scenes, and have been the handiwork of researchers focused on specific engineering challenges. Although not visible to the general public, research and systems-building in areas such as document retrieval, text classification, fraud detection, recommendation systems, personalized search, social network analysis, planning, diagnostics and A/B testing have been a major success — these are the advances that have powered companies such as Google, Netflix, Facebook and Amazon. One could simply agree to refer to all of this as “AI,” and indeed that is what appears to have happened. Such labeling may come as a surprise to optimization or statistics researchers, who wake up to find themselves suddenly referred to as “AI researchers.” But labeling of researchers aside, the bigger problem is that the use of this single, ill-defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play. The past two decades have seen major progress — in industry and academia — in a complementary aspiration to human-imitative AI that is often referred to as “Intelligence Augmentation” (IA). Here computation and data are used to create services that augment human intelligence and creativity. A search engine can be viewed as an example of IA (it augments human memory and factual knowledge), as can natural language translation (it augments the ability of a human to communicate). Computing-based generation of sounds and images serves as a palette and creativity enhancer for artists. While services of this kind could conceivably involve high-level reasoning and thought, currently they don’t — they mostly perform various kinds of string-matching and numerical operations that capture patterns that humans can make use of. Hoping that the reader will tolerate one last acronym, let us conceive broadly of a discipline of “Intelligent Infrastructure” (II), whereby a web of computation, data and physical entities exists that makes human environments more supportive, interesting and safe. Such infrastructure is beginning to make its appearance in domains such as transportation, medicine, commerce and finance, with vast implications for individual humans and societies. This emergence sometimes arises in conversations about an “Internet of Things,” but that effort generally refers to the mere problem of getting “things” onto the Internet — not to the far grander set of challenges associated with these “things” capable of analyzing those data streams to discover facts about the world, and interacting with humans and other “things” at a far higher level of abstraction than mere bits. For example, returning to my personal anecdote, we might imagine living our lives in a “societal-scale medical system” that sets up data flows, and data-analysis flows, between doctors and devices positioned in and around human bodies, thereby able to aid human intelligence in making diagnoses and providing care. The system would incorporate information from cells in the body, DNA, blood tests, environment, population genetics and the vast scientific literature on drugs and treatments. It would not just focus on a single patient and a doctor, but on relationships among all humans — just as current medical testing allows experiments done on one set of humans (or animals) to be brought to bear in the care of other humans. It would help maintain notions of relevance, provenance and reliability, in the way that the current banking system focuses on such challenges in the domain of finance and payment. And, while one can foresee many problems arising in such a system — involving privacy issues, liability issues, security issues, etc — these problems should properly be viewed as challenges, not show-stoppers. We now come to a critical issue: Is working on classical human-imitative AI the best or only way to focus on these larger challenges? Some of the most heralded recent success stories of ML have in fact been in areas associated with human-imitative AI — areas such as computer vision, speech recognition, game-playing and robotics. So perhaps we should simply await further progress in domains such as these. There are two points to make here. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited — we are very far from realizing human-imitative AI aspirations. Unfortunately the thrill (and fear) of making even limited progress on human-imitative AI gives rise to levels of over-exuberance and media attention that is not present in other areas of engineering. Second, and more importantly, success in these domains is neither sufficient nor necessary to solve important IA and II problems. On the sufficiency side, consider self-driving cars. For such technology to be realized, a range of engineering problems will need to be solved that may have little relationship to human competencies (or human lack-of-competencies). The overall transportation system (an II system) will likely more closely resemble the current air-traffic control system than the current collection of loosely-coupled, forward-facing, inattentive human drivers. It will be vastly more complex than the current air-traffic control system, specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine-grained decisions. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction. As for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. Such an argument has little historical precedent. Did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer? Should chemical engineering have been framed in terms of creating an artificial chemist? Even more polemically: if our goal was to build chemical factories, should we have first created an artificial chemist who would have then worked out how to build a chemical factory? A related argument is that human intelligence is the only kind of intelligence that we know, and that we should aim to mimic it as a first step. But humans are in fact not very good at some kinds of reasoning — we have our lapses, biases and limitations. Moreover, critically, we did not evolve to perform the kinds of large-scale decision-making that modern II systems must face, nor to cope with the kinds of uncertainty that arise in II contexts. One could argue  that an AI system would not only imitate human intelligence, but also “correct” it, and would also scale to arbitrarily large problems. But we are now in the realm of science fiction — such speculative arguments, while entertaining in the setting of fiction, should not be our principal strategy going forward in the face of the critical IA and II problems that are beginning to emerge. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda. It is not hard to pinpoint algorithmic and infrastructure challenges in II systems that are not central themes in human-imitative AI research. II systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent. Such systems must cope with cloud-edge interactions in making timely, distributed decisions and they must deal with long-tail phenomena whereby there is lots of data on some individuals and little data on most individuals. They must address the difficulties of sharing data across administrative and competitive boundaries. Finally, and of particular importance, II systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods. Such II systems can be viewed as not merely providing a service, but as creating  markets . There are domains such as music, literature and journalism that are crying out for the emergence of such markets, where data analysis links producers and consumers. And this must all be done within the context of evolving societal, ethical and legal norms. Of course, classical human-imitative AI problems remain of great interest as well. However, the current focus on doing AI research via the gathering of data, the deployment of “deep learning” infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills — with little in the way of emerging explanatory principles — tends to deflect attention from major open problems in classical AI. These problems include the need to bring meaning and reasoning into systems that perform natural language processing, the need to infer and represent causality, the need to develop computationally-tractable representations of uncertainty and the need to develop systems that formulate and pursue long-term goals. These are classical goals in human-imitative AI, but in the current hubbub over the “AI revolution,” it is easy to forget that they are not yet solved. IA will also remain quite essential, because for the foreseeable future, computers will not be able to match humans in their ability to reason abstractly about real-world situations. We will need well-thought-out interactions of humans and computers to solve our most pressing problems. And we will want computers to trigger new levels of human creativity, not replace human creativity (whatever that might mean). It was John McCarthy (while a professor at Dartmouth, and soon to take a  position at MIT) who coined the term “AI,” apparently to distinguish his  budding research agenda from that of Norbert Wiener (then an older professor at MIT). Wiener had coined “cybernetics” to refer to his own vision of intelligent systems — a vision that was closely tied to operations research, statistics, pattern recognition, information theory and control theory. McCarthy, on the other hand, emphasized the ties to logic. In an interesting reversal, it is Wiener’s intellectual agenda that has come to dominate in the current era, under the banner of McCarthy’s terminology. (This state of affairs is surely, however, only temporary; the pendulum swings more in AI than  in most fields.) But we need to move beyond the particular historical perspectives of McCarthy and Wiener. We need to realize that the current public dialog on AI — which focuses on a narrow subset of industry and a narrow subset of academia — risks blinding us to the challenges and opportunities that are presented by the full scope of AI, IA and II. This scope is less about the realization of science-fiction dreams or nightmares of super-human machines, and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives. Moreover, in this understanding and shaping there is a need for a diverse set of voices from all walks of life, not merely a dialog among the technologically attuned. Focusing narrowly on human-imitative AI prevents an appropriately wide range of voices from being heard. While industry will continue to drive many developments, academia will also continue to play an essential role, not only in providing some of the most innovative technical ideas, but also in bringing researchers from the computational and statistical disciplines together with researchers from other  disciplines whose contributions and perspectives are sorely needed — notably  the social sciences, the cognitive sciences and the humanities. On the other hand, while the humanities and the sciences are essential as we go forward, we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope — society is aiming to build new kinds of artifacts. These artifacts should be built to work as claimed. We do not want to build systems that help us with medical treatments, transportation options and commercial opportunities to find out after the fact that these systems don’t really work — that they make errors that take their toll in terms of human lives and happiness. In this regard, as I have emphasized, there is an engineering discipline yet to emerge for the data-focused and learning-focused fields. As exciting as these latter fields appear to be, they cannot yet be viewed as constituting an engineering discipline. Moreover, we should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term “engineering” is often  invoked in a narrow sense — in academia and beyond — with overtones of cold, affectless machinery, and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be. In the current era, we have a real opportunity to conceive of something historically new — a human-centric engineering discipline. I will resist giving this emerging discipline a name, but if the acronym “AI” continues to be used as placeholder nomenclature going forward, let’s be aware of the very real limitations of this placeholder. Let’s broaden our scope, tone down the hype and recognize the serious challenges ahead. Michael I. Jordan Acknowledgments : There are a number of individuals whose comments during the writing of this article have helped me greatly, including Jeff Bezos, Dave Blei, Rod Brooks, Cathryn Carson, Tom Dietterich, Charles Elkan, Oren Etzioni, David Heckerman, Douglas Hofstadter, Michael Kearns, Tammy Kolda, Ed Lazowska, John Markoff, Esther Rolf, Maja Mataric, Dimitris Papailiopoulos, Ben Recht, Theodoros Rekatsinas, Barbara Rosario and Ion Stoica. And I would like to add a special thanks to Cameron Baradar at The House, who first encouraged me to contemplate writing such a piece. Bio:  Michael I. Jordan is Professor of Computer Science and Statistics at the University of California, Berkeley. He has worked for over three decades in the computational, inferential, cognitive and biological sciences, first as a graduate student at UCSD and then as a faculty member at MIT and Berkeley. One of his recent roles is as a Faculty Partner and Co-Founder at AI@The House — a venture fund and accelerator in Berkeley. This fund aims to support not only AI activities, but also IA and II activities, and to do so in the context of a university environment that includes not only the engineering disciplines, but also the perspectives of the social sciences, the cognitive sciences and the humanities."
"How I Eat For Free in NYC Using Python, Automation, Artificial Intelligence, and Instagram","ow I Eat For Free in NYC Using Python, Automation, Artificial Intelligence, and Instagra","Living and working in the big apple comes with big rent. I, along with most other city-dwellers who live inside a crammed closet we call an apartment, look to cut costs anywhere we can. It’s no secret one way to curtail expenses, at least we’re told, is to cook at home instead of eating out all of the time. As a Hell’s Kitchen resident this is near impossible. Everywhere I look there is a sushi bar, Mexican restaurant or some delicious looking pizzeria within arm’s length that can break my willpower in the blink of an eye. I fall victim to this way more than I’d like to admit. Well,  I used  to fall victim to this — until recently. Not wanting to give up the dining experiences I enjoyed so dearly, I decided I’d create my own currency to finance these transactions. I’ve been dining at restaurants, sandwich shops, and other eateries for free ever since. I’m going to explain to you how I’m receiving these free meals from some of the best restaurants in New York City. I’ll admit — it’s rather technical and not everyone can reproduce my methodology. You’ll either need a background in Data Science/Software Development or  a lot  of free time on your hands. Since I have the prior, I sit back and let my code do the work for me. Oh, and you guessed it, you’ll need to know how to use Instagram as well. If you’re part of the technical audience, I will briefly go over some of the technologies and programming languages I use but I will not be providing code or anything like that. I will explain my use of logistic regression, random forests, AWS, and automation — but not in depth. This article will be more theory based. If you’re a non-technical reader, everything here can still be done, it’s just going to take some time and effort. These methods are tedious which is why I decided to automate most of them. Now to get into it. I’ll start with the answer and then go through how I got there. In today’s digital age, a large Instagram audience is considered a valuable currency. I had also heard through the grapevine that I could monetize a large following — or in my desired case — use it to have my meals paid for.  So I did just that. I created an Instagram page that showcased pictures of New York City’s skylines, iconic spots, elegant skyscrapers — you name it. The page has amassed a following of over 25,000 users in the NYC area and it’s still rapidly growing. I reach out to restaurants in the area either via Instagram’s direct messaging or email and offer to post a review to my followers in return for a free entree or at least a discount. Almost every restaurant I’ve messaged came back at me with a compensated meal or a gift card. Most places have an allocated marketing budget for these types of things so they were happy to offer me a free dining experience in exchange for a potential promotion. I’ve ended up giving some of these meals away to my friends and family because at times I had too many queued up to use myself. The beauty of this all is that  I automated the whole thing.  And I mean 100% of it. I wrote code that finds these pictures or videos, makes a caption, adds hashtags, credits where the picture or video comes from, weeds out bad or spammy posts, posts them, follows and unfollows users, likes pictures, monitors my inbox, and most importantly — both direct messages and emails restaurants about a potential promotion. Since its inception, I haven’t even really logged into the account. I spend zero time on it. It’s essentially a robot that operates like a human, but the average viewer can’t tell the difference. And as the programmer, I get to sit back and admire its (and my) work. I’ll walk you through how I did what I did, from A all the way to Z. Some of this may seem like common sense, but when you’re automating a system to act like a human, details are important. The process can be broken down into three phases: content sharing, growth hacking, and sales & promotion. Now, none of the content my account posts is owned by me. I re-share other people's content on my page, with credit to them. If someone asks me to take down their photo, I do immediately. But since I am sourcing their page, I’ve only been thanked — never the opposite. Posting every day — multiple times a day — is indispensable. This is one of the main factors the Instagram algorithm uses to determine how much they are going to expose you to the public (via the “explore page”). Posting every day, especially at “rush hour” times, is much harder and more monotonous than you might think. Most people give up on this task after a few weeks, and even missing a day or two can be detrimental. So, I automated the content collecting and sharing process. I first thought about setting up a picture scraper from Google Images or from Reddit to get my content. One of the biggest struggles I came across was how particular Instagram is with the sizing of the picture being posted. Ideally, it’s a “square” picture, meaning its width equals its height, so it will reject an out-of-proportion post attempt. This made retrieving content very challenging. I ultimately decided to scrape directly from other Instagram feeds because the picture will come in precisely the right size as it is. It also allows me to know exactly where the picture came from, which will come in handy during the auto-crediting process. I collected a list of fifty other Instagram accounts that posted quality pictures of NYC. I then set up a scraper to go through and download media from these other accounts. In addition to the actual content, I scraped a bunch of metadata along with the picture such as the caption, the number of likes, and the location. I set the scraper to run every morning at 3:00 AM or when my inventory was empty. From this, I now have a central location with related content in the right format. Not everything someone posts on Instagram is re-sharable. A lot of the time people are trying to sell something, shouting another page, or it could flat out just be bad or unrelated content. Take these two posts as an example: The above two posts are from the same NYC-based Instagram account. The one on the left is a normal natural post in their niche — one I would be happy re-sharing on my page. The one on the right, however, is an advertisement. Without any context, if I put this on my page it would be rather confusing and out of place. The caption got cut off, but it’s actually promoting an NYC-based app. You can see the difference in the number of likes — 8200 vs. 1000. I need to be able to automatically weed out posts like those on the right, and re-share posts like that of the left. Therefore, I can’t just blindly re-share all the content that I scrape. And since this will be an automated process, I needed to create an algorithm that can weed out the bad from the good. The first part of my “cleaner” has some hard-coded rules and the second is a machine learning model that refines the content even further. Cleaner Part 1 — Hard-Coded Rules: The first thing I did was refine my inventory on some specific guidelines from the metadata. I was rather strict because there is no shortage of content for me to share. If there was even a slight red-flag, I trashed the picture. I can always scrape more content, but if my algorithm posts something spammy or inappropriate to my page there maybe thousands of people that see it before I recognize and remove it. The preliminary step was to have my algorithm look at the caption. If the text includes any text related to “link in bio”, “buy now”, “limited time”, or the related, I immediately have it fail the test. These are typical of posts looking to sell something rather than quality content for entertainment purposes. The next thing I looked at is if the comments were disabled. If they were, I failed the picture. Disabled comments from my experience were linked to controversial posts and not worth the risk. The final thing I looked at was if there was more than one person tagged in the picture. A lot of the times, one tag in a picture is a credit to where it came from, so I actually found that to be beneficial. But if the picture had multiple tags, It would lead to confusion when it came to crediting or what the purpose of the post even was. From these rules, I was able to get most of the spammy and undesirable posts into the trash and out of my folder. However, just because a post isn’t trying to sell something doesn’t mean it’s a good, quality post. Also, my hard-coded rules may still miss some sales-y content, so I wanted to run them through a secondary model once I was done with part one. Cleaner Part 2— Machine Learning Model: As I was going through my now-cleaner repository of pictures, I noticed there were still some lingering items that weren’t particularly desirable to post. I wasn’t going to be able to sit there and manually move out the bad ones as I planned for this to be completely automated. I wanted to run each through another test. I had a ton of metadata on each of the posts, including the number of likes, captions, time of post, and much more. My original goal was to try to predict which pictures would garner the most likes. However, the issue was that bigger accounts naturally had more likes so it wasn’t a fair barometer. My follow-up thought would be to make the response variable equal to the like ratio (number of likes/number of followers) and try to predict that. After looking at each picture and its respective ratio, I still didn’t trust the correlation. I didn’t feel those with higher ratios were necessarily the best photos. Just because an account was “popular” didn’t mean it had better content than a relatively unknown photographer with fewer likes. I decided to change my outlook from a regression model to a classification model and just decide if the picture is good enough to post or not — a simple yes or no. Before even looking at any of the other metadata, I scraped a large number of photos and manually went through them, labeling them as 0 (bad) or 1 (good). This is extremely subjective, so I’m theoretically making a model to my own consciousness. However, it seems to be pretty universally agreed upon as to which content is unappealing and which is favorable. I generated my own dataset. The response variable was 0/1 (bad/good) with a large number of features. The metadata of each post gave me the following information: From these seven explanatory variables, I engineered a few more features that I thought would be useful. For example, I changed the number of comments and likes to ratios against followers. I extracted the number of hashtags from the caption and made that its own column, and did the same with the number of accounts mentioned in the caption. I cleaned up and vectorized the rest of the caption to be used in Natural Language Processing. Vectorizing is the process of removing peripheral words (“the”, “and”, etc.) and converting the remaining into a numeric field that can be analyzed mathematically. After all was said and done, I had the resulting data: I played around with a number of classification algorithms such as Support Vector Machines and Random Forests but landed on a basic Logistic Regression. I did this for a few reasons, first being Occam’s Razor — sometimes the simplest answer is the right one. No matter which way I spun or re-engineered the data, logistic regression performed the best on my test set. The second and more important reason was that, unlike some other classification algorithms, I can set a threshold score while making predictions. It’s common for classification algorithms to output a binary class (in my case 0 or 1) but logistic regression actually yields a decimal between 0 and 1. For example, it may rate a post as 0.83 or 0.12. It’s common to set the threshold at 0.5 and rank everything greater than that to 1 and everything else to 0, but it would be case dependent. Since this task is critical, and there is an abundance of media available, I was extremely strict on my threshold and set it to 0.9 and rejected anything that fell below that benchmark. After I implemented my model, the inventory of pictures and videos was A) cleaned by a hard set of rules and then B) only the cream of the crop was chosen by my logistic regression algorithm. I am now able to move on to captioning and crediting each post. I now had a system of automatically gathering relevant content and removing the tangential or spammy images— but I’m not done yet. If you’ve used Instagram before, you know that each post has a caption that exists under the picture or video. Being that I can’t actually see these pictures, nor do I have the time to sit there and caption them all, I needed to make a generic caption that can be used for any of them. The first thing I did was make a final template. It looked something like this: Where the three sets of {}’s needed to be filled in by my script. Let’s go through each three one-by-one. I created a text file with a number of predefined generic captions that could go with any picture. These were either quotes about NYC, broad questions, or just basic praise. Some of these included: For each post, one of my captions was randomly chosen. I have such a large list that I’m not worried about them being used too often or overlapping. So, for our example, let’s pick the first one — “Who can name this spot?”. 2. Credit This was one of the harder tasks — automatically crediting the source. What was particularly tricky was that the Instagram page that the media came from wasn’t necessarily the right person to credit. Often, that account was also re-sharing the content and crediting the owner in their caption or tagging them in the photo. I decided I would credit the page where it came from no matter what. I would then add more credits if I could possibly decipher the original owner as well. I felt I would cover all of my bases this way. Let’s take a look at this post by @likenewyorkcity on Instagram. We can see that even though he or she was the one who shared it, the real owner is @geoffrey.parry who is tagged in the picture and mentioned in the caption. Ideally, I would like my code to be able to look at this picture and return: The first part of that is easy; just inputting which account it came from. The second part was a little more challenging. I used REGEX to look for a number of keywords such as “by” or “photo:” and then look for the “@” symbol that followed right after. From there, I grabbed the username and believed that to be the second part of my credit. If none of those keywords existed in the caption, I checked if there was anyone tagged in the picture. If there was, I figured they deserved the credit. I understand this is an imperfect method, but more times than not that’s why someone was tagged and it was a risk worth taking. I very often capture exactly the right credit. In fact, many times I’ve had people comment on my pictures and say “thank you for sharing!” (I’ve added an example of that below). 3. Hashtags Instagram allows you to add 30 hashtags to your picture which will then be displayed on that hashtag’s feed. I created a file with over 100 related hashes: and randomly chose 30 to add each time. I did that so after a while, I can compare which hashtags lead me to a greater number of likes. 4. Final Template After the three steps were said and done, I was able to fill in my template and have a caption that can go along with any post. Here’s an example of one of my final products: I used a generic caption that could go with any picture of NYC. I credited both the Instagram account it came from and the original source. If you look at the comments, you can see the original owner thanking me for sharing. And I added thirty hashtags to boost my post as well. I now have a central repository of relevant media and a process of generating a caption for each of these posts. Now, it’s time to do just that — post. I spun up an EC2 instance on AWS to host my code. I chose this route because it’s more reliable than my personal machine — it’s always on and connected to the internet and I knew it would all fit under the free-tier limits. I wrote a Python script that randomly grabs one of these pictures and auto-generates a caption after the scraping and cleaning process is completed. Using my API, I was able to write code that does the actual posting for me. I scheduled a cron-job to run around 8:00 AM, 2:00 PM, and 7:30 PM every day. At this point, I’ve completely automated the content finding and posting process. I no longer have to worry about finding media and posting every day, it’s being done for me. It isn’t enough to just post — I need to enact some methodologies to grow my following as well. And since I won’t ever be on the account myself doing any of this manually, I’ll need to automate that too. The idea was to get my account exposed to an interested audience by interacting directly with those people. The interaction script that I wrote runs from 10:00 AM to 7:00 PM EST, the time range that I believed Instagram to be most active. Throughout the day, my account methodically follows, unfollows, and likes relevant users and photos in order to have the same be done back to me. Following (More Data Science) If you use Instagram, I’m sure you’ve been part of this before whether you realize it or not. This method is very common for accounts that are trying to increase their following. One day you follow an interesting Instagram page in the fitness niche, and the next day you’re being followed by a bunch of bodybuilders and fitness models. This seems extremely trivial, and it is, but it’s  very effective . The issue here is that you can’t just follow willy-nilly on Instagram. Their algorithm is very very strict, so they will cut you off or even ban your account if you go overboard and follow too many accounts in one day. Additionally, you can be following at most 7,500 users at one time on Instagram. After a lot of testing, I’ve found you can get away with following 400 people and unfollowing 400 people in a single day. Therefore, each follow is extremely precious. You don’t want to waste a follow on someone who is unlikely to follow you back because you only have so many users that you can follow in one day. I decided to capture the metadata of my activity and make a model to predict how likely someone would be to follow you back, so I wouldn’t waste a precious follow on someone who was unlikely to return the favor. I spent a few minutes manually gathering 20+ bigger accounts in the same niche as me. I had no initial data, so the first few weeks would be me randomly performing these actions to grow my following, but more importantly, I needed to capture as much metadata as possible so I can make my model. I cycled through these 20+ related accounts and followed the users who followed them, liked their pictures, or commented on their posts. With each follow I captured as much metadata as possible about the user into a CSV file. Some of this metadata included their follower/following ratio, if they were public or private, or if they had a profile picture or not. Every day, my script would go through this CSV and label the missing response variable, which is if they followed back or not. I gave each user two full days before labeling him or her 0, 1, or 2 — 2 being the most desirable outcome. 0 indicated that the user did not follow back, 1 indicated that they followed back but didn’t interact with me in my last ten pictures (liking or commenting), and 2 indicated if they followed back AND interacted on one of my last ten posts. My dataset looked something like this: Before running this data through an ML model, I did some exploratory data analysis and found that: From just the above insights, I was able to refine my initial search of users. I adjusted my settings to only follow in the morning and to look primarily for females. Now I was finally able to make a machine learning model to predict the likelihood of a follow back based on a user’s metadata before interacting with them. This allows me to not waste one of my already limited daily follows on someone who has a very small chance of following me back. I chose to use the Random Forest algorithm to classify the follow back outcome. I originally was using a number of different decision trees before I had a set structure or outcome variable because I wanted to see the visual flowcharts that come along with them. The Random Forest is an enhancement of the decision tree that provides a number of tweaks to correct many of the inconsistencies in the individual trees. I was consistently seeing an accuracy of over 80% on my test data after modeling to my training data, so it was an effective model for me. I implemented this in my code on my scraped users to optimize follow usage and saw tremendous growth in my following. Unfollowing After two days, I would unfollow the people I had followed. This gave me enough time to capture if they would follow me back or not. This allowed me to collect data to continue to grow. You have to unfollow the people you follow for two reasons. The first is that you cannot be following over 7,500 people at any time. The second is because — although artificial — you want to have your follower/following ratio as high as possible as it is a sign of a more desirable account. This is an easy task because there aren’t any decisions that need to be made. You follow 400 people in a day, and two days later you unfollow those exact people. Liking Liking can also supplement your account. I didn’t put nearly as much effort in choosing the pictures to like as liking isn’t proven to give you that much of gain in followers compared to the following method described above. I simply gave a predefined set of hashtags, looped through their feeds, and liked the pictures in hopes those users would return the favor. At this point, I have a complete self-sustaining robotic Instagram. My NYC page, on its own, is finding relevant content, weeding out bad potential posts, generating credits and a caption, and posting throughout the day. In addition, from 7:00 AM to 10:00 PM, it is growing its presence by automatically liking, following, and unfollowing with an intrigued audience which has been further redefined by some data science algorithms. The best part is that it seems more human than most accounts in the same niche. For a month or two, I sat back and watched my product grow. I would see an increase of anywhere between 100 and 500 followers a day all the while enjoying some beautiful pictures of the city I love. I was able to go about my life; work at my job, go out with friends, see a movie— never having to worry about spending any time manually growing my page. It had the formula to do its thing while I did my thing. Once I had 20,000 followers I decided it was time to use this page to get some free meals. Again, I automated my sales pitch too. I made a direct message template which I tried to keep as generic as possible. I wanted to be able to use the same message whether it was a restaurant, a theatre, a museum, or a store. This is what I came up with: Here, I just need to impute the account name and the number of followers I have at the time of the message. My goal was to find business Instagrams and give them my pitch. A business profile is slightly different from a normal one — it allows the user to add their email, phone number, directions, and other buttons on their page. But most importantly, they have a category label right on their profile. The above is an example of a business profile. Right under the name in the top left, it says “Korean Restaurant” and it has call-to-action buttons such as call, email, and directions at the top. I wrote a Python script that looks for these pages and automatically sends them a message from my account. The script takes two parameters, a starting hashtag, and a string to look for in the category label. In my case, I used the hashtag “Manhattan” and the string “restaurant”. What this script does is it goes to the hashtag feed and loads a bunch of photos. It then loops through the posts until it hits one that has users tagged in the photo. If it does, it goes into the tags and checks if they are a business page. If it is, it looks at the category. If the category includes the word “restaurant”, it sends them my message. The nice part about business profiles is that they often have emails on their page. If they do, I automatically send them an email follow-up to my Instagram direct message. I can change the hashtag to something like #TimesSquare and I can change the string to something like “museum” if my goals change down the road. If I go into my account, I will see the message that it auto-generated and sent. And if I go to my Gmail outbox, I’ll see: Finally, I have a script that monitors my inbox for any responses and alerts me if so. If there is a response, I finally do some manual work and negotiate with my potential client. Along with the posting process and the growth process, this runs throughout the day without the need for any human manipulation. The results are better than you might initially imagine. I have restaurants basically throwing gift cards and free meals my way in exchange for an Instagram promotion. Due to the power of AI, automation, and data science — I am able to sit back and relax while my code does the work for me. It acts as a source of entertainment while at the same time being my salesman. I hope this helps inspire some creativity when it comes to social media. Anyone can use these methods whether they are technical enough to automate or if they need to do it by hand. Instagram is a powerful tool and can be used for a variety of business benefits. Feel free to reach out with any questions! www.linkedin.com/in/chris-buetti www.lionize.ai LinkTree:  https://linktr.ee/chrisbuetti"
Artificial Intelligence + Blockchain = Passive Income (Forever?),rtificial Intelligence + Blockchain = Passive Income (Forever?,"The benefits of artificial intelligence are no longer limited to high-profile corporations, movie supervillains, and programmers performing fruitless experiments. What was once science fiction has now finally become reality, and when paired with blockchain technology it is no surprise that AI is making leaps and bounds. But the question that always comes to mind when a new technology comes to light is “Can I make money on it?” When looking at blockchain technology infused with artificial intelligence, the answer could be positive indeed! Thanks to  agency.howtotoken.com  for support in creating this topic (First platform with proven ICO contractors) You’ve probably heard about  world chess champions losing to computers , or perhaps you’re familiar with  self-driving cars  — both of these examples are fueled by artificial intelligence. We create AI to enable computers to solve problems for themselves, essentially giving them the ability to write code in response to new problems they encounter. When  paired with blockchains , AI is able to be better understood by humans, operate more efficiently, and make blockchains in general more efficient. In order for machines to learn, they need massive amounts of data to analyze, just like humans. The difference is that humans analyze their data passively, we use our five senses to take in the world and we store it away to create our belief in how the world functions. Trent McConaghy explains in  his article on Medium , “Because blockchains introduced three new characteristics: People inspired by Bitcoin were happy to overlook the traditional database-centric shortcomings, because these new benefits had potential to impact industries and society at large in wholly new ways.” He continues by adding, “These three new ‘blockchain’ database characteristics are also potentially interesting for AI applications.” Remember, machines don’t have senses, they need data, and blockchains can help them acquire that data faster and more cleanly (thanks to decentralization/shared control). Some AI may learn better than others, and by sharing IP/testing models on an exchange, there is the potential for faster growth (and profit for developers). Once AI starts learning, it becomes far easier for us to trace the path it took to a solution if that path is stored on an immutable trail. Considering that one of the primary reasons  we are creating AI in the first place is to make the lives of humans easier , one such obvious avenue is being explored by using AI and the blockchain to create both active and passive income streams. Blockchain technology provides users with the ability to create a trustless system on which they can run smart contracts, track a ledger, and more. AI and machine learning allows for hands-off programs that are designed to become smarter and more efficient over time. As  Tshilidzi Marwala and Bo Xing explain  in their University of Johannesburg paper, there are eight key ways where AI can help blockchains; four of which are pertinent to our discussion: It’s obvious to see how much of an impact AI will have on blockchains, and companies that are getting a headstart in the industry will almost certainly benefit greatly from such foresight. From increased scalability, enhanced security/privacy, and greater efficiency, AI will make it cheaper, safer, and easier to operate blockchains in general. There is a collection of examples where we can currently see AI and the blockchain working together on specific problems, such as: Trading cryptocurrencies is a risky market in the same way that the stock market can be dangerous, and one of the worst ways to increase this danger is by involving emotions. If the market starts crashing you may have a panic attack and quickly sell, only to watch the market bounce back higher than it ever was, leaving you at a loss. One simple way to prevent these emotional mistakes is by using an AI-based cryptobot to trade for you, such as  Cryptohopper  or  Autonio . Crypto bots are built on four principles: algorithms, market prediction, stop losses, and AI. Through these companies it is common for users to subscribe to “signals,” or external services that provide trading strategies based on the data they’ve collected and analyzed using AI. If the average trader removes themselves (and their emotions) from the equation, and instead base their trading on strategies generated by artificial intelligence, they are far more likely to succeed. As we’ve mentioned before in a different article,  musicians in today’s market struggle to earn their keep .  Musiclife  is a company that is working hard to change that by using blockchain technology infused with artificial intelligence. On the Musiclife platform, artists are automatically paid through smart contracts every time their music is streamed. This alone helps to ensure that the artists make more money for their work compared to traditional streaming platforms. But the real kicker here is Musiclife’s pricing model. Using an AI pricing method, the music price through the platform automatically adjusts based on current market playback data, so the more popular a song is the more money the artist will make. Given the potential of the AI + blockchain music streaming industry, it comes as no surprise to see big names like Spotify throwing their hat into the ring and  buying Niland , a startup focused on using AI for user-preference predictions. For those of us that aren’t musicians or professional cryptocurrency traders, however, these technologies may feel a bit detached from our daily lives. Luckily, there is a third option for making money in this industry, both during our lives and even potentially long after we’ve passed. A legacy used to be something that only the rich and powerful could afford to think about, but as decentralization technology helps to disrupt the status quo the possibility of creating lasting change is opening up to everyone. Whether your goal is to provide for your family after your death, or to have the work you’ve dedicated your life to carry on after your passing, avatars may be what you seek. Created by  EverLife , avatars are created using a combination of artificial intelligence and blockchain technology that gives you a platform where you can create your digital legacy. By harnessing a combination of machine learning, smart contracts, teachable skills, and secure avatar-to-avatar communications, there are a few key things you can do with your avatar: When creating avatars, EverLife chose to implement AI into their systems to ensure growth and that expansion would continue long after user input ceased. They understood that blockchains alone wouldn’t be sufficient, which is the same conclusion that  Tshilidzi Marwala and Bo Xing come to : “The DNA of the 4IR (Industrial Revolution) is AI, while blockchain represents one of the most disruptive technologies that may transform the whole economic system. Though blockchain holds various promises, this technology is still in its infancy.“ What EverLife, Marwala, and Xing are saying is that although the blockchain may be a groundbreaking technology, only by intertwining it with artificial intelligence will we truly see its full potential. As Matt Turck  points out in his article , the last 15 years was defined by social, mobile, and cloud-based services. Turck then goes on to say that “[t]here’s a rationale for making the argument that AI, the blockchain, and the Internet of Things is the new social, mobile, and cloud. Those trends are still very much emerging, but their potential impact is massive.” In the same way that social, mobile, and cloud development spawned many huge companies that exist today (Spotify, Airbnb, Twitter, Facebook, Uber, etc), one could speculate that the next 10–15 years will see a surplus of AI, blockchain, and IoT-based companies succeeding. Will our lives benefit from this? The better question would be to ask if our lives benefitted from all of the companies created during the last 10–15 years. We find ourselves currently standing on the precipice of change — a shift in the major industries is coming before us that has the potential of leading to sweeping innovations across a wide spectrum of our lives. The only question left is whether or not you, the average crypto-enthusiast, will seize the opportunity to make a profit during these changing times. Kirill Shilov  — Founder of Geekforge.io and Howtotoken.com. Interviewing the top 10,000 worldwide experts who reveal the biggest issues on the way to technological singularity. Join my  #10kqachallenge:   GeekForge Formula ."
The fourth industrial revolution: a primer on Artificial Intelligence (AI),he fourth industrial revolution: a primer on Artificial Intelligence (AI,"“The last 10 years have been about building a world that is mobile-first. In the next 10 years, we will shift to a world that is AI-first.” (Sundar Pichai, CEO of  Google , October 2016) From Amazon and Facebook to Google and Microsoft, leaders of the world’s most influential technology firms are highlighting their enthusiasm for Artificial Intelligence (AI). But what is AI? Why is it important? And why now? While there is growing interest in AI, the field is understood mainly by specialists. Our goal for this primer is to make this important field accessible to a broader audience. We’ll begin by explaining the meaning of ‘AI’ and key terms including ‘machine learning’. We’ll illustrate how one of the most productive areas of AI, called ‘deep learning’, works. We’ll explore the problems that AI solves and why they matter. And we’ll get behind the headlines to see why AI, which was invented in the 1950s, is coming of age today. As venture capitalists, we look for emerging trends that will create value for consumers and companies. We believe AI is an evolution in computing as, or more, important than the shifts to mobile or cloud computing. “It’s hard to overstate,” Amazon CEO Jeff Bezos wrote, “how big of an impact AI is going to have on society over the next 20 years.” We hope this guide cuts through the hype and explains why — whether you’re a consumer or executive, entrepreneur or investor — this emerging trend will be important for us all. Interested in more venture capital insights?  Sign up  for our blog posts. For a map of 226 AI startups in the UK,  explore this post . Are you an AI entrepreneur?  Get in touch . Coined in 1956 by Dartmouth Assistant Professor John McCarthy, ‘Artificial Intelligence’ (AI) is a general term that refers to hardware or software that exhibits behaviour which appears intelligent. In the words of Professor McCarthy, it is “the science and engineering of making intelligent machines, especially intelligent computer programs.” Basic ‘AI’ has existed for decades, via rules-based programs that deliver rudimentary displays of ‘intelligence’ in specific contexts. Progress, however, has been limited — because algorithms to tackle many real-world problems are too complex for people to program by hand. Complicated activities including making medical diagnoses, predicting when machines will fail or gauging the market value of certain assets, involve thousands of data sets and non-linear relationships between variables. In these cases, it’s difficult to use the data we have to best effect — to ‘ optimise’  our predictions. In other cases, including recognising objects in images and translating languages, we can’t even develop rules to describe the  features  we’re looking for. How can we write a set of rules, to work in all situations, that describe the appearance of a dog? What if we could transfer the difficulty of making complex predictions — the  data optimisation  and  feature specification  — from the programmer to the program? This is the promise of modern artificial intelligence. Machine learning (ML)  is a sub-set of AI. All machine learning is AI, but not all AI is machine learning (Figure 1, above). Interest in ‘AI’ today reflects enthusiasm for machine learning, where advances are rapid and significant. Machine learning lets us tackle problems that are too complex for humans to solve by shifting some of the burden to the algorithm. As AI pioneer Arthur Samuel wrote in 1959, machine learning is the ‘field of study that gives computers the ability to learn without being explicitly programmed.’ The goal of most machine learning is to develop a prediction engine for a particular use case. An algorithm will receive information about a domain (say, the films a person has watched in the past) and weigh the inputs to make a useful prediction (the probability of the person enjoying a different film in the future). By giving ‘computers the ability to learn’, we mean passing the task of optimisation — of weighing the variables in the available data to make accurate predictions about the future — to the algorithm . Sometimes we can go further, offloading to the program the task of specifying the features to consider in the first place. Machine learning algorithms learn through training. An algorithm initially receives examples whose outputs are known, notes the difference between its predictions and the correct outputs, and tunes the weightings of the inputs to improve the accuracy of its predictions until they are optimised. The defining characteristic of machine learning algorithms, therefore, is that  the quality of their predictions improve with experience . The more data we provide (usually up to a point), the better the prediction engines we can create (Figures 2 and 3, below. Note that the size of data sets required are highly context dependent — we cannot generalise from the examples below.) There are more than 15 approaches to machine learning, each of which uses a different algorithmic structure to optimise predictions based on the data received. One approach — ‘ deep learning’  — is delivering breakthrough results in new domains and we explore this below. But there are many others which, although they receive less attention, are valuable because of their applicability to a broad range of usage cases. Some of the most effective machine learning algorithms beyond deep learning include: Each approach has its advantages and disadvantages and combinations may be used (an ‘ ensemble’  approach). The algorithms selected to solve a particular problem will depend on factors including the nature of the available data set. In practice, developers tend to experiment to see what works. Use cases of machine learning vary according to our needs and imagination. With the right data we can build algorithms for myriad purposes including: suggesting the products a person will like based on their prior purchases; anticipating when a robot on a car assembly line will fail; predicting whether an email was mis-addressed; estimating the probability of a credit card transaction being fraudulent; and many more. Even with general machine learning — random forests, Bayesian networks, support vector machines and more — it’s difficult to write programs that perform certain tasks well, from understanding speech to recognising objects in images. Why? Because we can’t specify the features to optimise in a way that’s practical and reliable. If we want to write a computer program that identifies images of cars, for example, we can’t specify the features of a car for an algorithm to process that will enable correct identification in all circumstances. Cars come in a wide range of shapes, sizes and colours. Their position, orientation and pose can differ. Background, lighting and myriad other factors impact the appearance of the object. There are too many variations to write a set of rules. Even if we could, if wouldn’t be a scalable solution. We’d need to write a program for every type of object we wanted to identify. Enter  deep learning (DL) , which has revolutionised the world of artificial intelligence. Deep learning is a sub-set of machine learning — one of the more than 15 approaches to it. All deep learning is machine learning, but not all machine learning is deep learning (Figure 4, below). Deep learning is useful because it avoids the programmer having to undertake the tasks of  feature specification  (defining the features to analyse from the data) or  optimisation  (how to weigh the data to deliver an accurate prediction) — the algorithm does both. How is this achieved? The breakthrough in deep learning is to  model the brain, not the world . Our own brains learn to do difficult things — including understanding speech and recognising objects — not by processing exhaustive rules but through practice and feedback. As a child we experience the world (we see, for example, a picture of a car), make predictions (‘car!’) and receive feedback (‘yes!’). Without being given an exhaustive set of rules, we learn through training. Deep learning uses the same approach. Artificial, software-based calculators that approximate the function of neurons in a brain are connected together. They form a ‘ neural network ’ which receives an input (to continue our example, a picture of a car); analyses it; makes a determination about it and is informed if its determination is correct. If the output is wrong, the connections between the neurons are adjusted by the algorithm, which will change future predictions. Initially the network will be wrong many times. But as we feed in millions of examples, the connections between neurons will be tuned so the neural network makes correct determinations on almost all occasions. Practice makes (nearly) perfect. Using this process, with increasing effectiveness we can now: Deep learning is not well suited to every problem. It typically requires large data sets for training. It takes extensive processing power to train and run a neural network. And it has an ‘explainability’ problem — it can be difficult to know how a neural network developed its predictions. But by freeing programmers from complex feature specification, deep learning has delivered successful prediction engines for a range of important problems. As a result, it has become a powerful tool in the AI developer’s toolkit. Given its importance, it’s valuable to understand the basics of how deep learning works. Deep learning involves using an artificial ‘ neural network ’ — a collection of ‘neurons’ (software-based calculators) connected together. An artificial neuron has one or more inputs. It performs a mathematical calculation based on these to deliver an output. The output will depend on both the ‘ weights ’ of each input and the configuration of ‘input-output function’ in the neuron (Figure 5, below). The input-output function can vary. A neuron may be: A neural network is created when neurons are connected to one another; the output of one neuron becomes an input for another (Figure 6, below). Neural networks are organised into multiple layers of neurons (hence ‘deep’ learning). The ‘input layer’ receives information the network will process — for example, a set of pictures. The ‘output layer’ provides the results. Between the input and output layers are ‘hidden layers’ where most activity occurs. Typically, the outputs of each neuron on one level of the neural network serve as one of the inputs for each of the neurons in the next layer (Figure 7, below). Let’s consider the example of an image recognition algorithm — say, to recognise human faces in pictures. When data are fed into the neural network, the first layers identify patterns of local contrast — ‘low level’ features such as edges. As the image traverses the network, progressively ‘higher level’ features are extracted — from edges to noses, from noses to faces (Fig. 8, below) At its output layer, based on its training the neural network will deliver a probability that the picture is of the specified type (human face: 97%; balloon 2%; leaf 1%). Typically, neural networks are trained by exposing them to a large number of labelled examples. Errors are detected and the weights of the connections between the neurons tuned by the algorithm to improve results. The optimisation process is extensively repeated, after which the system is deployed and unlabelled images are assessed. The above is a simple neural network but their structure can vary and most are more complex. Variations include connections between neurons on the same layer; differing numbers of neurons per layer; and the connection of neuron outputs into the previous levels of the network (‘recursive’ neural networks). Designing and improving a neural network requires considerable skill. Steps include structuring the network for a particular application, providing a suitable training set of data, adjusting the structure of the network according to progress, and combining multiple approaches. AI is important because it tackles profoundly difficult problems, and the solutions to those problems can be applied to sectors important to human wellbeing — ranging from health, education and commerce to transport, utilities and entertainment. Since the 1950s, AI research has focused on five fields of enquiry: AI is valuable because in many contexts, progress in these capabilities offers revolutionary, rather than evolutionary, capabilities. Example applications of AI include the following; there are many more. In the coming years, machine learning capabilities will be employed in  almost all sectors in a wide variety of processes . Considering a single corporate function — for example, human resource (HR) activity within a company — illustrates the range of processes to which machine learning will be applied: Over time we expect the adoption of machine learning to become  normalised . Machine learning will become a part of a developer’s standard toolkit, initially improving existing processes and then reinventing them. The  second-order consequences  of machine learning will exceed its immediate impact. Deep learning has improved computer vision, for example, to the point that autonomous vehicles (cars and trucks) are viable. But what will be their impact? Today, 90% of people and 80% of freight are transported via road in the UK. Autonomous vehicles alone will impact: AI research began in the 1950s; after repeated false dawns, why is now the inflection point? The effectiveness of AI has been transformed in recent years due to the development of new algorithms, greater availability of data to inform them, better hardware to train them and cloud-based services to catalyse their adoption among developers. While deep learning is not new — the specification for the first effective, multi-layer neural network was published in 1965 — evolutions in deep learning algorithms during the last decade have transformed results. Our ability to recognise objects within images has been transformed (Figure 9, below) by the development of  convolutional neural networks  (CNN). In a design inspired by the visual cortexes of animals, each layer in the neural network acts as a filter for the presence of a specific pattern. In 2015, Microsoft’s CNN-based computer vision system identified objects in pictures more effectively (95.1% accuracy) than humans (94.9% accuracy). “To our knowledge,” they wrote, “our result is the first to surpass human level performance.” Broader applications of CNNs include video and speech recognition. Progress in speech and handwriting recognition, meanwhile, is improving rapidly (Figure 10, bel0w) following the creation of  recurrent neural networks  (RNNs). RNNs have feedback connections that enable data to flow in a loop, unlike conventional neural networks that ‘feed forward’ only. A powerful new type of RNN is the ‘Long Short-Term Memory’ (LSTM) model. With additional connections and memory cells, RNNs ‘remember’ the data they saw thousands of steps ago and use this to inform their interpretation of what follows — valuable for speech recognition where interpretation of the next word will be informed by the words that preceded it. From 2012, Google used LSTMs to power the speech recognition system in Android. Just six weeks ago, Microsoft engineers reported that their system reached a word error rate of 5.9% — a figure roughly equal to that of human abilities for the first time in history. Graphical Processing Units (GPUs)  are specialised electronic circuits that are slashing the time required to train the neural networks used for deep learning. Modern GPUs were originally developed in the late 1990s to accelerate 3D gaming and 3D development applications. Panning or zooming cameras in 3D environments makes repeated use of a mathematical process called a matrix computation. Microprocessors with serial architectures, including the CPUs that power today’s computers, are poor suited to the task. GPUs were developed with massively parallel architectures (the Nvidia M40 has 3,072 cores) to perform matrix calculations efficiently. Training a neural network makes extensive use of matrix computations. It transpired, therefore, that GPUs useful for 3D gaming were well suited to accelerate deep learning. Their effect has been considerable; a simple GPU can offer a 5x improvement in training time for a neural network, while gains of 10x or much greater are possible on larger problems. When combined with software development kits tuned for widely used deep learning frameworks, the improvements in training speed can be even greater (Figure 11, below). The neural networks used for deep learning typically require large data sets for training — from a few thousand examples to many millions. Fortunately, data creation and availability has grown exponentially. Today, as we enter the ‘third wave’ of data,  humanity produces 2.2 exabytes (2,300 million gigabytes) of data every day ; 90% of all the world’s data has been created in the last 24 months. The ‘first wave’ of data creation, which began in the 1980s and involved the creation of documents and transactional data, was catalysed by the proliferation of internet-connected desktop PCs. To this, a ‘second wave’ of data has followed — an explosion of unstructured media (emails, photos, music and videos), web data and meta-data resulting from ubiquitous, connected smartphones. Today we are entering the ‘third age’ of data, in which machine sensors deployed in industry and in the home create additional monitoring-, analytical- and meta-data. Given that much data created today is transmitted via the internet for use, ballooning internet traffic serves as a proxy for the enormous increase in humanity’s data production. While as a species we transferred 100GB of data per day in 1992, by 2020 we will be transferring 61,000GB per second (Figure 12, below — note the logarithmic scale). Beyond increases in the availability of general data, specialist data resources have catalysed progress in machine learning. ImageNet, for example, is a freely available database of over 10 million hand-labelled images. Its presence has supported the rapid development of object classification deep learning algorithms. Developers’ use of machine learning is being catalysed by the provision of cloud-based machine learning infrastructure and services from the industry’s leading cloud providers. Google, Amazon, Microsoft and IBM all offer  cloud-based infrastructure  (environments for model-building and iteration, scalable ‘GPUs-as-a-service’ and related managed services) to reduce the cost and difficulty of developing machine learning capabilities. In addition, they offer a burgeoning range of  cloud-based machine learning services  (from image recognition to language translation) which developers can use directly in their own applications. Google Machine Learning offers easily accessible services for: vision (object identification, explicit content detection, face detection and image sentiment analysis); speech (speech recognition and speech-to-text); text analysis (entity recognition, sentiment analysis, language detection and translation); and employee job searching (opportunity surfacing and seniority-based matching). Microsoft Cognitive Services includes more than 21 services within the fields of vision, speech, language, knowledge and search. The public’s  interest in AI has increased six-fold in the last five years  (Figure 13, below), with a still greater increase in the number of investments in AI companies by venture capital firms (Figure 14, below). We have entered a virtuous circle, in which progress in machine learning is attracting investment, entrepreneurship and awareness. The latter, in turn, are catalysing further progress. The benefits of machine learning will be numerous and significant. Many will be visible, from autonomous vehicles to new methods of human-computer interaction. Many will be less apparent, but enable more capable and efficient day-to-day business processes and consumer services. As with any paradigm shift, at times inflated expectations will exceed short-term potential. We expect a period of disillusionment regarding AI at some point in the future, to be followed by a longer and lasting recognition of its value as machine learning is used to improve and then reimagine existing systems. Historically, industrial revolutions transformed production and communication through new sources of power and transmission. The first industrial revolution used steam power to mechanise production in the 1780s. The second used electricity to drive mass production in the 1870s. The third used electronics and software to automate production and communication from the 1970s. Today, as software eats the world, our primary source of value creation is the processing of information. By enabling us to do so more intelligently, machine learning will yield benefits both humble and historic. We’ll be publishing more AI and venture capital insights. To avoid missing out,  Sign up  for our blog posts. For a map of 226 AI startups in the UK and key trends shaping the market,  explore this post . If you’re an AI entrepreneur,  Get in touch ."
Google is redefining mobile with artificial intelligence,oogle is redefining mobile with artificial intelligenc,"At Google’s annual developer conference in Mountain View today, the company took the wraps off the  next version of Android . It’s not named yet, so simply called ‘P’ but what  is  clear is that Google is executing on its clear lead in AI across every surface it develops for. Today, we saw how Google is redefining mobile with machine learning at its core. Let’s take a quick look at how it’s redefining Android, and what that means for the future of mobile."
Machine Learning (ML) vs. Artificial Intelligence (AI) — Crucial Differences,achine Learning (ML) vs. Artificial Intelligence (AI) — Crucial Difference,"October 15, 2018, by  Roberto Iriondo  — Last updated: October 31, 2021 sponsors.towardsai.net R ecently, a report was released regarding the misuse of companies claiming to use artificial intelligence [ 29 ] [ 30 ] on their products and services. According to  the Verge  [ 29 ], 40% of European startups claiming to use AI don’t use the technology. Last year,  TechTalks , also stumbled upon such misuse by companies claiming to use machine learning and advanced artificial intelligence to gather and examine thousands of users’ data to enhance user experience in their products and services [ 2 ] [ 33 ]. Unfortunately, there’s still much confusion within the public and the media regarding what genuinely is artificial intelligence [ 44 ] and what exactly is machine learning [ 18 ]. Often the terms are being used as synonyms. In other cases, these are being used as discrete, parallel advancements, while others are taking advantage of the trend to create hype and excitement to increase sales and revenue [ 2 ] [ 31 ] [ 32 ] [ 45 ]. 📚 Check out our editorial recommendations on the  best machine learning books . 📚 Below we go through some main differences between AI and machine learning. Quoting Interim Dean at the School of Computer Science at CMU, Professor and Former Chair of the Machine Learning Department at Carnegie Mellon University,  Tom M. Mitchell : A scientific field is best defined by the central question it studies. The field of Machine Learning seeks to answer the question: “How can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes? [ 1 ]” Machine learning  (ML) is a branch of artificial intelligence, and as defined by Computer Scientist and machine learning pioneer [ 19 ] Tom M. Mitchell: “ Machine learning is the study of computer algorithms that allow computer programs to automatically improve through experience. ” [ 18 ] — ML is one of the ways we expect to achieve AI. Machine learning relies on working with small to large datasets by examining and comparing the data to find common patterns and explore nuances. contribute.towardsai.net For instance, if you provide a machine learning model with many songs that you enjoy, along with their corresponding audio statistics (dance-ability, instrumentality, tempo, or genre). Then, it oughts to be able to automate (depending on the supervised machine learning model used) and generate a recommender system [ 43 ] as to suggest you with music in the future that (with a high percentage of probability rate) you’ll enjoy, similarly as to what Netflix, Spotify, and other companies do [ 20 ] [ 21 ] [ 22 ]. In a simple example, if you load a machine learning program with a considerable large dataset of x-ray pictures along with their description (symptoms, items to consider, and others), it oughts to have the capacity to assist (or perhaps automatize) the data analysis of x-ray pictures later on. The machine learning model looks at each picture in the diverse dataset and finds common patterns found in pictures with labels with comparable indications. Furthermore, (assuming that we use an acceptable ML algorithm for images), when you load the model with new pictures, it compares its parameters with the examples it has gathered before to disclose how likely the pictures contain any of the indications it has analyzed previously. The type of machine learning from our previous example, called “ supervised learning ,” where supervised learning algorithms try to model relationships and dependencies between the target prediction output and the input features, such that we can predict the output values for new data based on those relationships, which it has learned from previous datasets  [15]  fed. Unsupervised learning , another type of machine learning, is the family of machine learning algorithms, which have main uses in pattern detection and descriptive modeling. These algorithms do not have output categories or labels on the data (the model trains with unlabeled data). Reinforcement learning , the third popular type of machine learning, aims at using observations gathered from the interaction with its environment to take actions that would maximize the reward or minimize the risk. In this case, the reinforcement learning algorithm (called the agent) continuously learns from its environment using iteration. A great example of reinforcement learning is computers reaching a super-human state and beating humans on computer games  [3] . Machine learning can be dazzling, particularly its advanced sub-branches, i.e., deep learning and the various types of neural networks. In any case, it is “magic” (Computational Learning Theory)  [16] , regardless of whether the public, at times, has issues observing its internal workings. While some tend to compare deep learning and neural networks to the way the human brain works, there are essential differences between the two [ 2 ]  [4]  [ 46 ]. Artificial intelligence, on the other hand, is vast in scope. According to Andrew Moore [ 6 ] [ 36 ] [ 47 ], Former-Dean of the School of Computer Science at  Carnegie Mellon University , “Artificial intelligence is the science and engineering of making computers behave in ways that, until recently, we thought required human intelligence.” That is a great way to define AI in a single sentence; however, it still shows how broad and vague the field is. Fifty years ago, a chess-playing program was considered a form of AI [ 34 ] since game theory and game strategies were capabilities that only a human brain could perform. Nowadays, a chess game is dull and antiquated since it is part of almost every computer’s operating system (OS) [ 35 ]; therefore, “until recently” is something that progresses with time [ 36 ]. Assistant Professor and Researcher at CMU  Zachary Lipton  clarifies on Approximately Correct  [7] , the term AI “is aspirational, a moving target based on those capabilities that humans possess but which machines do not.” AI also includes a considerable measure of technology advances that we know. Machine learning is only one of them. Prior works of AI utilized different techniques. For instance, Deep Blue, the AI that defeated the world’s chess champion in 1997, used a method called tree search algorithms  [8]  to evaluate millions of moves at every turn [ 2 ] [ 37 ] [ 52 ] [ 53 ]. As we know it today, AI is symbolized with Human-AI interaction gadgets by Google Home, Siri, and Alexa, by the machine-learning-powered video prediction systems that power Netflix, Amazon, and YouTube. These technological advancements are progressively becoming essential in our daily lives. They are intelligent assistants who enhance our abilities as humans and professionals — making us more productive. In contrast to machine learning, AI is a moving target [ 51 ], and its definition changes as its related technological advancements turn out to be further developed  [7] . Possibly, within a few decades, today’s innovative AI advancements ought to be considered as dull as flip-phones are to us right now. The term “artificial intelligence” came to inception in 1956 by a group of researchers, including Allen Newell and Herbert A. Simon  [9] . Since then, AI’s industry has gone through many fluctuations. In the early decades, there was much hype surrounding the industry, and many scientists concurred that human-level AI was just around the corner. However, undelivered assertions caused a general disenchantment with the industry along with the public and led to the AI winter, a period where funding and interest in the field subsided considerably [ 2 ] [ 38 ] [ 39 ] [ 48 ]. Afterward, organizations attempted to separate themselves from the term AI, which had become synonymous with unsubstantiated hype and used different names to refer to their work. For instance, IBM described Deep Blue as a supercomputer and explicitly stated that it did not use artificial intelligence  [10] , while it did [ 23 ]. During this period, various other terms, such as big data, predictive analytics, and machine learning, started gaining traction and popularity [ 40 ]. In 2012, machine learning, deep learning, and neural networks made great strides and found use in a growing number of fields. Organizations suddenly started to use the terms “machine learning” and “deep learning” for advertising their products [ 41 ]. Deep learning began to perform tasks that were impossible to do with classic rule-based programming. Fields such as speech and face recognition, image classification, and natural language processing, which were at early stages, suddenly took great leaps [ 2 ] [ 24 ] [ 49 ], and in March 2019–three of the most recognized deep learning pioneers won a Turing award thanks to their contributions and breakthroughs that have made deep neural networks a critical component to nowadays computing [ 42 ]. Hence, to the momentum, we see a gearshift back to AI. For those who are used to the limits of old-fashioned software, the effects of deep learning almost seemed like “magic”  [16] . Especially since a fraction of the fields that neural networks and deep learning are entering were considered off-limits for computers, and nowadays, machine learning and deep learning engineers are earning high-level salaries, even when they are working at non-profit organizations, which speaks to how hot the field is [ 50 ]  [11] . Sadly, this is something that media companies often report without profound examination and frequently go along with AI articles with pictures of crystal balls and other supernatural portrayals. Such deception helps those companies generate hype around their offerings [ 27 ]. Yet, down the road, as they fail to meet the expectations, these organizations are forced to hire humans to make up for their so-called AI  [12] . In the end, they might end up causing mistrust in the field and trigger another AI winter for the sake of short-term gains [ 2 ] [ 28 ]. I am always open to feedback, please share in the comments if you see something that may need revisited. Thank you for reading! The author would like to extensively thank  Ben Dickson , Software Engineer and Tech Blogger, for his kindness to allow me to rely on his expertise and storytelling, along with several members of the AI Community for the immense support and constructive criticism in preparation of this article. DISCLAIMER:  The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University nor other companies (directly or indirectly) associated with the author(s). These writings do not intend to be final products but rather a reflection of current thinking and a catalyst for discussion and improvement. You can find me on  my website ,  Medium ,  Instagram ,  Twitter ,  Facebook ,  LinkedIn . Introduction to Machine Learning |  Matt Gormley  |  School of Computer Science , Carnegie Mellon University |  http://www.cs.cmu.edu/~mgormley/courses/10601/ AI for Everyone | Andrew Ng | Coursera |  https://www.coursera.org/learn/ai-for-everyone Machine Learning Course | Google |  https://developers.google.com/machine-learning/crash-course/ Intro to Machine Learning | Udacity |  https://www.udacity.com/course/intro-to-machine-learning–ud120 Machine Learning Training | Amazon Web Services |  https://aws.amazon.com/training/learning-paths/machine-learning/ Introduction to Machine Learning | Coursera |  https://www.coursera.org/learn/machine-learning [1] The Discipline of Machine learning | Tom M. Mitchell |  http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf [2] Why the difference between AI and machine learning matters | Ben Dickson | TechTalks |  https://bdtechtalks.com/2018/10/08/artificial-intelligence-vs-machine-learning/ [3] Types of Machine Learning Algorithms You Should Know | David Fumo | Towards Data Science |  https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861 [4] Watch our AI system play against five of the world’s top Dota 2 Professionals | Open AI |  https://openai.com/five/ [5] Differences between Neural Networks and Deep Learning | Quora |  https://www.quora.com/What-is-the-difference-between-Neural-Networks-and-Deep-Learning [6] What Machine Learning Can and Cannot Do | WSJ |  https://blogs.wsj.com/cio/2018/07/27/what-machine-learning-can-and-cannot-do/ [7] Carnegie Mellon Dean of Computer Science on the Future of AI | Forbes |  https://www.forbes.com/sites/peterhigh/2017/10/30/carnegie-mellon-dean-of-computer-science-on-the-future-of-ai [8] From AI to Ml to AI: On Swirling Nomenclature & Slurried Thought | Zachary C. Lipton | Approximately Correct |  http://approximatelycorrect.com/2018/06/05/ai-ml-ai-swirling-nomenclature-slurried-thought/ [9] Tree Search Algorithms | Introduction to AI |  http://how2examples.com/artificial-intelligence/tree-search [10] Reinventing Education Based on Data and What Works, Since 1955 | Carnegie Mellon University |  https://www.cmu.edu/simon/what-is-simon/history.html [11] Does Deep-Blue use AI? | Richard E. Korf | University of California |  https://www.aaai.org/Papers/Workshops/1997/WS-97-04/WS97-04-001.pdf [12] Artificial Intelligence: Salaries Heading Skyward | Stacy Stanford | Machine Learning Memoirs |  https://medium.com/mlmemoirs/artificial-intelligence-salaries-heading-skyward-e41b2a7bba7d [13] The rise of ‘pseudo-AI’: how tech firms quietly use humans to do bots’ work | The Guardian |  https://www.theguardian.com/technology/2018/jul/06/artificial-intelligence-ai-humans-bots-tech-companies [14] Simplify Machine Learning Pipeline Analysis with Object Storage | Western Digital |  https://blog.westerndigital.com/machine-learning-pipeline-object-storage/ [15] Dr. Andrew Moore Opening Keynote | Artificial Intelligence and Global Security Initiative |  https://youtu.be/r-zXI-DltT8 [16] The 50 Best Public Datasets for Machine Learning | Stacy Stanford |  https://medium.com/datadriveninvestor/the-50-best-public-datasets-for-machine-learning-d80e9f030279 [17] Computational Learning Theory | ACL |  http://www.learningtheory.org/ [18] Machine Learning Definition | Tom M. Mitchell| McGraw-Hill Science/Engineering/Math; (March 1, 1997), Page 1 | h ttp://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html [19] For pioneering contributions and leadership in the methods and applications of machine learning. |  “Prof. Tom M. Mitchell .”  National Academy of Engineering . Retrieved October 2, 2011. [20] Recommender System | Wikipedia |  https://en.wikipedia.org/wiki/Recommender_system [21] Spotify’s “This Is” playlists: the ultimate song analysis for 50 mainstream artists | James Le |  https://towardsdatascience.com/spotifys-this-is-playlists-the-ultimate-song-analysis-for-50-mainstream-artists-c569e41f8118 [22] How recommender systems make their suggestions | Bibblio |  https://medium.com/the-graph/how-recommender-systems-make-their-suggestions-da6658029b76 [23] Deep Blue | Science Direct Assets |  https://www.sciencedirect.com/science/article/pii/S0004370201001291 [24] 4 great leaps machine learning made in 2015 | Sergar Yegulalp |  https://www.infoworld.com/article/3017250/4-great-leaps-machine-learning-made-in-2015.html [25] Limitations of Deep Learning in AI Research | Roberto Iriondo | Towards Data Science |  https://towardsdatascience.com/limitations-of-deep-learning-in-ai-research-5eed166a4205 [26] Forty percent of ‘AI startups’ in Europe don’t use AI, claims report | The Verge |  https://www.theverge.com/2019/3/5/18251326/ai-startups-europe-fake-40-percent-mmc-report [27] This smart toothbrush claims to have its very own ‘embedded AI’ | The Verge | h ttps://www.theverge.com/circuitbreaker/2017/1/4/14164206/smart-toothbrush-ara-ai-kolibree [28] The Coming AI Autumn | Jeffrey P. Bigham |  http://jeffreybigham.com/blog/2019/the-coming-ai-autumnn.html [29] Forty percent of ‘AI startups’ in Europe don’t use AI, claims report | The Verge |  https://www.theverge.com/2019/3/5/18251326/ai-startups-europe-fake-40-percent-mmc-report [30] The State of AI: Divergence | MMC Ventures |  https://www.mmcventures.com/wp-content/uploads/2019/02/The-State-of-AI-2019-Divergence.pdf [31] Top Sales & Marketing Priorities for 2019: AI and Big Data, Revealed by Survey of 600+ Sales Professionals | Business Wire |  https://www.businesswire.com/news/home/20190129005560/en/Top-Sales-Marketing-Priorities-2019-AI-Big [32] Artificial Intelligence Beats the Hype With Stunning Growth | Forbes |  https://www.forbes.com/sites/jonmarkman/2019/02/26/artificial-intelligence-beats-the-hype-with-stunning-growth/#4e8507391f15 [33] Misuse of AI can destroy customer loyalty: here’s how to get it right | Compare the Cloud |  https://www.comparethecloud.net/articles/misuse-of-ai-can-destroy-customer-loyalty-heres-how-to-get-it-right/ [34] Timeline of Artificial Intelligence | Wikipedia |  https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence#1950s [35] Computer Chess | Wikipedia |  https://en.wikipedia.org/wiki/Computer_chess [36] Artificial Intelligence at Carnegie Mellon University |Machine Learning Department at Carnegie Mellon University |  https://www.youtube.com/watch?v=HH-FPH0vpVE [37] Search Control Methods in Deep Blue | Semantic Scholar |  https://pdfs.semanticscholar.org/211d/7268093b4dfce8201e8da321201c6cd349ef.pdf [38] Is Winter Coming? | University of California, Berkeley |  https://pha.berkeley.edu/2018/12/01/is-winter-coming-artificial-intelligence-in-healthcare/ [39] AI Winter | Wikipedia |  https://en.wikipedia.org/wiki/AI_winter [40] A Very Short History of Data Science | Forbes |  https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/#3c828f2055cf [41] Deep Learning Revolution | Wikipedia |  https://en.wikipedia.org/wiki/Deep_learning#Deep_learning_revolution [42] Turing Award Winners 2018 | ACM |  https://amturing.acm.org/byyear.cfm [43] Recommender System | Wikipedia |  https://en.wikipedia.org/wiki/Recommender_system [44] The discourse is unhinged: how the media gets AI alarmingly wrong | The Guardian |  https://www.theguardian.com/technology/2018/jul/25/ai-artificial-intelligence-social-media-bots-wrong [45] Retailers moved from AI hype to reality in 2018 | iXtenso |  https://ixtenso.com/technology/retailers-moved-from-ai-hype-to-reality-in-2018.html [46] Deep Learning & The Human Brain, Inspiration not Imitation | Imaginea |  https://www.imaginea.com/sites/deep-learning-human-brain-inspiration-not-imitation/ [47] Carnegie Mellon Dean of Computer Science on the Future of AI | Forbes |  https://www.forbes.com/sites/peterhigh/2017/10/30/carnegie-mellon-dean-of-computer-science-on-the-future-of-ai/#164487aa2197 [48] History of AI Winters | Actuaries Digital |  https://www.actuaries.digital/2018/09/05/history-of-ai-winters/ [49] Recent Advances in Deep Learning: An Overview | Arxiv |  https://arxiv.org/pdf/1807.08169.pdf [50] Tech Giants Are Paying Huge Salaries for Scarce AI Talent | New York Times |  https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html [51] Artificial Intelligence is a Moving Target | AnswerRocket |  https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html [52] Search Control Methods in Deep Blue | Semantic Scholar |  https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html [53] Search Tree Algorithms on Deep Blue | Stanford University |  http://stanford.edu/~cpiech/cs221/apps/deepBlue.html  DDI"
How Quantitative UX Research Differs from Data Analytics,ow Quantitative UX Research Differs from Data Analytic,"When we introduce ourselves as quantitative UX researchers, people often get curious about the “quantitative” part of our title. One question they ask is how our work is similar to, or different from, the work of product analysts or product data scientists. We hope this article will provide a comprehensive answer. The confusion is understandable. For one thing, we both love data. We’re trained to conduct quantitative analyses with variety of different data sources, including experiments, surveys, and logged behaviors. Many of us, across both groups, come from quantitative disciplines such as social psychology, statistics, computer science, and economics, which help us drive insights and elevate our teams’ understanding of product usage through large amounts of data. At Facebook, the Data Analytics and the UX Research team share the same mission of explaining phenomena we observe in data. We both focus on making meaningful and interpretable inferences about data, relationships between variables, and explanations for changes or patterns in the data. (By contrast, machine learning engineers and those in other big data roles focus on predicting unknowns as accurately as possible.) Both groups use the same primary tool: statistics. We write code in data analysis software like R, Python, or SPSS. We spend a lot of time exploring and visualizing data to drive our hypothesis generation. We visualize more complex relationships of data points using libraries in R and Python. We also need to have knowledge of distributed data storage systems like Presto and Hive (some SQL knowledge is often sufficient to work with those tools). Most of our work ends up, hopefully, in presentations that clearly and concisely communicate our results.    However, there are also fundamental differences between the two roles. Here are 4 major distinctions between quantitative UX research and data analytics. (Note: these differences are influenced by our work at Facebook and may not all apply everywhere.) For a technology company to perform well, it has to focus relentlessly on both improving business metrics and delighting its users. Insights about how and why metrics are changing help the company build better products and grow their business value. Understanding users — their motivations, their experiences, and how the product fits into their life — is also critically important.   Quantitative UX research delivers insights about people. UX researchers often approach research projects with questions such as: What are the human motivations for using these products? How do people perceive and use the product? How do they react emotionally and physically to it? What do they like and dislike about specific features? What role does the product play in their daily life?    Data scientists, on the other hand, often start by asking questions related to how the product is performing, or is expected to perform, in the marketplace. How does a product feature change behavioral metrics, such as clicks or time spent? How much adoption of the product did we receive on various devices? Which features are used and which ones are abandoned?    Despite the different motivating problems, data scientists and quantitative UX researchers have similar workflows as they collect and analyze data in order to discover important interactions and relationships between technology and people. User actions tell us about what is happening — for example, how many times they clicked on something, how often they come back to the app, or how much time they spent on the site.    User intents, on the other hand, are about the relationship between people who use the product and (un)available product features. For example, out of 10 clicks, how many of those clicks were out of interest and how many were due to frustration? What brings users back to the app and how do they feel about it? How much of the time they spent on the site was time well-spent, and how much was spent looking for something they couldn’t find or trying to figure out a feature that wasn’t intuitive?    Data scientists are less concerned with such questions than with with metrics and collective performance based on user actions or lack of action. They’re interested in the timing, variety, and magnitude of users’ signals — things like views, clickthrough rates, time spent, and churn.   UX researchers, including quantitative ones, are mainly interested in understanding how people use our products, what problems they may have, and what works differently for them. Quantitative researchers seek to gain insights about the intent of people’s product usage through patterns in the data they collected. They also try to measure quality of experience using self-reported data (surveys) or behavioral data.    While data scientists are more concerned about how many people used a new feature and what they did afterward, UX researchers aim to understand how many people used the feature in various contexts, what motivates them to use the feature, and how they felt about the experience. Like data scientists, quantitative UX researchers may use a multitude of statistical tools to gather insights from data. While the main suite of tools used by the two groups are roughly similar, each group uses the tools differently, since they’re pursuing different goals.   Compared to UX researchers, data scientists are more often motivated to improve the predictive accuracy of their models. (The most accurate models are black box machine learning models, which are hard to interpret by their nature.)    UX researchers are more often motivated by inference. In many cases, we’re not looking to predict future phenomena but to better understand the factors underlying experience or behavior. That’s why UX researchers more often use social science models that are more interpretable but have lower predictive accuracy. In data science, activity logs are the primary source of data. Quantitative UX researchers use a combination of log data and self-reported survey data. Depending on the research questions, we may use only one source of data or combine multiple methods of data collection and analysis.   Analyzing survey data requires a different methodology than analyzing log data. To accurately make sense of survey data, the quantitative researcher must consider, and model, the survey design and data collection process. Survey data is therefore typically analyzed with some form of regression, in which   survey design elements are incorporated into the model. For activity log data, Data Scientists typically consider these elements of data collection to be ignorable.   Log data is frequently several orders of magnitude larger than survey data. As a result, overfitting and algorithm speed are critical issues in learning from log data that typically don’t arise in analyzing survey data. While regression models are also used in analyzing log data, Data Scientists frequently use methods involving  regularization   in order to handle issues introduced by data size. Despite the real differences between data science and quantitative UX research, there are undoubtedly many cases where the two roles are almost interchangeable. But they can also be highly complementary, taking full advantage of a diverse range of backgrounds and skills. In fact, at Facebook, some of the most impactful, satisfying, and fulfilling research projects are collaborations between the two. The need for both groups shows no signs of fading. The number and variety of meaningful research questions about the relationship between technology and people ensure that data science and quantitative UX research will continue to exist in parallel, driving the business and the technology forward to serve people better. While we’ve shared some of the ways Quantitative UX Research and Data Analytics are similar or different from each other, our observations are influenced by the context of our organization and the nature of our work. We welcome further discussions about these two roles in other organizations, and fields. What kind of problems do you solve as a Quantitative UX Researcher or a Data Scientist? We’d love to hear from you — please comment below! Authors:   Mary Nguyen , Researcher at Facebook;  Saide Bakshi , Researcher at Facebook; and Alex Whitworth, Data Scientist at Facebook  (from left to right) Illustrator:  Drew Bardana"
Top 6 Data Analytics Tools in 2021,op 6 Data Analytics Tools in 202,"When it comes to data analytics tools, we always have questions. What is the difference between so many data analysis tools? Which is better? Which one should I study? Although this is a commonplace topic, it is really important, and I have been working hard to pursue the answer to this ultimate problem. If you go online to search for relevant information in this area, it is difficult to see a fair point of view. Because the reviewers who evaluate a certain data analytics tool may be from a different perspective, with some personal feelings. Today, let us put aside these personal feelings. And I am trying to talk objectively with you about my personal views on data analysis tools on the market, for your reference. I have chosen a total of 6 tools in three types. Next let me introduce them one by one. With a variety of powerful features such as form creation, PivotTable, VBA, etc., Excel’s system is so large that no analytics tool can surpass it, ensuring that people can analyze data according to their needs. However, some people may think that they are very proficient in computer programming languages, and disdain to use Excel as a tool because Excel can’t handle big data. But think about it, do the data we use in our daily life exceed the limit of big data? In my opinion, Excel is a versatile player. It works best for small data, and with plugins it can handle millions of data. To sum up, based on the powerful features of Excel and its user scale, my opinion is that it is an indispensable tool. If you want to learn data analysis,  Excel  is definitely the first choice. Business intelligence is born for data analysis, and it is born at a very high starting point. The goal is to shorten the time from business data to business decisions and use data to influence decisions. The product goal of Excel is not like this. Excel can do a lot of things. You can use Excel to draw a curriculum, make a questionnaire, or use it as a calculator, or even use it for drawing. If you master VBA, you can also make a small game. But these are not really data analysis functions. But BI tools are specialized in data analysis. Take the common BI tools such as Power BI, FineReport, and Tableau for example. You will find that they are designed according to the data analysis process. First, data processing, data cleaning, and then data modeling, finally  data visualization  that uses presentation of charts to identify problems and influence decision-making. These are the only way for data analysis, and there are some pain points of employees in this process. For example, the repetitive and low value-added work of cleaning data can be simplified with BI tools. If the amount of data is large, the traditional tool Excel cannot complete the PivotTable. If we use Excel to do graphical displays, it will take a lot of time to edit the chart, including color and font settings. These pain points are where BI tools can bring us change and value. Now let’s compare the three popular BI tools on the market: Power BI, FineReport, and Tableau. The core essence of Tableau is actually the PivotTable and PivotChart of Excel. It can be said that Tableau is keenly aware of this feature of Excel. It entered the BI market earlier and carried forward this core value. From the perspective of development history and current market feedback, Tableau is better at visualization. I don’t think this is because of how cool its charts are, but its design, color, and user interface give us a simple and fresh feeling. This is indeed like Tableau’s own propaganda, investing a lot of academic energy to study what kind of charts people like, how to give users the ultimate experience in operation and visual. As Tableau advertises, their team puts a lot of academic energy into researching what kind of charts people like, and how to give users the ultimate experiencein terms of operation and vision. In addition, Tableau has added data cleaning function and more intelligent analysis functions. This is also a predictable product development advantage for Tableau. The advantage of Power BI lies in its business model and data analysis capabilities. Power BI was previously a plug-in for Excel, and its development was not ideal. So it got out of Excel and developed into a BI tool. As a latecomer, Power BI has iterative updates every month and catches up very fast. Power BI currently has three licensing methods: Power BI Free, Power BI Pro, and Power BI Premium. Just like Tableau, the features of its free version are not complete. But they are almost enough for personal use. And the data analysis of Power BI is powerful. Its PowerPivot and DAX language allow me to implement complex advanced analysis in a way similar to writing formulas in Excel. What makes FineReport unique is that its self-service data analysis is very suitable for business users. With a simple drag and drop operation, you can design various styles of reports with FineReport and easily build a data decision analysis system. FineReport can directly connect to all kinds of databases, and it is convenient and quick to customize various styles to make weekly, monthly, and annual reports. Its format is similar to the interface of Excel. The features include report creation, report permission assignment, report management, data entry, etc. In addition, the visualization function of FineReport is also very prominent, providing a variety of  dashboard templates  and a number of self-developed visual plug-in libraries. In terms of price, the personal version of FineReport is completely free and all features are open. R  and  Python  are the third type of tools I want to talk about. Although softwarelike Excel and BI tools have been designed with the utmost effort to consider the most application scenarios of data analysis, they are essentially customized. If the software doesn’t design a feature, or develop a button for a feature, chances are that you won’t be able to complete your work with them. The programming language is different for this. It is very powerful and flexible. You can write code to do anything you want. For example, R and Python are the indispensable tools for data scientists. From a professional perspective, they are definitely powerful than Excel and BI tools. So what are the application scenarios that R and Python can realize, while it is difficult for Excel and BI tools to achieve? In terms of R language, it is best at statistical analysis, such as normal distribution, using algorithm to classify clusters, and regression analysis. This kind of analysis is like using data as an experiment. It can help us answer the following questions. For example, the distribution of data is a normal distribution, a triangular distribution or other types of distribution? What is the discrete situation? Is it within the statistical controllable range we want to achieve? What is the magnitude of the effect of different parameters on the results? And there is also hypothetical simulation analysis. If a certain parameter changes, how much impact will it bring? For example, we want to predict the behavior of a consumer. How long will he stay in our store? How much will he spend? We can find out his personal credit status and make a loan amount based on his online consumption record. Or we can push different items based on his browsing history on the web page. This also involves the current popular concepts of machine learning and artificial intelligence. The above comparison illustrates the difference between several softwares. What I want to summarize is that what is real is reasonable. Excel, BI tools or programming languages have overlapping functions, but they are also complementary tools. The value of each depends on the kind of application being developed and situation at hand. Before you choose a data analytics tool, you must first understand your own work: whether you will use the application scenarios I just mentioned. Or think about your career direction: whether it is toward the data science or business analysis. 8 Best Reporting Tools& Software To Improve Your Business 5 Most Popular Business Intelligence (BI) Tools in 2021 9 Data Visualization Tools That You Cannot Miss in 20 21 Top 16 Types of Chart in Data Visualization Top 10 Map Types in Data Visualization A Step-by-Step Guide to Making Sales Dashboards"
Data Analytics with Python by Web scraping: Illustration with CIA World Fact-book,ata Analytics with Python by Web scraping: Illustration with CIA World Fact-boo,"In  a data science project, almost always the most time consuming and messy part is the data gathering and cleaning. Everyone likes to build a cool deep neural network (or XGboost) model or two and show off one’s skills with cool 3D interactive plots. But the models need raw data to start with and they don’t come easy and clean. Life, after all, is not Kaggle where a zip file full of data is waiting for you to be unpacked and modeled :-) But why gather data or build model anyway ? The fundamental motivation is to answer a business or scientific or social question.  Is there a trend ?  Is this thing related to that ?  Can the measurement of this entity predict the outcome for that phenomena ? It is because answering this question will validate a hypothesis you have as a scientist/practitioner of the field. You are just using data (as opposed to test tubes like a chemist or magnets like a physicist) to test your hypothesis and prove/disprove it scientifically.  That is the ‘science’ part of the data science. Nothing more, nothing less… Trust me, it is not that hard to come up with a good quality question which requires a bit of application of data science techniques to answer. Each such question then becomes a small little project of your which you can code up and showcase on a open-source platform like Github to show to your friends. Even if you are not a data scientist by profession, nobody can stop you writing cool program to answer a good data question. That showcases you as a person who is comfortable around data and one who can tell a story with data. Let’s tackle one such question today… Is there any relationship between the GDP (in terms of purchasing power parity) of a country and the percentage of its Internet users? And is this trend similar for low-income/middle-income/high-income countries? Now, there can be any number of sources you can think of to gather data for answering this question. I found that an website from CIA (Yes, the ‘AGENCY’), which hosts basic factual information about all countries around the world, is a good place to scrape the data from. So, we will use following Python modules to build our database and visualizations, Let’s talk about the program structure to answer this data science question. The  entire boiler plate code is available here  in my  Github repository . Please feel free to fork and star if you like it. Here is how the  front page of the CIA World Factbook  looks like, We use a simple urllib request with a SSL error ignore context to retrieve this page and then pass it on to the magical BeautifulSoup, which parses the HTML for us and produce a pretty text dump. For those, who are not familiar with the BeautifulSoup library, they can watch the following video or read this  great informative article on Medium . So, here is the code snippet for reading the front page HTML, Here is how we pass it on to BeautifulSoup and use the  find_all  method to find all the country names and codes embedded in the HTML. Basically, the idea is to  find the HTML tags named ‘option’ . The text in that tag is the country name and the char 5 and 6 of the tag value represent the 2-character country code. Now, you may ask how would you know that you need to extract 5th and 6th character only? The simple answer is that  you have to examine the soup text i.e. parsed HTML text yourself and determine those indices . There is no universal method to determine this. Each HTML page and the underlying structure is unique. This step is the essential scraping or crawling as they say. To do this,  the key thing to identify is how the URL of each countries information page is structured . Now, in general case, this is may be hard to get. In this particular case, quick examination shows a very simple and regular structure to follow. Here is the screenshot of Australia for example, That means there is a fixed URL to which you have to append the 2-character country code and you get to the URL of that country’s page. So, we can just iterate over the country codes’ list and use BeautifulSoup to extract all the text and store in our local dictionary. Here is the code snippet, For good measure, I prefer to serialize and  store this data in a  Python pickle object   anyway. That way I can just read the data directly next time I open the Jupyter notebook without repeating the web crawling steps. This is the core text analytics part of the program, where we take help of  regular expression  module  to find what we are looking for in the huge text string and extract the relevant numerical data. Now, regular expression is a rich resource in Python (or in virtually every high level programming language). It allows searching/matching particular pattern of strings within a large corpus of text. Here, we use very simple methods of regular expression for matching the exact words like “ GDP — per capita (PPP): ” and then read few characters after that, extract the positions of certain symbols like $ and parentheses to eventually extract the numerical value of GDP/capita. Here is the idea illustrated with a figure. There are other regular expression tricks used in this notebook, for example to extract the total GDP properly regardless whether the figure is given in billions or trillions. Here is the example code snippet.  Notice the multiple error-handling checks placed in the code . This is necessary because of the supremely unpredictable nature of HTML pages. Not all country may have the GDP data, not all pages may have the exact same wordings for the data, not all numbers may look same, not all strings may have $ and () placed similarly. Any number of things can go wrong. It is almost impossible to plan and write code for all scenarios but at least you have to have code to handle the exception if they occur so that your program does not come to a halt and can gracefully move on to the next page for processing. One thing to remember is that all these text analytics will produce dataframes with slightly different set of countries as different types of data may be unavailable for different countries. One could use a  Pandas left join  to create a dataframe with intersection of all common countries for which all the pieces of data is available/could be extracted. After all the hard work of HTML parsing, page crawling, and text mining, now you are ready to reap the benefits — eager to run the regression algorithms and cool visualization scripts! But wait, often you need to clean up your data (particularly for this kind of socio-economic problems) a wee bit more before generating those plots. Basically, you want to filter out the outliers e.g. very small countries (like island nations) who may have extremely skewed values of the parameters you want to plot but does not follow the main underlying dynamics you want to investigate. A few lines of code is good for those filters. There may be more  Pythonic  way to implement them but I tried to keep it extremely simple and easy to follow. The following code, for example, creates filters to keep out small countries with < 50 billion of total GDP and low and high income boundaries of $5,000 and $25,000 respectively (GDP/capita). We use  seaborn regplot  function  to create the scatter plots (Internet users % vs. GDP/capita) with linear regression fit and 95% confidence interval bands shown. They look like following. One can interpret the result as There is a strong positive correlation between Internet users % and GDP/capita for a country. Moreover, the strength of correlation is significantly higher for low-income/low-GDP countries than the high-GDP, advanced nations.  That could mean access to internet helps the lower income countries to grow faster and improve the average condition of their citizens more than it does for the advanced nations . This article goes over a demo Python notebook to illustrate how to crawl webpages for downloading raw information by HTML parsing using BeautifulSoup. Thereafter, it also illustrates the use of Regular Expression module to search and extract important pieces of information what the user demands. Above all, it demonstrates how or why there can be no simple, universal rule or program structure while mining messy HTML parsed texts. One has to examine the text structure and put in place appropriate error-handling checks to gracefully handle all the situations to maintain the flow of the program (and not crash) even if it cannot extract data for all those scenarios. I hope readers can benefit from the provided Notebook file and build upon it as per their own requirement and imagination. For more web data analytics notebooks,  please see my repository. If  you have any questions or ideas to share, please contact the author at  tirthajyoti[AT]gmail.com . Also you can check author’s  GitHub repositories  for other fun code snippets in Python, R, or MATLAB and machine learning resources. If you are, like me, passionate about machine learning/data science, please feel free to  add me on LinkedIn  or  follow me on Twitter."
Building a Successful Modern Data Analytics Platform in the Cloud,uilding a Successful Modern Data Analytics Platform in the Clou,"I worked with dozens of companies migrating their legacy data warehouses or analytical databases to the cloud. I saw the difficulty to let go of the monolithic thinking and design and to benefit from the modern cloud architecture fully. In this article, I’ll share my pattern for a scalable, flexible, and cost-effective data analytics platform in the AWS cloud, which was successfully implemented in these companies."
Data Analytics is hard… Here’s how you can excel,ata Analytics is hard… Here’s how you can exce,"I really enjoyed working in data. Since my sophomore year, I already knew that I want to use technology as a way to solve real business problems. It gives me meaning to work and I hope it does for you as well. Every day I would squeeze every bit of my time to read and learn. I would sign up for the local Data Hackathon and organize Analytics related events to learn from industry leaders. I spoke in conferences and university events to mentor my juniors on how to succeed as a data analyst in large tech companies. I imagined myself as a fellow student to influence as many of my juniors as possible to learn and give back to communities. If you say that this end goal might sound too noble, you are right! In fact, as a Christian, I learnt that time is precious, the mother earth is a terminal where we only reside temporarily. Nothing in the world is lasting including time. So we need to create and deliver as much value as possible to prepare for the everlasting life with God ahead. For me, my religion and data career has given me meanings to excel, to learn and to contribute. After my admittance at Google as a Data Analyst to develop better ML Models to fight abuse, I have received many requests to share my life journey and tips for my juniors to ponder on. Therefore, I really hoped that this blog could fulfill that demand to give you the starting points to be successful as a data professional. As what we have known, data science and analytics have become a fast moving industry with the highest growth over the past few years. Just within a few years many universities started providing specializations in Data Science with thousands of sign ups from all around the world. Even in Singapore, there were no such programs 5 years ago, but now, the entrance of this degree has become as hard as getting into law and business school. However, despite the rising trend, there are so many uncertainties about where the excitement is going. The job market for data scientist is becoming more and more saturated and delusional.  Many startups are starting to realize that they are moving too fast in data science and start laying off their data scientists. Therefore, to secure your future, you will need to mature quickly and differentiate yourself from your peers. Just like a war, You would need to start preparing your armory. The best way, is to contribute more: learn, write, speak, specialize, and chill more Keep experimenting with your learning styles (kinesthetic, auditory, visual). When I was learning, my friends always gave me lists of curated Machine Learning materials. But after reading and listening to lots of videos, I realized that I was a kinesthetic type (learnt by doing) and I retained very few from listening. Knowing that, I created my own projects which I documented on Github.  Surprisingly, these projects had become the key for me to get into Visa and Google . Similarly, do not blindly follow the conventional learning materials that your friends suggest. Do your due diligence and always have a trial and error mindset. Soon, you will find your best learning style to boost your skills. For me, I usually use many different kinds of sources from Youtube and research paper. Personally I enjoyed  Sentdex  and  Computerphile . Highly recommend for you to watch these videos and even better, reproduce them. Furthermore, I am also taking a part time  online master degree at Georgia Tech  which exposed me to deeper technical rigor of machine learning and statistics. Always experiment, trial and error for you to learn quickly about this exciting industry. Write articles, share your codes to github, even better create a Youtube channel. During my university time, I started writing online tutorials for my juniors to tackle their university projects. Once I formed those tutorials, I would set up some small sessions to nurture a study group and shared some machine learning related models. By sharing, I had fun and learnt to articulate my thoughts. Similarly, I also believe you will learn more as you write more. Every time you write some projects, you will be able to reflect on a certain model/test/code review. It will allow you to close your gap of knowledge and figure out how to up skill and find better solutions. So far, I find  Medium  as one of the best channels to write on. It gives a sleek and standardized look for everyone to write, which free you the hassle to deal with the visual layout. Everything in Medium is already optimized for you to read and write. Even better, you can also sign up for the metered paywall. This will give the opportunity for curators to distribute your articles and improve your readership. So far, I have released 30 articles with a few producing $300 USD per article. Honestly, I think the best benefit of writing is that you get the chance to articulate your learning. It does not matter whether you blog online or even make a youtube channel. The goal is to maximize your time to learn and promote yourself. Teach your fellow peers or any conferences out there! When you speak, you are distributing your knowledge for others to use. You are promoting both yourself and your company. In the long run, you would be the more valuable data scientist because you have inspired your juniors to do the same. You will differentiate yourself with your peers. Furthermore, speaking will give you further meaning to learn. The more you learn the more you speak. Similarly the more you speak, the more you learn. Initially, I would send emails to university, data conference, and youth groups to see if I could share some of my writings in data science. I have been fortunate that a few students groups at Singapore Management University (SMU) and National University of Singapore (NUS) have been receptive to my requests. When I speak about my knowledge, I not only inspire my juniors but also learn from them to communicate my thoughts well. Try to dig deep into a certain data science technique to complement your strengths (Business, Social Science, Sales, etc) Have your own mindset and stick with it. Most of the common misconceptions are that business students would lose to IT students due to the needs of technical expertise. This is not accurate. A lot of superstar analysts I know come from various backgrounds such as Social Science, Business, and Economics. They use analytics to complement their expertise. Therefore, stay calm and leverage your strengths rather than indulging on the latest Kaggle’ish analytics trends such as Random Forest, XGBoost, etc For example, if you come from a finance background, you can develop your own stocks analysis project. If you come from operations and inventory management, you can focus on JIT (Just In Time) analytics to minimize bottlenecks and maximize efficiency. The sky’s the limit when you use analytics to make data driven analysis on your domain. For me, I come from a software development background. But I used business analytics to communicate my skills by developing a simple product. One day, I asked my friends who are finance students how they analysed stocks. Their answers surprised me as they used most time to copy and paste each data from Google Finance into an excelsheet and analysed them. As a response, I created a simple program to web scrape and generate intrinsic values using Value Investing methodology.  This has become my flagship product that I share with my fellow students at SMU. Avoid burnout and have fun while hacking Personal relationship is important in life. Therefor keep hacking with your friends. Use your Saturdays to learn and Sundays to chill. Furthermore, Join a learning minded companies. Visa and Google are a few of them. The best thing about working in tech companies is that you get paid to learn critical skills that would be in demand in the years ahead. For me, I am very happy to avoid burning out by chilling with my friends at church. Whenever I have free time, I would work out.  I would train for marathon, triathlon, and even cycling trip overseas (recently 515 km all over 3 days at Java Island Indonesia) . It is very busy, but it is very fulfilling and I would re-energize once the new Monday starts. Soli Deo Gloria. I really hope this has been a great read and a source of inspiration for you to develop and innovate. Please  Comment  out below for suggestions and feedback. Just like you, I am still learning how to become a better Data Scientist and Engineer. Please help me improve so that I could help you better in my subsequent article releases. Thank you and Happy coding :) Vincent Tatan is a Data and Technology enthusiast with relevant working experiences from Google LLC, Visa Inc. and Lazada to implement microservice architectures, business intelligence, and analytics pipeline projects . Vincent is a native Indonesian with a record of accomplishments in problem-solving with strengths in Full Stack Development, Data Analytics, and Strategic Planning. He has been actively consulting SMU BI & Analytics Club, guiding aspiring data scientists and engineers from various backgrounds, and opening up his expertise for businesses to develop their products . Vincent also opens up his 1 on 1 mentorship service on  BestTop  and 10to8 to coach how you can land your dream Data Analyst/Engineer Job at Google, Visa or other large tech companies.  book your appointment with him here  if you are looking for mentorship. Lastly, please reach out to Vincent via  LinkedIn ,  Medium  or  Youtube Channel"
Advice to a Data Analytics Newbie.,dvice to a Data Analytics Newbie,"Data Analysts are recognized as some of the most sought after tech talents with increasing demand and companies are prepared to pay extremely competitive salaries. It’s no surprise that more and more people are looking to switch into the field from other fields including non-technical fields. However, making a career switch isn’t just about the salary specs and job security but you also want to be certain that it is a role you will excel in. What do Data Analysts do? The job description of a data analyst as well as the tools needed to get the job done varies from company to company so for now you only need to get familiar with the general roles and tools. As a newbie to the Data analytics field, you may get overwhelmed with overload of information, as there are a lot of available information on the internet. I believe setting both big and small learning goals is a technique that will help you make progress. Here are some tips that have helped me since I started my journey in July 2021 and I hope you find them useful as you start your own journey. If you answered YES to the above questions, you‘d be an excellent data analyst! 3. Join Data communities:  The importance of community in the tech space cannot be over emphasized, as a collaborative effort is necessary for technological advancement. There are communities on different platforms (LinkedIn, Discord, slack, WhatsApp, Twitter, etc.) that you can join to stay motivated, learn from others and ask for help when stuck. I would recommend joining ‘The All about Data’ community. It is an online data community of over 6000 members from varying data fields, ready to assist you. You can join the community on  Twitter ,  Facebook  and  discord . 4. Get mentors:  Mentors can   influence and help you directly and indirectly on your journey. You can follow and even reach out to some professionals for tips and recommendations. Here is a list of 10 data professionals that you can follow online that constantly contribute to the growth of data community, 5 .  Love knowledge:  As an aspiring data analyst, you should be hungry for knowledge and have a ready-to-learn mindset. Choosing this career path means, you’ve agreed and signed up for a life of consistent learning and growing. Data Analytics is still developing so even the professionals are still learning new things. Are you up for the task? 6. Have a ‘Growth over Goal’ mindset : Some days you will meet and exceed your goals, other days you may not but you are still learning and growing regardless. 7. Go slow and steady:  Going fast and hard leads to burnout but going steady ensures you make consistent progress. There is a lot to learn, so do not try to learn everything so fast. Draw up a learning curriculum for the road map you want to follow and move at a reasonable pace. Learning is a lifelong process. Below is a list of tools and skills to include in your data analytics learning curriculum. 8.   Understand the process:  Data analytics is much more than the tools involved. In as much as we all want to master the tools, knowledge about the data analysis process is important. I’d recommend taking the Coursera Google Data Analytics professional course, it explains the analysis process from asking the right questions to giving actionable recommendations from the results. (Check out my  article  on how to get financial aid for the course.) 9.   Self-Development is important:  Don’t just focus on the technical skills, learn other skills like time management, proper planning, presentation, communication, etc. You can read good self-development books and work hard towards becoming your best self. These skills will help you stand out. 10. Share your work online:  The feedback and recommendations you get online will help you grow and improve your skills. However, before sharing your work online with professionals, share with your friends and make the suggested corrections before sharing with a professional for review as they are busy and get a lot of requests daily. Some of the mentors I mentioned earlier, selflessly make out time to review projects for newbies. Follow me on  Twitter  for more data related information."
The Surprising Link between a Company’s Growth and its Data Analytics Strategy,he Surprising Link between a Company’s Growth and its Data Analytics Strateg,"As  a Business Intelligence Analyst, I often wondered, “How does a company’s Data Strategy drive growth?” Typically, vendors would provide the same kinds of answers: Yawn. We’ve all heard these pitches before and working in BI I knew there’s more to better decisions than just more data, more dashboards and more visualizations, especially since everyone just looks at pivot tables. Not to mention getting to insights faster doesn’t ensure you’ll get to the  correct  insight. Faster does not equal better. The other thing to notice is that a lot of contemporary wisdom is just “industry jargon du jour”, which changes almost constantly. I wanted to go “beneath the veneer” to uncover the fundamental relationship between a company’s maturity with respect to its data strategy, and the effect of data maturity on growth. What I found shocked me: Your current Data Strategy may actually be  impeding  your growth. How could this be?  To understand how this is possible, we first need a definition of “Data Maturity.” Measuring Maturity with the  Dell Data Maturity Model I’m sure every business student has seen this picture (or one very similar to it): What I wanted to see was another curve that quantified the maturity over time of a company’s Data Strategy, superimposed upon this growth curve. What I needed first was a way to measure Data Maturity, and then plot that over time. To do this I drew from the teachings of a leading tech company: Dell. CIO.com has an  article  in which they define the “Dell Data Maturity Model” (or DDMM). Given Dell’s cachet this seemed like a good place to start. For anyone who doesn’t eat, breathe or sleep the industry jargon these descriptions may be a little abstract. So, let me build on this by illustrating the  pains  a company feels just before it transitions from one stage of the DDMM to the next. If you find yourself nodding along with one set of pains more than any other you have a good idea of where you are in the Dell Data Maturity Model. You may have noticed the gap between  Data Proficient  and  Data Savvy  is much wider as compared to the other stages. This wasn’t a formatting mistake. There are so many challenges involved in making that particular leap that, naturally, the gap is wider. Most companies never make the transition. In fact, a lot of businesses erroneously begin investing in becoming  Data Driven  (AI, Machine Learning, Data Mining, etc.) by hiring Data Scientists before achieving the  Data Savvy  stage. As a result, most Data Scientists are frustrated that they spend so much time cleaning and plumbing data because the foundation was never laid. In Dell’s Model, Data Maturity grows in steps or stages. But the one dimension missing from this diagram is  time . That is, how long does a company typically spend in each stage of the DDMM? This is an important piece that we’ll dive into right after we talk about the different stages of Growth. Stage 1: The Startup Phase The company has only a few people, anywhere from 1 to 3 employees and a few systems. At this phase you’re building a car and trying to drive it at the same time. Growth will be slow and data-driven decision-making usually takes the form of ad-hoc spreadsheets. Stage 2: The Scale-up Phase This stage is all about trying to scale-out your production and sales efforts and just get product out the door. People are buying as fast as you’re producing, and you have little concern for inefficiencies as you skip over bumps in the road. Think of it this way: Your company is a brand-new sports car and you’re driving around with the parking-brake up and a parachute strapped to the back. But it doesn’t matter. Your growth engine is so powerful you’re just blazing past everyone else. But, a big enough pot-hole or bump in the road, and all that friction catches up with you. Stage 3: The Enterprise Phase The fun, youthful years of exponential growth have come to a close. It’s inevitable but all high performing companies eventually settle into Sustainable Growth. This isn’t a bad thing because your brand recognition also makes you King of the Hill. It just means growth over time will be linear, instead of exponential, and inefficiencies play a bigger role in slowing down growth. Why Data Maturity Lags Growth over Time This is where the rubber meets the road. How long does a company spend in each of these Data Maturity stages, and when   does a “transition” occur relative to growth? Let’s quickly define a “transition” as a step-wise improvement in a company’s Data Maturity. These are typically tied to events like hiring a BI Consultant, eventually a BI Team, or building a Data Warehouse. The purchase of technology is typically a part of all these transitions. To track Data Maturity against Growth over time, we combine a few things: In my experience, these things don’t just neatly line up when you overlay them, and that’s the reason we find an interesting pattern in most companies. If you add time to the Dell Data Maturity Model you end up with a  Data Maturity Curve  which roughly illustrates how long most successful companies stay in each stage of the DDMM. After overlaying revenue growth, in my experience, you see this pattern emerge: Now, as you can see, Data Maturity grows in steps, typically the result of transitions caused by “precipitating events” (more on that below). The Data Maturity Curve isn’t so much a “curve” as it is a series of steps. If, for the purpose of illustration, we fit a curve to those steps and overlaid that, we’d see the following: While I understand that (cross)correlation does not imply causation, what I expected to see was that as companies implement data assets, technologies or strategies, its Data Maturity Curve would lead ahead of its Growth Curve. And while higher Data Maturity doesn’t necessarily cause higher Growth, a leading Data Maturity Curve would at least have been in-line with my expectations. What I’ve found is the opposite. The maturity of a company’s Data Strategy (its Data Maturity Curve) often  lags  its Growth Curve . Time and time again I have found this to be true of companies. A business’s Data Strategy (the different stages of the DDMM) is always behind the current state of growth. So, if you’re an executive leading a growing company, and reading this article right now, your company has already outgrown its current data strategy. Why is this? Because data-related pains don’t suddenly appear overnight. Unlike any other pain that physically prevents you from doing business (POS failures, internet outages, etc.), Data Pains are subtle, pernicious, chronic pains that compound. It’s a lot like lower back pain, something many of us just learn to live with. It’s there. It sucks. But we get on with our day. Startups in the Data Aware stage barely feel it. But if left untreated that pain can show up in the worst and most unexpected ways, and you will pay good money to make it go away. Anyone who’s spent an evening lying on the couch due to back pain will understand this. It’s these sorts of “precipitating events” that motivate step-wise improvements in a company’s Data Maturity. The above graphs are meant to illustrate a specific point: The maturity of a company’s data strategy  lags  company growth. So, you’re almost always behind the curve when it comes to your Data Strategy. But the keen reader should realize that, although we have graphed pain (and maturity) over time, they are not causally linked to time. They’re linked to  growth . This is the key distinction here. The pains themselves are tied to growth, not time, technology or the data itself. Untreated pains compound not over  time  but over  growth . Likewise,  new  pains occur due to growth, not the passage of time. You may even call them Data Growing Pains. Data-related pains tend to become unbearable at certain levels of growth. In the above graph we use revenue as a measurement of growth, but this is also very true of headcount, number of locations, countries in which the company does business, number of SKUs, and so on. This chart shows approximately where data pains become almost unbearable for  Apparel  businesses: Different industries will have different milestones in terms of revenue or headcount at which data pains become unbearable, but the general shape of the Pain vs. Growth curve will be the same. The pain will drop soon after a company undergoes a “transition” to move from one stage of maturity to the next. What happens if you never invest in your Data Strategy? Some executives believe the first step of data integration “is a showstopper,” leaving them stuck at a nascent level of Data Maturity despite their growth. This is a  bad  situation to be in, because not only will you inherit all the new problems associated with growth, the original problems which went unsolved just get worse. Fortunately, most executives realize the need for Business Intelligence.  Unfortunately , due to a lack of available technology, industry hype around Big Data and AI, or poor guidance from industry experts, most companies never cross the chasm between  Data Proficient  (having a BI system) and  Data Savvy  (Decision Support System enabled Centre of Excellence). In fact, some companies try to skip the  Data Savvy  stage entirely and go straight to  Data Driven  (using Data Science). As a result, most companies’ Pain v. Growth curve actually looks like this: The silver lining here is that, while the pain will continue to worsen with growth, it doesn’t get exponentially worse. Most companies think that the solution here is to invest in AI and Big Data, and while they certainly have the data quantity to do this, they don’t have the foundations in place to act  nimbly  on the insights (or opportunities) that come from Big Data or AI. It’s also important to realize that if a company never expands beyond a certain level of growth, it’ll never feel the (new) pains that accompany that (new) growth. If the business never adds countries, stores, SKUs, warehouses, customer types and so on, they’ll never need to transition to a higher stage of Data Maturity. This is important for any executive who believes he or she can simply “deal with it later” and “focus on other priorities.” You cannot delay your data pains unless you also plan on delaying your company’s growth. In a future post we’ll describe what these pains look like and how they manifest in the day-to-day operations of a business. How Process can be a Competitive Advantage You may be thinking, “OK, Company Growth causes Data Pains which, when solved, result in step-wise maturity in our Data Strategy. But who cares? We’re still growing in spite of that.” Maybe you’ll continue to grow. Maybe not. The question you should be asking is, “How much money are we leaving on the table by not solving these problems?” Another question to ask is, “How long before these inefficiencies start to catch up with us?” and “Will it cause a premature slow-down in growth?” The best question to ask is, “How bad is this problem for our larger competitors?” Dashboarding and Business Intelligence have become commoditized and easily accessible for most companies. This is a good thing, and especially beneficial to companies on the smaller side. Tech giants are filing patents on hardware and software that make it faster and more economical to crush Big Data and incorporate AI. This is useful to ultra-big enterprises. But what about everyone in-between? If you review the pains listed in this article, you’ll notice a lot of them relate to the  process  of using data to make decisions. Collection, cleaning and integration play a (very big) part in the early stages of growth, but process, people and business unit integration play an equally big part in later stages. While most of the data-industry is focusing on the sexy things like AI and Big Data, the truth is most companies will have problems with the  process  of making decisions with the data they  currently  have long before they are ready for AI and Big Data. Many large enterprises never truly make the leap between  Data Proficient  to  Data Savvy , with leadership choosing instead to hire Data Scientists and invest in sexier technologies. This presents a huge opportunity for Scaleups. When you think about patents you think about technology as a competitive advantage. Not many people think about  process , which is as equally valid a thing to patent. And while I’m not saying you need to run out and file a patent on how you make decisions with data, what I  am  saying is that if you can nail a process that allows you to be nimble with your data early in your growth curve then you have a huge advantage over larger companies that didn’t. A Scaleup’s major competitive advantage is (still) having the ability to jump into a meeting room, pull up some numbers and make decisions. If a company can retain that agility throughout its growth it can continue to run circles around its larger rivals. The net effect is reducing the lag between the Growth Curve and the Data Maturity Curve and, ultimately, the data pains associated with growth. In future we’ll dive deeper into  how  this can be done. Key Takeaways This story is published in  Noteworthy , where thousands come every day to learn about the people & ideas shaping the products we love. Follow our publication to see more product & design stories featured by the  Journal  team."
Architecting a Successful Modern Data Analytics Platform in the Cloud,rchitecting a Successful Modern Data Analytics Platform in the Clou,"After we discussed the concepts for  Building a Successful Modern Data Analytics Platform in the Cloud , it is time to architect it. This post will review the reference architectures for our scalable, flexible and robust design in both Amazon Web Services (AWS) and Microsoft Azure. I worked in the last 20 years with hundreds of companies to harness the ever-evolving technology tools to bring to life business ideas. Some companies were startups born to the cloud age…"
A 22-week curriculum to learn Data Analytics in 2022 + FREE Resources, 22-week curriculum to learn Data Analytics in 2022 + FREE Resource,"It’s always day one in tech! As the entire world countdowns to welcome the new year with hope and aspirations, I sit in front of my laptop making a roadmap to upskill and get job ready in 2022. In this roadmap each week discusses a different topic and has roughly 4–5 hours of content so you can spend the rest of the time building things and not get stuck in tutorial hell. Always remember tech is NOT a pen paper industry, get your hands dirty, add those projects and build a portfolio. SQL is the meat and potatoes of data analytics and is used in almost any job in the data industry. A great tutorial for the same is - mode.com Any one of the data visualization tool is a must have. You can choose any but I prefer Tableau because of it’s simplicity. Excel is a very important tool for Data Analysts. Be sure to take it all in with VBA, macros and learn while building. Statistics is vast and immensely useful when it comes to data and it’s better to know how to whip out all the nitty gritty of the data in a giphy with it. Here’s a free tutorial by Udacity: classroom.udacity.com Python is a helpful tool to automate things and one can do almost everything ( visualization , data cleaning , wrangling , modelling ) with it’s vast set of libraries. Here is a free tutorial by Udemy : https://www.udemy.com/course/master-data-analysis-with-python-intro-to-pandas/ Although you need just one data viz tool, I consider the basics of one more a plus. Google Analytics is a great tool for data collection and analysis, and while it is not a must I consider it a win win. Google has a free course and certification for it . analytics.google.com This is not a must have and you can totally skip it. I read about it and found it’s a great tool to generate CRM reports a for Business Analysts, so I added it to the list. https://trailhead.salesforce.com/en/users/strailhead/trailmixes/build-your-business-analyst-career-on-salesforce A very important and useful application widely used by almost any firm. I just felt it required extra attention. Here’s a free course by Udacity - classroom.udacity.com As much as we hate DSA it is still an integral part of the hiring process for many organizations. It’s better to be safe and practice it at least in a familiar language Python. Here’s a free course by Google for it: classroom.udacity.com Now that you are familiar with all the tools it is necessary to know to get data and work with it. Internet is the biggest source of data and being able to get it out of any website puts you ahead. Numpy is one of the most important libraries , it helps you transform and utilize numerical data to your liking. Python has a lot of useful libraries. One of the essential ones for a Data Analyst is matplotlib which is used for data visualization. With Big Data emerging as the next big thing, analysts who can leverage it to gain insights would be extremely beneficial to the companies. Here’s a free SQL course on Google’s BigQuery platform: www.kaggle.com Business not just want to gain insights from existing data but also want to forecast the unknown. Machine learning comes into play here and Kaggle’s platform with it’s micro courses are the best place to get started. www.kaggle.com All of this would make you familiar with all the skills required to start a career in data. But all of this by itself is not enough, you need to practice build projects and network. Happy new year! A blog on projects dropping soon. Stay tuned!"
You Should Master Data Analytics First Before Becoming a Data Scientist,ou Should Master Data Analytics First Before Becoming a Data Scientis,"While it may seem obvious at first to state that knowing Data Analytics before learning Data Science is key, it might surprise you then how many people jump right into Data Science without the right foundation of analyzing and presenting data. There are certain benefits to having either an internship, entry-level position, or any position really in Data Analytics beforehand. It is also important to note that this form of experience can be acquired by completing online courses and specializations in Data Analytics. That being said, if you already have a formal education in Data Science, you might already be learning the foundation of Data Analytics in one course only — most likely, which is why it is essential to add a few Data Analytics-focused learnings into your portfolio. However, the best way is to have some sort of Data Analytics practiced with other people as you will see below when I discuss the top four benefits of mastering Data Analytics before learning Data Science. As you specialize in Data Analytics, it is no surprise that you would become efficient at exploring data. As a Data Scientist, this is usually the first step of the Data Science process, so if you skip practicing this step, your model could result in error, confusion, and misleading results. You must keep in mind that garbage in creates garbage out. Just because you throw a dataset at a Machine Learning algorithm does not mean it will answer the business question at hand. You will have to find anomalies in the data, aggregations, missing values, transformations, preprocessing, and much more. Understanding the data first is of course important so being a master at Data Analysis is crucial. There are a few Python ( and R as wel l) libraries that help do this automatically. However, I often find, with large datasets that they take way too long and can cause your kernel to crash and you have to restart. That is why it is important to have a manual eye at the data too. That being said, there is a large dataset mode for the library that I will present below that can skip some of the expensive and longer-lasting computations. The parameter for this situation is within the profile report of the Pandas Profiling library:  minimal=True . Here is one particular library that is plenty easy to use: Pandas profiling  [3], can be viewed in your Jupyter Notebook. Some of the unique features of this library include, but are not limited to type inference, unique values, missing values, descriptive statistics, frequent values, histograms, text analysis, and file as well as image analysis. Other than this library, overall, there are countless ways to practice exploratory data analysis, so if you have not already, find a course and master analyzing data. Data Scientists can often learn complex Machine Learning algorithms pretty quickly in their education, skipping the important part of communicating with stakeholders to achieve a goal and articulate the Data Science process. If you have not noticed already, you will have to become a master at translating a business use case into a Data Science model. A Product Manager or other stakeholder will not come up to you and ask you to create a supervised Machine Learning algorithm with 80% accuracy. What they will do is tell you about some data, and what problem they keep seeing, you will have little guidance on Data Science, which of course is expected, because that is your job. You will have to come up with the idea of regression, classification, clustering, boosting, bagging, etc. You will have to work with them as well in order to set up success criteria — for example, what does 100 RMSE mean — and how can you address and translate it to meaningful business problems to stakeholders. So, how can you learn collaboration? Working as a Data Analyst beforehand often requires plenty of collaboration more often than that of a Data Scientist. You will create metrics, make visualizations, and develop analytical insights from working with others almost daily or at least weekly as a Data Analyst. This practice is vital in becoming a better Data Scientist as we have learned from above. Benefits of stakeholder collaboration practice through Data Analytics roles: As you can see collaborating with stakeholders is an important part of both the Data Analyst and Data Scientist positions. As a Data Scientist, you will have to perform feature engineering, where you will isolate key features that contribute to the prediction of your model. In school or wherever you learned Data Science, you may have a perfect dataset that is already made for you, but in the real world, you will have to use SQL to query your database to start finding the necessary data. In addition to the columns that you already have in your tables, you will have to make new ones — usually, these are new features that can be aggregated metrics like  clicks per user , for example. As a Data Analyst, you will practice SQL the most, and as a Data Scientist, it can be frustrating if all you know is Python or R — and you can not rely on Pandas all the time, and as a result, you cannot even start the model building process without knowing how to efficiently query your database. Similarly, the focus on analytics can allow you to practice creating subqueries and metrics like the one stated above so that you can add a few to at least, say 100, new features that are completely created from you that could be more important than the base data that you have now. Benefits of feature creation: A Data Analyst usually will master visualizations because they have to present findings in a way that is easily digestible for others in the company. Having a complex table full of values can be confusing and frustrating to read, so having the ability to highlight important metrics, insights, and results is extremely beneficial to know as a Data Scientist, too. Similarly, when you are finished with your complex Machine Learning algorithm that you have utilized to build your final model, you will be excited to share your results; however, stakeholders will need to know only the highlights and key takeaways. The best way to do this process through visualization, and here are some of the key ways to create those visualizations: Of course, there are more, but here are the ones I often see used the most. By articulating insights and results through visualizations, you also help yourself to learn the process and takeaways better. So the question is, should you become a Data Analyst first before becoming a Data Scientist?  I say yes — or at least some form of it, whether that be an internship, job, a similar job like that of a Business Analyst, or becoming certified in a Data Analytics course. In addition to the four benefits that I have discussed above, another one to highlight is that it could certainly help you to land a job as a Data Scientist if you have the title or experience of Data Analytics on your resume. To summarize, here are some of the important benefits to becoming a master in Data Analytics first before becoming a Data Scientist: I hope you found my article both interesting and useful. Please feel free to comment down below if you have become a Data Analyst first in some way before becoming a Data Scientist. Has it helped you in your Data Science career now? Do you agree or disagree, and why? Please feel free to check out my profile and other articles, as well as reach out to me on LinkedIn. [1] Photo by  NEW DATA SERVICES  on  Unsplash , (2018) [2] Photo by  Lukas Blazek  on  Unsplash , (2017) [3] Pandas,  Pandas Profiling , (2021) [4] Photo by  DocuSign  on  Unsplash , (2021) [5] Photo by  Myriam Jessier  on  Unsplash , (2020) [6] Photo by  William Iven  on  Unsplash , (2015)"
"Web Scraping, Regular Expressions, and Data Visualization: Doing it all in Python","eb Scraping, Regular Expressions, and Data Visualization: Doing it all in Pytho","As with most interesting projects, this one started with a simple question asked half-seriously: how much tuition do I pay for five minutes of my college president’s time? After a chance pleasant discussion with the president of my school ( CWRU ), I wondered just how much my conversation had cost me. My search led to  this article , which along with my president’s salary, had this table showing the salaries of private college presidents in Ohio: While I could have found the answer for my president, (SPOILER ALERT, it’s $48 / five minutes), and been satisfied, I wanted to take the idea further using this table. I had been looking for a chance to practice  web scraping  and  regular expressions  in Python and decided this was a great short project. Although it almost certainly would have been faster to manually enter the data in Excel, then I would not have had the invaluable opportunity to practice a few skills! Data science is about solving problems using a diverse set of tools, and web scraping and regular expressions are two areas I need some work on (not to mention that making plots is always fun). The result was a very short — but complete — project showing how we can bring together these three techniques to solve a data science problem. The complete code for this project is available as a  Jupyter Notebook  on  Google Colaboratory  (this is a new service I’m trying out where you can share and collaborate on Jupyter Notebooks in the cloud. It feels like the future!) To edit the notebook, open it up in Colaboratory, select file > save a copy in drive and then you can make any changes and run the Notebook. While most data used in classes and textbooks just appears ready-to-use in a clean format, in reality, the world does not play so nice. Getting data usually means getting our hands dirty, in this case pulling (also known as scraping) data from the web. Python has great tools for doing this, namely the  requests  library for retrieving content from a webpage, and  bs4  (BeautifulSoup) for extracting the relevant information. These two libraries are often used together in the following manner: first, we make a GET request to a website. Then, we create a Beautiful Soup object from the content that is returned and parse it using several methods. The resulting soup object is quite intimidating: Our data is in there somewhere, but we need to extract it. To select our table from the soup, we need to find the right  CSS selectors . One way to do this is by going to the webpage and inspecting the element. In this case, we can also just look at the soup and see that our table resides under a  <div>  HTML tag with the attribute  class = ""entry-content""  . Using this info and the  .find  method of our soup object, we can pull out the main article content. This returns another soup object which is not quite specific enough. To select the table, we need to find the  <ul>  tag (see above image). We also want to deal with only the text in the table, so we use the  .text  attribute of the soup. We now have the exact text of the table as a string, but clearly is it not of much use to us yet! To extract specific parts of a text string, we need to move on to regular expressions. I don’t have space in this article (nor do I have the experience!) to completely explain regular expressions, so here I only give a brief overview and show the results. I’m still learning myself, and I have found the only way to get better is practice. Feel free to go over  this notebook  for some practice, and check out the Python  re   documentation  to get started (documentation is usually dry but  extremely  helpful). The basic idea of regular expressions is we define a pattern (the “regular expression” or “regex”) that we want to match in a text string and then search in the string to return matches. Some of these patterns look pretty strange because they contain both the content we want to match and special characters that change how the pattern is interpreted. Regular expressions come up all the time when parsing string information and are a vital tool to learn at least at a basic level! There are 3 pieces of info we need to extract from the text table: First up is the name. In this regular expression, I make use of the fact that each name is at the start of a line and ends with a comma. The code below creates a regular expression pattern, and then searches through the string to find all occurrences of the pattern: Like I said, the pattern is pretty complex, but it does exactly what we want! Don’t worry about the details of the pattern, but just think about the general process: first define a pattern, and then search a string to find the pattern. We repeat the procedure with the colleges and the salary: Unfortunately the salary is in a format that no computer would understand as numbers. Fortunately, this gives us a chance to practice using a Python  list comprehension  to convert the string salaries into numbers. The following code illustrates how to use string slicing,  split  , and  join , all within a list comprehension to achieve the results we want: We apply this transformation to our salaries and finally have the all info we want. Let’s put everything into a  pandas  dataframe. At this point, I manually insert the information for my college (CWRU) because it was not in the main table. It’s important to know when it’s more efficient to do things by hand rather than writing a complicated program (although this whole article kind of goes against this point!). This project is indicative of data science because the majority of time was spent collecting and formatting the data. However, now that we have a clean dataset, we get to make some plots! We can use both  matplotlib  and  seaborn  to visualize the data. If we aren’t too concerned about aesthetics, we can use the built in dataframe plot method to quickly show results: To get a better plot we have to do some work. Plotting code in Python, like regular expressions, can be a little complex, and it takes some practice to get used to. Mostly, I learn by building on answers on sites like Stack Overflow or by reading  official documentation . After a bit of work, we get the following plot (see notebook for the details): Much better, but this still doesn’t answer my original question! To show how much students are paying for 5 minutes of their president’s time we can convert salaries into $ / five minutes assuming 2000 work hours per year. This is not necessarily a publication-worthy plot, but it’s a nice way to wrap up a small project. The most effective way to learn technical skills is by doing. While this whole project could have been done manually inserting values into Excel, I like to take the long view and think about how the skills learned here will help in the future. The process of learning is more important than the final result, and in this project we were able to see how to use 3 critical skills for data science: Now, get out there and start your own project and remember: it doesn’t have to be world-changing to be worthwhile. I welcome feedback and discussion and can be reached on Twitter  @koehrsen_will ."
15 Stunning Data Visualizations (And What You Can Learn From Them),5 Stunning Data Visualizations (And What You Can Learn From Them,"We’re drowning in data. Everyday,  2.5 quintillion bytes of data are created . This is the equivalent of 90% of the world’s information–created in the last two years alone. Now this is what we call “big data.” But where does it come from? Everywhere, from sensors and social media sites to digital images and videos. We have more data than we know what to do with, so it’s time now to organize and make sense of it all. This is where data visualization comes into the picture. In the seismic shift awaiting us, referred by some as the  Industrial Revolution of Data , we have to get better and more efficient at creating innovative data visualization that make the complex easy to understand. In the hopes of inspiring your own work, we’ve compiled 15 data visualizations that will not only blow your mind, they will also give you a clearer understanding of what makes a good visualization–and what makes a bad one. The interactive piece  The Daily Routines of Famous Creative People  is a perfect example of a data visualization that combines all the necessary ingredients of an effective and engaging piece: It combines reams of data into a single page; it uses color to easily distinguish trends; it allows the viewer to get a global sense of the data; it engages users by allowing them to interact with the piece; and it is surprisingly simple to understand in a single glance. The Year in News  is a good example of how an expertly executed data visualization can reveal patterns and trends hiding beneath the surface of mountains of data. By analyzing 184.5 million Twitter mentions, Echelon Insights was able to provide a bird’s eye view of what America was talking about in 2014. This U.S. age pyramid created by the Pew Research Center is a noteworthy example of how shifts and trends over time can be effectively communicated through the use of well-executed animation. Not only does this type of data visualization pack a whole lot of information into a single visual–23 bar charts were combined into a single GIF composite–it can be easily shared on social media and embedded anywhere. With so many data visualizations out there nowadays, it can be hard to find a unique angle that hasn’t been explored already. Not the case, though, with this wonderfully imaginative series of infographics created by designer Marion Luttenberger. Using real-life images as the basis of his infographics, Luttenberger was able to craft an entire annual report for an organization that provides aid to drug addicts in Austria–and still communicate the organization’s mission clearly and effectively. An effective way to communicate complex ideas is by using symbols and metaphors. Take, for example, this ambitious data visualization of the Internet created by Ruslan Enikeev. By using the metaphor of planets in a solar system, Enikeev is able to create a “Map of the Internet” that helps users visualize the relative reach and influence of every site out there. The amount of website traffic, for example, is represented by the size of the circle on the map. One of the great strengths of data visualizations is their unsurpassed ability to put isolated pieces of information into a bigger context. The goal of this insightful interactive piece by Nikon is to give users a sense of the size of objects, both big and small, by using comparisons. Next to the Milky way, for example, a common object such as a ball or a car seem smaller than we ever imagined. Another hallmark of an effective data visualization is its ability to summarize a ton of information and, in the process, save you time and effort. This data visualization, for instance, represents 100 years of the evolution of rock in a single page. Not only does it simplify information for you–condensing a century’s worth of information into a piece that can be viewed in less than a minute–it also provides actual audio samples for each genre, from electronic blues to dark metal. As humans, we cannot help but see the Universe and life from our own self-centered and completely unique point-of-view. This data visualization, however, gives us some perspective on our own lives–and the events of the current day–by placing them in the larger context of time, from the current year to the current millennium. In line with the objective of making the complex easy to understand, this infographic provides a visual representation of a coffee bean’s journey, from bean to cup. By breaking the process down into parts, this data visualization does its job of giving the reader bite-sized pieces of information that are easily digestible. A good infographic will not only do the hard work of digesting complex data, it may also stimulate readers’ imagination by allowing them to conjure up different hypothetical situations and possibilities, as is done in this example. By presenting an interactive, game-like experience, this infographic quickly engages the user and keeps them interested from beginning to end. This infographic takes dense material, such as indicators and figures, and presents it in a beautiful, clean and captivating format. Not only is the design deceptively simple and functional, it also provides the user with many options for interacting with the graphic, such as adding countries, indicators and type of relation. An effective data visualization not only conveys information in a convincing manner, it also narrates a story worth telling. This piece, for example, tells the story of every known drone strike and victim in Pakistan. By distilling information into an easily understandable visual format, this infographic dramatically brings to light disturbing facts that should not go unnoticed. This data visualization not only has all of the previous qualities mentioned, it also allows the user to have direct access to all the original raw data (view link on bottom right corner). Also, by using bubbles shaped in accordance with the size of the data breach, the viewer can get a solid overview of the data breach “landscape.” And if viewers want to get into the details of the information, they can also go as deep or as superficially as they want by navigating the different filters and raw data. In line with the global trend of democratizing access to information and empowering users, this data visualization does an excellent job of demystifying the process of balancing the national budget. By placing budget balancing in the hands of everyday users, this project taps into the power of collective thinking to solve big problems. This piece goes beyond a common data visualization to become an educational and interactive minisite. By combining enough data and information to fill an encyclopedia into a single interactive application, this data visualization becomes a useful classroom tool for students learning about wind, ocean and weather conditions. Interactive data visualizations are unique in that they appeal to several of the five senses: to the  sense of hearing  through audio, the  sense of sight  through stunning visuals and the  sense of touch  through the interactive experience of clicking, hovering and scrolling through content. While you may think that data visualizations are too costly and time consuming to produce on your own, you can explore several free tools out there that allow non-designers and non-programmers to create their own interactive content. Visme, for example, is an online tool that allows you to create interactive charts, graphs and maps. You can try it for free  here . And if you want to receive additional tips and guides on becoming a better visual communicator, don’t forget to sign up for our weekly newsletter below. The  original version of this post  first appeared on  Visme’s Visual Learning Center ."
5 Quick and Easy Data Visualizations in Python with Code, Quick and Easy Data Visualizations in Python with Cod,"Want to be inspired? Come join my  Super Quotes newsletter . 😎 Data Visualization is a big part of a data scientist’s jobs. In the early stages of a project, you’ll often be doing an Exploratory Data Analysis (EDA) to gain some insights into your data. Creating visualizations really helps make things clearer and easier to understand, especially with larger, high dimensional datasets. Towards the end of your project, it’s important to be able to present your final results in a clear, concise, and compelling manner that your audience, whom are often non-technical clients, can understand. Matplotlib is a popular Python library that can be used to create your Data Visualizations quite easily. However, setting up the data, parameters, figures, and plotting can get quite messy and tedious to do every time you do a new project. In this blog post, we’re going to look at 5 data visualizations and write some quick and easy functions for them with Python’s Matplotlib. Just before we jump in, check out the  AI Smart Newsletter   to read the latest and greatest on AI, Machine Learning, and Data Science! Scatter plots are great for showing the relationship between two variables since you can directly see the raw distribution of the data. You can also view this relationship for different groups of data simple by colour coding the groups as seen in the first figure below. Want to visualise the relationship between three variables? No problemo! Just use another parameters, like point size, to encode that third variable as we can see in the second figure below. All of these points we just discussed also line right up with the first chart. Now for the code. We first import Matplotlib’s pyplot with the alias “plt”. To create a new plot figure we call  plt.subplots()  . We pass the x-axis and y-axis data to the function and then pass those to  ax.scatter()  to plot the scatter plot. We can also set the point size, point color, and alpha transparency. You can even set the y-axis to have a logarithmic scale. The title and axis labels are then set specifically for the figure. That’s an easy to use function that creates a scatter plot end to end! Line plots are best used when you can clearly see that one variable varies greatly with another i.e they have a high covariance. Lets take a look at the figure below to illustrate. We can clearly see that there is a large amount of variation in the percentages over time for all majors. Plotting these with a scatter plot would be extremely cluttered and quite messy, making it hard to really understand and see what’s going on. Line plots are perfect for this situation because they basically give us a quick summary of the covariance of the two variables (percentage and time). Again, we can also use grouping by colour encoding. Line charts fall into the “over-time” category from our first chart. Here’s the code for the line plot. It’s quite similar to the scatter above. with just some minor variations in variables. Histograms are useful for viewing (or really discovering)the distribution of data points. Check out the histogram below where we plot the frequency vs IQ histogram. We can clearly see the concentration towards the center and what the median is. We can also see that it follows a Gaussian distribution. Using the bars (rather than scatter points, for example) really gives us a clearly visualization of the relative difference between the frequency of each bin. The use of bins (discretization) really helps us see the “bigger picture” where as if we use all of the data points without discrete bins, there would probably be a lot of noise in the visualization, making it hard to see what is really going on. The code for the histogram in Matplotlib is shown below. There are two parameters to take note of. Firstly, the  n_bins  parameters controls how many discrete bins we want for our histogram. More bins will give us finer information but may also introduce noise and take us away from the bigger picture; on the other hand, less bins gives us a more “birds eye view” and a bigger picture of what’s going on without the finer details. Secondly, the  cumulative  parameter is a boolean which allows us to select whether our histogram is cumulative or not. This is basically selecting either the Probability Density Function (PDF) or the Cumulative Density Function (CDF). Imagine we want to compare the distribution of two variables in our data. One might think that you’d have to make two separate histograms and put them side-by-side to compare them. But, there’s actually a better way: we can overlay the histograms with varying transparency. Check out the figure below. The Uniform distribution is set to have a transparency of 0.5 so that we can see what’s behind it. This allows use to directly view the two distributions on the same figure. There are a few things to set up in code for the overlaid histograms. First, we set the horizontal range to accommodate both variable distributions. According to this range and the desired number of bins we can actually computer the width of each bin. Finally, we plot the two histograms on the same plot, with one of them being slightly more transparent. Bar plots are most effective when you are trying to visualize categorical data that has few (probably < 10) categories. If we have too many categories then the bars will be very cluttered in the figure and hard to understand. They’re nice for categorical data because you can easily see the difference between the categories based on the size of the bar (i.e magnitude); categories are also easily divided and colour coded too. There are 3 different types of bar plots we’re going to look at: regular, grouped, and stacked. Check out the code below the figures as we go along. The regular barplot is in the first figure below. In the  barplot()  function,  x_data  represents the tickers on the x-axis and  y_data  represents the bar height on the y-axis. The error bar is an extra line centered on each bar that can be drawn to show the standard deviation. Grouped bar plots allow us to compare multiple categorical variables. Check out the second bar plot below. The first variable we are comparing is how the scores vary by group (groups G1, G2, ... etc). We are also comparing the genders themselves with the colour codes. Taking a look at the code, the  y_data_list  variable is now actually a list of lists, where each sublist represents a different group. We then loop through each group, and for each group we draw the bar for each tick on the x-axis; each group is also colour coded. Stacked bar plots are great for visualizing the categorical make-up of different variables. In the stacked bar plot figure below we are comparing the server load from day-to-day. With the colour coded stacks, we can easily see and understand which servers are worked the most on each day and how the loads compare to the other servers on all days. The code for this follows the same style as the grouped bar plot. We loop through each group, except this time we draw the new bars on top of the old ones rather than beside them. We previously looked at histograms which were great for visualizing the distribution of variables. But what if we need more information than that? Perhaps we want a clearer view of the standard deviation? Perhaps the median is quite different from the mean and thus we have many outliers? What if there is so skew and many of the values are concentrated to one side? That’s where boxplots come in. Box plots give us all of the information above. The bottom and top of the solid-lined box are always the first and third quartiles (i.e 25% and 75% of the data), and the band inside the box is always the second  quartile  (the  median ). The whiskers (i.e the dashed lines with the bars on the end) extend from the box to show the range of the data. Since the box plot is drawn for each group/variable it’s quite easy to set up. The  x_data  is a list of the groups/variables. The Matplotlib function  boxplot()  makes a box plot for each column of the  y_data  or each vector in sequence  y_data ; thus each value in  x_data  corresponds to a column/vector in  y_data . All we have to set then are the aesthetics of the plot. There are your 5 quick and easy data visualisations using Matplotlib. Abstracting things into functions always makes your code easier to read and use! I hope you enjoyed this post and learned something new and useful."
The Architecture of,he Architecture o,"Multilayered Storytelling through “Info-spatial” Compositions Information Design  is playing an increasingly critical role in everyday journalism. The movement from word and picture to “words within diagrams” is building a new form of truth-telling and storytelling — and with it, a new journalistic aesthetic. At  Accurat  we’ve been working side-by-side with the newsroom of  Corriere della Sera  for more than 2 years, designing a series of of exploratory data-visualizations originally published for  La Lettura , their Sunday cultural supplement. We aim to deliver rich visual narratives able to maintain the complexity of the data while making this complexity more accessible and understandable. Simply put, we publish compound and complex stories told through data visualizations. Within this collaboration we always aimed at  opening new perspectives in the newspaper-editorial field , and higher aim at educating readers’ eyes to  get familiar with new visual ways  to convey  the richness of the data stories we are telling  rather than simplifying them. This article will elucidate and share our  design method , based upon layering multiple sub-narratives over a main construct, through a  dissection of the spatial build-up of the visualizations ; it draws from different contributions I published on the   Parsons Journal for Information Mapping,  on the recently published  “New challenges for data design: articles & interviews”  Springer book, and by talks I delivered at the  Eyeo Festival ,   Resonate Festival  and  Visualized Conference . La Lettura  is the Sunday cultural supplement of Corriere della Sera, the highest circulation newspaper in Italy. The supplement is conceived as a  collection of long-form articles  about cultural and sociological phenomena, new media and communication related topics. The aim of the issue is to provide readers with  a product they can read throughout their week,  with deep essays usually written by sociologists, professional writers, art and literature critics, historians, philosopher or modern thinkers. We here try to  push the boundaries of what visualization can do  on data with multiple attributes and a high density: we are not going for simple diagrams to express basic concepts,  we somehow instead embrace complexity . We intend to deliver rich visual narratives able to maintain the nuances of the data but still making all this different aspects and flavors more accessible and understandable. For each story we consider and pursue a  topic  we believe may be of particular interest to explore, ranging from current affairs to historical or cultural issues. Sometimes choices are driven by a fascination we have, sometimes by a compelling dataset we find and we would start from, other times we choose to present events and topics that are hot at the moment. We then  analyze and compare different kinds of datasets  trying to identify and reveal a central story, hopefully a not-so-expected one. We start from a question or an intuition we have and work from here , then try to  put the information in context  and find some further facts and materials to potentially correlate. Every time we aim at moving away from mere quantity in order to pursue a qualitative transformation of raw statistical material into something that will provide new knowledge: unexpected parallels, not common correlation or  secondary tales, to enrich the main story with . In this respect our work here cannot be considered data-visualization in the pure sense: we are not just providing insight into numbers but into social issues or other qualitative aspects as well. In addition, since we publish on a full-spread format within the cultural Sunday supplement of the highest circulation Italian newspaper,  the leading narrative and the visual ways through which we display information have to be both catchy and attractive : once the first attention of the audience is “caught” by the aesthetic features of the image, the presentation of the information must be clear as might be expected. The clarity does not need to come all at once, however; we also like the idea of  providing several and consequent layers of exploration on the multiple dataset we analyze . We call it a  “non-linear storytelling”  where people can get lost in singular elements, minor tales, and last-mile textual elements within the greater visualization. To achieve this  multi-layered storytelling  with data even when visualizations are static and printed,  everything depends on the concept of layering, establishing hierarchies and making them clear : this is the case for both the data analysis (the stories we desire to tell), and the visual composition (i.e. the main architecture and the aesthetic value we desire to present), inviting readers to “get lost” within the narrative(s) and engage at deeper levels. Our design method, based upon layering multiple sub-narratives over a main construct prescribes this specific phenomenon. The following is a dissection of how we build our pieces , where the editorial process of selecting, analyzing, comparing, building hierarchies, etc., is in direct conjunction to the visual development of the layers. 1. Composing the main architecture of the visualization Composing the main architecture: this acts as the formalized base through which the main story will be mapped and displayed, upon this, one will see the most relevant patterns emerging from the story: the essential “map” that conceptually identifies where we are. This base is essentially a matrix or pattern that will serve as our organizer. It may be composed of cells, or distances, or other interrelated multiples. 2. Positioning singular elements within the main framework. This process will test the effectiveness of the main architecture; the placement of elements reveals or confirms weaknesses and strengths, which may lead to modification of the main architecture. 3. Constructing shaped elements of dimensionality and form Constructing shaped elements of dimensionality and form (essentially polygons) with quantitative and qualitative parameters and positioning these within the main architecture. As these elements have form they must also be identified through colors according to opportunities to establish categorizations, thus advancing clarity and relationships that serve to enhance the story. 4. Elucidating internal relationships between elements (if any) These links, directives, and qualifiers serve to give the story a comprehensive texture and correlate dependencies within the story. 5. Labeling and identifying Through the addition of explanatory labels and short texts we provide requisite last mile clarity throughout the presentation. 6. Supplementing the greater story through the addition of minor or tangential tales elements. We consider this a very important step to contextualize the phenomena in a wider world. These components link the story to external ideas, other times, or other places. Elements that are rendered here may come from very diverse sources — analysis that is undertaken once we have strongly established the core story. These elements, which may take the form of small images, textual components, graphic symbols, etc., are to be located where they best help to enrich the overall comprehension: they must not distract from the main story. 7. Providing small visual explanations such as a legend or key that assists readers and the general public who may not be familiar with norms of data visualization. These are composed to enlighten the layered idea of the visualization, often constructed as miniatures of the layers themselves. The process usually involves simplification of the general architecture (e.g. the x and y axes, base timelines, or map components) as well as minimal explicit shapes, colors, and dimensions of singular elements. These explanations also provide units of measurement for distances and volumes, etc. 8. Fine-tuning and stylizing of elements’ shapes, colors, and weights to make hierarchies pop out. By visually highlighting the most relevant elements and lightening the other background layers of information, we should be able to allow information to be selectively and sequentially revealed, helping readers discover stories by themselves and recognizing the patterns or interrelationships from one element within the story to another. The final fine-tuning of the piece is the  necessary effort required to please readers’ eyes: a well-balanced image where negative space and light elements play their role aesthetically. Is the process always so linear? Obviously, the answer is no. It is  a constant iteration of explorations and a mixture of different approaches  we can start to resolve the design problem with; with the constant goal in mind to allow people understand the stories, or, better said:  see the stories. Whenever the main purpose of visualizations is to open readers’ eyes to  new knowledge  and to  reveal something new about the world , or to engage and entertain the audience about a topic,  it’s impractical to avoid a certain level of visual complexity indeed. The world is complex, compound, rich in information that can be combined in endless ways, therefore catching new points of view or discovering something that you didn’t know before often cannot happen at a glance: this process of “revelation” often needs and require an  in-depth investigation of the context. Consequently, we like to think at these kind of data visualizations we presented as visual ways to convey the  richness , the involvement and feelings (being engagement or concern) that we experience in our everyday lives rather than simplifications of the world. One of the important challenges for data-visualization design nowadays is to experiment on and find  proper ways to express the data complexity,  and more broadly the complexity and the multiple possible interpretations and contextualization of any phenomena in the contemporary world; which in opposition to reductions  require the comprehension of the relations between the whole and the parts, at any time . And, as in the physical world,  aesthetics  plays an important role in shaping people’s reactions and responses to any products, acting as the bridge between it and people’s emotion and feeling. Since the goal here is neither to visualize data for rapid decision making processes, nor are we representing information for scientific purposes, the  opportunity to experiment with new visual metaphors is wide open, and the exploratory nature of this work is clear. We can, every time, try to push forward how we can “compose” data visualizations that achieve (in our idea) aesthetic beauty and elegance through new visual metaphors, intentionally avoiding typical and already tested styles of representation. To us, elegance is not only beauty and prettiness, our intention is to make things not just understandable but also  appealing , conveying the information in an effective way but also  catching the attention  (the eyes!) of the particular audience we are creating it for, as a trigger to their curiosity, and to the willingness to explore the piece. Many times standard visual models like bar charts, scatter plots, regular timelines and maps are the best way to convey data and messages indeed. To us, that doesn’t mean they are “an end” , though. We here simply believe that keeping on exploring the realm of possibilities in the representation of information, could lead up to  refining and perfecting the core of this “science”, of this field,  even passing through failures and mistakes. We are moving fast towards  infinite possibilities for data analysis and display,  with theoretical models, abstract formal languages, and open knowledge developed in other disciplines more than available to get inspirations from. Experimental visualizations design should always aim at  balancing conventions and familiar forms people are comfortable with, and novelty:  truly imaginative visuals able to attract individuals into the exploration, able  to transform the strange of any visual experiment we include into the known , and ultimately able to invite readers to explore the richness of the stories lying behind. With  this body of work  we try to test and to understand  what works and what doesn’t , what people like and what they don’t, and how we can even think of educating readers’ eyes to some new visual metaphors and models. Drawing the parallel with arts such as painting and music through centuries we know how much these disciplines have been able to constantly re-invent themselves even when a reinvention was not strictly necessary, opening new worlds and possibilities.  The interesting question to us here is:   how far we can go? Accurat  is a data-driven research, design and innovation firm. We help our clients understand, communicate and leverage their data assets through static and interactive data visualizations, interfaces and analytical tools that provide comprehension, insight and engagement. We have offices in Milan and New York. A very special and sincere thank to Professor  William Bevington  who made me think critically about what I was doing and about my mental process, and who knew it all well before me, helping me shaping all this and putting it down on paper."
D3 is not a Data Visualization Library,3 is not a Data Visualization Librar,"D3.js is an incredibly successful library yet there’s a disconnect between what people think D3 does and what it actually does. As a result, learning D3 is intimidating and confusing. By better understanding its structure and separating it into more manageable pieces, it can be easier to choose which parts of the library to learn and which parts to avoid — key lessons not only for D3 novices but for expert users like myself that might want to reexamine how they use D3. I wrote  a book about D3 (twice) , so I’m sure you’re thinking that this is some kind of clickbait title and that I’m going to make a subtle play on words or say something like  D3 is not a Data Visualization Library it is THE Data Visualization Library . But no, along with key functionality that lets you do data visualization, D3 also consists of other functionality that is only tangentially related to data visualization. You might be surprised by that given what the D3 home page looks like. But much of D3 has little to do with graphics and many of the parts that do aren’t necessary to learn to create effective data visualization. We can take the  D3 API page  and visualize it as a hierarchy by graphically nesting the functions into the sections and subsections described in the documentation. Here’s how  d3-selection  looks using this method: Along with doing this for the entire docs, I grouped the functionality into a few broad semantic categories. Obviously, this method of visualizing the API doesn’t account for true complexity because some sections have many small nearly duplicate functions whereas others only have a few complex functions but it gives a reasonable graphical overview. The size and complexity of the library has always made it difficult to teach and almost every lesson and book (mine included) focuses first on establishing the JQuery part of the library, which is all about DOM manipulation to create and manipulate elements on a web page. But take a look at that diagram above. If you want to learn how to use D3 for data visualization you don’t need to learn anything on the right hand side and you almost certainly don’t need to learn everything on the left. In fact, if you do, you might be setting yourself up for a worse chance at success in the long run. I’ll explain as I walk through the areas of the D3 API. So much of what you read in D3 tutorials focuses on its DOM manipulation functionality. This includes the  select/enter/exit/update  pattern you’ve probably seen a thousand times but also convenience functions for dragging, zooming, dispatching events and even using Fetch (the D3 flavor of which is unsurprisingly called  d3-fetch ). There are useful and interesting functions (like  d3-zoom ) along with particular D3 flavors of existing ES2015 functionality if you prefer the D3 way of writing code. But this whole section is unnecessary if you’re using something like Vue to create your DOM elements and actively conflicts with other forms of DOM management necessitating hacks or mixed systems. If you’re  working in a team environment , it’s far more likely the rest of your team will be more familiar with other methods of DOM management than D3, so there are good reasons to avoid this aspect of D3 altogether. It stands to reason why this is so tied up with data visualization: You can’t visualize things unless they actually exist (in the DOM) but because it is so prominent in the API it comes across to new audiences as if D3 can only be used if you use D3 to manipulate the DOM. It also makes sense given that when D3 was originally being developed (v3 was released in 2013). There’s another reason for selections and that’s tied to animation, which I’ll get into below. There’s another significant portion of the library that exposes a host of functions that are a part of the data transformation, cleaning and formatting process. Some of them, like the  min  and  max  functions in  d3-array  are syntactic sugar for operations that can be handled with vanilla JavaScript. Others, like  set  and  map  are D3 flavors that aren’t quite the same as ES2015  Set  and  Map . The formatting utilities are there if you like python style numerical formatting, something I find less intuitive than  numeral , as well as time formatting, which in all my experience with D3 I’ve never used preferring  moment  or (before Moment and now more recently) vanilla JavaScript time formatting options. There are, additionally, some interpolation functions that are used internally to power the scales and color functionality, as well as a host of random number generators. You need to know how to measure and format data to do data visualization but there are many ways to do that, often times in the process leading up to creating the dataset. There are also other libraries that might be more commonly used in industry or native ES2015 functions that larger teams will have a better chance of understanding than D3 particular functions. d3-transition  is one of the most convenient ways to animate graphical elements on the web and is intimately tied to  d3-selection . But I’ve found that animation in data visualization, while impressive and eye-catching, is a much lower priority than understanding how different chart types present data, how to properly design charts, understanding form in data visualization, and other aspects of data visualization practice. Given that you need to use D3 selections to get access to animation via  d3-transition  it’s a heavy investment that might be better spent on leveling up in other ways. Another aspect of animation that makes it optional as far as learning to do it with D3 is that there are other libraries that do good animation, like  GSAP , as well as animation solutions that are native to the method you (or your team) are using for DOM management. We’re getting closer to data visualization here. Parsing data like CSVs and its various flavors (tab-delimited, comma-delimited) is an important part of data visualization and I haven’t seen a library that does it so easily as  d3-dsv . But much of my production data comes processed and available as JavaScript data structures and doesn’t require processing CSVs.  d3-quadtree  is an amazingly fun and useful library for spatial search but its direct application is pretty rare in the real world (under the hood quadtrees are used for things like the network diagrams built with  d3-force ). Binning and nesting functions from  d3-collection  and  d3-array  also fall into this category. The real meat of D3 for data visualization is in its functions for decorating data with drawing instructions, creating new drawable data from source data, generating SVG paths and creating data visualization elements (like an axis) in the DOM from your data and methods. It’s useful to separate the graphical functionality of D3 into  generators ,  components  and  layouts . Since layouts and generators don’t produce DOM elements but only raw material for creating DOM elements, you still need to pass the data they create to the DOM,  a process I describe using React in an earlier article . Understanding not only how to use layouts and generators but also how they work is key to understanding data visualization. Because most complex data visualization is combinatorial, understanding how these different functions can interact to produce a different kind of chart is key to understanding how to design data visualization well. It’s unfortunate that people learning D3 spend so much time learning its DOM management functionality and have only a superficial understanding of the actual data visualization functionality. d3-shape  has a bunch of really valuable functions that draw SVG paths from arrays of data that are useful in themselves and as models of how you can build your own generators. Its built-in canvas rendering functionality is nice but there’s an existing solution for that in vanilla JavaScript: Path2D . Besides the graphical functions,  scales  and  interpolators  don’t create graphics but are key to projecting graphics into visual space (whether within the boundaries of a chart or with the color, stroke-width or other channels used to communicate visually). Scales don’t just interpolate numbers to numbers but transform from one mode to another (such as with quantizing scales) and expose convenience functions like calculating ticks suitable for axes. d3-hierarchy  has hierarchical layouts (like dendrograms, treemaps and sunbursts) but also a hierarchical data structure that provides convenient ways to slice up and analyze hierarchical data. d3-force  has a simple constraint-based force-directed layout that is effective and generic enough to be used in most network visualization. d3-color  and  d3-color-schemes  are not the only way to deal with color and if you’re just starting out, I’d suggest working with  chroma.js  rather than D3’s color utilities. The axis and brush functions in D3 suffer from the same problem that  d3-selection  does in that they reach into the DOM and create elements themselves. That means they’re harder to integrate into an application that’s using Vue or React to manage the DOM. Some functionality, like the canvas-to-SVG of  d3-path , can be useful in some corner cases, but the real value of learning D3 comes from learning the different visualization methods, like hulls, contours, voronoi polygons and chord diagrams. Understanding how they take different forms of data and derive drawing instructions can help you to understand how data can be transformed for your applications and how those transformations interplay. While definitely a part of the data visualization area of D3,  d3-geo  is likely too specific and not as easily integrated across different design cases as the broader data visualization pieces. Its popularity is well justified as D3 has become something of a playground for neogeographers experimenting with cartograms, raster reprojection and other geographic geekery. It consists of a million projections, an entire projection streaming system, functions for translating GeoJSON to paths, finding centroids and bounding boxes (use  d3-polygon  if you want this for generic geometries). A lot of spherical math and spherical shapes and steradians. D3 lets you make simple choropleth maps easily but with a high learning curve if you’re not a GIS professional or a map nerd. Given the innovations happening in the WebGL mapping space, if you’re looking to make maps you might want to first explore  kepler.gl  or Mapbox. People have a hard time learning D3. If you’re expected to learn the DOM manipulation part first it can be a barrier especially when you’re working on a project where that’s already being handled. The point of learning D3 is to learn how to create data visualization products, and that’s really just part of the library that you can focus on instead of the supplemental functionality. It limits the combinatorial quality of D3 if people think of it as a self-contained ecosystem. Data visualization and charts are about mapping data attributes to visual features. That should be the focus of anyone trying to learn a library that does data visualization, not DOM management. There are lots of tools for managing the DOM and all of them can integrate the data visualization functions in D3, which will only increase the amount of sophisticated data visualization being done. You might find using D3 for everything to be particularly suitable for your practice. That’s great! You should buy my book, since it explains how to use all those bits. But I’ve found that this is often not the case with people coming to learn D3 and as a result the data visualization community loses out on contributions from developers who really just wanted to do data visualization. For them, I hope I’ve done a decent job of defining where they should look and what they should focus on when it comes to learning D3. For those who have already mastered D3, I hope this has helped describe just how complex and eclectic and opinionated the library can appear to outsiders (And I didn’t even get into all the function chaining…). Let’s not assume that the way we learned or use D3 is the only way to do it, even if that means evolving our practice."
11 Javascript Data Visualization Libraries for 2019,1 Javascript Data Visualization Libraries for 201,"We live in an era of data explosion, when nearly every application we develop uses or leverages data to improve the experience delivered to the users. Sometimes, the best feature we can deliver to our users is the data itself. However, table and number charts are often tiring to read and it can be hard to draw actionable insights from large data tables. Instead, we can use neat data visualization methods that leverage the brain’s ability to identify and process data in a visual way. To help you get started and easily add beautiful data visualization to your favorite application, here are some of the best Javascript data visualization libraries around in 2019 (unranked). Feel free to comment and add your own suggestions and insights! Tip : Use  Bit  to quickly reuse UI components between your apps. Empower your team with a cloud-library to speed your app development. It’s free. https://bit.dev At 80k stars D3.js is probably the most popular and extensive Javascript data visualization library out there. D3 is built for manipulating documents based on data and bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the capabilities of modern browsers without coupling to a proprietary framework, combining visualization components and a data-driven approach to DOM manipulation. It allows you to bind arbitrary data to a Document Object Model (DOM), and then apply data-driven transformations to the document. Here’s a great  example gallery . Note: some say D3  isn’t a data visualization library  at all… :) An extremely popular (40k stars) library of open source HTML 5 charts for responsive web applications using the canvas element. V.2 provides mixed chart-types, new chart axis types, and beautiful animations. Designs are simple and elegant with 8 basic chart types, and you can combine the library with  moment.js  for time axis. You can also check out the library  on cdnjs . This incredibly popular library (45K stars; 1K contributors) in built for creating 3d animations using WebGL. The project’s flexible and abstract nature means it’s also useful for visualizing data in  2 or 3  dimensions. For example You can also use  this designated module  for 3D graph visualization with WebGL, or try  this online playground . Interesting choice to consider. github.com Baidu’s Echarts project (30k stars) is an interactive charting and visualization library for the browser. It’s written in pure JavaScript and is based on the  zrender  canvas library. It supports rendering charts in the form of Canvas, SVG (4.0+), and VML In addition to PC and mobile browsers, ECharts can also be used with node-canvas on node for efficient server-side rendering (SSR). Here’s a link to the full  example gallery , where each example can be played with (and themed) in an interactive playground. github.com Highcharts JS is a 8K stars and widely popular JavaScript charting library based on SVG, with fallbacks to VML and canvas for old browsers. It claims to eb used by 72 out of the world’s 100 largest companies, which makes it (probably) the most popular JS charting API in the world (Facebook, Twitter). github.com MetricsGraphics.js (7k stars) is a library optimized for visualizing and laying out time-series data. It’s relatively small (80kb minified), and provides a narrow yet elegant selection of line charts, scatterplots, histograms, bar charts and data tables, as well as features like rug plots and basic linear regression. Here’s a link to an  interactive example gallery . github.com Recharts is a chart library build with React and D3 that lets you deploy as declarative React components. The library provides native SVG support, lightweight dependency tree (D3 submodules) is highly customizable via component props. You can find live examples in the docs website. github.com A 10k stars Javascript “vector library” for working with vector graphics in the web. The library uses the SVG W3C Recommendation and VML as a base for creating graphics, so every graphical object is also a DOM object and you can attach JavaScript event handlers. Raphaël currently supports Firefox 3.0+, Safari 3.0+, Chrome 5.0+, Opera 9.5+ and Internet Explorer 6.0+. github.com At 8k stars C3 is a D3-based reusable chart library for web applications. The library provides classes to every element so you can define a custom style by the class and extend the structure directly by D3. It also provides a variety of APIs and callbacks to access the state of the chart. By using them, you can update the chart even after it’s rendered. Take a look at  these examples . github.com React-vis  (4k stars) is Uber’s set of React components for visualizing data in a consistent way, including line/area/bar charts, heat maps, scatterplots, contour plots, hexagon heatmaps and much more. The library does not require any previous knowledge with D3 or any other data-vis library, and provides low-level modular building-block components such as X/Y axis. A great match for working with  Bit   and a very useful library to consider. github.com React virtualized  (12k stars) is a set of React components for efficiently rendering large lists and tabular data. ES6, CommonJS, and UMD builds are available with each distribution and the project supports a Webpack 4 workflow. Note that  react ,  react-dom  must be specified as peer dependencies in order to avoid version conflicts. Give it a try. github.com Victory  is a collection of React composable React components for building interactive data visualization, built by Formidable Labs and with over 6k stars. Victory uses the same API for web and React Native applications for easy cross-platform charting. An elegant and flexible way to leverage React components in favor of practical data visualization. I like it. These libraries is a neat combination  with  Bit  when using individual components, to share and sync them across apps. At 2k stars Carto is a Location Intelligence & Data Visualization tool for discovering insights underlying location data. You can upload geospatial data (Shapefiles, GeoJSON, etc) using a web form and visualize it in a dataset or on a map, search it using SQL, and apply map styles using CartoCSS. Here are a bunch of  video demos  to help you get the idea and get started. github.com At over 5K stars Raw is a connection link between spreadsheets and data visualization built to create custom vector-based visualizations on top of the  d3.js  library. It works with tabular data (spreadhseets and comma-separated values) as well as with copied-and-pasted texts from other applications. Based on the SVG format, visualizations can be edited with vector graphics applications for further refinements, or directly embedded into web pages. Here’s an  example gallery  to explore before diving in. github.com At over 11k stars Metabase is a rather quick and simple way to create data dashboards without knowing SQL (but with SQL Mode for analysts and data pros). You can create canonical  segments and metrics , send data to Slack (and view data in Slack with  MetaBot ) and more. Probably a great tool to visualize data internally for your team, although some maintenance might be required. github.com At nearly 2k stars tauCharts is a D3-based and data-focused charting library. The library provides a declarative interface for fast mapping of data fields to visual properties, and its architecture allows you to build  facets  and extend chart behavior with reusable plugins. It also looks pretty good, right? github.com Note that some of these are unmaintained. github.com github.com github.com github.com github.com github.com github.com github.com github.com github.com github.com github.com blog.bitsrc.io blog.bitsrc.io blog.bitsrc.io"
20 ideas for better data visualization,0 ideas for better data visualizatio,"Applications we design are becoming increasingly data-driven. The need for quality data visualization is high as ever. Confusing and misleading graphics are all around us, but we can change this by following these simple rules. Choosing the wrong chart type, or defaulting to the most common type of data visualization could confuse users or lead to data misinterpretation. The same data set can be represented in many ways, depending on what users would like to see. Always start with a review of your data set and user interview. You can learn more on how to pick the right representation for your data, and how to design effective dashboards in my article about  Dashboard design . When using horizontal bars, plot  negatives values on the left side  and  positive on the right side  of a baseline. Do not plot negative and positive values on the same side of the baseline. Truncation leads to misrepresentation. On the example below, looking at the chart on the left, you can quickly conclude that value B is more than 3 times greater than D when in reality the difference is far more marginal. Starting at zero baseline ensures that users get a much more accurate representation of data. For line charts always limiting the y-axis scale to start at zero may render the chart almost flat. As the main goal for a line chart is to represent the trend, it's important to adapt the scale based on the data set for a given period and keep the line occupying two-thirds of the y-axis range. The line chart is composed of “markers” that are connected by lines, often used to visualize a trend in data over intervals of time — a time series. This helps to illustrate how values change over time and works really well with short time intervals, but when data updates infrequently this may cause confusion. Ex. Using a line chart to represent yearly revenue, if values are updated monthly will open the chart to interpretation. Users may assume the lines connecting the “markers” are representing actual values when in reality true revenue numbers at that specific time are unknown.  In such scenarios using a vertical bar chart can be a better option. Smoothed line charts may be visually pleasing but they misrepresent the actual data behind them, also excessively thick lines obscure the real “markers” positions. Often, to save space for your visualization, you may be inclined to use dual-axis charts when there are two data series with the same measure, but different magnitudes. Not only are those charts hard to read, but they also represent a comparison between 2 data series in completely misleading way. Most users will not pay close attention to the scales and just scan the chart, drawing wrong conclusions. A pie chart is one of the most popular and often misused charts. In most cases, a bar chart is a much better option. But if you decided on a pie chart here are a few recommendations on how to make it work: Without proper labeling, no matter how nice is your graph — it won’t make sense. Labeling directly on the chart is super helpful for all viewers. Consulting the legend requires time and mental energy to link the values and corresponding segments. Putting the values on top of slices may cause multiple problems, from readability issues to challenges with thin slices. Instead, add black labels with clear links to each segment. There are several ways commonly accepted in ordering pie slices: The same recommendation holds true for many other charts. Do not default to alphabetical sorting. Place the largest values on top (for horizontal bar charts) or left (for vertical bar charts) to ensure the most important values take the most prominent space, reducing the eye movements, and time required to read a chart. A pie chart in general is not the easiest chart to read, as it's very hard to compare similar values. When we take the middle out and create a donut chart, we free as space to display additional information but sacrifice clarity, taken to extremes it renders the chart useless. Unnecessary styling is not only distracting, it may cause misinterpretation of the data and users making false impressions. You should avoid: Color is an integral part of effective data visualization, consider those 3 color palette types when designing: A  Qualitative  color palette works best for the display of categorical variables. Colors assigned should be distinct to ensure accessibility. A  Sequential  color palette works best for numeric variables that need to be placed in a specific order. Using hue or lightness or a combination of both, you create a continuous color set. A  Divergent  color palette is a combination of two sequential palettes with a central value in the middle(usually zero). Often divergent color palettes will communicate positive and negative values. Make sure color also matches the notion of “negative” and “positive” performance. Check out a handy tool —  ColorBrewer  that can help you generate various color palettes. According to the National Eye Institute, about 1 in 12 humans are color blind. Your charts are only successful if they are accessible to a broad audience. Make sure typography is communicating information and helping users focus on data, rather than distracting from it. This simple trick will ensure users will be able to scan the chart much more effectively, without straining their neck) If your task is to add interactive charts to web and mobile projects, one of the first questions you should ask is what charting library will we use? Modern charting libraries have many of the previously mentioned interactions and rules baked in. Designing based on a defined library will ensure ease of implementation and will give you a ton of interaction ideas. Help users explore by changing parameters, visualization type, timeline. Draw conclusions to maximize value and insight. In the example below, you can see the IOS Health app using a combination of various kinds of data presentation to its benefit. For all who would like to learn more about this topic, I highly recommend reading  “The Wall Street Journal Guide to Information Graphics: The Dos and Don’ts of Presenting Data, Facts, and Figures” by Dona M. Wong . Many of the ideas in this article are inspired by this book."
"Data Visualization with Bokeh in Python, Part III: Making a Complete Dashboard","ata Visualization with Bokeh in Python, Part III: Making a Complete Dashboar","Creating an interactive visualization application in Bokeh Sometimes I learn a data science technique to solve a specific problem. Other times, as with Bokeh, I try out a new tool because I see some cool projects on Twitter and think: “That looks pretty neat. I’m not sure when I’ll use it, but it could come in handy.” Nearly every time I say this, I end up finding a use for the tool. Data science requires knowledge of many different skills and you never know where that next idea you will use will come from! In the case of Bokeh, several weeks after trying it out, I found a perfect use case in my work as a data science researcher. My  research project  involves increasing the energy efficiency of commercial buildings using data science, and, for a  recent conference , we needed a way to show off the results of the many techniques we apply. The usual suggestion of a powerpoint gets the job done, but doesn’t really stand out. By the time most people at a conference see their third slide deck, they have already stopped paying attention. Although I didn’t yet know Bokeh very well, I volunteered to try and make an interactive application with the library, thinking it would allow me to expand my skill-set and create an engaging way to show off our project. Skeptical, our team prepared a back-up presentation, but after I showed them some prototypes, they gave it their full support. The final interactive dashboard was a stand-out at the conference and will be adopted by our team for future use: While not every idea you see on Twitter is probably going to be helpful to your career, I think it’s safe to say that knowing more data science techniques can’t possibly hurt. Along these lines, I started this series to share the capabilities of  Bokeh , a powerful plotting library in Python that allows you to make interactive plots and dashboards. Although I can’t share the dashboard for my research, I can show the basics of building visualizations in Bokeh using a publicly available dataset. This third post is a continuation of my Bokeh series, with  Part I focused on building a simple graph,  and  Part II showing how to add interactions to a Bokeh plot . In this post, we will see how to set up a full Bokeh application and run a local Bokeh server accessible in your browser! This article will focus on the structure of a Bokeh application rather than the plot details, but the full code for everything can be found on  GitHub.  We will continue to use the  NYCFlights13 dataset , a real collection of flight information from flights departing 3 NYC airports in 2013. There are over 300,000 flights in the dataset, and for our dashboard, we will focus primarily on exploring the arrival delay information. To run the full application for yourself, make sure you have Bokeh installed ( using  pip install bokeh ),  download the  bokeh_app.zip  folder  from GitHub, unzip it, open a command window in the directory, and type  bokeh serve --show bokeh_app . This will set-up a  local Bokeh server  and open the application in your browser (you can also make Bokeh plots available publicly online, but for now we will stick to local hosting). Before we get into the details, let’s take a look at the end product we’re aiming for so we can see how the pieces fit together. Following is a short clip showing how we can interact with the complete dashboard: Here I am using the Bokeh application in a browser (in Chrome’s fullscreen mode) that is running on a local server. At the top we see a number of tabs, each of which contains a different section of the application. The idea of a dashboard is that while each tab can stand on its own, we can join many of them together to enable a complete exploration of the data. The video shows the range of charts we can make with Bokeh, from histograms and density plots, to data tables that we can sort by column, to fully interactive maps. Besides the range of figures we can create in Bokeh, another benefit of using this library is interactions. Each tab has an interactive element which lets users engage with the data and make their own discoveries. From experience, when exploring a dataset, people like to come to insights on their own, which we can allow by letting them select and filter data through various controls. Now that we have an idea of the dashboard we are aiming for, let’s take a look at how to create a Bokeh application. I highly recommend  downloading the code  for yourself to follow along! Before writing any code, it’s important to establish a framework for our application. In any project, it’s easy to get carried away coding and soon become lost in a mess of half-finished scripts and out-of-place data files, so we want to create a structure beforehand for all our codes and data to slot into. This organization will help us keep track of all the elements in our application and assist in debugging when things inevitably go wrong. Also, we can re-use this framework for future projects so our initial investment in the planning stage will pay off down the road. To set up a Bokeh application, I create one parent directory to hold everything called  bokeh_app  . Within this directory, we will have a sub-directory for our data (called  data ), a sub-directory for our scripts ( scripts ), and a  main.py  script to pull everything together. Generally, to manage all the code, I have found it best to keep the code for each tab in a separate Python script and call them all from a single main script. Following is the file structure I use for a Bokeh application, adapted from the  official documentation . For the flights application, the structure follows the general outline: There are three main parts:  data ,  scripts , and  main.py,  under one parent bokeh_app  directory. When it comes time to run the server, we tell Bokeh to serve the  bokeh_app  directory and it will automatically search for and run the  main.py  script. With the general structure in place, let’s take a look at  main.py  which is what I like to call the executive of the Bokeh application (not a technical term)! The  main.py  script is like the executive of a Bokeh application. It loads in the data, passes it out to the other scripts, gets back the resulting plots, and organizes them into one single display. This will be the only script I show in its entirety because of how critical it is to the application: We start out with the necessary imports including the functions to make the tabs, each of which is stored in a separate script within the  scripts  directory. If you look at the file structure, notice that there is an  __init__.py  file in the  scripts  directory. This is a completely blank file that needs to be placed in the directory to allow us to import the appropriate functions using relative statements (e.g.  from scripts.histogram import histogram_tab  ). I’m not quite sure why this is needed, but it works (here’s the  Stack Overflow answer  I used to figure this out). After the library and script imports, we read in the necessary data with help from the  Python  __file__  attribute . In this case, we are using two pandas dataframes (  flights  and  map_data  ) as well as US states data that is included in Bokeh. Once the data has been read in, the script proceeds to delegation: it passes the appropriate data to each function, the functions each draw and return a tab, and the main script organizes all these tabs in a single layout called  tabs . As an example of what each of these separate tab functions does, let’s look at the function that draws the  map_tab . This function takes in  map_data  (a formatted version of the flights data) and the US state data and produces a map of flight routes for selected airlines: We covered interactive plots in Part II of this series, and this plot is just an implementation of that idea. The overall structure of the function is: We see the familiar  make_dataset ,  make_plot , and  update  functions used to  draw the plot with interactive controls . Once we have the plot set up, the final line returns the entire plot to the main script. Each individual script (there are 5 for the 5 tabs) follows the same pattern. Returning to the main script, the final touch is to gather the tabs and add them to a single document. The tabs appear at the top of the application, and much like tabs in any browser, we can easily switch between them to explore the data. After all the set-up and coding required to make the plots, running the Bokeh server locally is quite simple. We open up a command line interface (I prefer Git Bash but any one will work), change to the directory containing  bokeh_app  and run  bokeh serve --show bokeh_app . Assuming everything is coded correctly, the application will automatically open in our browser at the address  http://localhost:5006/bokeh_app . We can then access the application and explore our dashboard! If something goes wrong (as it undoubtedly will the first few times we write a dashboard) it can be frustrating to have to stop the server, make changes to the files, and restart the server to see if our changes had the desired effect. To quickly iterate and resolve problems, I generally develop plots in a Jupyter Notebook. The Jupyter Notebook is a great environment for Bokeh development because you can create and test fully interactive plots from within the notebook. The syntax is a little different, but once you have a completed plot, the code just needs to be slightly modified and can then be copied and pasted into a standalone  .py  script. To see this in action, take a look at the  Jupyter Notebook  I used to develop the application. A fully interactive Bokeh dashboard makes any data science project stand out. Oftentimes, I see my colleagues do a lot of great statistical work but then fail to clearly communicate the results, which means all that work doesn’t get the recognition it deserves. From personal experience, I have also seen how effective Bokeh applications can be in communicating results. While making a full dashboard is a lot of work (this one is over 600 lines of code!) the results are worthwhile. Moreover, once we have an application, we can quickly share it using GitHub and if we are smart about our structure, we can re-use the framework for additional projects. The key points to take away from this project are applicable to many data science projects in general: That’s all for this post and for this series, although I plan on releasing additional stand-alone tutorials on Bokeh in the future. With libraries like Bokeh and plot.ly it’s becoming easier to make interactive figures and having a way to present your data science results in a compelling manner is crucial. Check out this  Bokeh GitHub repo  for all my work and feel free to fork and get started with your own projects. For now, I’m eager to see what everyone else can create! As always, I welcome feedback and constructive criticism. I can be reached on Twitter  @koehrsen_will ."
How to improve the process of Data Mining through implementation of common Rules?,ow to improve the process of Data Mining through implementation of common Rules,"Web Data Mining Companies  have started to hit a point in providing  web data mining services  where running out of the box is quite effective, however, after long experimenting only. In noteworthy of mentioning, for the beginners of  Data Mining Services , there are common rules that must be implemented in order to improve the process. These rules may be taken to skip sometimes; on the contrary, the application of these basic rules can assist the Data Miner in the complex extraction of Data. Web Data Mining Services  come out effortless with the use of tools, but at this juncture, you will learn how rule implementation works. The complexity of Business in the context of Data Extraction and pulling out benefits may sound cool and challenging but this requires experience and intellect in the field. Rule one states that in case, you have not handled any complex business Data mining projects in the past, don’t take hold of a complex project in the first place. Start with the resolution of common business issues through data mining. These business issues could be cross-selling opportunities, customer’s loyalty, and fraud identification/detection. Rule2: Leaving specific categories of Data is not an option More often than not,  Data Mining service  providers leave upon the business data that does not interest them. This is not an option if you want the outcome to be beneficial and add-on to business. While preparation of model, never skip any Data extracted that might make a difference in the predictions and plans. Overlooking the other side of the coin is an injustice to the process. Rule3: Stick to Sampling Strategy In noteworthy of mentioning, sometimes the use of advanced tools and techniques result in the creation of complexities as the unwanted populations’ database comes in the way. Sticking to a specific strategy can help in the management of activities and outcomes in the long run. Loginworks Softwares  recommend the professionals of  data mining service  to put hands off advanced tools prior to expertise in simpler data mining techniques. Connect with source url:- https://www.loginworks.com/data-mining-services-various-type/"
Data Mining Reveals Fundamental Pattern of Human Thinking,ata Mining Reveals Fundamental Pattern of Human Thinkin,"Back in 1935, the American linguist George Zipf made a remarkable discovery. Zipf was curious about the…"
BENIFITS ASSOCIATED WITH DATA MINING,ENIFITS ASSOCIATED WITH DATA MININ,"Data has been used from time immemorial by various companies to manage their operations.Data is needed by various organizations strategically aimed at expanding their business operations, reduction of costs, improve their marketing force and above all improve profitability. Data mining is aimed at the creation of information assets and uses them to leverage their objectives. In this article, we discuss some of the common questions asked about the  data mining technology .  Some of the questions we have addressed include: Data Mining Defined Data mining can be regarded as a new concept in the enterprise decision support system, usually abbreviated as DSS. It does more than complementing and interlocking with the DSS capabilities that may involve reporting and query. It can also be used in on-line analytical processing (OLAP), traditional statistical analysis and data visualization. The technology comes up with tables, graphs, and reports of the past business history. We may define data mining as modeling of hidden patterns and discovering data from large volumes of data.It is important to note that data mining is very different from other retrospective technologies because it involves the creation of models. By using this technology, the user can discover patterns and use them to build models without even understanding what you are after. It gives an explanation why the past events happened and even predicting what is likely to happen. Some of the information technologies that can be linked to data mining include neural networks, fuzzy logic, rule induction and genetic algorithms. In this article, we do not cover those technologies but focus on how data mining can be used to meet your business needs and you can translate the solutions thereafter into dollars. Setting Your Business Solutions and Profits One of the common questions asked about this technology is; what role can data mining play for my organization? At the start of this article, we described some of the opportunities that can be associated with the use of data. Some of those benefits include cost reduction, business expansion, sales and marketing, and profitability. In the following paragraphs, we look into some of the situations where companies have used data mining to their advantage. Business Expansion Equity Financial Limited wanted to expand their customer base and also attract new customers. They used the Loan Check offer to meet their objectives. Initiating the loan, a customer had to go to any branch of Equity branch and just cash the loan. Equity introduced a $6000 LoanCheck by just mailing the promotion to their existing customers. The equity database was able to track about 400 characteristics of every customer. The characteristics were about loan history of the customer, their active credit cards, current balance on the credit cards and if they could respond to the loan offer. Equity used data mining to shift through 400 customer features and also finding the significant ones. They used the data and build a model based on the response to the Loan Check offer. They then integrated this model to 500,000 potential customers from a credit bureau. They then selectively mailed the most potential customers that were determined by the data mining model. At the end of the process, they were able to generate a total of $2.1M in extra net income from 15,000 new customers. Reduction of Operating Costs Empire is one of the largest insurance companies in the country. In order to compete with other insurance companies, it has to offer quality services and at the same time reducing costs.Therefore it has to attack costs that may in form of fraud and abuse. This demands a considerable investigation skills and use of data management technology. The latter calls for data mining application that can profile every physician in their network based on claims records of every patient in their data warehouse. The application is able to detect subtle deviations on the physician behavior that are linked to her/her peer group. The deviations are then reported to the intelligence and fraud investigators as “suspicion index.” With this effort derived from data mining, the company was able to save $31M, $37M, and $41M in the first three years respectively from frauds. Sales Effectiveness and Profitability In this case, we look into pharmaceutical sector. Their sales representatives have a wide range of assortment tools they use in promoting various products to physicians. Some of the tools include product samples, clinical literature, dinner meetings, golf outings, teleconferences and many more. Therefore getting to know the promotions methods that are ideal for a particular physician is of valuable importance and it is likely to cost the company a lot of dollars in sales call and thereby more lost revenue. Through  data mining , a drug maker was able to link eight months of promotional activity based on corresponding sales found in their database. They then used this information to build a predictive model for each physician.The model revealed that for the six promotional alternatives, only three had a significant impact. Then they used the knowledge found in the data mining models and thereby customizing the ROI. Looking at those two case studies, then ask yourself, was data mining necessary? Getting Started All the cases presented above have revealed how data mining was used to yield results to the various businesses. Some of the results led to increased revenue and increased customer base. Others can be regarded as bottom-line improvements that impacted on cost savings and also improved productivity.In the next few paragraphs we try to answer the question; how can my company get started and start realizing the benefits of data mining. The right time to start your data mining project is now. With the emergence of specialized data mining companies, starting the process has been simplified and the costs greatly reduced. Data mining project can offer important insights into the field and also aggregate the idea of creating a data warehouse. In this article, we have addressed some of the common questions regarding data mining, what are the benefits associated with the process and how a company can get started. Now, with this knowledge your company should start with a pilot project and then continue building a data mining capability in your company; to improve profitability, market your products more effectively, expand your business and also reduce costs Connect with orignal sources:- https://www.loginworks.com/blogs/255-benefits-associated-with-data-mining/"
Cambridge Analytica: the Geotargeting and Emotional Data Mining Scripts,ambridge Analytica: the Geotargeting and Emotional Data Mining Script,"Last year, Michael Phillips, a data science intern at Cambridge Analytica, posted the following scripts to a set of “work samples” on his  personal GitHub account . The Github profile,  MichaelPhillipsData  is still around. It contains a selection of Phillips’ coding projects. Two of the “commits” — still online today — appear to be scripts that were used by Cambridge Analytica around the election. One of them even lists his email address. The rest of his current work, Phillips notes on his Github profile, he unfortunately “cannot share.” archive.is The first of Phillips’ two election data processing Github scripts is titled  GeoLocation.py ,* a list-completing and enrichment tool that can be used to: archive.is “complete an array of addresses with accurate latitudes and longitudes using the completeAddress functionIncludes another function compareAPItoSource for testing APIs with source latitude longitudes.” Phillips describes the geolocation list completion script as performing the following tasks (to enrich clients’ personal information files): “Essentially what it does is: For each address in the addresses file, try to get an accurate lng/lat quickly (comparing available datafrom Aristotle/IG to the zip code file data to determine accuracy), but if we can’t, we fetch it from ArcGIS.” >Don’t miss the line item called “Voter_ID” The second “work-related” script sitting on Phillips’ Github repo is called  Twitteranalysis.py . archive.is Phillips offers a quick starter for how the Twitter sentiment-mining code works: For starters, we will just get sentiment from textBlob for tweets containing keywords like “Trump”, “Carson”, “Cruz”, “Bern”, Bernie”, “guns”, “immigration”, “immigrants”, etc. Twitteranalysis.py  also finds the Twitter  user IDs  amongst the tweet sample it collects in order to “ retrieve all the user’s recent tweets and favorites .” Looking in more detail, it then: As a real-time social media mining tool which uses common tools like tweepy and matplotlib, this doesn’t appear to be science fiction or extremely complex. However, this is not what makes the code interesting as a key  research , political  evidence , and  cultural  object. The most fascinating part of the Twitter sentiment-miner that Phillips’ posted is how it appears to pull users’ IDs and find their “recent tweets” and favorites to  expand  the company’s corpus of keywords around specific objects of election “outrage” sentiment (ie, immigration, border control, etc.). Looking below, nearly all “ sentiments ” within the lines of code involve “hot-button” 2016 election topics such as  abortion ,  citizenship ,  naturalization ,  guns , the  NRA ,  liberals ,  Obama , and  Planned Parenthood. See for yourself, here’s the actual code: #each  sentiments  list will have tuples: ( sentiment ,  tweetID ) #note: could include many more keywords like “ feelthebern ” for example, but need neutral keywords to get true sentiments.  feelthebern  would be a biased term. In any case, here are the “sentiments” the script was set to look for via Twitter’s API: hillarySentiments  = [] hillaryKeywords = [‘hillary’, ‘clinton’, ‘hillaryclinton’] trumpSentiments  = [] trumpKeywords = [‘trump’, ‘realdonaldtrump’] cruzSentiments  = [] cruzKeywords = [‘cruz’, ‘tedcruz’] bernieSentiments  =[] bernieKeywords = [‘bern’, ‘bernie’, ‘sanders’, ‘sensanders’] obamaSentiments  = [] obamaKeywords = [‘obama’, ‘barack’, ‘barackobama’] republicanSentiments  = [] republicanKeywords = [‘republican’, ‘conservative’] democratSentiments  = [] democratKeywords = [‘democrat’, ‘dems’, ‘liberal’] gunsSentiments  = [] gunsKeywords = [‘guns’, ‘gun’, ‘nra’, ‘pistol’, ‘firearm’, ‘shooting’] immigrationSentiments  = [] immigrationKeywords = [‘immigration’, ‘immigrants’, ‘citizenship’, ‘naturalization’, ‘visas’] employmentSentiments  = [] emplyomentKeywords = [‘jobs’, ‘employment’, ‘unemployment’, ‘job’] inflationSentiments  = [] inflationKeywords = [‘inflate’, ‘inflation’, ‘price hike’, ‘price increase’, ‘prices rais’] minimumwageupSentiments  = [] minimumwageupKeywords = [‘raise minimum wage’, ‘wage increase’, ‘raise wage’, ‘wage hike’] abortionSentiments  = [] abortionKeywords = [‘abortion’, ‘pro-choice’, ‘planned parenthood’] governmentspendingSentiments  = [] governmentspendingKeywords = [‘gov spending’, ‘government spending’, ‘gov. spending’, ‘expenditure’] taxesupSentiments  = [] taxesupKeywords = [‘raise tax’, ‘tax hike’, ‘taxes up’, ‘tax up’, ‘increase taxes’, ‘taxes increase’, ‘tax increase’] taxesdownSentiments  = [] taxesdownKeywords = [‘lower tax’, ‘tax cut’, ‘tax slash’, ‘taxes down’, ‘tax down’, ‘decrease taxes’, ‘taxes decrease’, ‘tax decrease’] Drilling down to the list of terms that are linked to each election sentiment keyword (in the code as  #(nameOfTuple,  sentimentList ,  keywordList  ),  we can see: personSentimentList  = [(‘hillary’, hillarySentiments, hillaryKeywords), (‘trump’, trumpSentiments, trumpKeywords), (‘cruz’, cruzSentiments, cruzKeywords), (‘bernie’, bernieSentiments, bernieKeywords), (‘obama’, obamaSentiments, obamaKeywords)] issueSentimentList  = [(‘guns’, gunsSentiments, gunsKeywords), (‘immigration’, immigrationSentiments, immigrationKeywords), (‘employment’, employmentSentiments, emplyomentKeywords), (‘inflation’, inflationSentiments, inflationKeywords), (‘minimum wage up’, minimumwageupSentiments, minimumwageupKeywords), (‘abortion’, abortionSentiments, abortionKeywords), (‘government spending’, governmentspendingSentiments, governmentspendingKeywords), (‘taxes up’, taxesupSentiments, taxesupKeywords), (‘taxes down’, taxesdownSentiments, taxesdownKeywords) ] Phillips also provides a snippet of code “for taking random twitter IDs” to create a Twitter “ control group.”  This part of the code appears to “skim the most recent tweets that have mentioned one of our [Cambridge Analytica’s pre-defined] keywords.” Phillips explains in the notes within his code about the practicalities of sentiment mining — this is not big data (ie, “all the tweets”) that were being sought out: “it turned out that skimming all of the tweets found very very few occurances of keywords since “twitter is such a global/multilingual platform.” Next, Phillips provides a snippet to parse * any * text that CA was “looking for through  non-tweets  (like transcripts of some sort),” noting that the tool is set up to “find sentiment and  adds  [it] to the respective keywords’ data list”: Interesting functionality, indeed. The lines of code then follow with a function that Phillips states: “goes through tweets of each user, looks for keywords, and if the keyword is there, we find the sentiment for that tweet and add it to the sentiment data list” Finally, the code compiles the collected and refined Twitter data into a set. Phillips describes: “compiles the sentiment data for each keyword group into an easier to work with format (dataframe) … it is only meaningful if compared with a control group, since keyword selection is impossible to employ neutrally.” The final output of   the   Twitteranalysis.py  is a list of tweets and Twitter users (via user IDs) from a pre-defined set of keywords ( abortion ,  NRA ,  Hillary ,  Obama , lower  taxes ,  guns ,  immigration ,  liberals , etc.). All relate to #Election2016 campaign issues. Also, this code appears to be extensible — it can be used outside of Twitter, such as to mine the transcripts and recorded text from  focus   groups  and  survey   respondents . These scripts normally wouldn’t be  that  interesting. But provided both were added by a Cambridge Analytica intern (at least at the time) and contain a running dialog of what the tools do, how they work, and why they were built— and the fact that they are *still* available on Github — I thought I’d share. Almost every pronoun used in the script walkthroughs (see the archived Github links) is  inclusive  and  plural  — “our,” “we,” “we’re,” etc. Also, reference to convert to format “to put into the  neural   network .” Wait, there’s  one  more thing. When Phillips committed his original  Twitteranalysis.py  script, he accidentally  left the  working  Twitter API keys in the code  (via the consumer  key  and consumer “ secret ”). This contains the alphanumeric strings which are used for the developer account to access data from Twitter’s API. Interestingly, on Feb 23, 2017 (yes,  2017 ), Phillips  removed  the API keys: Two days later, another Github user added a comment about Phillip’s mistake: Was this API key Cambridge Analytica’s? Or SCL’s ? While both scripts— the first including Phillips’  @cambridgeanalytica.org  email address, clearly are voter data and election related, from the commentary in the script, it’s not clear who the API key belonged to. This might have been Phillips’ own account. Regardless, this code shows the  inner workings  of client voter file geo-data “enrichment” and presumably automated  voter database processing  for clients by Cambridge Analytica. This code also provides the proof in showing once and for all how Twitter users’ emotional reactions and real-time discussions even favorites/likes (pulled from the API) are  mined  in real time and used to create  test   phrases,  establish  control   groups , and apparently provide sets of  future  terms around keywords related to political campaign issues. The fact that Cambridge Analytica was using this kind of code to mine emotional responses that surface from users’ “recent tweets” from a defined set of 2016 presidential campaign “trigger words” is interesting. medium.com I’m confident Phillips provided this data in earnest, as he includes an excellent working description in the purposes and uses of these scripts. He was an CA intern who wanted to show his work to get a job in the future. Yet, this is part of the arsenal of tools used by Cambridge Analytica to  geolocate American voters  and harness American’s  real-time emotional sentiment  (see example below for Instagram targeting). I’d argue the question of the  ownership  of Cambridge Analytica — a foreign business previously registered in the United States as a  foreign  corporation (SCL Elections ) just became a bit more relevant. Foreign  influence— sound familiar? And that fact that a  working Twitter developer API key  — possibly one of Cambridge Analytica’s own — was left sitting on GitHub by a data  intern  for anyone to use is, well, another story. The code will likely be removed soon, so it’s available here: data.world medium.com"
Your Ultimate Data Mining & Machine Learning Cheat Sheet,our Ultimate Data Mining & Machine Learning Cheat Shee,"There are several areas of data mining and machine learning that will be covered in this cheat-sheet: Train-test-split  is an important part of testing how well a model performs by training it on designated training data and testing it on designated testing data. This way, the model’s ability to generalize to new data can be measured. In  sklearn , both lists, pandas DataFrames, or NumPy arrays are accepted in  X  and  y  parameters. Training a standard supervised learning model  takes the form of an import, the creation of an instance, and the fitting of the model. sklearn  classifier models  are listed below, with the branch highlighted in blue and the model name in orange. sklearn  regressor models  are listed below, with the branch highlighted in blue and the model name in orange. Evaluating model performance  is done with train-test data in this form: sklearn  metrics  for classification and regression are listed below, with the most commonly used metric marked in green. Many of the grey metrics are more appropriate than the green-marked ones in certain contexts. Each have their own advantages and disadvantages, balancing priority comparisons, interpretability, and other factors. Before clustering, the data needs to be standardized (information for this can be found in the Data Transformation section). Clustering is the process of creating clusters based on point distances. Training and creating a K-Means clustering model  creates a model that can cluster and retrieve information about the clustered data. Accessing the labels  of each of the data points in the data can be done with: Similarly, the label of each data point can be stored in a column of the data with: Accessing the cluster label of new data  can be done with the following command. The  new_data  can be in the form of an array, a list, or a DataFrame. Accessing the cluster centers of each cluster  is returned in the form of a two-dimensional array with: To  find the optimal number of clusters , use the silhouette score, which is a metric of how well a certain number of clusters fits the data. For each number of clusters within a predefined range, a K-Means clustering algorithm is trained and its silhouette score saved to a list ( scores ).  data  is the  x  that the model is trained on. After the scores are saved to the list  scores , they can be graphed out or computationally searched for to find the highest one. Dimensionality reduction is the process of expressing high-dimensional data in a reduced number of dimensions such that each one contains the most amount of information. Dimensionality reduction may be used for visualization of high-dimensional data or to speed up machine learning models by removing low-information or correlated features. Principal Component Analysis , or PCA, is a popular method of reducing the dimensionality of data by drawing several orthogonal (perpendicular) vectors in the feature space to represent the reduced number of dimensions. The variable  number  represents the number of dimensions the reduced data will have. In the case of visualization, for example, it would be two dimensions. Fitting the PCA Model : The  .fit_transform  function automatically fits the model to the data and transforms it into a reduced number of dimensions. Explained Variance Ratio : Calling  model.explained_variance_ratio_  will yield a list where each item corresponds to that dimension’s “explained variance ratio”, which essentially means the percent of information in the original data represented by that dimension. The sum of the explained variance ratios is the total percent of information retained in the reduced dimensionality data. PCA Feature Weights : In PCA, each newly creates feature is a linear combination of the former data’s features. These   linear weights can be accessed with  model.components_ , and are a good indicator for feature importance (a higher linear weight indicates more information represented in that feature). Linear Discriminant Analysis  (LDA, not to be commonly confused with Latent Dirichlet Allocation) is another method of dimensionality reduction. The primary difference between LDA and PCA is that LDA is a supervised algorithm, meaning it takes into account both  x  and  y . Principal Component Analysis only considers  x  and is hence an unsupervised algorithm. PCA attempts to maintain the structure (variance) of the data purely based on distances between points, whereas LDA prioritizes clean separation of classes. Feature Importance is the process of finding the most important feature to a target. Through PCA, the feature that contains the most information can be found, but feature importance concerns a feature’s impact on the target. A change in an ‘important’ feature will have a large effect on the  y -variable, whereas a change in an ‘unimportant’ feature will have little to no effect on the  y -variable. Permutation Importance  is a method to evaluate how important a feature is. Several models are trained, each missing one column. The corresponding decrease in model accuracy as a result of the lack of data represents how important the column is to a model’s predictive power. The  eli5  library is used for Permutation Importance. In the data that this Permutation Importance model was trained on, the column  lat  has the largest impact on the target variable (in this case the house price). Permutation Importance is the best feature to use when deciding which to remove (correlated or redundant features which actually confuse the model, marked by negative permutation importance values) in models for best predictive performance. SHAP  is another method of evaluating feature importance, borrowing from game theory principles in Blackjack to estimate how much value a player can contribute. Unlike permutation importance,  SH apley  A ddative Ex P lanations use a more formulaic and calculation-based method towards evaluating feature importance. SHAP requires a tree-based model (Decision Tree, Random Forest) and accommodates both regression and classification. PD(P) Plots , or partial dependence plots, are a staple in data mining and analysis, showing how certain values of one feature influence a change in the target variable. Imports required include  pdpbox  for the dependence plots and  matplotlib  to display the plots. Isolated PDPs : the following code displays the partial dependence plot, where  feat_name  is the feature within  X  who will be isolated and compared to the target variable. The second line of code saves the data, whereas the third constructs the canvas to display the plot. The partial dependence plot shows the effect of certain values and changes in the number of square feet of living space on the price of a house. Shaded areas represent confidence intervals. Contour PDPs : Partial dependence plots can also take the form of contour plots, which compare not one isolated variable but the relationship between two isolated variables. The two features that are to be compared are stored in a variable  compared_features . The relationship between the two features shows the corresponding price when only considering these two features. Partial dependence plots are chock-full of data analysis and findings, but be conscious of large confidence intervals. Standardizing or scaling  is the process of ‘reshaping’ the data such that it contains the same information but has a mean of 0 and a variance of 1. By scaling the data, the mathematical nature of algorithms can usually handle data better. The  transformed_data  is standardized and can be used for many distance-based algorithms such as Support Vector Machine and K-Nearest Neighbors. The results of algorithms that use standardized data need to be ‘de-standardized’ so they can be properly interpreted.  .inverse_transform()  can be used to perform the opposite of standard transforms. Normalizing  data puts it on a 0 to 1 scale, something that is, similarly to standardized data, makes the data mathematically easier to use for the model. While normalizing doesn’t transform the shape of the data as standardizing does, it restricts the boundaries of the data. Whether to normalize or standardize data depends on the algorithm and the context. Box-cox transformations  involve raising the data to various powers to transform it. Box-cox transformations can normalize data, make it more linear, or decrease the complexity. These transformations don’t only involve raising the data to powers, but also to fractional powers (square rooting) and logarithms. For instance, consider data points situated along the function  g ( x ). By applying the logarithm box-cox transformation, the data can be easily modelled with linear regression. sklearn  automatically determines the best series of box-cox transformations to apply to the data to make it better resemble a normal distribution. Because of the nature of box-cox transformation square-rooting, box-cox transformed data must be strictly positive (normalizing the data before hand can take care of this). For data with negative data points as well as positive ones, set  method = ‘yeo-johnson’  for a similar approach to making the data more closely resemble a bell curve. Be sure to bookmark this page for easy reference if you find it helpful. Often, data mining and analysis will require visualization — feel free to check out another cheat sheet for visualization. While you’re creating visualizations and performing machine learning operations, you may want to take a look at the data manipulation and cleaning cheat sheet. medium.com medium.com medium.com"
Data Mining Has Revealed Previously Unknown Russian Twitter Troll Campaigns,ata Mining Has Revealed Previously Unknown Russian Twitter Troll Campaign,By Emerging Technology from the arXiv
Data Mining in Brief,ata Mining in Brie,"Data mining is a very popular topic nowadays. Unlike a few years ago, everything is bind with data now and we are capable of handling these kinds of large data well. By collecting and inspecting these data, people were able to discover some patterns. Even the whole data set is a junk, there are some hidden patterns that can be extracted by combining multiple data sources to provide valuable insights. This is called as  Data Mining. Data mining is often combined with various sources of data including enterprise data that is secured by an organization and has privacy issues and sometimes multiple sources are integrated including third party data, customer demographics and financial data etc. The amount of data available is a critical factor here. Since we are going to discover patterns in sequential or non-sequential data, correlations, to determine if the amount of obtained data is of good quality, as much as data available is good. Let’s start with an example. Assume we got some data related to login logs for a web application. As a whole, this set of data has no value. It may contain the username of a user, login timestamp, time spent to log out, activities have done etc. If we take an overview look at this, it is a whole mess. But we can analyze this to do extract some useful information. For example, this data can be used to find out a regular habit of a particular user. Further, it will help to find out the peak hours of the system. This extracted information can be used to increase the efficiency of the system and make more user-friendly. However, data mining is not a simple task. It takes a certain amount of time and it requires a special procedure as well. The basic steps of data mining are follows There are different kinds of models associated with data mining In  Descriptive Modeling , it detects the similarities between the collected data and the reasons behind them. This is very important in constructing the final conclusion from the data set. Predictive Modeling  is used to analyze the past data and predict the future behavior. Past data give some kind of hint about the future. With the significant development of web, text mining has added as a related discipline to data mining. It is required to process, filter and analyze data properly to create such predictive models. Data mining is useful in many ways. For marketing, it can be applied effectively. Using data mining we can analyze the behavior of customers and we can do advertising by getting more close to them. It will help to identify trends of customers for goods in the market and it allows the retailer to understand the purchase behavior of a buyer. In education domain we can identify the learning behaviors of students and the learning institutions can upgrade their modules and courses accordingly. We can use data mining to solve natural disasters as well. If we can collect some information, we can use them to predict things like land sliding, rainfall, tsunami etc. There are much more applications in data mining nowadays. They can vary from very simple things like marketing to very complex domains like making environmental disaster predictions etc. Thanks for reading… Cheers!"
Data Mining Reveals How The “Down-Vote” Leads To A Vicious Circle Of Negative Feedback,ata Mining Reveals How The “Down-Vote” Leads To A Vicious Circle Of Negative Feedbac,"In the 1930s, the American psychologist Burrhus Skinner popularised the notion of operant conditioning, the notion that an individual’s future behaviour is determined by the punishments and rewards he or she has received in the past. It means that specific patterns of behaviour can be induced by punishing unwanted actions while rewarding those that are desired. And it certainly works with rats and pigeons. This idea has since become one of the foundations of behavioural psychology and is an important driver of the way online social networks are designed and operate. Many have systems that allow people to like, or up-vote, certain types of content while disliking, or down-voting, others. An up-vote can be thought of as a reward designed to encourage while the down-vote is a punishment designed to discourage. In theory, this should guide contributors towards producing better content that is more likely to be rewarded. At least, that’s what the theory of operant conditioning predicts. But does that actually happen on real social networks? Today, we find out thanks to the work of Justin Cheng at Stanford University in Palo Alto and a couple of buddies. These guys have measured how up-voting and down-voting influences the behaviour of a large number of contributors to different social networks. And they say that the results are far from reassuring. The evidence is that a contributor who is down-voted produces lower quality content in future that is valued even less by others on the network. What’s more, people are more likely to down-vote others after they have been down voted themselves. The result is a vicious spiral of increasingly negative behaviour that is exactly the opposite of the intended effect. Cheng and co began by compiling a dataset of the comments associated with news articles on four online communities: CNN.com, a general news site; Breitbart.com, a political new site; IGN.com, a computer gaming news site; and Allkpop.com, a Korean entertainment site. The data includes 1.2 million threads with 42 million comments and 114 million votes from 1.8 million different users. These guys conducted a survey on Amazon’s Mechanical Turk asking people to rate the quality of comments from these communities and then worked out the how the percentage of up-votes the post received correlated with the human evaluation. This confirmed that the percentage of up-votes is indeed a good measure of the quality of a post. Cheng and co then built a machine learning algorithm that predicts a post’s quality by examining the words it contains. They trained the algorithm on half of the posts in the community and found that its ratings correlated well with the subjective opinions of humans. So the algorithm is an automatic way of rating the quality of every post in their dataset. Then came the actual experiment. They used the machine learning algorithm to find posts of equal quality but with a twist: they matched posts into pairs in which one had been positively received by the community while the other had been negatively received. In other words, one of these posts received more up votes while the other received more down votes. They then assessed the future output of the authors of these posts to measure the effect of positive and negative evaluations. The results are something of an eye-opener. “We find that negative feedback leads to significant behavioural changes that are detrimental to the community,” say Cheng and co. “Not only do authors of negatively-evaluated content contribute more, but also their future posts are of lower quality, and are perceived by the community as such,” they say. And it gets worse: “These authors are more likely to subsequently evaluate their fellow users negatively, percolating these effects through the community.” By contrast, positive feedback does not appear to influence authors much at all. It does not encourage them to write more and does not improve the quality of their posts. Curiously, authors that receive no feedback, are more likely to leave the community entirely. “Surprisingly, our findings are in a sense exactly the opposite than what we would expect under the operant conditioning framework,” say Cheng and co. That points to an obvious strategy for improving the quality of comments on any social network site. Clearly, providing negative feedback to “bad” users does not appear to be a good way of preventing undesired behaviour. So how can unwanted behaviour be stopped? “Given that users who receive no feedback post less frequently, a potentially effective strategy could be to ignore undesired behaviour and provide no feedback at all,” say Cheng and co. That’s an interesting study that provides a fascinating insight into the complex nature of social interactions. And it backs up certain kinds of real life experience. Anyone with children will know that it is possible to unintentionally reward bad behaviour with the increased attention that it generates. So sometimes it’s better to ignore unwanted behaviour than to focus on it. But at the same time, parents also need an effective way of stepping in and actively preventing more serious incidents of undesirable behaviour when necessary. The work of Cheng and co clearly suggested ignoring bad behaviour is an effective way of discouraging it, and one that social network sites might profitably explore. But at the same time, these sites will need a way to step in and actively prevent certain types of behaviour when necessary. What’s needed now, of course, is a test of this idea. There are certainly social networks that allow up voting but not down voting (Medium being one of them). An interesting question is whether it results in the same rich tapestry of opinion that clearly flourishes on social network sites that allow both types of voting. In other words, does this kind of manipulation have other consequences that Cheng and co have not yet accounted for. Clearly, there’s interesting work ahead in teasing these things apart. Ref:  arxiv.org/abs/1405.1429  : How Community Feedback Shapes User Behavior Follow the Physics arXiv Blog on Twitter at  @arxivblog , on  Facebook  and by hitting the Follow button below"
Data Mining Tools,ata Mining Tool,"Huge amount of data generated every second and it is necessary to have knowledge of different tools that can be utilized to handle this huge data and apply interesting data mining algorithms and visualizations in quick time. Data Mining  is the set of methodologies used in analyzing data from various dimensions and perspectives, finding previously unknown hidden patterns, classifying and grouping the data and summarizing the identified relationships. The tasks of data mining are twofold: Four most useful data mining techniques: For doing quick analysis on data using any data mining technique it is important to have hands on knowledge of different tools. All the tools mentioned below has its own peculiarity in terms of implementation and each has its own merits. It all boils down to the requirement of task. Most important thing is to know that tools exist which can immensely enhance the efficiency of a data scientist or a student working on some project, so that you can focus more the things that matter that is is gaining useful insights and making projections. It also takes the pain of implementing any standard algorithm from scratch but at the same time gives you the power to modify the code of tool (open source ) as per requirements. There are many tools apart from mentioned below and I encourage you to check that out as well. The list I have provided are the one that are most common and used widely in leading companies as well as academia. Also most of them are open source == Awesome :) . Rapid Miner This is very popular since it is a ready made, open source, no-coding required software, which gives advanced analytics. Written in Java, it incorporates multifaceted data mining functions such as data pre-processing, visualization, predictive analysis, and can be easily integrated with WEKA and R-tool to directly give models from scripts written in the former two. Besides the standard data mining features like data cleansing, filtering, clustering, etc, the software also features built-in templates, repeatable work flows, a professional visualisation environment, and seamless integration with languages like Python and R into work flows that aid in rapid prototyping. Download  |  Site  |  Tutorial Weka Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes. Download  |  Site  |  Tutorial Orange Python users playing around with data sciences might be familiar with Orange. It is a Python library that powers Python scripts with its rich compilation of mining and machine learning algorithms for data pre-processing, classification, modelling, regression, clustering and other miscellaneous functions. Orange also comes with a visual programming environment and its workbench consists of tools for importing data, and dragging and dropping widgets and links to connect different widgets for completing the workflow. Download  |  Site  |  Tutorial R R is a free software environment for statistical computing and graphics written in C++. R Studio is IDE specially designed for R language.It is one of the leading tools used to do data mining tasks and comes with huge community support as well as packaged with hundreds of libraries built specifically for data mining. Download  |  Site  |  Tutorial Knime Primarily used for data preprocessing — i.e. data extraction, transformation and loading, Knime is a powerful tool with GUI that shows the network of data nodes. Popular amongst financial data analysts, it has modular data pipe lining, leveraging machine learning, and data mining concepts liberally for building business intelligence reports. Download  |  Site  |  Tutorial Rattle Rattle, expanded to ‘R Analytical Tool To Learn Easily’, has been developed using the R statistical programming language. The software can run on Linux, Mac OS and Windows, and features statistics, clustering, modelling and visualisation with the computing power of R. Rattle is currently being used in business, commercial enterprises and for teaching purposes in Australian and American universities. Download  |  Site  |  Tutorial Tanagra TANAGRA is a free open source data mining software for academic and research purposes. It proposes several data mining methods from exploratory data analysis, statistical learning, machine learning and databases area. TANAGRA is more powerful, it contains some supervised learning but also other paradigms such as clustering, factorial analysis, parametric and non parametric statistics, association rule, feature selection and construction algorithms.The main purpose of Tanagra project is to give researchers and students an easy-to-use  data mining software , conforming to the present norms of the software development in this domain (especially in the design of its GUI and the way to use it), and allowing to analyse either real or synthetic data. Download  |  Site  |  Tutorial XL Miner XLMiner is the only comprehensive data mining add-in for Excel, with neural nets, classification and regression trees, logistic regression, linear regression, Bayes classifier, K-nearest neighbors, discriminant analysis, association rules, clustering, principal components, and more. XLMiner provides everything you need to sample data from many sources — PowerPivot, Microsoft/IBM/Oracle databases, or spreadsheets; explore and visualize your data with multiple linked charts; preprocess and ‘clean’ your data, fit data mining models, and evaluate your models’ predictive power. The drawback of XL Miner is that is paid add in for excel but there is 15 day free trial option. The software has great features and its integration in excel makes life easier. Download  |  Site  |  Tutorial . “ The goal is to turn data into information, and information into insight.” — Carly Fiorina (  former executive, president, and chair of Hewlett-Packard Co.) . [1]  http://opensourceforu.com/2017/03/top-10-open-source-data-mining-tools/ [2]  https://thenewstack.io/six-of-the-best-open-source-data-mining-tools/ [3]  http://blog.galvanize.com/four-data-mining-techniques-for-businesses-that-everyone-should-know/ [4]  http://www.rdatamining.com/resources/tools [5]  https://www.invensis.net/blog/data-processing/12-data-mining-tools-techniques/ [6]  https://www.predictiveanalyticstoday.com/top-free-data-mining-software/ [7]  https://www.kdnuggets.com/software/index.html"
Data Engineering Roadmap For 2021,ata Engineering Roadmap For 202,"Maybe it’s the 6 figure salaries, the opportunity to work with cool technology or people are finally learning that data engineering is where everything starts in the data field. Whatever the reason, people are noticing."
5 Data Engineering Projects To Add to Your Resume, Data Engineering Projects To Add to Your Resum,"All signs point towards an auspicious future for data engineering. Dice’s 2020 tech jobs report cites data engineering as the fastest-growing field in 2020, increasing by a staggering 50%,  while data science roles only increased by 10% . You can rest assured that the influx of…"
Functional Data Engineering — a modern paradigm for batch data processing,unctional Data Engineering — a modern paradigm for batch data processin,"Batch data processing  — historically known as ETL —  is extremely challenging . It’s time-consuming, brittle, and often unrewarding. Not only that, it’s hard to operate, evolve, and troubleshoot. In this post, we’ll explore how applying the  functional programming paradigm  to  data engineering  can bring a lot of  clarity  to the process. This post distills fragments of wisdom accumulated while working at Yahoo, Facebook, Airbnb and Lyft, with the perspective of well over a decade of data warehousing and data engineering experience. Let’s start with a quick primer/refresher on what functional programming is about, from the  functional programming Wikipedia page : In  computer science ,  functional programming  is a  programming paradigm  — a style of building the structure and elements of  computer programs  — that treats  computation  as the evaluation of  mathematical functions  and avoids changing- state  and  mutable  data. It is a  declarative programming  paradigm, which means programming is done with  expressions [1]  or declarations [2]  instead of  statements . In functional code, the output value of a function depends only on the  arguments  that are passed to the function, so calling a function f twice with the same value for an argument x produces the same result f(x)each time; this is in contrast to  procedures  depending on a  local  or  global state , which may produce different results at different times when called with the same arguments but a different program state. Eliminating  side effects , i.e., changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming. Functional programming brings  clarity . When functions are “pure” — meaning they do not have side-effects — they can be written, tested, reasoned-about and debugged in isolation, without the need to understand external context or history of events surrounding its execution. As ETL pipelines grow in complexity, and as data teams grow in numbers, using methodologies that provide clarity isn’t a luxury, it’s a necessity. Reproducibility and replicability are foundational to the  scientific method . The greater the claim made using analytics, the greater the scrutiny on the process should be. In order to be able to trust the data, the process by which results were obtained should be transparent and reproducible. While business rules evolve constantly, and while corrections and adjustments to the process are more the rule than the exception, it’s important to insulate compute logic changes from data changes and have control over all of the moving parts. Let’s take an example where some raw data provided by some other company has been identified as faulty, and the decision has been made to re-import corrected data and reprocess a certain time-range to correct the issue. Now we know that all of the data structures derived from that table, for that time range, need to be reprocessed, or  backfilled . In theory, what we want to do is to apply the exact same compute scheme as we did on the original pass, on top of the new, corrected data. Note that it’s also important for the related datasets used in this computation to be identical as they were at the time of the original computation. For example, if one of the downstream processes joins to a dimension to enrich the data, we’d want for that dimension to be identical to how it was when computing the original process. If we cannot provide this guarantee, it implies that performing the same computation, for the same period may yield different results each time it is computed. Similarly, if we want to change a business rule and perform a backfill all of the downstream computation, we need the guarantee that the change of logic is the only moving part. We need that same guarantee that the blocks of data used in the computation are identical to the ones used when re ran the original process, or in other words, that the sources have have not been altered. To put it simply, immutable data along with versioned logic are key to reproducibility. To make batch processing functional, the first thing is to avoid any form of side-effects outside the scope of the task. A pure task should be  deterministic  and  idempotent , meaning that it will produce the same result every time it runs or re-runs. This requires forcing an overwrite approach, meaning re-executing a pure task with the same input parameters should overwrite any previous output that could have been left out from a previous run of the same task. Having idempotent tasks is vital for the  operability  of pipelines. When tasks fail, or when the compute logic needs to be altered for whatever reason, we need the certainty that re-running a task is safe and won’t lead to double-counting or any other form of bad state. Without this assumption, making decisions about what needs to be re-executed requires the full context and understanding of all potential side-effects of previous executions. Note that contrarily to a pure-function, the pure-task is typically not “returning” an object in the programming sense of the term. In the context of a SQL  ELT-type approach  which has become common nowadays, it is likely to be simply overwriting a portion of a table (partition). While this may look like a side-effect, you can think of this output as akin to the immutable object that a typical pure-function would return. Also, for the sake of simplicity and as a best practice, individual tasks should target a single output, the same way that a pure-function shouldn’t return a set of heterogenous objects. In some cases it may appear difficult to avoid side-effects. One potential solution is to re-think the size of the unit of work. Can the task become pure if it encompasses other related tasks? Can it be purified by breaking down into a set of smaller tasks? It’s also important that all transitional states within the pure-task are insulated much like locally scoped variables in pure-functions. Multiple instances of the pure task should be fully independent in their execution. If temporary tables or dataframes are used, they should be implemented in a way that task instances cannot interfere with one another so that they can be parallelized. One of the way to enforce the functional programming paradigm is to use immutable objects. While purely functional languages will prevent mutations by enforcing the use of immutable objects, it’s possible to use functional practices even in a context where the exposed objects are in fact mutable. While your database of choice may allow you to update, insert, and delete at will, it doesn’t mean you should. DML operations like UPDATE, APPEND and DELETE are performing mutations that ripple in side-effects. Thinking of partitions as immutable blocks of data and systematically overwriting partitions is the way to make your tasks functional. A pure task should always fully overwrite a partition as its output. Your data store may not have support for physical partitions, but that does not prevent you from taking a functional approach. To work around this limitation, one option is to implement your own partitioning scheme by using different physical tables as the output of your pure tasks that can be UNIONed ALL into views that act as logical tables. Results may vary depending on how smart your database optimizer is. Alternatively, it’s also possible to logically partition a table and to systematically DELETE prior to INSERTing using a partitioning key that reflects the parameters used to instantiate the task. Note that where a TRUNCATE PARTITION is typically a “free” metadata operation, a DELETE operation may be expensive and that should be taken into considerations. If you decide to go that route, note that on many database engines it may more efficient to check whether the DELETE operation is needed first to avoid unnecessary locking. The staging area is the conceptual loading dock of your data warehouse, and while in the case a real physical retail-type warehouse you’d want to use a transient staging area and keep it unobstructed, in most modern data warehouse you’ll want to accumulate and persist all of your source data there, and keep it unchanged forever. Given a persistent immutable staging area and pure tasks, in theory it’s possible to recompute the state of the entire warehouse from scratch (not that you should), and get to the exact same state. Knowing this, the retention policy on derived tables can be shorter, knowing that it’s possible to backfill historical data at will. Business logic and related computations tend to change over time, sometimes in retroactive fashion, sometimes not. When retroactive, you can think of the change as making existing downstream partitions “dirty” and it calls for them to be recomputed. When non-retroactive, the ETL logic should capture the change and apply the proper logic to the corresponding time range. Logic that changes over time should always be captured inside the task logic and be applied conditionally when instantiated. This means that ideally the logic in source control describes how to build the full state of the data warehouse throughout all time periods. For example, if a change is introduced as a new rule around how to calculate taxes for 2018 that shouldn’t be applied prior to 2018, it would be dangerous to simply update the task to reflect this new logic moving forward. If someone else was to introduce an unrelated change that required “backfilling” 2017, they would apply the 2018 rule to 2017 data without knowing. The solution here is to apply conditional logic within the task with a certain effective date, and depending on the slice of data getting computed, the extra tax logic would apply where needed. Also note that in many cases business rules changes over time are best expressed with data as opposed to code. In those cases it’s desirable to store this information in “parameter tables” using effective dates, and have logic that joins and apply the right parameters for the facts being processed. An oversimplified example of this would be a “taxation rate” table that would detail the rates and effective periods, as opposed to hard-coded rates wrapped in conditional blocks of code. By nature, dimensions and other referential data is slowly changing, and it’s tempting to model this with UPSERTs (UPdating what has changed and inSERTing new dimension members) or to apply other  slowly-changing-dimension methodology  to reflect changes while keeping track of history. But how do we model this in a functional data warehouse without mutating data? Simple. With  dimension snapshots  where a new partition is appended at each ETL schedule. The dimension table becomes a collection of dimension snapshots where each partition contains the full dimension as-of a point in time.  “But only a small percentage of the data changes every day, that’s a lot of data duplication!” . That’s right, though typically dimension tables are negligible in size in proportion to facts. It’s also an elegant way to solve SCD-type problematic by its simplicity and reproducibility. Now that storage and compute are dirt cheap compared to engineering time, snapshoting dimensions make sense in most cases. While the traditional type-2 slowly changing dimension approach is conceptually sound and may be more computationally efficient overall, it’s cumbersome to manage. The processes around this approach, like managing surrogate keys on dimensions and performing surrogate key lookup when loading facts, are error-prone, full of mutations and hardly reproducible. In case of very large dimensions, mixing the snapshot approach along with SCD-type methodology may be reasonable or necessary. Though in general it’s sufficient to have the current attribute only in the current snapshot, and denormalize dimension attributes into fact table where the attribute-at-the-time-of-the-event is important. For the rare cases where attribute-at-the-time-of-the-event importance was not foreseen and denormalized into fact upfront, you can always run a more expensive query that joins the facts to their time-relative dimension snapshots as opposed to the latest snapshot. You could almost think about the “dimension snapshotting” approach as a type of further  denormalization  that has similar tradeoffs as other types of denormalization. Another modern approach to store historical values in dimensions is to use complex or nested data types. For example if we wanted to keep track of the customer’s state over time for tax purposes, we could introduce a “state_history” column as a map where keys are effective dates and values are the state. This allows for history tracking without altering the grain of the table like a type-2 SCD requires, and is much more dynamic than a type-3 SCD since it doesn’t require creating a new column for every pice of information. Clarity around the unit of work is also important, and the unit of work should be directly aligned to output to a single partition. This makes it trivial to map each logical table to a task, and each partition to a task instance. By being strict in this area it makes it easy to directly map each partitions in your data store to its corresponding compute logic. For example, in this Airflow “tree view” where squares represents task instances of a DAG of tasks over time, it’s comforting to know that each row represents a task that corresponds to a table, and that each cell is a task instance that corresponds to a partition. It’s easy to track down the log file that correspond to any given partition. Given that tasks are idempotent, re-running any of these cells is a safe operation. This set of rule makes everything easier to maintain since a clear scope on the units of work make everything clear and maintainable. As a side note, Airflow was designed with functional data engineering in mind, and provides a solid framework to orchestrate idempotent, pure-tasks at scale. Full disclosure is due here, the author of this post happens to be the creator of Airflow, so no wonder the tooling works well with the methodology prescribed here. While you may think of your workflow as a directed acyclic graph (DAG) of tasks, and of your data lineage as a graph made of tables as nodes, the functional approach allows you to conceptualize a more accurate picture made out of task instances and partitions. In this more detailed graph, we move away from individual rows or cells being the “atomic state” that can be mutated to a place where partitions are the smallest unit that can be changed by tasks. This data lineage graph of partitions is much easier to conceptualize to the alternative where any row could have been computed by any task. With this partition-lineage graph, things become clearer. The lineage of any given row can be mapped to a specific task instance through its partition, and by following the graph upstream it’s possible to understand the full lineage as a set of partitions and related task instances. A common pattern that leads to increased “state complexity” is when a partition depends on a previous partition on the same table. This leads to growing complexity linearly over time. If for instance, computing the latest partition of your user dimension uses the previous partition of that same table, and the table has 3 years of history with daily snapshot, the depth of the resulting graph grows beyond a thousand and the complexity of the graph grows linearly over time. Strictly speaking, if we had to reprocess data from a few months back, we may need to reprocess hundreds of partition using a DAG that  cannot be parallelized . Given that backfills are common and that past dependencies lead to high-depth DAGs with limited parallelization, it’s a good practice to avoid modeling using past-dependencies whenever possible. In cases where cumulative-type metrics are necessary (think live-to-date customer spending as part of a customer dimension for instance), one option is to model the computation of this metric in a specialized framework or somewhat independent model optimized for that purpose. A cumulative computation framework could be a topic for an entire other blog post, let’s leave this out of scope for this post. Late arriving facts can be problematic with a strict immutable data policy. Unfortunately late arriving data is fairly common, especially in given the popularity of mobile phones and occasional instability of networks. To bring clarity around this not-so-special case, the first thing to do is to dissociate the event’s time from the event’s reception or processing time. Where late arriving facts may exist and need to be handled, time partitioning should always be done on event processing time. This allows for landing immutable blocks of data without delays, in a predictable fashion. This effectively brings in two tightly related time dimensions to your analytics and allows to do intricate analysis specific to late-arriving facts. Knowing when events were reported in relation to when they occurred is useful. It allows for showing figures like “total sales in February as of today”, “total sales in February as of March 1st” or “how much sales adjustments on February since March 1st”. This effectively provides a time machine that allows you to understand what reality looked like at any point in time. When defining your partitioning scheme based on event-processing time, it means that your data is not longer partitioned on event time, and means that your queries that will typically have predicates on event time won’t be able to benefit from  partition pruning  (where the database only bothers to read a subset of the partitions). It’s clearly an expensive tradeoff. There are a few ways to mitigate this. One option is to partition on both time dimensions, this should lead to a relatively low factor of partition multiplication, but raises the complexity of the model. Another option is to author queries that apply predicates on both event time and on a relatively wider window on processing time where partition pruning is needed. Also note that in some cases, read-optimized stores may not suffer much from the inability the database optimizer to skip partitions through pruning as execution engine optimizations can kick in to reach comparable performance. For example, if your engine is processing ORC or Parquet files, the execution will be limited to reading the file header before moving on to the next file. Rules are meant to be broken and in some cases it’s rational to do so. A common pattern we’ve observed is to trade perfect immutability guarantees in exchange for earlier SLAs (Service License Agreement on acceptable delays make data available). Let’s take an example where we want to compute aggregates that depend on the user dimension, but that this user dimension usually lands very late in the day. Perhaps we care more about our aggregate landing early in the day than we care about accuracy. Given this preference, it’s possible to join onto the latest partition available at the time the other dependencies are met. Note that this can easily limited to a specified time range (say 2–3 partitions) to insure a minimum level of accuracy. Of course this means that re-processing the aggregation table later in the future may lead to slightly different results. While this post may be a first in formalizing the functional approach to data engineering, these practices aren’t experimental or new by any means. Large, data driven, analytically mature corporations have been applying these methods for years, and reputable tooling that prescribes this approach has been widely adopted in the industry. It’s clear that the functional approach contrasts with with decades of data-warehousing literature and practices. It’s arguable that this new approach is more aligned with how modern read-optimized databases function as they tend to use immutable blocks or segments and typically don’t allow for row-level mutations. This methodology also skews on treating storage as cheap and engineering time as expensive, duplicating data for the sake of clarity and reproducibility. At the time where Ralph Kimball authored  The Datawerouse Toolkit , databases used for warehousing were highly mutable, and data teams were small and highly specialized. Things have changed quite a bit since then. Data is larger, read-optimized stores are typically built on top of immutable blocks, we’ve seen the rise of distributed systems, and the number and proportion of people partaking in the “analytics process” has exploded. More importantly, given that this methodology leads to more manageable and maintainable workloads, it empowers larger teams to push the boundaries of what’s possible further by using reproducible and scalable practices."
Your 2022 Data Engineering Roadmap,our 2022 Data Engineering Roadma,"What’s up dear readers ? I’ve been absent for almost two months. It was for a good reason, I was working on one of my Dreams (I’ll talk about it in a Future blog post) Note: I have received no compensation for writing this piece. Please consider supporting my and others’ writing by  becoming a Medium member with this link . A new year means, new resolutions. A good resolution to have in 2022 is doing a career pivot and become a Data Engineer. I am sharing with you an updated version of my Data Engineering Roadmap. For every skill I’ve identified as essential, I am linking to THE BEST online course you can find on the matter. I am assuming you’re completely new to the tech industry so feel free to skip the steps and concepts you’re already comfortable with. Learn one / or two of the following Programming Languages (Python — Java — Scala) Let’s connect on LinkedIn . I work as a Lead Cloud Data Engineer for  DataScientest.com , a leading Data-driven French EdTech Startup specialized in Data and Cloud Training. I am a Certified Data Engineer on the Top 3 leading Cloud Platforms: AWS, Google, Microsoft Azure. In addition, I hold 2x Kubernetes and 6x additional AWS certifications. I happen to suffer from a severe physical disability tying me to the use of an Electric Wheelchair. I am a Disability Rights advocate and a happily married father of two boys."
Data engineering in 2020,ata engineering in 202,"It is incredible how fast data processing tools and technologies are evolving. And with it, the nature of the data engineering discipline is changing as well. Tools I am using today are very different from what I used ten or even five years ago, however, many lessons learned are still relevant today. I have started to work in the data space long before  data engineering became a thing  and  data scientist became the sexiest job of the 21st century . I ‘officially’ became a big data engineer six years ago, and I know firsthand the challenges developers with a background in “traditional” data development have going through this journey. Of course, this transition is not easy for software engineers either, it is just different. Even though technologies keep changing — and that’s the reality for anyone working in the tech industry — some of the skills I had to learn are still relevant, but often overlooked by data developers who are just starting to make the transition to data engineering. These usually are the skills that software developers often take for granted. In this post, I will talk about the evolution of data engineering and what skills “traditional” data developers might need to learn today (Hint: it is not Hadoop). Data teams before the Big Data craze were composed of business intelligence and ETL developers. Typical BI / ETL developer activities involved moving data sets from location A to location B (ETL) and building the web-hosted dashboards with that data (BI). Specialised technologies existed for each of those activities, with the knowledge concentrated within the IT department. However, apart from that, BI and ETL development had very little to do with software engineering, the discipline which was maturing heavily at the beginning of the century. As the data volumes grew and interest in data analytics increased, in the past ten years, new technologies were invented. Some of them died, and others became widely adopted, that in turn changed demands in skills and teams’ structures. As modern BI tools allowed analysts and business people to create dashboards with minimal support from IT teams, data engineering became a new discipline, applying software engineering principles to ETL development using a new set of tools. Creating a data pipeline may sound easy, but at big data scale, this meant bringing together a dozen different technologies (or more!). A data engineer had to understand a myriad of technologies in-depth, pick the right tool for the job and write code in Scala, Java or Python to create resilient and scalable solutions. A data engineer had to know their data to be able to create jobs which benefit from the power of distributed processing. A data engineer had to understand the infrastructure to be able to identify reasons for failed jobs. Conceptually, many of those data pipelines were typical ETL jobs — collecting data sets from a number of data sources, putting them in a centralised data store ready for analytics and transforming them for business intelligence or machine learning. However, “traditional” ETL developers didn’t have the necessary skills to perform these tasks in the Big Data world. I have reviewed many articles describing what skills data engineers should have. Most of them advise learning technologies like Hadoop, Spark, Kafka, Hive, HBase, Cassandra, MongoDB, Oozie, Flink, Zookeeper, and the list goes on. While I agree that it won’t hurt to know these technologies, I find that in many cases today, in 2020, it is enough to “know about them” — what particular use cases they are designed to solve, where they should or shouldn’t be used and what are the alternatives. Rapidly evolving cloud technology has given rise to a huge range of cloud-native applications and services in recent years. In the same way as modern BI tools made data analysis more accessible to the wider business several years ago, modern cloud-native data stack simplifies data ingestion and transformation tasks. I do not think that technologies like Apache Spark will become any less popular in the next few years as they are great for complex data transformations. Still, the high rate of adoption of cloud data warehouses such as Snowflake and Google BigQuery indicates that there are certain advantages they provide. One of them is that Spark requires highly specialised skills, whereas ETL solutions on top of cloud data platforms are heavily reliant on SQL skills even for big data — such roles are much easier to fill. BI / ETL developers usually possess a strong understanding of database fundamentals, data modelling and SQL. These skills are still valuable today and mostly transferable to a modern data stack — which is leaner and easier to learn than the Hadoop ecosystem. Below are three areas I often observe “traditional” data developers having gaps in their knowledge because, for a long time, they didn’t have tools and approaches software engineers did. Understanding and fixing those gaps will not take a lot of time, but might make a transition to a new set of tools much smoother. SQL code is a code, and as such, software engineering principles should be applied. I am a big fan of  DBT  — an open-source tool which brings software engineering best practices to SQL world and simplifies all these steps. It does  much more  than that so I strongly advise to check it out. 2. Good understanding of the modern cloud data analytics stack We tend to stick with the tools we know because they often make us more productive. However, many challenges we are facing are not unique, and often can be solved today more efficiently. It might be intimidating trying to navigate in the cloud ecosystem at first. One workaround is to learn from other companies’ experiences. Many successful startups are very open about their data stack and the lessons they learnt on their journey. These days, it is common to adopt a version of a cloud data warehouse and several other components for data ingestion (such as Fivetran or Segment) and data visualisation. Seeing a few architectures is usually enough to get a 10,000-foot view and know what to research further when needed — e.g. dealing with events or streaming data might be an entirely new concept. 3. Knowing a programming language in addition to SQL As much as I love Scala, Python seems to be a safe bet to start with today. It is reasonably easy to pick up, loved by data scientists and supported pretty much by all components of cloud ecosystems. SQL is great for many data transformations, but sometimes it is easier to parse complex data structure with Python before ingesting it into a table or use it to automate specific steps in the data pipeline. This is not an exhaustive list, and different companies might require different skills, what brings me to my last point … Data processing tools and technologies have evolved massively over the last few years. Many of them have evolved to the point where they can easily scale as the data volume grows while working well with the “small data” too. That can significantly simplify both the data analytics stack and the skills required to use it. Does it mean that the role of a data engineer is changing? I think so. It doesn’t mean it gets easier — business demands grow as technology advances. However, it seems that this role might become more specialised or split into a few different disciplines. New tools allow data engineers to focus on core data infrastructure, performance optimisation, custom data ingestion pipelines and overall pipeline orchestration. At the same time, data transformation code in those pipelines can be owned by anyone who is comfortable with SQL. For example,  analytics engineering  is starting to become a thing. This role sits at the intersection of data engineering and data analytics and focuses on data transformation and data quality. Cloud data warehouse engineering is another one. Regardless of whether the distinction in job titles will become widely adopted or not, I believe that “traditional” data developers possess many fundamental skills to be successful in many data engineering related activities today — strong SQL and data modelling are some of them. By understanding the modern cloud analytics data stack and how different components can be combined together, learning a programming language and getting used to version control, this transition can be reasonably seamless."
Data Engineering: Major Technologies To Learn In 2022,ata Engineering: Major Technologies To Learn In 202,"Data Engineering is growing so fast that it is flooded with an array of tools and technologies, and it is easy to get lost in learning them. The effective way to learn is to categorise these technologies and understand one really well from each category. The core idea will be the same with some differences for different tools. Let’s dive in: Python, Scala Python is easy to learn and dynamic. PySpark is the python interface for Apache Spark which is adapted more than the native Scala API. Airflow is written in Python. Most of the popular big data tools have support for Python APIs. Scala is a typed language and is used in major technology companies. It runs on JVM and supports JAVA libraries. Apache Kafka, Apache Spark, Apache Flink, all of them have been written in Java and Scala. Hadoop is the idea behind a lot of the technologies. It will help in understanding basic concepts like scalability, replication, failure tolerance, partitioning. HDFS is the storage part of Hadoop, which is a distributed file system.  MapReduce, a batch processing algorithm published in 2004, was subsequently implemented in various open-source data systems, including Hadoop, MongoDB, CouchDB, etc.  Although the importance of MapReduce is declining, it is worth understanding because it provides a clear picture of why and how batch processing is useful. Below tools ingest raw data into the platform. Apache Kafka, AWS Kinesis, Cloud Pub/Sub Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. It provides durability really well compared with queues like RabbitMQ, Redis Queue. AWS Kinesis is provided by AWS, while Cloud Pub/Sub is provided by Google Cloud. This category is to store real-time data with fast access. Apache Kafka, AWS Kinesis, Cloud Pub/Sub Most of the tools used in ingestion with event streaming supports Fast Storage as well. This category stores the data for batch processing. HDFS, AWS S3, Google Cloud Storage In this category, all the data is stored for the purpose of data lake, or staging layer, which is used for batch processing. Below tools are being used for processing the streaming data. Apache Flink, Spark Streaming, Amazon Kinesis Analytics, Cloud Dataflow, ksqlDB, Kafka Streams Apache Flink is a fast-growing solution for real-time data with minimum latency. Spark Streaming is also widely used but it processes data in really small windows. Amazon Kinesis Analytics and Cloud DataFlow are the stream processing solutions provided by AWS and Google Cloud respectively. These tools are being used for batch processing. Apache Spark, AWS Glue, Dataproc, AWS EMR, Presto, Trino Apache Spark is the most widely used for ETL operations. It has a lot of APIs supported in Python, Scala, Java and R. It is an in-memory compute engine which has a lot of benefits over MapReduce.  AWS Glue and AWS EMR are spark-based managed solutions from AWS. Dataproc is provided by Google Cloud. Presto is an open-source distributed SQL query engine for running interactive analytic queries against data sources of all sizes. These tools are being used to orchestrate the data pipelines. Apache Airflow, Google Cloud Composer, Astronomer, AWS Step Functions All the pipelines need to be scheduled and dependencies need to be defined. Apache Airflow is python based workflow engine that creates pipelines dynamically. Google Cloud Composer and Astronomer are Airflow based managed solutions while AWS Step functions are provided by AWS which is not dependent on Airflow. Below tools are the ones where we can store the data for analytics and reporting. AWS Redshift, BigQuery, Snowflake, PostgreSQL All the curated and transformed data will be stored in Data Warehousing for analytics and reporting purposes.  AWS Redshift is a massively parallel processing tool provided by AWS, while BigQuery, Snowflake are also widely used, majorly for ELT (extract-load-transform) operations. To serve the data consumers. NoSQL Databases — Cassandra, DynamoDB Relational Databases — PostgreSQL, Mysql This layer is where APIs and data consumers will be served with minimal latency. Cassandra is one of the highly available NoSQL databases which is being used.  DynamoDB is a highly scalable database provided by AWS. PostgresSQL and Mysql are relational databases.   This article will guide you   in choosing the databases for your use case. These categories and the technologies will ensure that you build the tree of knowledge while focusing on the major part, root and stem while going in the branches when required."
The State of Data Engineering 2022,he State of Data Engineering 202,"Note: This article was  originally published  by Einat Orr on June 20th, 2022. A year has passed since we shared the  State of Data Engineering 2021 . And since we released that article last May, not much has changed in the data landscape. In fact, we had discussions internally about whether we should even do an update for 2022. We kid. It was another year worthy of its own prime-time drama, and we’re back to share our updated, digestible snapshot of it all! What has changed this year? The major theme we saw play out in the past year is  consolidation . Companies expanding their scope to break into new categories, or simply coming into existence with the value-prop of replacing several existing tools in the data stack. Let’s take a look at the updated diagram, this year in a higher resolution! Let’s cover in section in more detail, shall we? This layer includes streaming technologies and SaaS services that provide pipelines from operational systems to data storage. The evolution worth mentioning here is the dramatic rise of  Airbyte . Founded in 2020 and pivoting to its current offering only towards the end of that year, Airbyte is an open-source project  used today by over 15,000 companies . The community counts  over 600 contributors . It’s rare to see such exponential growth in usage and community. Airbyte has just launched its commercial offering and expanded to reverse ETL (a category we don’t cover in our diagram) by acquiring Grouparoo, an open source for reverse ETL connectors. We believe reverse ETL to be a very different product then ETL, as it requires integrating the data into the operational systems in a way that benefits the users’ workflow within that system. Curious to see how things turn out. In 2021, we included data warehouses and lakehouses as part of the data lake layer. But this year, we decided to keep the data lake category solely for the object storage technologies used as data lakes. We moved all warehouses and lakehouses to the analytics engines category. Why? Most of the architectures data engineers deal with today are complex enough to include both object storage and an analytics engine. So you either only need an analytics database (in which case you don’t have a data lake but a data warehouse that serves as the analytics engine) or you need both. And when you need both, you usually perform some of the analysis over the object storage and some over the analytics engine. That’s why they need to work very well together. This dependency occurs at different layers. Large data sets would be managed in your object storage while artifacts and serving layer datasets would be stored within analytics engines and databases. The idea that one of them would conquer the other is something that we’re not seeing in architectures around us. What we see in reality is these solutions co-exist. There are several reasons behind this architecture and one of them is definitely cost considerations. Querying a massive volume of data in  Snowflake  or  BigQuery  is expensive. So instead of having analytics databases managing your entire lake, you manage everything you can in object storage with cheaper compute over it and leave all the must-haves for the analytics engines. We consider a lakehouse to be an analytics engine  (although in Databricks it includes both the data lake and analytics engine ). This architecture features an optimized version of Spark SQL to create an analytics engine over the Delta table format. This delivers the increased performance and costs expected from an analytics engine. The same rule applies to Dremio over Iceberg, or to Snowflake supporting Iceberg as external tables to its database. A lot of things are happening in the metadata space! The two layers of metadata, this one and the organizational one at the top of our chart, are becoming a focus in many organizations. Reviewing the evolution of our challenges as scalable data practitioners, we have spent the past decade innovating around storage and computers — all to make sure they support the scale of data. Today, we face mostly manageability problems and can solve them by producing and managing metadata. This layer includes different aspects of metadata, let’s go through them. We’ve seen interesting advancements in open table formats in the past year. They’re becoming a standard for holding structured data in a data lake. A year ago,  Delta Lake  was a  Databricks  project with an actual commercial product called Delta. Then this year, we have  Apache Hudi  commercialized by  Onehouse  and  Apache Iceberg  commercialized by  Tabular . Both companies were founded by the creators of these open-source projects. So, the entire space went from being open-source to fully backed up by commercial entities. This brings a question mark to how much influence other players would have on the open-source project now that there’s a commercial interest behind it. Since all three open-source projects are part of the apache/Linux Foundation, the risk to the community is low. This doesn’t seem to calm the  passionate dispute  between the creators and fans of all these three projects over who is “really” open source and who has the best solution. Netflix will soon jump on this story as excellent material for a drama show. 🙂 We’re seeing  Hive Metastore  being pulled out of architectures where it’s possible to replace it with open table formats. Not all organizations are making full use of Metastore capabilities, and if their only use case was virtualizing tables then open table formats and the commercial offerings around them provide a good option.  Other use cases  for Metastore have yet to receive a better alternative solution. The concept of Git for data is taking hold in the community. dbt is encouraging analysts to use best practices over different versions of data (dev, stage and production), though does not support the creation and maintenance of those data sets in data lakes. There is a growing demand from DataOps teams to provide cross-organizational data version control to manage data sets that have different revisions over time. A few examples for different revisions to a data set are:  recalculation , necessary for algorithms and ML models, or of  backfills  from operational systems as often happens in BI, or  deletion  of a subset due to regulations such as the right to be forgotten under GDPR. This trend is clear from the dramatic growth in the adoption of  lakeFS  and its community, which I’ve witnessed first-hand. lakeFS serves both structured and unstructured data operations, shining in cases where both exist. Unfortunately, public data on usage for Dremio’s  Project Nessie  is hard to come by. Interestingly, it is also available as a free service named Arctic. This is likely a strategic decision made to compete with Tabular. We applied some changes to the compute layer this year to reflect the ecosystem better. First, we removed virtualization as a category altogether. It doesn’t seem to be catching on at the moment. We then split the compute engines into two categories: distributed compute and analytics engines. The main difference between them is how opinionated these tools are regarding their storage layer. While distributed compute includes compute engines that aren’t opinionated about storage, the analytics engines category contains compute engines (distributed or not) that are opinionated. The general distributed compute engines allow engineers to distribute anything that is SQL or any other code. Sure, they might be opinionated about the programming language, but they will carry out a general distribution over (usually) federated data. This could be data stored across many formats and sources. The two interesting additions to the distributed compute category are  Ray  and  Dask . Ray is an open-source project that allows engineers to scale any compute-intensive Python workload, used mainly for machine learning. Dask is also a distributed Python engine based on Pandas. You might have thought that  Spark  would be the distributed engine ruling the scene with no competition in sight. So, it’s pretty exciting to witness the rise of new technologies gaining traction in this category. The analytics engines category includes all data warehouses such as  Snowflake ,  BigQuery ,  Redshift ,  Firebolt , and the good old  PostgreSQL . It also contains lakehouses like  Databricks  lakehouse,  Dremio , or  Apache Pinot . All are very opinionated about the data format they support to provide better performance to their querying engines. Since all analytics engines use the data lake as their deep storage or storage, it’s worth mentioning that  Snowflake  now supports  Apache Iceberg  as one of the external table formats that can be read by Snowflake directly from the lake. This is a new layer populated by existing categories. Orchestration tools were part of the metadata layer last year, but we moved it over to the compute engine where it really belongs — it’s about orchestrating pipelines across compute engines and data sources. Together with orchestration, we get observability tools that also grew a lot during 2021 to support more data sources resulting in any compute engine. Did anything eye-catching happen around orchestration?  Airflow  is still the biggest open-source product.  Astronomer  has backed it for a few years already and since the company jumped on the cloud bandwagon, it’s now directly competing with cloud providers on the managed Airflow front. Another very interesting move by Astronomer is the  acquisition of Datakin  which provides data lineage. This makes one wonder —  what happens when an orchestration tool has lineage capabilities? This, in theory, could help data teams build safer, more resilient pipelines. By knowing which datasets depend on missing, corrupt or low quality data, it makes impact analysis considerably easier by tying together the logic (managed by orchestration tools) and their output (managed in lineage tools). Whether this materializes into an integral part of the orchestration ecosystem is to be seen. The observability category, established by  Monte Carlo  data, is also led by it. Monte Carlo’s frequent fund raising is an indication of its products’ rapid adoption in the market. The product keeps evolving, offering more integrations, for example, the databricks ecosystem, and additional observability and root cause analysis features. It is this success that is probably driving the growth of the category, at least from the perspective of the number of companies exploring this front today. Several companies were founded or went out of stealth in 2021, most interesting is  Elementary , another open source project out of YC This layer is meant for the users of data architectures created through the former layers: data scientists and analysts who unlock insights from data. We split this category into three subcategories: When I set out to review this space, someone told me I should name the category: “Prepare to be disappointed.” While the category includes great tools, none of them is really end-to-end. They offer a good solution to certain steps in the ML process but lack at their offering for other aspects of the ML pipeline. Still, the approach of offering an end-to-end solution was popular in 2021. Several mission-specific tools entered the path to become an end-to-end offering. Examples include  Comet ,  Weights & Biases ,  Clear.ml  and  Iguazio. This subcategory doesn’t escape the end-to-end trap either, but the tools listed in it take on a different approach to the functionality they provide. They put the data itself and its management at the center of their missions. The two new joiners to this space are  Activeloop  and  Graviti . They were built by experienced data practitioners who understand the criticality of data management to the success of any data operation. We share this sentiment at lakeFS. A unique approach is taken by  DagsHub , which provides an E2E solution that is data-centric but bases its offering on open source solutions. They excel at each of the ML lifecycle phases, providing their unique usability and ease of integration touch. In such a confusing space, this is a solid approach to get a good end-to-end solution that also delights users. We’re watching the company with anticipation…. This subcategory includes tools focusing on the monitoring and observability of model quality. Very much like the data observability category, this space is growing and gaining momentum. In early 2022  Deepchecks  went open source, and had quickly gained traction, contributors, and partners. In the category of notebooks, we see  Hex  getting more traction and validation by investment from both  Databricks  and  Snowflake . Hex offers more orchestration capabilities within its notebook. The same is true for  Ploomber  which emerged to offer orchestration capabilities to  Jupyter . The category of tools meant for analysts established itself over the past year and gained some competition.  dbt  proved itself as a standard for analysts and, in 2021, it released integrations with the scalable data engineering stack, including object storage, HMS, and the Databricks offering. While collaborating with the ecosystem, in 2021 Databricks  launched the GA of its ‘live tables’ product , which is in direct competition with dbt. My sense of the ecosystem is that the need for a data catalog is now clear to companies of all sizes, and we will see it become a standard. Commercial offerings based on open-source projects show  good levels of adoption . Meanwhile, long-time providers for enterprise solutions,  Alation  and  Collibra , continue to expand their offerings. And the security and permissions supplier  BigID  is attempting to offer a catalog as well. Immuta  stays persistent in its focus on data access control, and its unique technology is now compatible with additional data sources. To accelerate its growth, the company  raised $90 million  in series D funding back in mid-2021. While the number of companies in the space keeps growing, we can see signs of consolidation in product offerings across several of these categories. MLOps is trending towards end-to-end, notebooks are going into orchestration, and orchestration is turning towards lineage and observability. At the same time, we see open table formats going into the Metastore functionality. And in the governance layer, security and permission tools go into catalogs and the other way around… Are those signs of a (still) limited market, maybe in MLOps? Does this consolidation reflect the need for differentiation due to fierce competition (which may be the case in orchestration)? Or is it an opportunity to fill a void that may be the case with open table formats or data-centric ML? Maybe it’s all because users want to use fewer tools that do more? I’m going to leave these questions open in hope to initiate a discussion about the state of data engineering in 2022. What other projects are gaining steam in 2022? Which tools are on their way to becoming de facto standards in the industry? Share your thoughts in the comments section!"
Introduction to Data Engineering,ntroduction to Data Engineerin,"According to the recently published  Dice 2020 Tech Job Report , data engineer was the fastest-growing tech occupation in 2019, with a 50% year-over-year growth in the number of open job positions. As data engineering is a relatively new job category, I often get questions about what I do from people who are interested in pursuing it as a career. In this blog post, I will share my own story of becoming a data engineer and answer some frequently asked questions…"
Three Questions to Help You Prepare for a Data Engineering Interview,hree Questions to Help You Prepare for a Data Engineering Intervie,"Data science is just one of the modern data-driven  fields in our new data world. Another job that is even more prevalent than data scientist is data engineer. Now, being a data engineer does not have all the hype behind it of being a data scientist. However, companies like Google, Facebook, Amazon, and Netflix all need great data engineers! Data engineering requires a combination of knowledge, from data warehousing to programming, in order to ensure the data systems are designed well and are as automated as possible. The question is:  How do you prepare for an interview for a data engineering position? Many of the questions will require you to understand data warehouses, scripting, ETL development, and possibly some NO-SQL if the company uses a different form of data storage system like  CouchDB . In case you are preparing for a data engineering interview, here are some questions that might help you. We are focusing on conceptual questions. However, you should also work on some technical skills like  SQL , Python, and etc. Before scrolling any further, why not join our team’s newsletter to keep up to date on data science, data engineering and tech!  Learn more here . As a data engineer,  you control what is possible in the final product . A data scientist can’t build algorithms or metrics without having the right data and the data at the right granularity. This means a data engineer needs to understand the entire product. A data engineer can’t just get away with building systems based off of requirements. They need to ask why they are building certain tables and objects. It’s helpful if the  stakeholders  already have a general outline of what they want. If they don’t have an outline, we would want to work with them to develop a general idea of what metrics and algorithms will exist. This drives all the major decisions, including what data should be pulled, how long should it be stored, if should it be archived, and etc. Once a general outline exists, the next step would be drilling into the why of each metric. This is because as you’re building different tables at different  data granularities , certain issues might arise. Should the  unique key  be on columns A and B, or A, B, and C. Well, that depends, why is this important? What does that row signify? Is it customer level, store level, or maybe brand level? Once your team has gone through the process of working on the outline with your stakeholders and gained an understanding of the why, the next step is to think through as many  operational scenarios  as possible. Will you ever need to reload data? Do your ETLs allow for it? Is it efficient? What happens when X occurs? How do you handle case Y? You can’t spend all day doing this, but trying to think of all the issues that could occur will help you develop a more robust system. It also helps create a system that actually meets requirements. From there, it’s about developing the design, creating test cases, and testing the tables, stored procedures, and scripts, and then pushing to production. How that occurs usually changes from team to team. If you took a database course in college, then you probably learned about how to set up a standard normalized database. This style of database is optimized for transactions that involve Insert, Update, and Delete SQL statements. These are standard operational databases. They need to focus on making transactions quickly without getting bogged down by calculations and data manipulations. Thus, their design is a little more cumbersome for an analysis. Generally, you will have to join several tables just to get a single data point. A data warehouse is not concerned as much with dealing with millions of fast transactions every second. Instead, a data warehouse is usually built to support a data analytics product and analysis. This means performance is not geared towards transactions — instead, it’s aimed at  aggregations , calculations, and select statements. A data warehouse will have a slightly  denormalized  structure compared to an operational database. In most data warehouses, a majority of tables will take on two different attributes: a historical transaction table and tables that contain categorical style data. We reference these as fact and dimension tables. The fact table is essentially in the center, unlike in a normalized database where you might have to join across several tables to get one data point. A standard data warehouse usually has a focus on the fact tables, and all the dimension tables join to provide categorical information to the fact table. It’s also typically bad practice to join fact table to the fact table, but sometimes it can occur if the data is created correctly. Here is an example of the structure of a data warehouse: These are not the only tables that exist in a data warehouse. There are aggregate tables, snapshots, partitions, and more. The goal is usually a report or dashboard that can be automatically updated quickly. Data warehouses also have a lot of other nuances, like  slowly changing dimensions . However, that is a whole other can of worms. As a data engineer, you will run into performance issues. Either you developed an ETL when the data was smaller and it didn’t scale, or you’re maintaining older architecture that is not scaling. ETLs feature multiple components, multiple table inserts, merges, and updates. This makes it difficult to tell exactly where the  ETL  issue is occurring. The first step is  identifying the problem , so you need to figure out where the bottleneck is occurring. Hopefully, whoever set up your ETL has an ETL log table somewhere that tracks when components finish. This makes it easy to spot bottlenecks and the biggest time sucks. If not, it will not be easy to find the issue. Depending on the urgency of the issue, we would recommend setting up an ETL log table and then rerunning to identify the issue. If the fix is needed right away, then you will probably just have to go piece-by-piece through the ETL to try to track down the long-running component. This also depends on how long the ETL takes to run. There are ways you can approach that as well depending on what the component relies on. Issues can vary wildly, and they can include table locks, slow transactions, loops getting stuck, and etc. Once you have identified the issue, then you need to figure out a fix. This depends on the problem, but the solutions could require adding an index, removing an index, partitioning tables, and batching the data in smaller pieces (or sometimes even larger pieces — it seems counterintuitive, but this would depend on table scans and indexes). Depending on the storage system you are using, it’s good to look into the activity monitor to see what is happening on an  I/O  level. This will give you a better idea of the problem. When you look at the activity monitor, you can see if there is any data being processed at all. Is there too much data being processed, none, or table locks? Any of these issues can choke an ETL and would need to be addressed. If you Google some of the performance issues, then you will find some people blaming the architecture for a lot of the problems. We don’t disagree with them. However, this doesn’t mean you should throw in the towel. There are always various ways to manage performance if you can’t touch the actual structure. Even beyond indexes, there are some parallel processing methods that can be used to speed up the ETL. You can also add temporary support tables to lighten the load. This question is more to note that it is important to have a scripting language on your side as a data engineer. Typically, most shops we have worked in won’t just rely on ETL tools like SSIS. Instead, they will often use custom scripts and libraries to load data. It might seem like overkill, but it is often easier. Using tools like SSIS are great if you need all the fancy functions, like sending emails and some of the other add-ons. Even these could be scripted instead of written. So we do recommend having some scripting abilities as well. It allows you to easily automate data flow and analytical tasks. These are just some general questions to help get ready for your data engineering interview. Besides these questions, you should also look into the concept of  slowly changing dimensions , automation with python or PowerShell, some basic Linux commands, and design concepts. If you do have a data engineer interview, then we wish you good luck!"
The State of Data Engineering in 2021,he State of Data Engineering in 202,"Note: This article was written by Einat Orr, PhD and published on the  lakeFS blog  on May 5, 2021. Let’s start with the obvious: the  lakeFS project  doesn’t exist in isolation. It belongs to a larger ecosystem of data engineering tools and technologies adjacent and complementary to  the problems  we are solving. What better way to visualize our place in this ecosystem, I thought, than by creating a cross-sectional LUMAscape to depict it. What’s more, I believe it is critical to understand where lakeFS resides to identify opportunities where we can bring additional value to users by addressing pain points in today’s practices. With that said, I’m excited to share what we created (and continuously maintain) internally with the larger community! Note at the end I conclude with a few thoughts and predictions about the space generally. Without further ado… The section of the data ecosystem lakeFS inhabits can be described as open, flexible analytic platforms capable of supporting the core functions of modern data teams: What we see in the LUMAscape above are the major components typical of these platforms. We’ll cover each in more detail, starting from the bottom and working upwards. The first step is getting data into the system and there are three main strategies for doing so. The first strategy is basic uploading or dumping of data files in batch style. This is easy enough to implement yourself using core functions available in most programming languages or common data-transformation libraries like Pandas and Spark. For example, it can be as simple as: The second approach of streaming data requires more advanced technologies that combine high throughput messaging systems with compute capabilities within their consumers. Most prevalent is the open-source project  Kafka  and its many managed offerings. Competing with Kafka are public cloud services such as  AWS Kinesis  and  Google Pub/Sub . Finally there are other open source options such as  Flink  and Spark Streaming one can choose. It is common to want to ingest data from operational systems like a SalesForce CRM, Hubspot account, or Zuora subscription containing financial information, as well as other internal databases. Rather than implement the fetching data from these tools yourself, it is increasingly common to employ a managed approach by using any of the five tools in this section —  Segment ,  Stitch,  Fivetran ,  Snowplow , and  Matillio n — for their pre-built data connectors. No reason to reinvent the wheel, unless your data volumes grow large enough that it becomes prohibitively expensive. The exact definition of a data lake can change depending on who you ask. It is easier to know one when you see it. And there are two main architectures you’ll encounter: Let’s look at examples of each. In the first type of lake architecture, object stores hold any type of data in a cost-effective way with a rich ecosystem of applications that can consume data directly from it. Examples can include all the major cloud object storage services (AWS S3, ADLS, GCS), as well as other providers such as  Wasabi ,  Pure  and even open source projects such as  MinIO . Analytics engines provide an SQL interface to tabular, relational datasets. Some engines like  Snowflake ,  Druid ,  Firebolt ,  Redshift  (and other tools commonly referred to as “data warehouses”) integrate proprietary storage services with the analytics engine, creating self-contained data lake functionality. If your data lands in an object store however, it makes sense to use an analytics engine like Databricks  Lakehouse ,  Dremio , or  BigQuery  that offers solely computation. Where there’s data, there is metadata. Metadata is used to define schema, data types, data versions, relations to other datasets and so on. It is useful for improving discoverability, manageability, and enforcement of good practices. The following sections of the LUMAscape all leverage metadata to achieve these aims. One of the empowering characteristics of data lakes in an object store is that we choose the format data is stored in. This is also one of the most influential decisions, as it impacts the performance and functionality of the lake directly. Open table formats like  Hudi, Iceberg, and Delta  are designed to meet mutability requirements (think GDPR) and maximize performance of even the largest tables. They achieve this through managing metadata files over the dataset, allowing for fast access and mutations during read or write operations. Metastores play the important role of abstracting files in object storage into the familiar construct of a query-able table. One relic of the Hadoop ecosystem that is most likely to survive is Hive Metastore, a virtualization layer that provides tabular access to the content of the object storage. It also plays a role in managing schema, aiding in the discovery of data lake content, and improving read performance through partition management. Hive is the sole metastore on the market, with managed or compatible versions available on all public clouds. It will be interesting to see if a new player emerges to overtake Hive, as has occurred with most other Hadoop-era technologies. Or perhaps an existing tool or combination of multiple (potentially format + discovery) will make hive redundent. The data in data-intensive applications should have a lifecycle similar to the ones used to manage code. Lifecycle management tools allow for this through CI/CD operations and isolated data development environments (instead of shared buckets). Both lakeFS and the  Nessie project  approach the problem by enabling git-like operations over collections of data. Notably, Nessie leverages (and depends on) the metadata created by the Iceberg data format, whereas lakeFS employs a general data model that is format agnostic. Data pipelines that run over the data lake require orchestration of tasks. A data pipeline may include the execution of hundreds or even thousands of jobs represented by a DAG, where the input of one job may depend on the output of several upstream jobs. Managing all of this is no simple task itself, and thankfully open source solutions like  Airflow ,  Dagster , and  Prefect  offer useful abstractions and services to simplify it. At this point, data is not only present in the system, but flowing smoothly thanks to metadata tools. Now it’s time to crunch it! When working with data volumes common to most lakes, distributed compute engines are a must to handle the load. When it burst onto the scene in 2006, Hadoop was a significant improvement (and open source, no less). Since then the category has only continued to improve to the point of allowing near real-time computation via both SQL and code interfaces. Distributed compute today is dominated by Spark — offered as an open source technology, as a service on the major cloud providers, and other vendors. This category of data virtualization aims high. Regardless of data’s location, it aspires to provide access to it via a single endpoint. Trino  (formerly PrestoSQL) is the first open source project to offer such federated capabilities. Today, all public clouds offer their managed version of Trino, and other virtualization technologies like Denodo are entering the market. Not all users of the platform will be engineers, hence tools are required to close the gap between the technology and user capabilities. Here we cover a few of the areas where tools exist to improve the experience for BI and DS functions on analytic platforms. The processes involved in the development and maintenance of machine learning models have received much attention in the last few years. This category includes dozens of different tools, including homegrown in large enterprises and released to open source, like  MetaFlow  (Netflix), DisDat (Intuit), and KubeFlow (Google). Also relevant are commercial companies with an open source strategy such as  Pachyderm ,  DVC , and  Clear . What’ll be interesting to see play out is whether an end-to-end solution can win out in managing the ML model lifecycle. Or whether architectures will consist of multiple tools with a more specialized focus. An in-depth analysis of this category can be found  here . The organization and execution of transformational queries poses challenges for analysts. As a result, tools like  dbt  and  dataform  have exploded in popularity, providing the equivalent of an IDE for running data-intensive code/SQL. Notebook environments like  Jupyter  came onto the scene and mostly made code tutorials in blogs a bit nicer looking. Since then notebook environments have become preferred interfaces for everything from exploratory analysis, to ML model training, and even production ETL jobs. Newer products in this space like  Deepnote  and  Hex  focus on enabling collaboration in the notebook space, among other usability enhancements. The second metadata layer doesn’t describe the data itself, but rather contains organizational metadata. The tools in this final, topmost section aim to enhance the usability of data platforms in organizational settings. In the last 18 months, 10 new open source projects were released from large companies (see the Love Letters section below) that offer an organizational data catalog. These discovery tools allow a user to easily find datasets, visualize connections between them, contact the creators, and see how they are used. As an org scales, it is important to make this information easily found to sustain a data-driven culture that is efficient and consistent. Enterprises in many verticals are committed to data auditing, reproducibility and regulation. The tools in this category simplify data management for these purposes, sometimes involving custom solutions per customer. Finally, the quality and observability category offers rule or machine learning based data quality  monitoring and testing . In an ideal world, tests will cover all data source, be implemented in all stages of the data lifecycle. Errors and anomalies are accepted as a given in complex data systems. The idea is to identify them before your consumers do. Though perhaps as this category matures, our expectations around data quality will rise with it. Phew! We made it through the analytics gauntlet! Now that we’ve touched on the major components of modern analytics platforms, I’d like to share a few trends. The first problem we faced with big data was the  feasibility  of processing data at such a high scale. In solving the scale problem, people developed technologies we know today like Kafka, Spark, Presto, Snowflake, etc. Now the problem people face is one of manageability. They no longer ask if   they can handle a dataset but rather:  How can I move faster when developing data-intensive applications? How do I utilize all of my data  (Discoverability)  and ensure it is high-quality  (Quality, Observability) ?  Or,  how do I ensure reproducibility, auditability, and governance of my data? This context explains the explosion of tools in the categories along the topmost row of the Lumascape: Data Discovery, Quality & Observability, and Lineage, Management & Governance. As people want to do more with their data — run more analyses, put more models into production, etc. — effective use of these tools will play an important role in enabling this. It is this domain of metadata management that I expect to see growing in the next few years. Sophisticated data organizations like Netflix and Uber were the first to encounter the problems related to large-scale analytics. In response, they developed their own internal solutions like the  Iceberg and Hudi  data formats respectively to address these issues. Years later, the rest of the world is catching up and one example is the adoption of these data formats, now open-sourced. We see the same pattern in the Orchestration space, where Airflow, originally developed at AirBnb, is now an open source product with huge adoption, and competitors like Prefect and Dagster emerging. A final category worth highlighting is Discovery, where it seems every notable company developed an internal Data Catalogue tool that now is available as an open-source or paid service. Some examples are  Amundsen  (Lyft),  Datahub  (LinkedIn),  Metacat  (Netflix),  Databook  (Uber), and  Dataportal  (Airbnb). One thing the LUMAscape highlights is the fractured nature of the data engineering ecosystem. And it stands to reason we’ll see a degree of consolidation in the future. The question is what type of consolidation? One option is for an end-to-end solution to emerge more in-line with the closed Snowflake platform. The other option is consolidation around an ecosystem based on open standards, aligned with the Databricks approach. My belief is that if a consolidated solution emerges, it will form through allowing organizations to pick and choose the pieces of the puzzle that make sense for them. The final system results in a platform with a total added value greater than the individual parts. From a vendor perspective, it means my chips are placed on the DataBricks approach (though there is certainly room for both companies to succeed). As one final note, I see a parallel situation in the MLOps space. Vying for market share are products offering closed, end-to-end solutions to model management. My bet however, is that in the mid-term, the ecosystem will remain fractured with tools satisfying their niche remaining successful. To learn more about lakeFS and its benefits in data lake architectures, check out our  Github repo , or say “Hi” in our  Slack group !"
"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture, and Unifield architecture"," big data architectures, Data Streaming, Lambda architecture, Kappa architecture, and Unifield architectur","Although data analysis is hidden behind the business system, it has a very important role. The results of data analysis play a pivotal role in decision-making and business development. With the development of big data technology, the exposure of proper terms such as data mining and data exploration is…"
Enterprise Data Architecture,nterprise Data Architectur,"Job ad: “Wanted: truck driver to drive a trailer load of tropical plants from Atlanta to St. Petersburg. Must know optimal planting conditions, desired soil characteristics, drought tolerance, and disease resistance of each of the 65 plant species on board.” Why data architecture?"
Modern Unified Data Architecture,odern Unified Data Architectur,"Today, most business value is derived from the analysis of data and products powered by data, rather than the software itself. The data generated by several application silos are combined and greatly enhanced to provide a better customer experience. Deriving value from the data includes building a unified data architecture and a collaborative effort of data engineering and data science teams. Data engineering involves building and maintaining the data infrastructure and data pipelines, and Data science involves transforming crude data into something useful and deriving insights through analytical and ML workloads. Modern unified data architecture includes infrastructure, tools and technologies that create, manage and support data collection, processing, analytical and ML workloads. Building and operating the data architecture in an organization require deployments to cloud and colocations, use of several technologies (open source and proprietary) and languages (python, sql, java), and involves different skilled resources (engineers, scientists, analysts, admins). It is cost-effective to have a centralized data infrastructure to avoid duplication of data and efforts as well as to maintain a single source of truth in the organization for efficient usage. Many organizations generate, process and store massive amounts of data regularly for business analysis and operations. The challenges faced by big data analytical and processing applications are summarized as 3Vs, 5Vs, 7Vs or even more. In this article, I consider the 7 key challenges of modern data architectures: As shown in Figure 1., these challenges are surfaced at different stages as the data flows through the modern big data architectures. To address these challenges, the separation of data ingestion, processing, storage, ML modeling and consumers into separate isolated components makes it possible to independently repair, scale or replace resources in these stages without impacting others. Data Producers Data producers generate data in a variety of ways in structured, unstructured or semi-structured format. Data producers can be transactional applications and operational systems that generate relational data, or they can be social media mobile apps, IoT devices, clickstreams or log files that generate non-relational data. The data sources can have different data mutation rates — data that comes from OLTP transaction applications experience heavy write operations and data that arrive from other OLAP systems can experience heavy read but low write operations. Data produced from relational databases typically have static schemas whereas distributed non-relational data stores have dynamic schemas. Data produced by dissimilar systems arrive in different formats such as json, csv, parquet, avro etc. Data Ingestion The huge volume of data generated by the providers is ingested into big data system through various techniques such as batch ingestion, micro-batches, change data capture, publish-subscribe, sync-async, and stream ingestion. Both push and pull mechanism of data extraction is employed along with features such as ordering, message delivery guarantees, delivery confirmation, message retention, message aging and watermarking. The data architecture should effectively handle the performance, throughput, failure rate requirements and avoid throttling in the system. Data ingestion through massive batch processing is used for complex processing and deep analysis; real-time streaming is used for quick feedback and anomaly detections. Typically, batch ingestions at scheduled intervals have predictable workloads and on-the-fly batch ingestions have unpredictable workloads. For stream ingestions, the data should be query-able as soon as it enters the system and provides immediate actionable insights. Data Processing Data processing involves various methods such as cleansing, profiling, validating, enriching, and aggregating datasets. It involves data modeling and mapping source-destination schemas. The data architecture should support both schema enforcement to avoid inadvertent changes (schema-on-write) and at the same time offer flexibility to modify schemas (schema-on-read) as the requirements evolve. For slow-moving datasets, batch processing techniques are employed to churn large datasets, perform complex transformations and generate deep insights. Previously, batch processing used to be long-running jobs, but lower latencies are possible by the use of distributed massive parallel processing engines such as Spark. For fast-moving datasets, real-time streaming techniques such as aggregating and filtering on rolling time-windows are employed to generate immediate insights by the use of Spark streaming or Flink. Languages such as python, java, scala and sql are predominantly used for data processing. Previously, Lambda/Kappa architectures provide unified analytics but separate paths for batch and real-time processing resulting in duplicate resources and effort. However, with modern architectures through the use of frameworks such as databricks, it is possible to combine batch and real-time processing into a single path. As the count and complexity of data processing jobs increases, complex DAGs (Directed Acyclic Graphs) and efficient pipelines can be built using workflow tools such as Airflow, Nifi, Luigi, etc. along with virtualization container services such as Docker or Kubernetes. As data velocity changes, processing jobs should scale elastically to handle data bursts and data accelerations due to a sudden spike in usage or demand. Data Storage The data architecture should effectively manage the massive amounts of data processed and stored in the system through distributed storage, object stores and purpose-built storage options (nosql db, columnar db, timeseries db etc. ). Previously multi-cluster distributed Hadoop systems have combined storage and compute at each node. However, modern solutions decouple storage from compute so the same data can be analyzed with variety of compute engines. Decoupled storage employs efficient columnar data indexing and compression techniques. Centralized storage avoids duplication of data copies distributed across multiple systems and provides better access control to users. Cloud data lakes are essential components in any modern data solutions and store unlimited amounts of data. The fundamental challenge with data lakes are they are typically append-only and updating records is hard. Delta lakes and HUDI solutions solve this challenge by bringing ACID properties to data lakes. The performance of data processing is improved through properly configuring settings such as partition, vacuuming, compaction, shuffling, etc. ML Modeling After the datasets are prepared by the pipelines built by the data engineers, the data scientists will perform further curation, validation and labeling the data for feature engineering and model building. Scaling out data preparation is not the same as scaling out ML models. Scaling out ML is hard and training models are typically not multithreaded. Once the ML models are trained then the models are deployed at scale on multiple nodes, and the inference endpoints are generated to provide predictions. The MLOps and DevOps will help the data engineers and data scientists to manage and automate the end-to-end ML workflows. The modern data architecture supports MLOps practices to enable automation and traceability of model training, testing, hyper-parameter updates and experiments so that ML models are deployed in production at scale. For tracking experiments and deploying ML models, open-source tools such as MLflow or Kubeflow are used. Deploying a model to the production is not the end. The models are continuously monitored for any drifts in data and model accuracy. When any decline in model quality is detected, then the data received by the model are captured and compared with the training datasets. The models are retrained, redeployed to production and inference endpoints are updated again, and this process continues for the ML lifecycle. The effectiveness of model deployments to production are improved using shadow deployments, canary deployments and A/B testing. Data Consumers At the end of the data and ML pipelines, the value of the data and data architecture is derived by the data consumers, harnessing data through analytical services, data science, and operational products. After all the processing, crunching and mining of data is performed, the goal is to provide actionable insights of value through interactive exploratory analysis, reports, visualization, data science and statistical modeling, so business can make evidence-based data-driven decisions. Depending upon the analytical maturity of the use cases, descriptive, predictive and prescriptive analysis are performed. Rich support for languages, query engines and libraries are available for analysis. Typical languages used for analysis are sql, python and R. Big data query engines such as hive, spark sql, cassandra cql, impala etc and Search engines such as Solr, Lucene, Elastic Search etc., are used. Data scientists use libraries such as pandas, matplotlib, numpy, scipy, scikitlearn, tensorflow, pytorch etc. Data powered applications support live operations of the business through APIs and microservices from the data platform. The data products can use the APIs that are built upon the data stores to provide enriched information or they can be referential ML endpoints that provide predictions and recommendations. Figure 2 shows the various open-source and proprietary products available at each stage in building modern data architecture. Metadata Management Metadata management includes data cataloging, data relationship and data lineage techniques. Data cataloging offers smarter data scanning methods to automatically deduce data structures and mappings. It describes the data traits such as quality, lineage and profile statistics of a dataset. The data architecture should allow users to append tagging and keywords to easily search data assets. Data architecture can provide enhanced features such as automatically exposing correlations, data corruptions, joins, relationships and predictions within the data. As the variety and number of datasets increases, the metadata of big data applications can itself become so large that the data architecture should include search tools for data discovery and serve as data inventory. Scalable metadata management is required for democratized data access and self-service management. Data Quality & Integrity Data Quality is to ensure an accurate, complete and consistent record of data is maintained over its entire flow through different pipeline stages as well as its lifecycle. This ensures that the data is reliable and trustworthy for planning, decision making and operations. To ensure integrity of the data, we need to have full traceability and lineage information when the data enters the system and through all stages till the data reaches the consumer end points. Several basic techniques can be employed to validate the data integrity between source and destination datasets at each processing step such as comparing rowCounts, nullCounts, uniqueCounts, and md5 checkSums. Data corruptions can be detected and corrected by ensuring that referential integrity, entity relations and constraints of datasets are defined and met. Data integrity is maintained by providing selective update access only for authorized users and services, establishing data governance policies and employing data stewards. Data Security The data architecture should provide stringent security, compliance, privacy and protection mechanisms for data in all the different layers. Only authenticated and authorized users or services can access the data. PII information should be masked and hashed out. Modern data architectures provide automatic anonymization when patterns such as email, ssn, and credit card are detected. Data encryption methods are applied for data at rest and for data in transit. Observability and site reliability engineering methods are employed for auditing and alert mechanisms. Modern data solutions utilize CI/CD and DevOps to manage and automate deployments and changes to the system by including build systems like Jenkins, configuration management systems like Puppet or Chef, and containers such as Dockers or Kubernetes. The major cloud providers (AWS, Azure and Google) offer end-to-end solutions to build unified integrated data architecture. In Figure 3, each of the stages is mapped to the services offered by the major cloud providers. The big data unified architecture has a plethora of tools and technologies available today and this is an area where rapid changes are happening. Each of these tools and technologies has certain strengths that make them the right choice for a particular scenario, however, they could be a terrible selection for a different use case. Hence for tool selection, understanding your organization’s use case and requirements are important, to begin with. Then follows the evaluation and experimentation of tools with clear and time-bound goals, before picking the right tool. In this space, open-source technologies and services offered by major cloud providers (AWS, Azure, GCP) are generally preferred rather than being locked to proprietary vendor solutions. This space it continuously evolving, so identifying the right technologies, and being flexible to change and iterate are important to meet your business needs and build a competitive advantage. References :"
Big Data Architecture in Data Processing and Data Access,ig Data Architecture in Data Processing and Data Acces,"I started my career as an Oracle database developer and administrator back in 1998. Over the past 20+ years, it has been amazing to see how IT has been evolving to handle the ever growing amount of data, via technologies including relational OLTP (Online Transaction Processing) database, data warehouse, ETL (Extraction, Transformation and Loading) and OLAP (Online Analytical Processing) reporting, big data and now AI, Cloud and IoT. All these technologies were enabled by the rapid growth in computational power, particular in terms of processors, memory, storage, and networking speed. The objective of this article is to summarize, first, the underlying principles on how to handle large amounts of data and, second, a thought process that I hope can help you get a deeper understanding of any emerging technologies in the data space and come up with the right architecture when riding on current and future technology waves. In a data pipeline, data normally go through 2 stages: Data Processing and Data Access. For any type of data, when it enters an organization (in most cases there are multiple data sources), it is most likely either not clean or not in the format that can be reported or analyzed directly by the eventual business users inside or outside of the organization. Data Processing is therefore needed first, which usually includes data cleansing, standardization, transformation and aggregation. The finalized data is then presented in the Data Access layer — ready to be reported and used for analytics in all aspects. Data Processing is sometimes also called Data Preparation, Data Integration or ETL; among these, ETL is probably the most popular name. Data processing and data access have different goals, and therefore have been achieved by different technologies. Data Processing for big data emphasizes “scaling” from the beginning, meaning that whenever data volume increases, the processing time should still be within the expectation given the available hardware. The overall data processing time can range from minutes to hours to days, depending on the amount of data and the complexity of the logic in the processing. On the other hand, data access emphasizes “fast” response time on the order of seconds. On a high level, the scalability of data processing has been achieved mostly by parallel processing, while fast data access is achieved by optimization of data structure based on access patterns as well as increased amounts of memory available on the servers. In order to clean, standardize and transform the data from different sources, data processing needs to touch every record in the coming data. Once a record is clean and finalized, the job is done. This is fundamentally different from data access — the latter leads to repetitive retrieval and access of the same information with different users and/or applications. When data volume is small, the speed of data processing is less of a challenge than compared to data access, and therefore usually happens inside the same database where the finalized data reside. As the data volume grows, it was found that data processing has to be handled outside of databases in order to bypass all the overhead and limitations caused by the database system which clearly was not designed for big data processing in the first place. This was when ETL and then Hadoop started to play a critical role in the data warehousing and big data eras respectively. The challenge of big data processing is that the amount of data to be processed is always at the level of what hard disk can hold but much more than the amount of computing memory that is available at a given time. The fundamental way of efficient data processing is to break data into smaller pieces and process them in parallel. In another word, scalability is achieved by first enabling parallel processing in the programming such that when data volume increases, the number of parallel processes will increase, while each process continues to process similar amount of data as before; second by adding more servers with more processors, memory and disks as the number of parallel processes increases. Parallel processing of big data was first realized by data partitioning technique in database systems and ETL tools. Once a dataset is partitioned logically, each partition can be processed in parallel. Hadoop HDFS (Highly Distributed File Systems) adapts the same principle in the most scalable way. What HDFS does is partition the data into data blocks with each block of a constant size. The blocks are then distributed to different server nodes and recorded by the meta-data store in the so called Names node. When a data process kicks off, the number of processes is determined by the number of data blocks and available resources (e.g., processors and memory) on each server node. This means HDFS enables massive parallel processing as long as you have enough processors and memory from multiple servers. Currently Spark has become one of the most popular fast engine for large-scale data processing in memory. Does it make sense? While memory has indeed become cheaper, it is still more expensive than hard drives. In the big data space, the amount of big data to be processed is always much bigger than the amount of memory available. So how does Spark solve it? First of all, Spark leverages the total amount of memory in a distributed environment with multiple data nodes. The amount of memory is, however, still not enough and can be costly if any organization tries to fit big data into a Spark cluster. Let’s consider what type of processing Spark is good for. Data processing always starts with reading data from disk to memory, and at the end writing the results to disks. If each record only needs to be processed once before writing to disk, which is the case for a typical batch processing, Spark won’t yield advantage compared to Hadoop. On the other hand, Spark can hold the data in memory for multiple steps for data transformation while Hadoop cannot. This means Spark offers advantages when processing iteratively on the same piece of data multiple times, which is exactly what’s needed in analytics and machine learning. Now consider the following: since there could be tens or hundreds of such analytics processes running at the same time, how to make your processing scale in a cost effective way? Clearly, simply relying on processing in memory cannot be the full answer, and distributed storage of big data, such as Hadoop, is still an indispensable part of the big data solution complementary to Spark computing. Another hot topic in data processing area is Stream processing. It offers great advantage in reducing processing speed because at a given point of time it only needs to process small amount of data whenever the data arrives. However, it is not as versatile as batch processing in 2 aspects: the first is that the input data needs to come in a “stream” mode, and the second is that certain processing logic that requires aggregation across time periods still need to be processed in batch afterwards. Lastly Cloud solutions provide the opportunity to scale the distributed processing system in a more dynamic fashion based on data volume, hence, the number of parallel processes. This is hard to achieve on premise within an enterprise because new servers need to be planned, budgeted and purchased. If the capacity is not planned well, the big data processing could be either limited by the amount of hardware, or extra purchase leads to wasted resources without being used. Processing on Cloud gains the big advantage of infrastructure elasticity which can give more guarantee to achieve the best scale in a more cost effective fashion. As compared to data processing, data access has very different characteristics, including: Given the above principles, there have been several milestones in the past 2 decades that reflect how to access the ever increasing amount of data while still returning the requested data within seconds: Below table gives some popular examples of each database type, but not intent to give a full list. Note that a database may combine more than 1 technologies. For example, Redis is a NoSQL database as well as in memory. In addition, data retrieval from Data Warehouse and Columnar Storages leverages parallel processes to retrieve data whenever applicable. Because there could be many choices of different types of databases depending on data content, data structure and retrieval patterns by users and/or applications, Data Access is an area an organization needs to evolve quickly and constantly. It should be also common to have different types of databases or tools at the same time for different purposes. As we can see, a big distinction between data processing and data access is that data access ultimately comes from customers’ and business’s needs, and choosing the right technology drives future new product developments and enhances users experience. On the other hand, data processing is the core asset of a company, and processing in scale and producing good quality of data is the essential enabler for a company to grow with its data. Many companies experience the stalking of their data processing system when data volume grows, and it is costly to rebuild a data processing platform from scratch. The principle of parallel data processing and scalability need to be carefully thought through and designed from the beginning. Data Processing also goes hand in hand with data management and data integration — all 3 are essential for the success of any data intensive organization. Furthermore, every organization is now facing many choices of big data solutions from both open source communities and third-party vendors. A clear understanding of the differences between data processing and data access can enable IT and business leaders to not only build a solid data architecture, but also make the right decisions to expand and modernize it at a steady pace."
Fundamentals of Data Architecture to Help Data Scientists Understand Architectural Diagrams Better,undamentals of Data Architecture to Help Data Scientists Understand Architectural Diagrams Bette,"Within a company using data to derive business value, although you may not be appreciated with your data science skills all the time, you always are when you manage the data infrastructure well. Everyone wants the data stored in an accessible location, cleaned up well, and updated regularly. Backed up by these unobtrusive but steady demands, the salary of a data architect is equally high or even higher than that of a data scientist. In fact, based on the salary research conducted by PayScale ( https://www.payscale.com/research/US/Country=United_States/Salary ) shows the US average salary of Data Architect is  $121,816 , while that of Data Scientist is  $96,089 . Not to say all data scientists should change their job, there would be a lot of benefits for us to learn at least the fundamentals of data architecture. Actually, there is one simple (but meaningful) framework that will help you understand any kinds of real-world data architectures. “Data Lake”, “Data Warehouse”, and “Data Mart” are typical components in the architecture of data platform. In this order, data produced in the business is processed and set to create another data implication. Three components take responsibility for three different functionalities as such: For more real-world examples beyond this bare-bone-only description, enjoy googling “data architecture” to find a lot of data architecture diagrams. Because different stages within the process have different requirements. In the data lake stage, we want the data is close to the original, while the data warehouse is meant to keep the data sets more structured, manageable with a clear maintenance plan, and having clear ownership. In the data warehouse, we also like the database type to be analytic-oriented rather than transaction-oriented. On the other hand, data mart should have easy access to non-tech people who are likely to use the final outputs of data journeys. Differently-purposed system components tend to have re-design at separate times. Then, configuring the components loosely-connected has the advantage in future maintenance and scale-up. Roughly speaking, data engineers cover from data extraction produced in business to the data lake and data model building in data warehouse as well as establishing ETL pipeline; while data scientists cover from data extraction out of data warehouse, building data mart, and to lead to further business application and value creation. Of course, this role assignment between data engineers and data scientists is somewhat ideal and many companies do not hire both just to fit this definition. Actually, their job descriptions tend to overlap. Last but not the least, it should be worth noting that this three-component approach is conventional one present for longer than two decades, and new technology arrives all the time. For example,  “ Data Virtualization ”  is an idea to allow one-stop data management and manipulation interface against data sources, regardless of their formats and physical locations. Now, we understood the concept of three data platform components. Then, what tools do people use? Based on  this “Data Platform Guide”  (in Japanese) , here’re some ideas: There are the following options for data lake and data warehouse. ETL happens where data comes to the data lake and to be processed to fit the data warehouse. Data arrives in real-time, and thus ETL prefers event-driven messaging tools. A workflow engine is used to manage the overall pipelining of the data, for example, visualization of where the process is in progress by a flow chart, triggering automatic retry in case of error, etc. The following tools can be used as data mart and/or BI solutions. The choice will be dependent on the business context, what tools your company is familiar with (e.g. are you Tableau person or Power BI person?), the size of aggregated data (e.g. if the data size is small, why doesn’t the basic solution like Excel or Google Sheets meet the goal?), what data warehouse solution do you use (e.g. if your data warehouse is on BigQuery, Google DataStudio can be an easy solution because it has natural linkage within the Google circle), and etc. When the data size stays around or less than tens of megabytes and there is no dependency on other large data set, it is fine to stick to spreadsheet-based tools to store, process, and visualize the data because it is less-costly and everyone can use it. Once the data gets larger and starts having data dependency with other data tables, it is beneficial to start from cloud storage as a one-stop data warehouse. (When the data gets even larger to dozens of terabytes, it can make sense to use on-premise solutions for cost-efficiency and manageability.) In this chapter, I will demonstrate a case when the  data is stored in Google BigQuery as a data warehouse.  BigQuery data is processed and stored in real-time or in a short frequency. The end-user still wants to see daily KPIs on a spreadsheet on a highly aggregated basis. This means  data mart can be small and fits even the spreadsheet solution . Instead of Excel, let’s use  Google Sheets  here because it can be in the same environment as the data source in BigQuery. Oh, by the way, do not think about running the query manually every day.  Try to find a solution to make everything running automatically without any action from your side. In this case study, I am going to use a sample table data which has records of NY taxi passengers per ride, including the following data fields: The sample data is stored in the BigQuery as a data warehouse. Technically yes, but at the moment this is only available through  Connected Sheets and you need an account of G Suite Enterprise, Enterprise for Education, or G Suite Enterprise Essentials account . Connected Sheets allows the user to manipulate BigQuery table data almost as if they play it on spreadsheet. See the GIF demonstration  in this page on “BenCollins” blog post. Connected Sheets also allows automatic scheduling and refresh of the sheets , which is a natural demand as a data mart. Although it demonstrates itself as a great option, one possible issue is that owing G Suite account is not very common. For more details about the setups, see  this blog post from “BenCollins” . To extract data from BigQuery and push it to Google Sheets, BigQuery alone is not enough, and we need a help of server functionality to call the API to post a query to BigQuery, receive the data, and pass it to Google Sheets. The server functionality can be on a server machine, external or internal of GCP (e.g. ‘Compute Engine’ instance on GCP; or ‘EC2’ instance on AWS). The code run can be scheduled using  unix-cron job . But one downside here is that it takes maintenance work and cost on the instance and is too much for a small program to run. ‘Google Cloud Functions’ is a so-called “serverless” solution to run code without the launch of a server machine. Putting code in Cloud Functions and setting a trigger event (e.g. scheduled timing in this case study, but also can be HTML request from some internet users), GCP automatically manages the run of the code. There are two steps in the configuration of my case study using NY taxi data. Step 1: Set up scheduling — set Cloud Scheduler and Pub/Sub to trigger a Cloud Function. Here, “Pub/Sub” is a messaging service to be subscribed by Cloud Functions and to trigger its run every day at a certain time. “Cloud Scheduler” is functionality to kick off something with user-defined frequency based on  unix-cron format . Combining these two, we can create regular messages to be subscribed by Cloud Function. See  this official instruction on how to do  it. Here are screenshots from my GCP set-up. Step 2: Set up code — prepare code on Cloud Functions to query BigQuery table and push it to Google Sheets. The next step is to set up Cloud Functions. In Cloud Functions, you define 1) what is the trigger (in this case study, “cron-topic” sent from Pub/Sub, linked to Cloud Scheduler which pulls the trigger every 6 am in the morning) and 2) the code you want to run when the trigger is detected. See  this official instruction  for further details, and here are screenshots from my set-up. The code to run has to be enclosed in a function named whatever you like (“nytaxi_pubsub” in my case.) The code content consists of two parts: part 1 to run a query on BigQuery to reduce the original BigQuery table to KPIs and save it as another data table in BigQuery, as well as make it a Pandas data frame, and part 2 to push the data frame to Sheets. Here’re the codes I actually used. Importantly, the authentication to BigQuery is automatic as long as it resides within the same GCP project as Cloud Function (see  this page  for explanation.) Yet, this is not the case about the Google Sheets, which needs at least a procedure to share the target sheet through Service Account. See  the description in gspread library  for more details. Finally, I got the aggregated data in Google Sheets like this: This sheet is automatically updated every morning, and as the data warehouse is receiving new data through ETL from the data lake, we can easily keep track of the NY taxi KPIs the first thing every morning. In a large company who hires data engineers and/or data architects along with data scientists, a primary role of data scientists is not necessarily to prepare the data infrastructure and put it in place, but knowing at least getting the gist of data architecture will benefit well to understand where we stand in the daily works. Data Lake -> Data Warehouse -> Data Mart is a typical platform framework to process the data from the origin to the use case. Separating the process into three system components has many benefits for maintenance and purposefulness. There are many options in the choice of tools. They are to be wisely selected against the data environment (size, type, and etc.) and the goal of the business. Finally in this post, I discussed a case study where we prepared a small size data mart on Google Sheets, pulling out data from BigQuery as a data warehouse. With the use of Cloud Scheduler and Pub/Sub, the update was made to be automatic."
Evolution of Data Architectures,volution of Data Architecture,"The separation of data from business operations and various analytical workloads (BI, Data Science, Cognitive Solutions, etc.) is as old as IT systems and business applications are. As analytical workloads are resource intensive, they need to be separated from the IT systems that run business operations so that operational workloads run smoothly without any resource constraints, thereby ensuring a positive customer experience. Our dependency on big data and business analytics has significantly increased over the years, with its market size  expected  to reach USD 684.12 billion by 2030. Globally, various industries invest in analyzing their massive volumes of data and creating effective data strategies. Data architectures are frameworks for how data strategies are supported through IT infrastructures. As the foundation of data strategies, data architectures play an essential role in effective strategy implementation. The evolution of data architectures over the years has accordingly shaped the effectiveness of data strategy. Data models, architectures, and storage have evolved with time, catering to diverse analytical workloads. In this article, we will introduce various data architectures that have evolved to meet continuously growing analytical needs. Each of these evolutions deserves a book to describe the complete details that cannot be produced in one article. However, the purpose is to describe high-level details of each of them here and point to additional literature available. Let’s begin. In the early days, an operational data store (ODS) was developed to cater to decision support systems, mainly targeting operational users who needed predefined reports. ODS stores only current data (6 months typically) for operational reporting and tactical decision making, such as a bank teller. It decides whether to offer an overdraft facility, increase the credit limit, etc., for a customer standing in the queue. The arrival of Business Intelligence tools has broadened the analytics user base to cover senior executives who prefer summary information in a graphical representation. Data marts and dimensional modeling techniques like star/snowflake schemas have been developed to support this user base. Data Marts are typically used for descriptive and diagnostic analytics focusing on specific subject areas, helping users to understand what happened, what is happening, and why, and also to conduct what-if analysis. While ODS and Data Marts serve two sets of different analytical users, they tend to get limited to specific functional areas. Enterprise Data Warehouses (EDW) have been developed to cater to the needs of cross-functional analysis. Data warehouses store historical data to find long-term patterns in the data. They have been designed with ER and dimensional modeling techniques depending on the organizational preferences. A typical enterprise data analytics architecture looks like this at this stage: ODS, Data Marts, and EDW implemented with traditional RDBMS such as Db2, Oracle, and SQL Server serve the purpose of canned reports and executive dashboards that could be delivered in batch mode as per predefined schedules, typically daily. For ad-hoc reports and interactive analysis, they have severe performance constraints. To serve these needs, multi-dimensional databases (MDDBs) such as Oracle Express, Cognos Power Play, Hyperion Essbase, etc., have been developed. These databases have been used for data marts for specific subject area analytics such as financial planning & budgeting, accounting, procurement, sales, marketing, etc., due to the size limitation of MDDBs (each cube could typically hold 2 GB of data). Users could perform interactive analysis with drill up/down/through, what-if analysis, and scenario planning with these MDDBs, though limited to a specific functional area. Analytical applications have to process data at an aggregate level to find new patterns. Traditional RDBMS like DB2, Oracle, and SQL Server that run-on general-purpose/commodity hardware lag behind in meeting the demands of these analytical workloads. DBMS like Teradata, appliances like Netezza, Neoview, Parallel Data Warehouse, and SAP HANA came into the market to address those needs. They run on special purpose hardware that uses massively parallel architecture and in-memory processing, giving a required performance boost. These appliances have been used to implement a flavor of Enterprise Data Warehouse. However, except for Teradata, all other appliance technologies have minimal success. ODS, EDW, and Data Marts deal with enterprise structured data only. They cannot process and analyze semi-structured (JSON, XML, etc.) and unstructured data (text, documents, images, videos, audio, etc.). In addition, they were developed before the cloud came into existence. Hence, there was tight integration between storage and computing resources. As a result, these resources had to be planned for peak load on the application, which will be underutilized most of the time when the load on the application is not high. With the arrival of big data technologies, another variant of data architecture came into existence, the data lake. While the purpose of the data lake is similar to that of EDW or data marts, it also caters to semi-structured and unstructured data. It is a more prominent implementation on cloud infrastructures such as AWS S3, Azure ADLS, or Google’s GCS. While data warehouses and data marts are built with a predefined purpose, a data lake is a raw storage of all types of data (at the lowest possible storage cost), which can be processed for specific purposes by spinning of a data warehouse, data marts, or data pipeline for data science and cognitive science applications. Since a data lake holds raw data, it does not require schema when writing, unlike data warehouses and data marts that need pre-defined schema when loading data into them. The low cost, object storage, and open format features of a data lake make it popular, as opposed to expensive and proprietary data warehouses. However, data lakes come with their own challenges, such as: This is what a typical data lake architecture looks like: Data warehouse and data lake architectures are centralized implementations that limit the scalability and availability of data for consumption. These implementations take a long time, limit domain understanding of the data, and are more technology-oriented than end user-oriented. They are designed and owned by data engineering specialists who are not readily available in large numbers, which is also a limitation of scalability and democratization of data for analysis. These data engineers are far away from business applications that generate the data; hence, it lacks business context and meaning of data. Data Mesh architecture/concept has been developed to address these challenges. In this approach, data is organized as data products along with various functional/subject areas or domains. They are owned by those responsible for business applications, so they understand the business context, meaning, and usage of the data. These data product owners take help from data engineers to design and distribute analytical data products. There will be a catalog of these analytical data products, which every consumer in the organization can see, understand the context of, use any given data product, and interpret accordingly. Core principles of Data Mesh are essentially: Data Mesh is still an approach for data architecture. There are no products available in the market yet that implement this architecture. Data Fabric is also trying to solve the same problems that Data Mesh is trying to do. However, their approaches are quite different. While Data Mesh is a domain and a business-oriented distributed approach, Data Fabric is a centralized meta data-driven and technology-centric approach. Data Fabric is developed with metadata, catalog, logical data model, and data delivery APIs. Part of the data is virtualized, while the remaining data is centralized, just like a data warehouse. It is complemented with centrally managed data life cycle management policies, such as: SAP Data Intelligence, IBM’s Cloud Pak for Data, Oracle Coherence, and Talend Data Fabric are some of the products available in this space. Denodo is another product that is more about data virtualization technology which is a core part of the data fabric approach. In the Data Lake architecture, each type of analytical workload requires its own data pipeline due to different data access requirements, leading to inconsistent understanding and usage of the same data. It also introduces one more layer of data storage in between analytical applications (consumers) and business applications (sources) that generate data. First, data has to come into the data lake and then move to the consuming applications, which could reduce the value of key insights by the time they are acted upon. Data lake does not support transactional applications and has many other limitations, as described in the section above. Lakehouse architecture is trying to address these issues by having a common interface for all types of data analytics workloads. It supports ACID properties of transactional applications. It essentially combines the advantages of both data warehouse and data lake architectures while addressing the challenges of both. Data architectures have been evolving to meet the growing demands for various analytical and cognitive workloads, leveraging innovations in cloud and big data technologies. Depending on where the organization stands on the data analytics maturity, the variety of data it holds, and the kind of analytical workloads it requires, a specific type of data architecture can be chosen. While the Lakehouse architecture holds the promise of the best of all worlds, it is new and yet to mature for broader adoption. Data architectures are at the core of all business data strategies; hence, paying attention to them is crucial. With the right data architectures for your specific use case, you can ensure the successful implementation of data strategies. References 1.   What is Operational Data Stores? 2.  Data Warehouse Concepts: Kimball vs. Inmon Approach 3.  What is a multidimensional database? 4.  Data Warehouse Appliance Vendors and Products 5.  Introduction to Data Lakes 6.  Data Mesh Principles and Logical Architecture 7.  Using Data Fabric Architecture to Modernize Data Integration 8.  Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics krtrimaIQ Cognitive Solutions is an agile start-up with the attitude and power of an enterprise, focused on applying Data Science, Cognitive Science and NextGen BI to build the Intelligent Enterprise. In Sanskrit,  “krtrima”  means  “artificial” , and  “IQ”  is the  Intelligent Quotient .  To know more about what we do, check out our  website , and follow us on  LinkedIn , and  Twitter ."
What is the Data Architecture We Need?,hat is the Data Architecture We Need,"In the new era of Big Data and Data Sciences, it is vitally important for an enterprise to have a centralized data architecture aligned with business processes, which scales with business growth and evolves with technological advancements. A successful data architecture provides clarity about every aspect of the data, which enables data scientists to work with trustable data efficiently and to solve complex business problems. It also prepares an organization to quickly take advantage of new business opportunities by leveraging emerging technologies and improves operational efficiency by managing complex data and information delivery throughout the enterprise. When compared with information architecture, system architecture, and software architecture, data architecture is relatively new. The role of Data Architects has also been nebulous and has fallen on the shoulders of senior business analysts, ETL developers, and data scientists. Nonetheless, I will use Data Architect to refer to those data management professionals who design data architecture for an organization. When talking about architecture, we often think about the analogy with building architecture. A conventional building architect plans, designs, and reviews the construction of a building. The design process involves working with the clients to fully gather the requirements, understanding the legal and environmental constraints of the location, and working with engineers, surveyors and other specialists to ensure the design is realistic and within the budget. The complexity of the job is indeed very similar to the role of a data architect. However, there are a few fundamental differences between the two architect roles: Given all these differences, a data architect could still learn from building architects and, in particular, take their top-down approach to improve data architecture design. In many organizations, there has been a lack of systematic, centralized, end-to-end data architecture designs. Below lists some of the main reasons: With these shortfalls, we often see a company with disjointed data systems and gaps between teams and departments. The disparities lead to the poor performance of the systems with many hand-offs, a long time to troubleshoot when a production data issue arises, a lack of accountability to reach the right solution across systems, and a lack of capability to assess the impact of a change. Lastly, the disjointed systems could cause tremendous effort to analyze and research when migrated or re-engineered to the next-gen platform. Given all these, a successful enterprise needs to have a top-down coherent data architecture designed based on the business processes and operations. In particular, just like what a building architect does, an enterprise data architect needs to build a blueprint at the conceptual and logical level first, before applying the technologies to the detailed application designs and implementations. In modern IT, business processes are supported and driven by data entities, data flows, and business rules applied to the data. A data architect, therefore, needs to have in-depth business knowledge, including Financial, Marketing, Products, and industry-specific expertise of the business processes, such as Health, Insurance, Manufacturers, and Retailers. He or she can then properly build a data blueprint at the enterprise level by designing the data entities and taxonomies that represent each business domain, as well as the data flow underneath the business process. In particular, the following areas need to be considered and planned at this conceptual stage: This conceptual level of design consists of the underlying data entities that support each business function. The blueprint is crucial for the successful design and implementation of Enterprise and System architectures and their future expansions or upgrades. In many organizations, this conceptual design is usually embedded in the business analysis driven by the individual project without guidance from the perspective of enterprise end-to-end solutions and standards. This level of design is sometimes called data modeling by considering which type of database or data format to use. It connects the business requirements to the underlying technology platforms and systems. However, most organizations have data modeling designed only within a particular database or system, given the siloed role of the data modeler. A successful data architecture should be developed with an integrated approach, by considering the standards applicable to each database or system, and the data flows between these data systems. In particular, the following 5 areas need to be designed in a synergistic way: The naming conventions and data integrity The naming conventions for data entities and elements should be applied consistently to each database. Also, the integrity between the data source and its references should be enforced if the same data have to reside in multiple databases. Ultimately, these data elements should belong to a data entity in the conceptual design in the data architecture, which can then be updated or modified synergistically and accurately based on business requirements. Data archival/retention policies The data archival and retention policies are often not considered or established until every late-stage on Production, which caused wasted resources, inconsistent data states across different databases, and poor performance of data queries and updates. To enforce the data integrity, data architects should define the data archival and retention policy in the data architecture based on Operational standards. Privacy and security information Privacy and security become an essential aspect of the logical database design. While the conceptual design has defined which data component is sensitive information, the logical design should have the confidential information protected in a database with limited access, restricted data replication, particular data type, and secured data flows to protect the information. Data Replications Data Replication is a critical aspect to consider for three objectives: 1) High availability; 2) Performance to avoid data transferring over the network; 3) De-coupling to minimize the downstream impact. Excessive data replications, however, can lead to confusion, poor data quality, and poor performance. Any data replication should be examined by data architect and applied with principles and disciplines. Data Flows and Pipelines How data flows between different database systems and applications should be clearly defined at this level. Again, this flow is consistent with the flow illustrated in the business process and data architect conceptual level. Besides, the frequencies of the data ingestion, data transformations in the pipelines, and data access patterns against the output data should be considered in an integrated view in the logical design. For example, if an upstream data source comes in real-time, while a downstream system is mainly used for data access of aggregated information with heavy indexes (e.g., expensive for frequent updates and inserts), a data pipeline needs to be designed in between to optimize the performance. As data architecture reflects and supports the business processes and flow, it is subject to change whenever the business process is changed. As the underlying database system is changed, the data architecture also needs to be adjusted. The data architecture, therefore, is not static but needs to be continuously managed, enhanced, and audited. Data governance, therefore, should be adopted to ensure that enterprise data architecture is designed and implemented correctly as each new project is being kicked off. Within a successful data architecture, a conceptual design based on the business process is the most crucial ingredient, followed by a logical design that emphasizes consistency, integrity, and efficiency across all the databases and data pipelines. Once the data architecture is established, the organization can see what data resides where and ensure that the data is secured, stored efficiently, and processed accurately. Also, when one database or a component is changed, the data architecture can allow the organization to assess the impact quickly and guides all relevant teams on the designs and implementations. Lastly, the data architecture is a live document of the enterprise systems, which is guaranteed to be up-to-date and gives a clear end-to-end picture. In summary, a holistic data architecture that reflects the end-to-end business process and operations is essential for a company to advance quickly and efficiently while undergoing significant changes such as acquisitions, digital transformation, or migration to the next-gen platform."
A Framework for Modern Data Architecture, Framework for Modern Data Architectur,"Today is the age of data. Every organization is becoming a “data company.” Today, if an organization is not thinking about its data as a strategic asset, then it has already missed the bus. Data has evolved over the past decade. The rate of evolution has been exponential. But have the Data Architecture practices managed to keep up with the same pace? McKinsey recently published an  article  that formulates the building of modern data architecture to drive innovation. The article explains the foundational shifts for modern data architecture. Being a practitioner in this field, I found Mckinsey’s view interesting. The goal of this article is two-fold: The article elaborates on the six foundational shifts that enable modern data architecture. The article elaborates on the six foundational shifts that enable modern data architecture. Shift #1: From on-premise to cloud-based platforms Cloud enables organizational agility. Shift #2: From batch to real-time processing Organizations will pivot from batch to real-time data ingestion and processing. Shift #3: From pre-integrated commercial solutions to modular, best-of-breed platforms Cloud enables modular components focused on functionalities rather than technologies. Shift #4: From point-to-point to decoupled data access Co-existence of point-to-point and decouple data access is required to get the right data to the right stakeholder and at the right time. Shift #5: From an enterprise warehouse to domain-based architecture A single repository of Enterprise Data Warehouse impedes the required agility for transforming data into insights. Shift #6: From rigid data models toward flexible, extensible data schemas Data models used in the right context serve as the blueprint for the modern data architecture. Having discussed these shifts, let us see which components in a cloud platform can enable these shifts. Microsoft’s Azure cloud platform will be used to exemplify the features. However, these capabilities can be mapped to other cloud platforms as well. In conclusion, there are three key takeaways:"
Modern Data Architecture Models,odern Data Architecture Model,"Data has come a long way, starting from the 1640s when the term “data” had its first use, to the 21st century, where AI has become integral to everyday life. As you can imagine, several software and hardware developments have co-evolved with data, bringing us to the here and now. One of the early challenges in data was ingesting it — how the data was to be used and the needs it served, weren’t nearly as interesting. The use cases were extremely narrow, mostly defaulting to basic business reporting. Today however, the focus has shifted from ingesting data to making it accessible in a way that would support a plethora of applications — parameters of accuracy, timeliness, reliability and trust at a massive scale, are paramount. The challenges today in data — a consequence of its scale and speed — are in the areas of data discoverability, governance and reliability. The market is flooded with tools for every data problem conceivable. But, is there a guiding philosophy on how to bring these multitude of tools together, or how to stitch the different roles in an org with these tools? What we need are data architectures that can provide directional guidance, allow for weighing trade-offs, are domain-agnostic and at the same time don’t put us at the risk of building something that quickly becomes obsolete. Data architecture is a relatively new term. In fact, one of the first references to data architecture is the mention of Data mesh as a model in this  article  in April, 2020. So, is data mesh the only model or one of many? A search will show  Data Fabric  and  Data Mesh  as two popular candidates for data architecture models. If you are looking for a short form introduction to the two models; Think of data fabric as a convergence of the modern data tools, stitched together to collect disparate data and move it within a system in a multi-hop manner. The objectives being, data discoverability, accessibility and management — for varied consumers and use cases. Data Mesh then, is the next step in the evolution of data architectures; brining in aspects of product management and decentralization to data. Contrasting one with the other, Data fabric allows for ingestion of data from any source, for any use case — without gating it for quality during ingestion — trust in data, data integrity are addressed through layers that logically come after ingestion. Whereas, Data mesh places strong emphasis on data quality and data being treated as a product, even before it can become part of the data ecosystem. Is one better than the other? Which one renders itself better to implementation? Here’s the long form of the two models. How should data move in a system, What characteristics should data retain and shed as it moves? In the Data fabric architecture, data follows a set of steps that determine its flow. The first step takes data through an integration phase. In the integration phase, data is ingested and then cleaned, transformed and loaded into storage. Then, there is the data quality phase where quality assessment is performed on the stored data. This data is then made available for different use cases through a combination of a data lake and a data warehouse, Typical use cases are BI, analytics and machine learning. Data governance policies are defined for the ingested data and a data catalog is used for discoverability. The above functions are mostly centralized — a team of data specialists are designing and implementing the different stages in the fabric and also setting up policies and access controls. Simply put, Data Fabric is how most data ecosystems move, store and access data today. The beauty of data fabric as an architecture model is the flexibility it offers — not all components are a must, there are multiple vendors with off-the-shelf solutions that can collect and process data from  any  source and  any  use case. Riding on the shift of software systems towards distributed domain design, data mesh is built on the principles of distributed architecture. There are three major components in a data mesh —  Decentralized Domain ownership of data  and the resulting  Data products ,  Self-serve data infrastructure  and  Federated Governance . Data Mesh has been designed to derive value from data at scale, in complex environments — complex not in data volume or velocity but in the number of use cases and the diversity of data sources. Since the complexity in not only technical, this architecture is modelled as a socio-technical construct. Domain owned data is probably the most critical shift in going from Data Fabric to a Mesh. The idea is quite simple — Who better to own and provide data for use, than the teams generating the very data? In this paradigm, business domains decide what data is useful and should be exposed for different use cases within the org. If that is true, are these the teams also building methods and tools to serve this data? No — This requires skills that the domains are not expected to have and is instead delegated to the data infrastructure that builds a self-serve data platform. Domains serve their data as a product — a product that meets well-defined standards that ensure interoperability with data from other domains. This data product lives as a node on the mesh. This is how the concept of ETLs is done with in the Data Mesh paradigm. Decentralized domain data ownership is the highlight in this architecture.  Ownership of design and deployment of the infrastructure that serves data, however is centralized — with the data platform team. Naturally, there arises a need for a body that balances these aspects, delineates decisions that lie localized with each domain from the decisions that are considered global. This group is the federated governance group that is carved out of both the data platform team and individual domains. Both the architecture models attempt to solve the problem of getting value from data at scale — while making data secure, accessible and easy to use and interpret. In a Data Fabric, a dataset gains value by being onboarded, catalogued and made available through a standardized set of governance rules. In a Data Mesh, a dataset gains value because of its usability as determined by its consumers (data scientists/data analysts). In a Data fabric, there is standardization in how data is cleaned, labeled and checked for quality. In a Data Mesh, the decision on how data is to be made consumption ready i.e the pre-processing steps lies with the domains that own the data. In a Data Fabric, the onus of understanding the data, interoperability of data sets generated by different services becomes a joint responsibility of the data engineering team and consumers of data — the analysts and the scientists. In a Data Mesh, it is the responsibility of the teams serving their data, to understand how the data could be used to generate value and design it in a way that meets the needs of the consumers. Data Fabric addresses and recommends solutions to the fundamental questions on ingestion and use of data. Data Mesh as a model, can become a solution when the fabric hits a wall on issues around data ownership and data quality. Also, an important pre-requisite for the Data Mesh architecture to be successful is domain oriented software architecture and teams in an organization. All things considered, it is a good idea for a data org to get started with the data fabric paradigm and adopt principles from data mesh as their data, their needs and the complexity of their data systems evolve."
A Data Scientist’s Guide to Data Architecture, Data Scientist’s Guide to Data Architectur,"You should always prepare your own data. This quote comes from a senior executive in the AI space when I asked him about his view on data a few years ago. He is not the only one thinking this way. The head of the department during my first job couldn’t trust another team to build the data pipeline and ended up writing a big portion of the ETL code himself and had the data team reporting to him directly. Why do these senior executives put so much emphasis on data? Because without a good grasp of data, you won’t (and shouldn’t) have much confidence in the insights and recommendations that you derive from the data. For data to move from its raw form all the way to being ready for analysis, it’s gone through a lot of processes. A good data scientist must question every step of the process relentlessly, and ask questions like: The purpose of this article is to go through some data architecture related topics using real-world examples so that you have some basic context and know what questions to ask your data team. As mentioned before, I started my career working for a major insurance company, in a team where all data processes are done by ourselves. Every month, we take a copy of a long list of tables from the production database and put it onto our SQL server which we call the staging database. Production database : this is the transaction database used by various departments to administer insurance policies and claims in real time. Staging database : this is a snapshot of the production database for all sorts of analysis work. Typically a snapshot is taken once a month, but sometimes it is taken more frequently upon ad-adhoc requests. The purpose of the staging database is so that we don’t interfere with the normal business operation. This should never happen, but it did happen once due to a data connection setting problem. We received phone calls from the IT department every 5 minutes, as everyone has been complaining the system is frozen… The staging environment is safe for the analytics team to work in. Moreover, you are guaranteed to have the same result as long as you run your code against the staging database. Once all the tables have been copied onto the staging database, we perform a series of data processing steps to turn these into a handful of denormalized tables. The raw tables in the production system are called normalized tables which is a core concept behind relational databases. Each table contains data about different things and can be linked together via primary and foreign keys. For analytics purposes, a large number of table joins are required before you can have something useful. For example, say you want to predict the probability of having a building insurance claim given the characteristics of the building. Ideally, you need a flat table where To create the flat table you need to join the claim table (which tells you whether or not there is a claim), the policy table (which has the policy information) and the risk table (which has the building information) together. This process is called  denormalization . The result is a  denormalized  or  flat table  where the data is well organized and ready to be analyzed. You may think that this is no big deal. We just perform a few joins and it’s done, isn’t it? What could go wrong? A lot can go wrong. You may have duplicated records. You may unintentionally gain or lose records. This is what I was told to do when I started my first job, and I believe I still benefit from these good habits today. If we skip these steps, we could end up wasting more time down the track. This is a lesson I learned again and again in many different projects. After a lot of efforts, we end up a few core tables that satisfy the needs of the entire team, which include: These are all flat tables and are produced monthly and put onto a common server. Whoever needs to do any analysis just makes a copy of these flat tables, knowing that the data can be trusted. ETL stands for  E xtraction,  T ransform and  L oad. It may sound fancy, but you’ve seen the process already. “Extract” is when we copy the data from the production database to the staging database. “Load” is when we deliver the final core datasets. “Transform” is all the work in between. Throughout the entire process, we had robust data checks every step along the way: At that time, most of the data integrity checks, as well as the original ETL process was written in SAS programs. Typically after adding these checks, the size of SAS programs increases by a factor of 3. Indeed, checking takes more effort than doing! Is it worthwhile though? Absolutely. Everyone in the team can pick up the dataset and immediately work with it confidently knowing it’s been checked every step along the way. In addition to robust data checks, we also spent a lot of time learning how data is generated in the first place. This was done by sitting with the people who input the data every day. In our case, it was the sales department for policy data and claims department for claims data. We sat alongside the frontline staff, listening to them answer customer’s questions, typing data into the system. We tried our best to understand how they did their job and built a good deal of domain expertise. Check out  this article  I wrote on the importance of building up domain expertise. While the above process might be good for detecting data quality problems, it doesn’t fix them going forward. That’s why a feedback loop is needed. Every month, we publish a set of data quality reports and shared them with the management team. Some of the things we look for are The data quality performance is tied to the KPI of staff and forms part of the metrics that drive their remuneration. This provides a powerful incentive for everyone to care about data quality. This would never be possible without the support from the senior management of the company. I believe a data-driven decision-making culture can never be driven from bottom up. It needs to come from the top down. So far, I have explained the traditional approach. We were using SAS back then. The word data engineer or data scientist wasn’t used by people. Nowadays, a lot has changed, but these concepts are still relevant. Next, I will discuss a few topics that became increasingly relevant in recent years. The data extracted from the IT system is called internal data because it is internally generated. External data is really powerful and can significantly enrich and complement internal data. The type of external data varies significantly by context. In the case of building insurance, here are some examples of the external data that can be used: In the case of motor insurance, here are some examples of additional external data that can be used: Most of the data discussed above is structured data. Examples of unstructured data include: As you can see, unstructured data typically require some additional processing to become useful for our tasks, e.g. insurance risk modeling. So far I have only discussed batch processing. The ETL process takes at least 2–3 days. Adding another few days for analysis, you won’t have any output until at least a week after the data becomes available. Real-time analytics is different. The moment data comes in, it gets processed and analyzed. Can you imagine that the moment you buy a product, Amazon says to you: “Dear customer, thanks for your purchase. Please wait for a week and we’ll recommend some products that interest you.” That never happens. They provide a recommendation instantly. This allows them to generate 35% of their revenue on Amazon.com from  recommendations . Real-time analytics is often within the realms of data engineers. Take the COVID-19 as an example, these are batch processing based static dashboard results that I created. It was a pain to update the results. By the time I publish these articles, the numbers were already out of date. Status Update of the Coronavirus (COVID-19) Outbreak  — published Mar 17 Status Update of the Coronavirus (COVID-19) Outbreak  — published Feb 29 Coronavirus outbreak — 5 questions to ask big data  — published Feb 1 Compare the above to  this interactive dashboard  which shows the spread of the COVID-19 virus around the world and in particular in Singapore since late January. The entire process happens automatically as new data comes in. Real-world analytics delivers the result just when it is most needed — who cares about what COVID-19 figures were a week ago? Let’s apply the same logic to the insurance sector. Why should the CEO of an insurance company care about the sales volume or loss ratios from 3 months ago? Yet, today most insurance companies still consume monthly or quarterly reports. With some exceptions such as real-time fraud analysis, the insurance industry tends to be slow to adopt these new technologies. As tools like R-Shiny and Dash make it much easier and more affordable to deploy real-time analytics I expect it to gain momentum in the near future. Within the data science world, there are a lot of sexy topics — AI, deep learning, big data… Data architecture is not one of these sexy topics. The company executives won’t ever attract any media attention by saying “we have built a world-class data integrity check”. It’s hard work. It takes a lot of discipline to get it right. However, if you do manage to get it right, it will probably deliver more value than any of those sexy topics. As a data scientist, it pays to spend some time learning about these concepts so that you can communicate effectively with your data team and ensure you have a robust data process that lays down a solid foundation for all your analytics work."
Microservice Principles: Decentralized Data Management,icroservice Principles: Decentralized Data Managemen,"Microservice philosophy favors decentralization in all aspects of software design. This focus on decentralization doesn’t just guide the organization of business logic. It also guides how data is persisted. In the traditional monolithic approach to software design it is common to use a monolithic data store, such as a SQL server that contains a single database with many tables. This central database is used as an engine for all data persistence, and often portions of the application logic are offloaded into the SQL server in the form of queries that use complex joins, or even stored procedures. In contrast microservice architecture favors decentralized data management, as covered by  Martin Fowler’s original 2014 paper  that defined microservices. This article extends the concept of decentralized data management by showing some of the modern architectural patterns for data management that lead to highly successful decentralized applications. In order to correctly organize data in a decentralized manner it is important to first understand how to model data using Representational State Transfer, or REST for short. REST was defined in 2000 by  Roy Fielding  and has guided the development of many massively scalable stateless systems ever since. The core principle of REST is to give each resource that is part of your application a URL, and then use standard HTTP verbs to interact with the resource. For example, the API for a basic social messaging app might be organized like this: This API has three primitive resource types: user, message, and friend. Each primitive type is served by a set of resource paths. In a monolith one central server would handle requests for all the resource paths, and typically this service would be backed by a single database that also stores all the resource types as tables: A microservices deployment employing decentralized data management would serve the three resource types using three services: one service for user resources, one service for message resources, and one service for friend relationships. In addition each service has its own database. The fact that each microservice has its own database does not mean that there need to be three database servers. In the early days of a platform the three databases will probably just have a logical distinction as three databases all hosted by a single physical SQL server. However, creating this logical distinction sets the platform up for easy physical scaling in the future. If this platform gains massive adoption the database administrator can split the three logical databases into three databases served by three different physical servers. A critical aspect of good decentralized data management is to avoid SQL JOIN. The need for joins usually starts from efforts to make an API easier for clients to consume. For example, the messaging app we are using as an example might have a timeline view. The timeline needs to have the latest message from each friend of the authenticated user as well as that friend’s name and avatar beside the message. With the basic REST API that we have defined the client would need to make many API calls to populate this view. For example, a user with two friends would require that the client make the following API requests in order to populate the view: A total of five requests would be made. One request to get the list of friends of the user, followed by two requests to get the name and avatar of each friend, and two requests to get the latest message from each friend. Obviously this is unacceptable from a performance standpoint because there is so much extra roundtrip latency between the client and the server before the view is ready to be displayed. A sensible solution to this problem would be to add a new route to the API: The client can then fetch this single timeline resource to get all the data it requires to render the timeline view. The technique used for implementing this new resource is a prime example of the difference between centralized data management, and decentralized data management. In a monolith the logic for serving this route would probably be coded as a SQL join, and offloaded to the database server, which would access all three tables to generate a result: In a decentralized data management architecture such a SQL join is not only not advised, but actually impossible if the data is properly separated using logical and/or physical boundaries. Instead each microservice should be the only gateway to accessing its own table. No single microservice has access to all three tables. To expose the timeline resource to a client we create an additional timeline microservice that lives on top of the three underlying data microservices and treats each as resources from which it fetches. This top level microservice joins the data from the underlying microservices and exposes the joined result to the client: This timeline service can make requests to the backing microservices in a matter of milliseconds because the timeline service and the other microservices are hosted in the same datacenter, and perhaps in containers that are hosted on the same physical machine. To further reduce roundtrip network penalties the timeline service could also take advantage of “bulk fetch” endpoints. For example the user microservice could have an endpoint that accepts a list of user ID’s and returns all matching user objects, so that the timeline service only has to make one request to the friends service, one request to the user service, and one request to the message service. The timeline service functions as a centralized place to define the logic for what a timeline is. If business requirements change and the client now needs to display the latest two messages from each friend then this can easily be changed in the timeline service without needing to make modifications to the other backing microservices that actually host the basic resources. Additionally the separation between how the data is stored and how the data is manipulated for display to users allows the underlying microservices to be refactored, as long as they continue to adhere to the resource format that the timeline service expects. The maintainers of the friend service could easily rewrite how the friend relationships are stored without breaking the timeline service. On the other hand the use of join queries requires all joins against a table be reviewed and updated if you need to update the table structure. One of the side effects of decentralized data management is the need to handle eventual consistency. In a centralized data store developers can use transactional capabilities to ensure that data is in a consistent state between multiple tables. However, this is not the case when data is separated into different logical or physical databases. For example, consider what would happen if a user fetched their timeline at the exact same moment that one of their friends deleted their account: Decentralized data modeling requires extra conditional handling to detect and handle such race conditions where underlying data has changed between requests. For the case of a simple social media application this is typically easy. But for a more complex application it may be necessary to keep some tables together in the same database to take advantage of database transaction. Typically these linked tables would also be handled by a single microservice. Alternatively if related data needs strong consistency but is still to be decentralized it may be necessary to use a  two-phase commit  in order to manipulate it safely. One significant advantage of decentralized data management is the ability to take advantage of polyglot persistence. Different types of data have different storage requirements: For the example social messaging app each message is actually a structured JSON document that contains metadata about media files, geolocation, etc. Additionally, it is expected that there will be many users posting messages, and the total number of messages that need to be persisted will grow quickly. For this scenario the team in charge of messages may choose to utilize a sharded MongoDB cluster to persist this structured JSON data. On the other hand the user and friendship tables have a simple, flat data model and do not grow as quickly, so those services are backed by a Postgres server. Because this application is utilizing decentralized data management principles it is able to take advantage of polyglot persistence and store the different types of data in different databases that serve the needs of that particular data type. Decentralized data management can be properly deployed by starting from REST basics to figure out the separations between different resource types. These separations should then drive microservice and database boundaries. Where multiple types of resources are required to serve a composite resource to a client this can be built by using a higher level microservice that joins data from different underlying microservices. This requires careful handling of eventual consistency, but it allows the use of polyglot persistence to store different types of data in storage providers that best handle that type of data."
Data Management Strategy: Introduction,ata Management Strategy: Introductio,"Everything you wanted to know about it and its main actors The objective of these series of articles is to obtain a clear idea of the benefits, needs and challenges involved in carrying out a Data Management initiative. Data Management projects will be transversal and will put in contact different departments of the organizations. One of the main challenges is to have all the business information available. It is very important to point out that Data Management methodologies focus on what should be done and not on how. Data Management managers manage these changes, but they are not the ones who carry them out. Even so, they will have a very important role in this type of projects, especially in data governance. They should act and be seen as the leaders who drive and boost the digitalization journey By the end of these series you will be able to understand the main concepts related to data management, or Data Management, which are: Within each of these concepts, we will explore the fundamental perspectives from the point of view of: In this first article, we will focus on the general concepts and the motivation behind developing a well structured Data Management strategy as the key component of the digitalization journey within an organization. In recent years there has been tremendous data growth in all organizations. Data has become the basis of competitiveness, productivity, growth and innovation in many companies. Facebook, Amazon, Spotify and Netflix are a few examples that we can think about and have completely revolutionized the industry thanks to its data strategy. The growing volume of data in the operational systems of companies and the appearances of internet, social media and multimedia, are supposing a revolution in the knowledge of the client its preferences and demands. As well as in the depth of the insights gained about companies inner processes, as we are able now to clearly track and monitor which parts are performing well and more importantly, which are not. This volume of data puts us in a special context in which data management becomes a fundamental part, as data has become an asset of companies. Currently, Data Management is a priority in organizations and to meet the strategic objectives of companies is becoming more necessary than ever to have quality data of corporate information. Data Management implies the definition of policies, roles, processes and responsibilities throughout the company on the definition and management of data. An effective data governance model requires a complete structure that facilitates collaboration between technology and business. It is important to be aware of the importance of data governance in all organizations and its impact on the generation of an holistic vision of business information. In addition, it is crucial to know the key factors to take into account when defining effective data governance and to know the technological and methodological aspects to carry out data governance techniques. The goal of Data Management is to increase the value of an organization’s data through data governance. In the past recent years we have experienced the rise of technologies related to Data Science, some of them are: Data Engineering focuses on building adequate infrastructures to facilitate the data flow within organizations and in the preparation of this data to be in a useful format. Data Analytics focuses on finding useful information from the data. This branch of Data Science is involved in the descriptive and diagnostic analysis of the data, that explains what happened and why it happened. It also involves the Data Visualization aspect (which is an entirely separated field) Machine Learning is the science (and art) that focuses on making computers learn from data. They do this by learning correlations between certain characteristics of past data that lead to certain outcomes, so when they are presented with new data, they can make accurate predictions. Deep Learning is a subfield of machine learning that focuses on replicating the learning mechanisms that intelligent beings use to learn. They do this by deconstructing complex concepts in simpler ones, and so, learning in a hierarchical way. Using Artificial Neural Networks to achieve this. These areas of knowledge have changed the rules of the game due to the technologies and the real time data processing capacity that has been obtained. We have more data from more sources in less time. In addition, the democratization of computing power has enabled us to be able to keep up with the processing of this data, thanks to technologies such as Amazon Web Services, Google Cloud Platform and Azure (to name a few). Infrastructure development is no longer an impossible task, the new challenge is to get value out of this data. Define which models to generate to extract business value from them. Data can be extracted from social networks, sensors, mobile devices… so that this processed information plays in our favor, we will have to develop analytical techniques and data management that provides a 360º vision of the company’s information. This will be done with a data governance initiative in our organization. A Gartner survey identified a new figure in organizations, the CDO or Chief Data Officer. This CDO is an agent of change and aims to enhance the value of the data. Data exists to extract business value and improve decision making. These roles belong to the top management of the organization and innovation. The CDO will be responsible for all data governance and will prepare the organization for the digital transformation that will involve the integration of data, previously separated into silos. The tasks of the CDO are: The CDO will assist in decision making by ensuring that data is a source of quality information and implementing all decisions to make the information credible. Although currently their role is not perfectly defined, in a few years organizations that do not apply data governance techniques will be rare, and surely in a clear disadvantage over those that apply them. It is to define effective methods so that the information is in where we need it, when we need it and with the characteristics we need. Data governance is a joint responsibility of management and IT. Data governance requires continuous improvement and development. To do so, it uses best practice frameworks. Corporate data management is about effectively managing the entire data lifecycle. We speak from the capture, storage, transformation, movements between the different systems and the use that is given to the data. Defining a data governance model is necessary for companies, which customers expect and governments demand. It is a need that must be addressed. It is a cyclical process in which it is necessary to continuously ensure the quality of this data. It is the proposed frame of reference for the management of DM. DAMA is an international structure that oversees the structure of DM. It is a compilation of good practices for data management. It identifies 11 functions for successful data management These functions will be condensed in the following ones, in order to have a more practical and agile approach: The selection of technologies used in a Data Management Initiative will vary depending on the specific needs and infrastructure of each organization. However, some of the most common and used technologies are: In the next articles, we will dive deep in each of those areas, learn who are the main actors and the best practices and technologies to use in each step along the Data Management journey. You can find below links to the articles of the series. Data Management Strategy: Part 1 — Data Governance & Metadata Management Data Management Strategy: Part 2 — Data Quality & Data Architecture Data anagement Strategy: Part 3 — Data Integration, Data Security and Master Data If you liked this post then you can take a look at my other posts on Data Science and Machine Learning  here . If you want to learn more about Machine Learning, Data Science and Artificial Intelligence  follow me on Medium , and stay tuned for my next posts!"
Master Data Management: an Essential Part of Data Strategy,aster Data Management: an Essential Part of Data Strateg,"First of all, what is Master Data Management (MDM)? Master data refers to the critical data that are essential to an enterprise’s business and often used in multiple disciplines and departments.  MDM is the establishment and maintenance of an enterprise level data service that provides accurate, consistent and complete master data across the enterprise and to all business partners. The concept of Master Data Management originated around 2008 when data warehousing and ERP applications became popular in many organizations. With the increase of data volume and the number of databases, and thus the increase in the   number of applications for users to enter and read the data, it became more and more important to make sure that correct master data definitions are used so that there is a single truth in the data without discrepancies, duplications or being out-of-date. The first example is related to customer information. In a big organization, there could be multiple customer databases populated and managed by multiple applications or department silos, and the same customer in the real world could receive multiple direct mails or notifications from the same company. As the data grows, the master data consists of not only the customer information, but also other key data assets, such as the data of prospects, suppliers, panelists and products. MDM has been a challenge to implement, because all three aspects of  processes ,  technology and  tools  are required to ensure that the   master data is coordinated and synchronized across the enterprise. With the recent explosion of big data and rapid progress of analytics and IoT, the consistency of referencing and applying high-quality master data has become unprecedentedly crucial. An enterprise should not only need to make sure it has its key data assets  efficiently and accurately  managed, but also embrace new data assets to fully realize the economic potential by joining or referencing the existing master data that the company already owns. As such, MDM needs to be an essential part of the data strategy for a company to grow and profit as well as   one of the core missions for C-Suites and Executives such as Chief Data Officer (CDO). There have been 2 main reasons for failures of MDM initiatives in the past 10 years: 1) Relying on only technology and tools without buy-in and support from business units; 2) Focusing on fixing and solving current data issues, without forward thinking. For MDM to be successful, it needs to be first a business-driven process and embraced by business departments and executives. In many cases, fundamental changes to business processeswill be required to establish and maintain unified master data and some of the most difficult MDM issues are not technical at all. Next,a forward looking strategy is crucial in placing   MDM as an essential part of data management in an organization, since it proactively lays the foundation for future success. Numerous experiences tell us that the implementation of MDM is easiest and smoothest when a dataset has just been introduced for ETL and into a business intelligence project. Trying to fix for existing data assets and processes often require high cost and large effort, which also likely leads to big impact on the current deliverables. Below is a comparison to illustrate the big differences between implementing MDM at the beginning versus fixing the issues of existing data and systems: Delayed Implementation of MDM 2. Data quality issues everywhere with no easy way to track down 3. Low customer satisfaction 4. Data asset potentials are not fully realized 5. Difficult to migrate to new data platform Implement MDM from the Beginning 2. Fewer data quality issues that are faster to fix 3. High customer satisfaction 4. Generate more revenue opportunity 5. Much easier to migrate to new data platform when needed With the above comparison, it is clear that MDM should be an essential part of any company’s data strategy, and should be forward-looking with long-term commitment. In other words, MDM needs to be treated as an investment, which will pay off in the long run and establish a solid foundation for a company’s growth and profitability in the areas of big data, analytics and IoT. Once a  MDM strategy is set, the next step is to implement the master data management within an enterprise. This is a big topic that can be covered in much depth in a book by itself. In this article, I would like to give a very high level introduction and point out 4 steps that are essential for a successful implementation of MDM. Each of the steps will warrant its own topic in the future with more elaborations and detailed examples. Step 1: Establish Data Governance Embraced by the Entire Organization This is the most critical and essential piece of MDM, and also the most difficult one. To enforce MDM requires the commitment of a data governance committee, which normally has the following structure: The main missions of the committee include the following: Below lists some of the key areas that the data governance committee should make decisions for: Step 2: Apply MDM to New Data Additions or New Applications Always keep in mind that the simplest and most efficient way to make MDM successful is to enforce data consistency when the data is created. MDM is a long-term project and requires the long-term commitment of a company. Any attempt to change the data in an  ad hoc  way renders the effort both ineffective and costly. Data governance policies and definitions are implemented throughout 2 channels: 1) via any new projects and application development; 2) by using a data governance software. Many organizations’ MDM implementations stalled because of the high cost and effort they faced when trying   to fix the existing systems and issues; they   did not realize that the best way to start with MDM is to apply it for going forward for   new projects, which will test   it out first and enable the organization to build up expertise and experience. In addition, most of the data governance should be implemented directly as part of data related projects into applications and reports. For example, data governance should enforce and propagate its definitions, policies and principles into the following technical implementations: Step 3: Select the Right MDM Software An ideal MDM software should have the following functionalities: There are many tools on the market that can do 1) and 2), but it is not easy to do 3) with   the same tool. This is the reason why a MDM software can be also a data integration tool at the same time, or  vice versa . Recent rapid progress in artificial intelligence (AI) has made such software more powerful with enhanced data management, which has a bright future in the coming years. Keep in mind that a tool is still a tool. Without the Governance committee and sponsorship from the C-Suite and executives, the software itself cannot play the magic and is not sufficient. In addition, constant communications and reinforcement by information stewards and data stewards in each department also play more essential roles than the software itself. Step 4: Leverage the MDM Capability to Manage Existing and Legacy Systems Many companies may not have the luxury to create new master datasets from scratch, which means they need to revamp the existing database and related applications. Applying MDM to the existing data assets often requires a large amount of effort, which could also fail or abort due to   complexity and cost. To make the MDM effort successful, careful planning is required to establish a road-map with multiple phases. Sometimes, it may be   a better strategy to apply MDM only partially, until the data or system is migrated to the new platform, while focusing on applying MDM to new master data that are being added or new applications and processes that are being built for the enhanced and new data sources to be joined with the master data. MDM has become a necessity for an organization to fully realize its datasets’ revenue and profit potential, but it is not easy to implement. MDM first needs to become a permanent part of the data strategy in order for the company to have a long-term commitment. Next, it requires consistent governance and sponsorship from the top management, as well as persistent efforts from information stewards of IT/CDO departments and data stewards of business departments. The challenge of fixing existing data and system issues should not stop the adoption of MDM for an enterprise. Instead, applying MDM to the new data sources and new applications will lay the foundation to gradually apply it successfully to the existing data and systems."
7 Useful Pandas Tips for Data Management, Useful Pandas Tips for Data Managemen,"The Premier league is big business. In fact, Premier League clubs have paid out more than £260m to football agents during 2018–19 - an increase of £49m on the previous 12 months. This statistic alone piqued my interest and drove me to delve deeper into Premier League spending for the 2018–19 season. To develop a comprehensive Financial Review of the Premier League for the season just finished, I used the Python Pandas module. In doing so, I used many features of the Pandas library which made the data management that little bit easier. This tutorial article includes these useful Pandas tips and features for the data gathering and management I undertook. Money spent on agents and estimated transfer spending were two key variables I wanted to ascertain for each Premier League Club. A quick search led me to the BBC page entitled;  Premier League: Clubs increase spending to £260m on football agents in 2018–19 . Below is the table I want to scrape. This is simple to do using Pandas. The trick here is not using the pd.read_html method( which, for this article could have constituted a tip in itself ), but the indexing. How did I know to index the zeroth [0] element? The answer resides, by switching across to the developers tool in Chrome and using the console tab. Here, I search for the table tag, and the result is an array with a length of 1. This means only one table is present on this particular web page. If I index, the first ( and only ) element I will be able to scrape the Table. However, now we run into a slight bump in the road, so time for Pandas Tip number 2!. When this table is scraped, a multi index is returned as shown. In most instances, it is preferable to have a single index. This will make data sorting and filtering easier. Now, the highest-level column ‘Premier League Spending 2018–19’ needs to be removed. To do this, I use the  .xs() method.  Within this method, the first parameter that I specify is the name of the column I want to get the cross section of. Here I specify ‘Premier League Spending 2018–19’. The  axis=1  gets the cross section of the column and the  drop_level  argument=True, returns the cross section without the multilevel index. Evidently, there is  now  a single-index, but there are still lots of spaces between the column names, which makes certain tasks, like filtering more cumbersome. Ideally, the columns should all be single string elements. The column names are very informative of their contents. I therefore only want to change them marginally, so downstream data manipulation is easier. Here, I use the  df.columns.str.replace  method, and replace all spaces, hyphens and asterisks with an underscore. These characters need to be removed because they can cause syntax errors when we attempt to filter (later on) for example. It is therefore best practice to remove/replace them. As a demonstration of how useful this string function is, I have include the equivalent code, commented out, that would be necessary to achieve the same outcome using the  df.rename  function. As shown, it is a lot simpler. Easy, when you know how! I now, perform some simple string manipulation functions as shown in the GitHub gist below to transform the string entries into numeric datatypes. This Table has some interesting data, but what I really want is a comprehensive view of the Premier League Finances for the 2018–19 season. So, onto Tip number 4. Navigating to the  Premier League news article , led to me information regarding the Premier League value of central payments to clubs during the 2018/19 season. This page included a link to a downloadable pdf (shown below). The next few tasks involve scraping this table and merging it with the spending table extracted from the  BBC Football web page . I imported the  read_pdf  method from tabula, and passed the file path to this method. I initially assigned the DataFrame the variable name df, and checked its type. A Pandas DataFrame is returned. Simple clean-up and renaming followed, but I included this tip as it really demonstrates, how simple it can be to transform a pdf table into a Pandas Dataframe amenable for analysis. The columns in the league_club_revenue DataFrame are a string datatype, and need to be converted to a numeric datatype. However, through careful inspection, all the column values, with the exception of ‘Club_name’ and ‘UK_live_Broadcast’ are prefixed by a ‘£’ sign, and inter-spaced with commas. To change these columns together, I simply create a list of the columns (Pandas series). I then iterate over these columns in a for loop within the DataFrame, and replace the ‘£’ and ‘,’ with empty strings, followed by their conversion using the .astype method. I also divided by 1 Million, because ultimately, I want to merge the ‘spending’ and ‘league_club_revenue’ DataFrames together, therefore I need the data to be consistent between these two DataFrames. I need all columns to express their values in £ millions. Almost one line of code cleans up the entire Dataframe! To merge and successfully align the data, I produce a new column in the league_club_revenue Dataframe. This is necessary as the teams are in a different order in both tables. I create the new column, called ‘Team’ with the teams in the order, they appear in the league_club_revenue DataFrame. I can then merge on this common column shared between the two Dataframes and the rows across each Dataframe will now align. The new DataFrame, Financial_review is now the merged product. The data formatting is consistent, with all column values, with the exception of ‘UK_live_broadcast’ and ‘Current_Position’ expressed as £millions (sterling). The newly merged DataFrame is now amendable for querying. To start with a basic query, I specify 2 conditions. I want to know the Teams who spent more than £8 Million on agent fees, and finished lower than 10th position ( as of 4th of April, when the BBC article on agent and Estimated fees was written ). The results may suggest that the agents for these teams could do a better job, as the teams they scout players for sit in the bottom half of the table! Now suppose, we want to query another condition. This time, however we do not want the entire record (row) returned. To achieve this, simply write the filter condition followed by a period and then the column you want to return. Here, I only want to know the name of the Teams who have been estimated to spend more than £60 million on transfers and received a payment merit of less than £20 million. It looks like Bournemouth, Brighton and Fulham spend big, but do not recoup that investment very well in Merit-based prize money. Its not looking good so far, financially for these teams compared to the rest! For comparison purposes, I have included what the returned result would look like if I did not use not notation followed by the column, ‘Team’. Here, the entire record or row is returned. Finally, I will conclude, by demonstrating how a simple aggregate function can be used. Firstly, the mean (in £ million) for agent spending for all the teams in the Premier league is calculated. Two different ways to achieve this result are shown, the first with square brackets, and the second using dot notation (more commonly used), and hence why illegal characters such as spaces needed to be removed and replaced with underscores earlier. Lets suppose you want to find out the mean UK Broadcast games for the top and bottom half teams of the Premier league separately. To gain further insight, it would be interesting to determine the difference between average Broadcast games for teams in the top half of the table versus the bottom half of the table. Simply use  .loc  then select the first 10 rows using [:10 followed by a comma, and finally the column in quotations followed by a closing square bracket. Repeat this for the second half of the table, [10: , and we can clearly see that teams in the top half have on average 8.49 more UK broadcast games compared with teams that reside in the bottom half. I hope this example has demonstrated some useful Pandas features to make data handling and management that little bit easier. If you liked the Premier League example, I have written an introductory article entitled ‘ Pandas in the Premier League ’ which shows how Pandas can help with initial data clean up."
Everything a Data Scientist Should Know About Data Management*,verything a Data Scientist Should Know About Data Management,"By  Phoebe Wong  &  Robert Bennett To be a real “ full-stack ” data scientist, or what many bloggers and employers call a “ unicorn ,” you’ve to master every step of the data science process — all the way from storing your data, to putting your finished product (typically a predictive model) in production. But the bulk of data science training focuses on machine/deep learning techniques; data management knowledge is often treated as an afterthought. Data science students usually learn modeling skills with processed and cleaned data in text files stored on their laptop, ignoring how the data sausage is made. Students often don’t realize that in industry settings, getting the raw data from various sources to be ready for modeling is usually  80% of the work . And because enterprise projects usually involve a massive amount of data that their local machine is not equipped to handle, the entire modeling process often takes place in the cloud, with most of the applications and databases hosted on servers in data centers elsewhere. Even after the student landed a job as a data scientist, data management often becomes something that a separate data engineering team takes care of. As a result, too many data scientists know too little about data storage and infrastructure, often to the detriment of their ability to make the right decisions at their jobs. The goal of this article is to provide a roadmap of what a data scientist in 2019 should know about data management — from types of databases, where and how data is stored and processed, to the current commercial options — so the aspiring “unicorns” could dive deeper on their own, or at least learn enough to sound like one at interviews and cocktail parties. The story of data science is really the  story of data storage . In the pre-digital age, data was stored in our heads, on clay tablets, or on paper, which made aggregating and analyzing data extremely time-consuming. In 1956, IBM introduced the first commercial computer with a magnetic hard drive,  305 RAMAC . The entire unit required 30 ft x 50 ft of physical space, weighed over a ton, and for $3,200 a month, companies could lease the unit to store up to 5 MB of data. In the 60 years since,  prices per gigabyte in DRAM  has dropped from a whopping $2.64 billion in 1965 to $4.9 in 2017. Besides being magnitudes cheaper, data storage also became much denser/smaller in size. A disk platter in the 305 RAMAC stored a hundred bits per square inch, compared to  over a trillion bits per square inch  in a typical disk platter today. This combination of dramatically reduced cost and size in data storage is what makes today’s big data analytics possible. With ultra-low storage cost, building the data science infrastructure to collect and extract insights from huge amount of data became a profitable approach for businesses. And with the profusion of  IoT devices  that constantly generate and transmit users’ data, businesses are collecting data on an ever increasing number of activities, creating a massive amount of high-volume, high-velocity, and high-variety information assets (or the “ three Vs of big data ”). Most of these activities (e.g. emails, videos, audio, chat messages, social media posts) generate  unstructured data , which accounts for almost 80% of total enterprise data today and is growing twice as fast as structured data in the past decade. This massive data growth dramatically transformed the way data is stored and analyzed, as the traditional tools and approaches were not equipped to handle the “three Vs of big data.” New technologies were developed with the ability to handle the ever increasing volume and variety of data, and at a faster speed and lower cost. These new tools also have profound effects on how data scientists do their job — allowing them to monetize the massive data volume by performing analytics and building new applications that were not possible before. Below are the major big data management innovations that we think every data scientist should know about. Relational Databases & NoSQL Relational Database Management Systems  (RDBMS) emerged in the 1970’s to store data as tables with rows and columns, using Structured Query Language (SQL) statements to query and maintain the database. A relational database is basically a collection of tables, each with a schema that rigidly defines the attributes and types of data that they store, as well as keys that identify specific columns or rows to facilitate access. The RDBMS landscape was once ruled by  Oracle  and  IBM , but today many open source options, like  MySQL ,  SQLite , and  PostgreSQL  are just as popular. Relational databases found a home in the business world due to some very appealing properties.  Data integrity  is absolutely paramount in relational databases. RDBMS satisfy the requirements of  Atomicity, Consistency, Isolation, and Durability (or ACID-compliant)  by imposing a number of constraints to ensure that the stored data is reliable and accurate, making them ideal for tracking and storing things like account numbers, orders, and payments. But these constraints come with costly tradeoffs. Because of the schema and type constraints, RDBMS are terrible at storing unstructured or semi-structured data. The rigid schema also makes RDBMS more expensive to set up, maintain and grow. Setting up a RDBMS requires users to have specific use cases in advance; any changes to the schema are usually difficult and time-consuming. In addition, traditional RDBMS were designed to  run on a single computer node , which means their speed is significantly slower when processing large volumes of data.  Sharding  RDBMS in order to scale horizontally while maintaining ACID compliance is also extremely challenging. All these attributes make traditional RDBMS ill-equipped to handle modern big data. By the mid-2000’s, the existing RDBMS could no longer handle the changing needs and exponential growth of a few very successful online businesses, and many non-relational (or NoSQL) databases were developed as a result (here’s  a story  on how Facebook dealt with the limitations of MySQL when their data volume started to grow). Without any known solutions at the time, these online businesses invented new approaches and tools to handle the massive amount of unstructured data they collected: Google created  GFS ,  MapReduce , and  BigTable ; Amazon created  DynamoDB ; Yahoo created  Hadoop ; Facebook created  Cassandra  and  Hive ; LinkedIn created  Kafka . Some of these businesses open sourced their work; some published research papers detailing their designs, resulting in a proliferation of databases with the new technologies, and NoSQL databases emerged as a major player in the industry. NoSQL databases  are schema agnostic and provide the flexibility needed to store and manipulate large volumes of  unstructured and semi-structured data . Users don’t need to know what types of data will be stored during set-up, and the system can accommodate changes in data types and schema. Designed to distribute data across different nodes, NoSQL databases are generally more horizontally scalable and fault-tolerant. However, these performance benefits also come with a cost — NoSQL databases are not ACID compliant and data consistency is not guaranteed. They instead provide “ eventual consistency ”: when old data is getting overwritten, they’d return results that are a little wrong temporarily. For example,  Google’s search engine index  can’t overwrite its data while people are simultaneously searching a given term, so it doesn’t give us the most up-to-date results when we search, but it gives us the latest, best answer it can. While this setup won’t work in situations where data consistency is absolutely necessary (such as financial transactions); it’s just fine for tasks that require speed rather than pin-point accuracy. There are now several different categories of NoSQL, each serving some specific purposes. Key-Value Stores, such as  Redis ,  DynamoDB , and  Cosmos DB , store only key-value pairs and provide basic functionality for retrieving the value associated with a known key. They work best with a simple database schema and when speed is important. Wide Column Stores, such as  Cassandra ,  Scylla , and  HBase , store data in column families or tables, and are built to manage petabytes of data across a massive, distributed system. Document Stores, such as  MongoDB  and  Couchbase , store data in XML or JSON format, with the document name as key and the contents of the document as value. The documents can contain many different value types, and can be nested, making them particularly well-suited to manage semi-structured data across distributed systems. Graph Databases, such as  Neo4J  and  Amazon Neptune , represent data as a network of related nodes or objects in order to facilitate data visualizations and graph analytics.  Graph databases  are particularly useful for analyzing the relationships between heterogeneous data points, such as in fraud prevention or Facebook’s friends graph. MongoDB is currently the  most popular NoSQL database , and has delivered substantial values for some businesses that have been struggling to handle their unstructured data with the traditional RDBMS approach. Here are  two industry examples : after MetLife spent  years  trying to build a centralized customer database on a RDBMS that could handle all its insurance products, someone at an internal hackathon built one with MongoDB within hours, which went to production in 90 days. YouGov, a market research firm that collects 5 gigabits of data an hour, saved 70 percent of the storage capacity it formerly used by migrating from RDBMS to MongoDB. Data Warehouse, Data Lake, & Data Swamp As data sources continue to grow, performing data analytics with multiple databases became inefficient and costly. One solution called  Data Warehouse  emerged in  the 1980’s , which centralizes an enterprise’s data from all of its databases. Data Warehouse supports the flow of data from operational systems to analytics/decision systems by creating a single repository of data from various sources (both internal and external). In most cases, a Data Warehouse is a relational database that stores processed data that is optimized for gathering business insights. It collects data with predetermined structure and schema coming from transactional systems and business applications, and the data is typically used for operational reporting and analysis. But because data that goes into data warehouses needs to be processed before it gets stored — with today’s massive amount of unstructured data, that could take significant time and resources. In response, businesses started maintaining  Data Lakes  in  the 2010's , which store all of an enterprise’s structured and unstructured data at any scale. Data Lakes store raw data, and could be set up without having to first define the data structure and schema. Data Lakes allow users to run analytics without having to move the data to a separate analytics system, enabling businesses to gain insights from new sources of data that was not available for analysis before, for instance by building machine learning models using data from log files, click-streams, social media, and IoT devices. By making all of the enterprise data readily available for analysis, data scientists could answer a new set of business questions, or tackle old questions with new data. A common challenge with the Data Lake architecture is that without the appropriate data quality and governance framework in place, when terabytes of structured and unstructured data flow into the Data Lakes, it often becomes extremely difficult to sort through their content. The Data Lakes could turn into  Data Swamps  as the stored data become too messy to be usable. Many organizations are now calling for more data governance and metadata management practices to prevent Data Swamps from forming. Distributed & Parallel Processing: Hadoop, Spark, & MPP While storage and computing needs grew by leaps and bounds in the last few decades, traditional hardware has not advanced enough to keep up. Enterprise data no longer fits neatly in standard storage, and the computation power required to handle most big data analytics tasks might take weeks, months, or simply not possible to complete on a standard computer. To overcome this deficiency, many new technologies have evolved to include multiple computers working together, distributing the database to thousands of commodity servers. When a network of computers are connected and work together to accomplish the same task, the computers form a  cluster . A cluster can be thought of as a single computer, but can dramatically improve the performance, availability, and scalability over a single, more powerful machine, and at a lower cost by using commodity hardware.  Apache Hadoop  is an example of distributed data infrastructures that leverage clusters to store and process massive amounts of data, and what enables the Data Lake architecture. When you think Hadoop, think “distribution.” Hadoop consists of  three main components : Hadoop Distributed File System (HDFS), a way to store and keep track of your data across multiple (distributed) physical hard drives; MapReduce, a framework for processing data across distributed processors; and Yet Another Resource Negotiator (YARN), a cluster management framework that orchestrates the distribution of things such as CPU usage, memory, and network bandwidth allocation across distributed computers. Hadoop’s processing layer is an especially notable innovation:  MapReduce  is a two step computational approach for processing large (multi-terabyte or greater) data sets distributed across large clusters of commodity hardware in a reliable, fault-tolerant way. The first step is to distribute your data across multiple computers (Map), with each performing a computation on its slice of the data in parallel. The next step is to combine those results in a pair-wise manner (Reduce). Google  published a paper  on MapReduce in 2004, which got  picked up by Yahoo programmers  who implemented it in the open source Apache environment in 2006, providing every business the capability to store an unprecedented volume of data using commodity hardware. Even though there are many open source implementations of the idea, the Google brand name MapReduce has stuck around, kind of like Jacuzzi or Kleenex. Hadoop is built for iterative computations, scanning massive amounts of data in a single operation from disk, distributing the processing across multiple nodes, and storing the results back on disk. Querying  zettabytes of indexed data  that would take 4 hours to run in a traditional data warehouse environment could be completed in 10–12 seconds with Hadoop and  HBase . Hadoop is typically used to generate complex analytics models or high volume data storage applications such as retrospective and predictive analytics; machine learning and pattern matching; customer segmentation and churn analysis; and active archives. But MapReduce  processes data in batches  and is therefore not suitable for processing real-time data.  Apache Spark  was built in 2012 to fill that gap. Spark is a parallel data processing tool that is optimized for speed and efficiency by  processing data in-memory . It operates under the same MapReduce principle, but runs much faster by completing most of the computation in memory and only writing to disk when memory is full or the computation is complete. This in-memory computation allows Spark to “run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.” However, when the data set is so large that  insufficient RAM becomes an issue  (usually hundreds of gigabytes or more), Hadoop MapReduce might outperform Spark. Spark also has an extensive set of data analytics libraries covering a wide range of functions:  Spark SQL  for SQL and structured data;  MLib  for machine learning,  Spark Streaming  for stream processing, and  GraphX  for graph analytics. Since Spark’s focus is on computation, it does not come with its own storage system and instead runs on a variety of storage systems such as  Amazon S3 ,  Azure Storage , and  Hadoop’s HDFS . Hadoop and Spark are not the only technologies that leverage clusters to process large volumes of data. Another popular computational approach to distributed query processing is called  Massively Parallel Processing (MPP) . Similar to MapReduce, MPP distributes data processing across multiple nodes, and the nodes process the data in parallel for faster speed. But unlike Hadoop, MPP is used in RDBMS and utilizes a  “share-nothing” architecture  — each node processes its own slice of the data with multi-core processors, making them many times faster than traditional RDBMS. Some MPP databases, like  Pivotal Greenplum , have  mature machine learning libraries  that allow for in-database analytics. However, as with traditional RDBMS, most MPP databases do not support unstructured data, and even structured data will require some processing to fit the MPP infrastructure; therefore it takes additional time and resources to set up the data pipeline for an MPP database. Since MPP databases are ACID-compliant and deliver much faster speed than traditional RDBMS, they are usually employed in high-end enterprise data warehousing solutions such as  Amazon Redshift, Pivotal Greenplum, and Snowflake . As an industry example, the  New York Stock Exchange  receives four to five terabytes of data daily and conducts complex analytics, market surveillance, capacity planning and monitoring. The company had been using a traditional database that couldn’t handle the workload, which took hours to load and had poor query speed. Moving to an MPP database reduced their daily analysis run time by eight hours. Cloud Services Another innovation that completely transformed enterprise big data analytics capabilities is  the rise of cloud services . In the bad old days before cloud services were available, businesses had to buy on-premises data storage and analytics solutions from software and hardware vendors, usually paying upfront perpetual software license fees and annual hardware maintenance and service fees. On top of those are the costs of power, cooling, security, disaster protection, IT staff, etc, for building and maintaining the on-premises infrastructure. Even when it was technically possible to store and process big data, most businesses found it cost prohibitive to do so at scale. Scaling with on-premises infrastructure also require an extensive design and procurement process, which takes a long time to implement and requires substantial upfront capital. Many potentially valuable data collection and analytics possibilities were ignored as a result. The on-premises model began to lose market share quickly when cloud services were introduced in the late 2000’s — the global cloud services market has been growing 15% annually in the past decade. Cloud service platforms provide subscriptions to  a variety of services  (from virtual computing to storage infrastructure to databases), delivered over the internet on a pay-as-you-go basis, offering customers rapid access to flexible and low-cost storage and virtual computing resources. Cloud service providers are responsible for all of their hardware and software purchases and maintenance, and usually have a vast network of servers and support staff to provide reliable services. Many businesses discovered that they could significantly reduce costs and improve operational efficiencies with cloud services, and are able to develop and productionize their products more quickly with the out-of-the-box cloud resources and their built-in scalability. By removing the upfront costs and time commitment to build on-premises infrastructure, cloud services also lower the barriers to adopt big data tools, and effectively democratized big data analytics for small and med-size businesses. There are several cloud services models, with public clouds being the most common. In a  public cloud , all hardware, software, and other supporting infrastructure are owned and managed by the cloud service provider. Customers share the cloud infrastructure with other “cloud tenants” and access their services through a web browser. A  private cloud  is often used by organizations with special security needs such as government agencies and financial institutions. In a private cloud, the services and infrastructure are dedicated solely to one organization and are maintained on a private network. The private cloud can be on-premises, or hosted by a third-party service provider elsewhere.  Hybrid clouds  combine private clouds with public clouds, allowing organizations to reap the advantages of both. In a hybrid cloud, data and applications can move between private and public clouds for greater flexibility: e.g. the public cloud could be used for high-volume, lower-security data, and the private cloud for sensitive, business-critical data like financial reporting. The  multi-cloud model  involves multiple cloud platforms, each delivers a specific application service. A multi-cloud can be a combination of public, private, and hybrid clouds to achieve the organization’s goals. Organizations often choose multi-cloud to suit their particular business, locations, and timing needs, and to avoid vendor lock-in. Building out a viable data science product involves much more than just building a machine learning model with scikit-learn, pickling it, and loading it on a server. It requires an understanding of how all the parts of the enterprise’s ecosystem work together, starting with where/how the data flows into the data team, the environment where the data is processed/transformed, the enterprise’s conventions for visualizing/presenting data, and how the model output will be converted as input for some other enterprise applications. The main goals involve building a process that will be easy to maintain; where models can be iterated on and the performance is reproducible; and the model’s output can be easily understood and visualized for other stakeholders so that they may make better informed business decisions. Achieving those goals require selecting the right tools, as well as an understanding of what others in the industry are doing and the best practices. Let’s illustrate with a scenario: suppose you just got hired as the lead data scientist for a vacation recommendation app startup that is expected to collect hundreds of gigabytes of both structured (customer profiles, temperatures, prices, and transaction records) and unstructured (customers’ posts/comments and image files) data from users daily. Your predictive models would need to be retrained with new data weekly and make recommendations instantaneously on demand. Since you expect your app to be a huge hit, your data collection, storage, and analytics capacity would have to be extremely scalable. How would you design your data science process and productionize your models? What are the tools that you’d need to get the job done? Since this is a startup and you are the lead — and perhaps the only — data scientist, it’s on you to make these decisions. First, you’d have to figure out how to set up the data pipeline that takes in the raw data from data sources, processes the data, and feeds the processed data to databases. The ideal data pipeline has low event latency (ability to query data as soon as it’s been collected); scalability (able to handle massive amount of data as your product scales); interactive querying (support both batch queries and smaller interactive queries that allow data scientists to explore the tables and schemas); versioning (ability to make changes to the pipeline without bringing down the pipeline and losing data); monitoring (the pipeline should generate alerts when data stops coming in); and testing (ability to test the pipeline without interruptions). Perhaps most importantly, it had better not interfere with daily business operations — e.g. heads will roll if the new model you’re testing causes your operational database to grind to a halt. Building and maintaining the data pipeline is usually the responsibility of a data engineer (for more details,  this article  has an excellent overview on building the data pipeline for startups), but a data scientist should at least be familiar with the process, its limitations, and the tools needed to access the processed data for analysis. Next, you’d have to decide if you want to set up on-premises infrastructure or use cloud services. For a startup, the top priority is to scale data collection without scaling operational resources. As mentioned earlier, on-premises infrastructure requires huge upfront and maintenance costs, so cloud services tend to be a better option for startups. Cloud services allow scaling to match demand and require minimal maintenance efforts, so that your small team of staff could focus on the product and analytics instead of infrastructure management. In order to choose a cloud service provider, you’d have to first establish the data that you’d need for analytics, and the databases and analytics infrastructure most suitable for those data types. Since there’d be both structured and unstructured data in your analytics pipeline, you might want to set up both a Data Warehouse and a Data Lake. An important thing to consider for data scientists is whether the storage layer supports the big data tools that are needed to build the models, and if the database provides effective in-database analytics. For example, some ML libraries such as Spark’s MLlib cannot be used effectively with databases as the main interface for data — the data would have to be unloaded from the database before it can be operated on, which could be extremely time-consuming as data volume grows and might become a bottleneck when you’ve to retrain your models regularly (thus causing another “heads-rolling” situation). For data science in the cloud, most cloud providers are working hard to develop their native machine learning capabilities that allow data scientists to build and deploy machine learning models easily with data stored in their own platform (Amazon has  SageMaker , Google has  BigQuery ML , Microsoft has  Azure Machine Learning ). But the toolsets are still developing and often incomplete: for example,  BigQuery ML  currently only support linear regression, binary and multiclass logistic regression, K-means clustering, and TensorFlow model importing. If you decide to use these tools, you’d have to test their capabilities thoroughly to make sure they do what you need them to do. Another major thing to consider when choosing a cloud provider is vendor-lock in. If you choose a proprietary cloud database solution, you most likely won’t be able to access the software or the data in your local environment, and switching vendor would require migrating to a different database, which could be costly. One way to address this problem is to choose vendors that support  open source  technologies ( here’s Netflix explaining why they use open source software ). Another advantage of using open source technologies is that they tend to attract a larger community of users, meaning it’d be easier for you to hire someone who has the experience and skills to work within your infrastructure. Another way to address the problem is to choose third-party vendors (such as  Pivotal Greenplum  and  Snowflake ) that provide cloud database solutions using other major cloud providers as storage backend, which also allows you to store your data in multiple clouds if that fits your startup’s needs. Finally, since you expect the company to grow, you’d have to put in place a robust cloud management practice to secure your cloud and prevent  data loss and leakages  — such as managing data access and securing interfaces and APIs. You’d also want to implement  data governance best practices  to maintain data quality and ensure your Data Lake won’t turn into a Data Swamp. As you can see, there’s so much more in an enterprise data science project than tuning the hyperparameters in your machine learning models! We hope this high-level overview has gotten you excited to learn more about data management, and maybe pick up a few things to impress the data engineers at the water cooler. Disclaimer: The views expressed in this article are our own and do not represent the views of our past or current employers."
How Knowledge Graphs Will Transform Data Management And Business,ow Knowledge Graphs Will Transform Data Management And Busines,"In late November the U.S. Federal Drug Administration approved Benevolent AI’s recommended arthritis drug  Baricitnib  as a COVID-19 treatment, just nine-months after the hypothesis was developed. The correlation between the properties of this existing Eli Lilly drug and a potential treatment for seriously ill COVID-19 patients, was made with the help of knowledge graphs, which represent data in context, in a manner that humans and machines can readily understand. Knowledge graphs   apply semantics to give context and relationships to data, providing a framework for data integration, unification, analytics and sharing. Think of them as a flexible means of discovering facts and relationships between people, processes, applications and data, in ways that give companies new insights into their businesses, create new services and improve R&D research. Benevolent AI , a six-year-old London-based company which has developed a platform of computational and experimental technologies and processes that can draw on vast quantities of biomedical data to advance drug development, built-in the use of knowledge graphs from day one. “In the human genome there are about 20,000 genes with a much smaller number of approved drugs. The number of entities out there isn’t very large but the number of connections between them ranks in the hundreds of millions and that is only scratching the surface of what’s known,” says Olly Oechsle, a senior software engineer at Benevolent AI. “Having a graph system that is able to help you navigate between all of those connections is vital.” Oechsle was one of seven panelists who participated in a December 7 roundtable discussion on the power of graphs organized by  DataSeries,  a global network of data leaders led by venture capital firm  OpenOcean  and moderated by The Innovator Editor-in-Chief Jennifer L. Schenker. Until recently knowledge graphs were mainly leveraged by young companies like Benevolent AI or tech companies like Google, Facebook, LinkedIn and Amazon. But large corporates in traditional sectors such as chemicals and finance are starting to discover the power of graphs and, as their number grows, it is expected to cause a paradigm shift in data management. What’s more, as graphs progress they could potentially lead to more robust and reliable AI systems. Knowledge graphs are a game changer that help companies move away from relational databases and leverage the power of natural language processing, semantic understanding and machine learning to better leverage their data, says panelist Michael Atkin, a principal at  agnos.ai , a specialist consultancy that designs and implements enterprise knowledge graphs. The advantages to business are clear. Graphs “are a prerequisite for achieving smart, semantic AI-powered applications that can help you discover facts from your content, data and organizational knowledge which would otherwise go unnoticed,” says Atkin. They help corporates organize the information from disparate data sources to facilitate intelligent search. They make data understandable in business terms rather than in symbols only understood by a handful of specialized personnel. And they speed digital transformation by delivering a “digital twin” of a company that encompasses all data points as well as the relationships between data elements. “By fundamentally understanding the way all data relates throughout the organization, graphs offer an added dimension of context which informs everything from initial data discovery to flexible analytics,” he says. “Graphs give corporates the ability to ask business questions and get business answers and value. These developments promise to enhance productivity and usher in a new era of business opportunity.” Breaking Data Silos Today, the infrastructure for managing data in most major corporations is based on decades old technology. Line of business and functional silos are everywhere. They are exacerbated by relational database management systems based on physical data elements that are stored as columns in tables and constrained by the lack of data standards. “Data meaning is tied to proprietary data models and managed independently,” explains Atkin. These data silos, when combined with external models for glossaries, entity relationship diagrams, databases and metadata repositories, lead to incongruent data and, due to the explosion of uniquely labeled elements, it is nearly impossible to align these silos. As a result, corporates end up with “data that is hard to access, blend, analyze and use, impeding application development, data science, analytics, process automation, reporting and compliance,” he says. Advantages of Semantic Technology Semantic technology — which uses formal semantics to help AI systems understand language and process information the way humans do, is seen as the best way to handle large volumes of data in multiple forms. It was designed specifically for interconnected data and is good at unraveling complex relationships. Semantic processing (which is now a World Wide Web Consortium standard) was a huge breakthrough for content management, says Atkin. And because it is an open standard it has propelled lots of companies into the world of knowledge management. It is the backbone of the Semantic Web. It is the infrastructure for bio-medical engineering in areas such as cancer research and for the human genome project. And, it is the basis for what Google and other tech companies are doing with knowledge graphs and graph neural networks (GNNs), a type of neural network which learns directly from the graph structure, helping make recommendations for applications like search, e-commerce and computer vision. A shift away from conventional relational databases to knowledge graphs gives corporates the opportunity to reap some of the same advantages: capturing the meaning of data as well as how concepts are connected. Semantic modeling eliminates the problem of hard-coded assumptions because it focuses on concepts, not specific data formats. Users automatically understand what the data represents even when it moves across organizational boundaries, allowing efficient reuse across systems and processes. “Instead of data silos we get data that is integrated and linked, and organizations become more efficient because ontologies are standardized and reusable,” says Atkin. This allows for economies of scale and, he says, “moving from data issues to data use cases.” How Businesses Are Using Knowledge Graphs Some corporates, like German multinational chemical company BASF, the largest chemical producer in the world, are already reaping the benefits. BASF is applying knowledge graphs to help the company digitalize existing knowledge and help with future research and development, says panelist Juergen Mueller, BASF’s Head of Knowledge Architecture. A large-scale knowledge graph powers the company’s internal knowledge system for R&D. It combines ontologies and natural language processing applied to more than 200 million scientific, technical documents. In addition, his team is applying knowledge graphs to aggregate data and information for project specific applications in R&D and business. “We do our own research and development to bring together explicit semantic expression with graph algorithms and graph neural networks,” says Mueller. “The goal is to create a seamless interaction between humans and machines in building and using knowledge in digital form to answer research and business questions.” One core field of focus is to figure out how to deal with ambiguity and uncertainty beyond just representing factual knowledge. “We want to represent the process of elaboration of early ideas and hypotheses to falsification or verification over time,” he says. “This would enable us to answer questions like ‘what was known at that point?’, ‘why did we come to that conclusion?’, or ‘what decisions do we need to review given new information?’ Knowledge graphs allow us to deal with high complexity that is not doable in other ways,” he says.” We need technology that’s really capable to scale into billions or even trillions of relationships.“ Graphs are already playing an important role in drug discovery and promise to do even more. As graphs progress, Benevolent AI’s Oechsle says he is looking forward to the time when all companies will be able to take their data and securely combine it with the knowledge of the world, in order to be able to answer things that are beyond the company’s internal knowledge. The world’s biggest banks are all experimenting with knowledge graphs, in part because they were mandated to fix their data problems after the 2008 financial crisis, says Atkin. There are two main objectives. One involves control functions such as inventory management, employee connection, data processes, governance, quality requirements and organizational functions to ensure operational resilience, he says. The other is focused on business value proposition objectives such as aggregating things in flexible ways so that banks can look at concepts from various perspectives. This includes product developments, consumer behavior and how they can service their customers in a better way. “The whole point is to be able to customize and personalize your data,” says Atkin. “The underlying drivers of the moment are the infrastructure control processes but the value proposition is personalization. Being able to make that connection between the two is hard, because you need an investment to drive this further.” That said, “competition is shifting from data being a competitive advantage to companies being at a competitive disadvantage if they don’t use knowledge graphs,” says Atkin. The mentality of ‘are we being left behind?’ has been injected in many corporations and that’s great to see.” It is hard to predict when — or if — knowledge graphs will become the core of most large corporates data management strategies, says panelist Francois Scharffe, co-founder of the  Knowledge Graph Conference , a global conference on knowledge graphs. “But we are already seeing a lot of interest in this space and an increasing excitement, which means that it is definitely closer to short/mid-term rather than a long-term timeline.” Making The Leap From Columns To Context Today making the leap from columns to context is achievable but not automatic. Most of the enterprise data is stored in a tables-columns format and it is accessed using the highly common but formal SQL language, while knowledge graphs’ data is stored in a a format with more emphasis on connections, that is slightly less strict. This means that in order to benefit from knowledge graphs, organizations must invest in new infrastructure, data must be transformed and new skills must be learned, says panelist Amit Weitzner, one of three brothers who co-founded  Timbr.ai , an Israeli startup that is helping governments and large corporates enhance their existing data management technologies with knowledge graph capabilities. (The company’s name, is short for Tim Berners Lee who, together with timbr.ai’s advisor Jim Hendler, pioneered the semantic Web). To make it easier, Timbr implemented the semantic Web standards developed originally for the Internet, in SQL, to make it possible for enterprises to modernize and enhance existing data technologies with knowledge graph capabilities. Timbr’s knowledge graph acts as a virtual layer that maps data from companies’ existing relational databases. Knowledge graphs are a key part of a larger shift towards a data-centric enterprise, so it must also involve rethinking the software architecture and making it more data-driven and declarative, says panelist Martynas Jusevičius, co-founder of  AtomGraph , a company which tries to solve that issue with knowledge graph management solutions and open-source software for publishing and managing connected data. While various solutions exist for placing virtual knowledge graphs on top of legacy data management systems, what is missing are user-friendly tools that help non-techies within the business leverage the knowledge graphs, says Jusevičius. “Software design needs to be rethought. If you use knowledge graphs, then your software has to be designed in such a way so that you can truly use all the potential functions that a graph can give you.” Technical And Organizational Challenges While a host of young companies are working on solutions to make it easier for corporates to implement knowledge graphs “we are not at a level that relationship databases have reached where you can confidently place bets with regards to technology choices,” says BASF’s Mueller. Another barrier is that knowledge graphs are still expensive to implement as there is a need for both data engineers and knowledge engineers that bring domain and ontology modeling expertise, says Scharffe. Figuring out how to get domain experts to express their knowledge in a digital format is critical. “Making implicit expert knowledge explicit in a systematic way for use by humans and machines is essential to drive future innovation” says BASF’s Mueller. “Just writing down findings in documents to be read by human experts is not sufficient.” Very costly natural language processing (NLP) techniques are required to reconstruct structure from such unstructured data into something machine processable or even searchable at scale. “Expressing knowledge through the construction of comprehensive ontologies has a steep learning curve for domain experts not familiar with semantic Web technologies,’ he says. “Therefore, we need new approaches and tools to make the capture and leverage of knowledge in digital form as easy, efficient, and fast as possible.” Perhaps the greatest challenge is a necessary mind-set shift by top executives at traditional companies. “The right leadership is needed to facilitate innovation and give strategic support for the required changes that accompany knowledge graphs,” says Atkin. It is important, also, for corporates to set expectations. “People want short term wins but don’t understand that it takes time to get to the true wins,” says Timbr.ai’s Weitzner. “If there is no fast return on investment, people tend to give up. If you can shorten this time and build a knowledge graph that will give you value in a certain function, then you will be able to reap value much faster and also foster a shift in mindset.” He recommends starting with one use case and setting clear KPIs. “The beauty of knowledge graphs is you can implement new use cases and onboard other data sources too,” he says. “Start with one pain point and solve a problem, even if it’s minor.” Leveraging The Marriage Of Graphs And Machine Learning Once knowledge graphs become widely used, they will not only radically change the way data is managed they promise to also impact machine learning and AI training, helping to guide and speed up the learning process for machines, says Scharffe. “A ML training process is reviving every possible data experience in order to generalize. If there is a graph in place that acts as a knowledgeable instructor then you will be able to create general rules,” he says. “We are not there yet but knowledge graphs are introducing this, so using a knowledge graph as a guide and instructor for a ML learning process is what the future might look like.” While corporations grapple with implementing graphs, tech companies are already embedding even more sophisticated functionality in the form of GNNs, a type of neural network which learns directly from the graph structure. BenevolentAI uses GNNs. “ It is our bread-and-butter for finding new ways in which diseases are founded,” says Oechsle.  DeepMind , the UK-based AI company owned by Alphabet, and other cutting-edge companies working on some of the most important questions in science, are also using GNNs to solve complex issues. DeepMind recently announced it can predict the structure of proteins, a breakthrough that could dramatically speed up the discovery of new drugs. Some of the models incorporated in solving protein folding — a problem that stumped scientists for 50 years — were solved by directly reasoning about the spatial graph of the folded protein, according to the DeepMind blogpost. GNNs are also playing an important role in drug discovery. “Based on a recent highly influential study from MIT you can ask whether a molecule inhibits a particular strain of [an infection] and predict yes or no and then once you have a model trained like this you can apply it to new molecules you have never seen before and still predict with a high probability the top 100 that will have the highest probability of working and send them to chemists for further study,” says Petar Velickovic, a senior research scientist at DeepMind. Velickovic directly worked on deploying GNNs to help Google Maps to more accurately predict travel times. Meanwhile, some of the world’s large e-commerce companies are using them to suggest diversified complimentary purchases. Social networks use them to create friend graphs while Pinterest, an image sharing and social media service, is using them to suggest additional relevant content to its users, says panelist Will Hamilton, who aided Pinterest with developing its GNN technology. For a long time there has been a divide between ML & AI techniques under development, explains Hamilton, an assistant professor of computer technology at McGill Univeristy and chair of CIFAR, a Canadian-based global research organization.“On one side we had these continuous signal processing things like convolutional neural networks, techniques, and mathematics coming from a perspective of how can we transform this signal and do continuous pattern recognition and how can we learn deep representation. On the other side, we have long history of logic AI with expert systems using the idea of symbolic representation.” Knowledge Graphs are the convergence of these two sides. “We’re actually starting to build GNNs which have deep mathematical relationships to signal processing and convolutions,” says Hamilton. “They are being trained and optimized and used to actually simulate logical observations — to do things like query and logical problem solving on top of the knowledge graph structure. This is very exciting as this the first time that we know of, where we had this successful merger of these two different streams of thoughts on a deep theoretical level. “ This theoretical bridge has the potential to really impact real world applications by making modern AI systems more reliable and robust, says Hamilton. For example, it is critical that interactive dialogue systems do not state false facts or use harmful language, he says. It is difficult to ensure this kind of robustness is purely statistical deep learning models, since they are inherently unpredictable. Merging the power of deep learning techniques with the robustness and systematicity of logical rule-based methods offers one major avenue to address these concerns. “If we can bridge this divide, then we can stop having to trade-off between high performing, not very robust deep learning techniques, versus very stable, but brittle, easy to predict rule-based systems,” says Hamilton. “The real opportunity is combining these two worlds and harvesting the best of both.” This content piece has been generated out of a partnership between DataSeries and The Innovator.  Jennifer Schenker , Editor-in-Chief at ‘ The Innovator ’ has moderated the ‘Power of Graphs’ roundtable and produced this summary."
"Data Management, Quality and Governance","ata Management, Quality and Governanc","Data quality is a journey, it doesn’t come in one day, and the focus should be more about improving data quality than having it right on day one. Having a data governance model, implementing testing for data quality are all things that help on this journey."
Building A scalable data management system for computer vision tasks,uilding A scalable data management system for computer vision task,"By Shirley Du |Engineer, Discovery On Pinterest, computer vision plays a central role in enabling search and discovery from every Pin and every image within a Pin. We train visual models to not just identify objects, but also predict attributes, related ideas, and exact product matches. As people conduct hundreds of millions of visual searches on Pinterest every month, these models are increasingly becoming more powerful with results surfaced through technologies like  Lens camera search ,  automated Shop the Look , and  visual search  across the platform. Successful training of machine learning models, especially deep learning models, typically relies on collecting high quality training data at scale. Furthermore, in production applications, many auxiliary tasks often require more engineering work than the model design itself: data collection, label cleaning, quality control, crowdsourcing, versioning, dataset storage, taxonomy management, etc. As a solution to help with such tasks, we designed a centralized, scalable, and flexible data management tool called Voyager. Voyager serves as a centralized hub for storing computer vision training data at Pinterest. The training and evaluation of deep learning models require datasets with various labels, formats and sizes, and Voyager allows us to create such datasets easily from scratch. This feature is particularly useful in exploratory applications because we only need a small dataset to start experimenting, and it’s most convenient when we can just bootstrap a dataset on our own. Imagine we wanted to bootstrap a dataset with 100 images of high heels. In Voyager, there are several ways to quickly define a taxonomy and add images to do this: Even though 100 images might not seem like a lot, when fine-tuning existing models, this may be enough to bootstrap a new category. What makes Voyager even more powerful is that it also supports dynamic taxonomy management, which mimics that of a file system. Users can manage their data similar to how they manage files and folders on laptops. This feature is especially useful in deep learning applications in terms of data collection. For example, in order to train our  Lens  model, we need training images that resemble the quality distribution of user input images, where these images usually have properties like low resolution, low light conditions, and are sometimes taken at unconventional angles. Hence, the convenience of having a file-system-like tool makes collecting such images that satisfy the above requirements fast and easy. In addition, to further improve the user experience and diversity of visual search results, we also focus on attributes of images, such as color, material, gender, and other useful information associated with the image. To collect this kind of data, Voyager allows our internal product specialists to label multiple attributes in real time. Voyager is not just a web tool. It is also a system that handles data collection, cleaning, visualization, and deep vision model training. This system is supported by an underlying unified data labeling schema. An important lesson we learned from developing our data management toolchain is that we needed to define a unified schema optimized for all computer vision tasks at Pinterest. Without a unified schema, we frequently ran into issues like difficult data visualization, incompatible dataset formats, brittle type and schema inference in vision dataset consumers, along with a host of other concerns. Thus, our schema satisfies the following requirements: The advantage of this schema is that on the data labeling side (web tool), we can easily render image-level and region-level information and write back to the database for modifications. On the deep learning model training side (training framework), the trainers can quickly deserialize and extract image metadata at once. By storing image raw bytes in this unified data structure, we also ensure the intactness of all images, which further guarantees the reliability of performance and improve the feasibility of reproducing and retraining the model. When things go at scale, modularizing the data management system is becoming increasingly important. Having separation of responsibilities is also becoming a crucial mission for our system. Therefore, having a consistent, unified, and backward-compatible schema is an important first step towards connecting modules together and achieving those goals. With a reliable unified schema, we are now ready to perform data cleaning in Voyager. However, we have continuously evolving datasets due to hundreds of product specialists cleaning our data at the same time. In industrial applications where massive data is being produced everyday and fed into machine learning models, it is even more common to have continuous management of the datasets. The reason is that the maintenance and iterations on models require easy access to existing data entries so that historical performance and analysis could be conveniently stored and tracked, and that reproducing model performance can be therefore reliable. Reproducibility is crucial to industrial model development. Without it, we risk claiming gains from making modifications to the deep learning models without realizing that the real source of improvement was random. To solve this problem, we implemented an automatic versioning system for keeping track of data. To achieve automatic versioning, we developed pipelines running under Voyager that publishes datasets in real time. These pipelines pick up the latest snapshot of our datasets and dumps them into the database. This way, it ensures reusability within the organization and reproducibility of model performances within the team. Crowdsourcing is the most efficient way to collect data at scale. However, ensuring data integrity and consistency is nontrivial because scene images vary and different crowdsourcing labelers and QA person might have different definitions of what an object is and standards of how to draw a bounding box for an object. Thus, ensuring data quality can be a headache. Take collecting bounding boxes for detection tasks as an example: in order to train our detection model in  automated Shop the Look  that gives Pinners the best experience to explore and shop objects contained in the scenes, we need training images that have well-drawn bounding boxes around all prominent objects following a consistent but sophisticated labeling guide. We ask the labelers to label one category at a time  (e.g., label all the chairs in the scene)  to ensure that bounding boxes are drawn at speed following our training guide. Then we ask labelers to verify each box is drawn correctly  (e.g., boxes are tight enough) . After boxes are verified, we send them to ask labelers to verify if each image is labeled at full recall  (e.g., all the chairs in the scene are correctly labeled) . We iterate this process until we get the quality that meet our standard. This process is generalized as a “targeted” data collection pipeline since we are collecting one category at a time. This pipeline is extremely quick and scalable for collecting detection data, and it is being adopted widely at Pinterest. After we gather targeted data from this pipeline, we ingest these results into our visualization & cleaning tool to perform internal QA. This workflow will, of course, also perform merging when there’s a conflict with our existing image dataset and dedup images from our test dataset. If the quality doesn’t pass our standard, we will ask the labelers to redo the task and repeat the process; otherwise, an export workflow will be kicked off to dump the cleaned data to our source of truth location. We believe that having a reliable, scalable, and maintainable data management system is crucial to deep learning model training as the vision tasks get more and more complicated. Our data management system significantly simplifies common deep learning tasks by reducing our turnaround time for the process of data collection, model training, debug, and more data collection. Looking ahead, we look forward to an even more powerful system that can allow us to do instant training — an instant feedback loop to debug into models. Please stay tuned! Acknowledgements: Our data management tool is a collaborative effort at Pinterest. Special thanks to Dmitry Kislyuk, Michael Feng, Andrew Zhai, Jeff Harris, Chuck Rosenberg for the collaboration on this project."
"How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions","ow LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solution",I recently started a new newsletter focus on AI education. TheSequence is a no-BS(…
Understanding decentralized data management in Web3,nderstanding decentralized data management in Web,"Web3  is a concept that is used to describe the evolution of the internet. It is the promise of an interconnected serverless internet, i.e. the decentralized web.  An internet where users are in control of their own data, identity and destiny. This goal for the internet to progress to is an advanced form of value to be built on distributed ledger technology, beyond the initial proven use cases of digital scarce value with Bitcoin, and then programmable money with Ethereum. The Web3 internet is built on fundamentally different infrastructure, utilizing blockchains and distributed peer-to-peer networks, compared to the current existing internet which rely on server-client relationships. To adapt to this new infrastructure,  new methods of handling and interacting with data in a decentralized context need to be developed  to account for the open, transparent and distributed nature in which Web3 applications operate. Completely new applications will be enabled by this technology, by building with structures that eliminate reliance on corrupt/censoring central authorities. They will leverage immutability and open economic incentive systems to generate highly accurate and verified information without trusting other parties. The following projects have built tools that serve as useful examples of how data can be managed in Web3. FOAM Protocol is an open protocol for decentralized, geospatial data markets, that has the goal to build a consensus-driven map of the world that can be trusted for every application. FOAM Protocol is a unique example of an application that can only exist effectively using the decentralized data management principles and incentivisation mechanisms that Web3 technology can provide. FOAM uses tokens to create economic incentives for the validation of geospatial data, to build a universal consensus-driven map and standard for location data and its exchange. It is what FOAM describes as ‘programmable space’. By creating economic incentives for an open-source location standard, a much more accurate and universal standard for location data and exchange should be able to develop on FOAM to solve problems of location data monetization, interoperability of location data and verification of location data. Supporting technologies and use cases such as IOT networks, supply chain, logistics and enhanced location intelligence (e.g. mobility AI). The core principles behind FOAM rely on the immutability and transparency of the Ethereum blockchain where values are represented in FOAM smart contracts, allowing participants to actively verify data. With base fundamentals of decentralized technology, FOAM is then able to build on top of this a verification layer built with a  token curated registry(TCR) ,  where users can stake reputation and tokens to vouch for the location data accuracy, and be rewarded for providing and verifying the correct data. This provably verified data can then either be leveraged into applications, such as in open tools like  FOAM’s visualiser  or available for exchange in data marketplaces. FOAM is a prime example of how data can be managed, verified and improved for a specific purpose and ultimately with Web3 we will see more applications arise that leverage the stronger verification and wisdom able to be sourced from economically incentivised crowds with open and transparent standards. DIRT Protocol provides another example of how data could be managed in a decentralized Web3 context by utilizing  Token Curated Registeries (TCRs) . DIRT Protocol is a standard for people to create openly editable, Community Moderated Datastores on Ethereum. The idea behind DIRT is that it creates a framework for data to be community owned and managed without any central authorities, and provide a means for the data to be accurate and readily available for any service or application that requires it. Essentially this functions in the same way as FOAM’s verification layer where any user can contribute to the shared database, but must place tokens at stake as an economic incentive to provide quality data and receive rewards for verifying the accuracy of the data. This framework enables an entirely new concept of ‘Trust as a service’ as described by DIRT, removing the likelihood of single party manipulation and improving accuracy through crowd-sourced participation. The first iteration of an application based on this standard is  OpenMarketCap  a token curated alternative to CoinMarketCap to create a more trustworthy cryptocurrency trade data source in an industry that suffers inaccurate and at times very dishonest data. ChainLink is a decentralized data management solution that focuses on data external to blockchain systems, aiming to solve the ‘smart contract connectivity’ or ‘oracle problem’. That is, due to the decentralization and tamper-proof properties of smart contracts, they lack connectivity to external data points as they form critical centralizing points of failure as oracle data inputs control the behaviour of the ‘trustless’ smart contract system. ChainLink provides a secure, decentralized tamper-proof blockchain middleware, bridging data sources, payments and APIs to smart contracts, as well as providing a framework for easing access to multiple inputs and outputs for smart contracts. ChainLink allows any user to run an oracle and take part in the network. ChainLink then aggregates the multiple oracle data sources for usage for each particular feed, such that the system can provide redundancy and accuracy for the end user data. Marketplace based incentives dictate the ChainLink system operation with data users making payments to the oracles providing the data sources, and therefore alongside the reputation and staking mechanisms, data users can dictate to what level they value data accuracy and redundancy as higher payments incentivise greater honesty amongst oracles and more participants as the potential reward increases. Currently the status of ChainLink’s mainnet, is that its smart contract aggregator only operates with a static list of four trusted oracles seen  here . It is planned that the number of oracles sourced from will expand and oracle participants will build reputation and stake LINK tokens as collateral which are lost if bad data is provided, but this is yet to be operational on mainnet. Ultimately ChainLink has demonstrated so far, that interacting with external data and managing its inputs, outputs and interactions with decentralized systems will be one of the most crucial building blocks for Web3 technology, and therefore projects that build solutions for the oracle problem will become significant players. It is likely that many iterations and improvements will be made and will be needed in oracle design such that interacting with blockchain applications and the outside world becomes fully seamless and secure for a raft of new decentralized applications to be fully realized. Band is an open protocol that facilitates the governance of data used in decentralized blockchain systems. The protocol functions as an open standard for data handling and management for any decentralized program or application that requires trusted and reliable data. Building on shared principles of ChainLink, FOAM and DIRT, Band Protocol provides the infrastructure for secure and effective Web3 applications to be made by packaging up the core fundamental tools for decentralized data management into simple building blocks with infrastructure developed for  off-chain oracles ,  identity management systems ,  token issuances  and  token curated registries . Combining these tools will mean that Band Protocol is effective in four key areas: While other aspects to Web3 may prove to be more critical in future, the mechanisms for data management are crucial problems that need to be solved for effective Web3 applications to be implemented today and at Band we believe we’ve created some of the most usable and holistic tools across the board for this purpose. We continue to build with our mainnet deployment fast approaching.  If any interested parties are seeking to integrate our technology, please contact the team if any assistance is needed. For more information see our  website ,  introductory blog  and  whitepaper . About Band Protocol Band Protocol is a data governance framework for Web3.0 application. It operates as an open-source standard and framework for the decentralized management of data. Build and manage off-chain oracles, reputation scores, identity management systems, token issuances and token curated registries. Website  |  Whitepaper  |  Telegram   |  Medium  |  Twitter  |  Reddit  |  Github"
Fundamental Data modeling and Data warehousing,undamental Data modeling and Data warehousin,"What is data modeling? Model is a representation of real data that provide us with characteristic, relation and rules that apply to our data. It doesn’t actually contain any data in it Data model give us insight about The data model helps us design our database. When building a plane, you don’t start with building the engine. You start by creating a blueprint anschematic. Creating database is just the same, you start with modelling the data. Some terminologies in data modeling Type of Data modeling Cardinality Crow foot notation Normalization vs Denormalization Normalization Trying to increase data integrity by reducing the number of copies of the data. Data that needs to be added or updated will be done in as few places as possible. Denormalization Trying to increase performance by reducing the number of joins between tables (as joins can be slow). Data integrity will take a bit of a potential hit, as there will be more copies of the data (to reduce JOINS). OLTP vs OLAP OLTP (Online Transaction Processing)  system captures and maintains transaction data in a database. Each transaction involves individual database records made up of multiple fields or columns. Examples include banking and credit card activity or retail checkout scanning. OLAP (Online Analytical Processing)  applies complex queries to large amounts of historical data, aggregated from OLTP databases and other sources, for data mining, analytics, and business intelligence projects. What is Data warhouse? Why build Data warehouse? Rules of Traditional Data warehouse Data mart A Data Mart is focused on a single functional area of an organization and contains a subset of data stored in a Data Warehouse Data Lake Data lake, an option of concept to store data beside Data warehouse Data warehouse vs Data lake What is dimensional modelling? Dimensional modeling always uses the concepts of facts (measures), and dimensions (context). Facts are typically (but not always) numeric values that can be aggregated, and dimensions are groups of hierarchies and descriptors that define the facts. For example, sales amount is a fact; timestamp, product, register#, store#, etc. are elements of dimensions. Dimensional models are built by business process area, e.g. store sales, inventory, claims, etc. Because the different business process areas share some but not all dimensions, efficiency in design, operation, and consistency, is achieved using conformed dimensions, i.e. using one copy of the shared dimension across subject areas Relational Model Dimensional Model What important in Dimensional modelling? Fact and Dimension Steps for Dimensional modelling Data schema modelling Database keys for dimensional modelling There are 2 type of key that used in dimensional modelling: There are 2 type of key used for primary key in modelling data warehouse: Tips in determining key in dimensional modelling schema: How to design dimension table? Dimension table provide context to measurement. There are two types of dimension table: How to design fact table? Type of fact table: How to create policy for historical data? 3 main policy for historical data (especially in dimension table): Data pipeline A data pipeline may be a simple process of data extraction and loading, or, it may be designed to handle data in a more advanced manner, such as training datasets for machine learning. Source: Data sources may include relational databases and data from SaaS applications. Conclusion Managing large quantities of structured and unstructured data is a primary function of information systems. Data models can describe the structure, manipulation, and integrity aspects of the data stored in data management systems such as relational databases. So this very important to learn about data modeling. References What is Dimensional Modeling in Data Warehouse? (guru99.com) Data model — Wikipedia Dimensional modeling — Wikipedia What is a Data Pipeline? Process and Examples (stitchdata.com) OLTP and OLAP: a practical comparison | Stitch resource (stitchdata.com)"
Inmon vs Kimball - the great data warehousing debate,nmon vs Kimball - the great data warehousing debat,"Pioneers Bill Inmon, known as the ‘father of data warehousing’ and Ralph Kimball, a thought leader in dimensional data warehousing, have an ongoing debate. According to Kimball:   “The data warehouse is nothing more than the union of all data marts”, to which Inmon responds: “You can catch all the minnows in the ocean and stack them together — they still do not make a whale”. Here’s what they’re arguing about: In a typical data warehouse, we begin with a set of OLTP data sources. These could be XL sheets, ERP Systems, Files or basically any other source of data. After the data arrives into the staging environment, ETL tools are used to process and transform the data and then feed it into the data warehouse. According to Inmon, data should be fed directly into the data warehouse straight after the ETL process. Kimball, however, maintains that after the ETL process, data should be loaded into data marts, with the union of all these data marts creating a conceptual (not actual) data warehouse. The Inmon approach is referred to as the top-down or data-driven approach, whereby we start from the data warehouse and break it down into data marts, specialized as needed to meet the needs of different departments within the organization, such as finance, accounting, HR etc. The Kimball approach is referred to as bottom-up or user-driven, because we start from the user-specific data marts, and these form the very building blocks of our conceptual data warehouse. It’s important to know from the outset which model best suits your needs so that it can be built into the data warehouse schema. To illustrate, we can consider a data warehouse to be like a filing cabinet, and the data marts its drawers. For Inmon, we transfer all the data into our filing cabinet (aka data warehouse) and we then decide which subject-specific drawer of the cabinet (aka data mart) to put the different files into. Conversely, for Kimball, we begin with a number of subject-specific drawers (data marts) that reflect who needs to use what data, and we can stack them into a cabinet formation (the data warehouse) if we want to, but at the end of the day, they’re just a load of drawers, whether we bring them together into a cabinet or not. It is the business needs of a certain entity that determine the correct approach for them. Here are some examples to illustrate: Conclusion In 2017,  Gartner estimated  that 60% of data warehouse implementations would have only limited acceptance or fail entirely. To enhance acceptance and success, it is important to set up your data warehouse correctly for your needs, from the very beginning. The implications of taking the wrong approach are costly and time-consuming. When considering which approach to take-the Inmon or the Kimball-consider factors such as your budget, data volume, data velocity, data variety and data veracity. Then watch out for pitfalls, such as inappropriate software, poor communication between the business and the team, poor cost estimation etc. This is where experience really counts. What  you  might think is the right approach may not be the one that  I  think is right, and without a proper understanding of all the implications, mistakes can be made. With the right experience, you can find a cost-effective method to build a time-efficient solution. We hope that our discussion of the Inmon and Kimball argument is useful as a starting point. Thanks for reading!"
A Guide To The Data Lake — Modern Batch Data Warehousing, Guide To The Data Lake — Modern Batch Data Warehousin,"The last decades have been greatly transformative for the Data Analytics landscape. With the decrease in storage costs and the adoption of cloud computing, the restrictions that guided the design of data tools were made obsolete and so — the data engineering tools and techniques  had  to evolve. I suspect many data teams are taking on complex projects to modernize their data engineering stack and use the new technologies at their disposal. Many others are designing new data ecosystems from scratch in companies pursuing new business opportunities made possible by advances in Machine Learning and Cloud computing. A  pre- Hadoop  batch data infrastructure was typically made of a Data Warehouse (DW) appliance tightly coupled with its storage (e.g. Oracle or Teradata DW), an Extract Transform Load (ETL) tool (e.g. SSIS or Informatica) and a Business Intelligence (BI) tool (e.g. Looker or MicroStrategy). The philosophy and design principles of the Data organization, in this case were driven by well-established methodologies as outlined in books such as Ralph Kimball’s  The Data Warehouse Toolkit  (1996) or Bill Inmon’s  Building The Data Warehouse  (1992). I contrast this approach to its  modern  version that was born of Cloud technology innovations and reduced storage costs. In a modern stack, the roles that were handled by the Data Warehouse appliance are now handled by  specialized  components like, file formats (e.g.  Parquet ,  Avro ,  Hudi ), cheap cloud storage (e.g.  AWS S3 ,  GS ), metadata engines (e.g.  Hive  metastore), query/compute engines (e.g. Hive,  Presto ,  Impala ,  Spark ) and optionally, cloud-based DWs (e.g.  Snowflake ,  Redshift ).  Drag and drop  ETL tools are less common, instead, a scheduler/orchestrator (e.g.  Airflow ,  Luigi ) and “ad-hoc” software logic take on this role. The “ad-hoc” ETL software is sometimes found in separate applications and sometimes within the scheduler framework which is  extensible  by design (Operator in Airflow, Task in Luigi). It often relies on external compute systems such as Spark clusters or DWs for heavy Transformation. The BI side also saw the rise of an open source alternative called  Superset  sometimes complemented by  Druid  to create roll-ups, Online analytical processing (OLAP) cubes and provide a fast read-only storage and query engine. I found myself working on a migration from a  pre-Hadoop  stack to a modern stack. It is not only a technology shift but also a major  paradigm shift . In some cases, there are modeling and architectural decisions where we should distance ourselves from  outdated  wisdom and best practices, and it is important to understand why we are doing it. I found  Maxime Beauchemin ’s resources to be extremely helpful, and studying/understanding Apache Airflow’s design choices brings a lot of practical understanding in implementing the approach he advocates for (Airflow was created by Maxime). This guide aims to take an  opinionated  approach to defining and designing a Data Lake. I pick specific technologies to make this guide more  practical , and I hope most of it is applicable to other tools in a modern stack. The motivations to pick a technology over another are generally a result of my experience (or lack thereof). For example, I will mention AWS tools because that is the cloud provider I have experience with. This blog post defines the E of ETL, and describes the role of the  Data Lake . An extraction, in data engineering, should be an  unaltered snapshot  of the state of  entities  at a given point in time. Concretely, it often involves calling APIs, scraping websites, getting files from Secure File Transfer Protocol (SFTP) servers, running periodically a query on a copy of an Operational Database and copying files from an S3 account. The result of the extraction is stored in a cheap, scalable, high availability Cloud storage such as S3 to be kept  forever  — or as long as compliance lets us. The data produced by these extractions is what constitutes the Data Lake. Different writers, bloggers, and professionals have different definitions of a “Data Lake” and a “Data Warehouse”. Sometimes,  their roles are not clearly stated or there is overlap  — to the point where it creates confusion and those two words are used interchangeably. The following definition of a Data Lake is simple, it clearly separates it from the Data Warehouse, and it formalizes the useful separation of raw data (part of the Data Lake) from the derived datasets (part of the Data Warehouse / Data Marts). A Data Lake is a repository that contains all of the  unprocessed  data produced or gathered by the business. Because no business logic is applied to the data at that point,  it remains true , and any analytics (tables, data science models) can be recreated from that source if business requirements were to change. This extraction of data from different sources is necessary, because getting the data from the source is often expensive (API calls, slow SFTP, operational database dumps) and sometimes impossible (APIs evolve, SFTPs get emptied, operational databases mutate records in-place). With the democratization of data, and the decentralization of analytics, it is important for a lake to be  discoverable , consider the following structure: The Data Lake consumers would expect any extract to be a top level prefix in the “myorg-data-lake” S3 bucket, making it easy to browse. However, the data does not need to be collocated in the same bucket, because we can use the Hive metastore to register any of the extracts as tables inside the same schema, providing a  central interface  to the data lake. Semi-structured and unstructured data is often cited as a characteristic of a Data Lake. However, I believe that conversion to a file format that  embeds a schema  (such as Parquet, and Avro), during the extraction process, has significant advantages. A schema is a way to define an interface to the dataset, making it easier to use by multiple teams with minimal communication required. It is also a way to perform a  lightweight validation  that the source system is still producing the expected data. When the schema is no longer valid, the extract  breaks , but it reduces the risk of erroneous analytics being produced or cryptic errors deep in the transformation processes. The data extracted might already have some sort of schema (database and API extracts), and storing as JSON / CSV would mean losing some valuable metadata. In the cases where the source system makes previous data unavailable quickly, extract it once  without any conversion , and run conversion on the raw copy. The Hive table can then be pointed at the converted copy. For example, in the case that we files are sourced from an SFTP server and those files disappear after ~1 hour: In this manner, the files are on S3 and the schema can be fixed without fearing the loss of any data. The resulting files are often  faster to process  as Big Data frameworks leverage the file metadata and special data layouts to optimize compute. Another benefit of using such file formats is the  reduced file size . Schemas, encoding techniques and special  data layouts , like Columnar storage, allow these libraries to avoid redundancies — reducing the volume of bytes needed to represent the same information. Many of the Hadoop-family frameworks are more efficient at processing a  small number of big files  rather than a big amount of small files. The reason is that there is overhead in reading the metadata on each file, and there is overhead in starting the processes that download the files in parallel. As a result,  merging extracted data  from multiple queries on a database, multiple API calls, or multiple SFTP files to reduce the number of files, can be a big time saver for downstream transformations. A  compression  library such as Snappy can also be used to reduce the amount of data going through the network, and it is generally worth doing as the CPU load introduced is negligible. The Data Lake should contain the invariable  truth  —   unmodified snapshots of  states  at given moments in time .  Any transformation at that point is undesirable —   with the exception of compliance processes (e.g. anonymization). If the data is altered before being copied to the Data Lake, it will deviate from the state that it aimed to capture in the source system. If that source system makes previous data unavailable, and the logic applied needs to be undone, the data will have to be further mutated. This is dangerous as it might end up in a state where  rolling back the changes  to get the raw extract is not possible. Sometimes a bug in the source system causes incorrect data to be produced. In those we might want it to be reflected in our capture (and analytics), or we might want it corrected. In the latter case, re-running the extraction process for the time-window concerned might be enough, or manual intervention might be required, but in both cases, the  physical partitions  are used to  overwrite  the concerned data only. Partitioning the extracted data is important to achieve idempotency and optimize extract sizes.  Maxime Beauchemin  makes a strong case for idempotent ETL in his  Functional Data Engineering blog post . The extract should always produce the same result given a “scheduled execution date”. This is sometimes  impossible  as we depend on external sources we do not have control over, but those extracts should be grouped into partitions nonetheless so downstream Transformations can be made idempotent. This is a simple example of an idempotent daily extract: In practice, we would parameterize the above SQL to derive lower and upper bounds from the execution date Compare it to the following  wrong example , where the result will change depending on an external factor (today’s date): In Airflow, the  execution date  is often used to implement idempotency. The execution date is a powerful concept, it is an  immutable date  given to a  pipeline  (Direct Acyclic Graph in Airflow) when it runs. If the DAG was scheduled to run on  2019–01–01 00:00:00  this is what the execution date will be. If that pipeline failed, or needed to re-run, the state of that specific run could be cleared, it would then get the  same execution date  and produce the same extract — with the condition that the extract is based on execution date and idempotent. This would make it possible to run multiple extract processes in parallel and is often used to backfill data. As an example, I have backfilled data from APIs using the same (simple) technique and using Airflow’s features to limit parallelism and, retry or timeout automatically the calls. It is a common use-case of Airflow and it is made possible by idempotent (and non-overlapping) extracts. Similarly, we can run  multiple instances  of our end-to-end ETL. This is especially useful when trying to re-process data for an  unusual  time-frame. For example, if the transformation code and defined resources were tested for a daily volume of data, it is hard to know how the same set up would behave for an entire month worth of volume. Instead, one instance of the ETL can be started per day of a month being reprocessed — potentially executed in parallel (this is made easy in Airflow). The physical partitioning of the extracts should be based on the date when the extract process is scheduled to run. The simplest example is of a daily extract that is processed in a daily batch: ds stands for date stamp, here: the execution date Note the  key=value  format which denotes a physical partition in the Hadoop ecosystem — physical partitions are turned into a column of the dataset and allow all files outside of selected partitions to be skipped (partition pruning) when used in a WHERE clause. The extraction process should be written in a way that overwrites a partition  based on the execution date it is given . The transformation processes can then, use the  ds as a filter to get to the data they need to process. Other time-frames follow the same principles, but under hourly we might want to consider  Streaming  — batch systems tend to have overheads that make them unusable for very short batches. Another interesting and common use-case is one where we extract the data at  a higher frequency than we transform it . This need arises when extracting a large time-frame from the source overloads it, the data is in risk of being unavailable later or extracting all the data at once right before transformation would delay downstream processes. As an example, we might want to create hourly extracts, but process the data every 12 hours. To design such a pipeline where tasks need to be scheduled at different cadences — we can use  logical branching   to   selectively run the transformation processes when we want to. This higher frequency of extraction can produce undesirably small files, in this case, before transformation, we merge the batch to be processed. When the files are so small that writing metadata (schema, statistics) into each one represents a substantial overhead the conversion to a schema embedded file should happen after the files are merged. In that case, consider the following structure: Where the merge + conversion + compression process would turn 12 partitions into 1, and the  sftp_clients  table would point to the parquet version and not the raw copy. In this post, I attempted to add some structure to the Data Lake and the Extract processes that feed it. I hope you found it useful!"
Guide to Data Warehousing,uide to Data Warehousin,"In this guide, I’ll try to cover several methodologies, explain their differences and when and why (in my point of view) one is better than the other, and maybe introduce some tools you can use when modeling DWH (Data Warehouse) or EDW (Enterprise Data Warehouse). I started my BI (Business Intelligence) career in a small company that consulted with other companies on how to improve their processes or just helped them build a BI system so that they can make the decisions themselves. How I imagined working during that time (almost straight out of the university): I would come to work, get equipment, someone would explain what I have to do, I would start doing it. I was so surprised when I came to work on the first day: I’ve got the book “ The Data Warehouse Toolkit” by Ralph Kimball , and my manager told me to read and learn. So, I started reading and trying to understand it. It took me some time. I don’t remember how long it took, but maybe a week or so to get a proper understanding (at least in my view). Terms which need an explanation and you might find in this article: Staging area  — a copy of raw/source data on the same database/machine we have our DWH Transformation area  — transformed staging data. Prepared to be loaded to DWH Fact table  — transactional or event-based data with some measures. i.e., sale information, warehouse products movement. Dimension table  — all information for a particular thing in one place. i.e., all product-related information, customer information. I am starting with a technique that I learned first mostly because it’s easy to comprehend. It was created by Ralph Kimball and his colleagues (hence the name). This approach is considered to be a  bottom-up design approach . For a more general audience, maybe it’s more familiar by dimensional modeling name. Personally, what I liked about this modeling — easy to design and analyze. It was designed to answer specific questions or help understand particular areas (i.e., HR, Sales). This approach allows for fast development, but we lose some flexibility. Usually, we need to rebuild the DWH (or some of its parts) to apply the changes. Most of BI reporting tools can understand this model, and you can quickly drag and drop reports (i.e., MS SQL Server Analysis Services, Tableau) Kimball flow: I’ll cover Star and Snowflake schemas in more detail in sections below. In Star Schema, we have a fact table and dimension tables (with all of the foreign keys in the fact table). You can find more details in this  Wikipedia article . In a nutshell, it looks like this: Pros: Cons: Snowflake is a Star schema with more layers. I.e., we have an address in our shop dimension. We can create an address dimension with address_PK, which would point to the dim_shop. You can read upon snowflake schema in this  Wikipedia article . Simplified view of it: Pros: Cons: I’m not saying that one modeling is better than the other; it all depends on the use cases, available resources, and the end goal. Weigh all of the options and think if you’ll be adding more groupings to the dimensions, if you want to add more atomic layers later (updating fact table with foreign keys to dimension tables). Practically I haven’t used it, so this will be a more theoretical overview. This methodology was created by Bill Inmon and is considered to be a  top-down approach.  We have to have a whole picture and model it accordingly to 3NF (Normal form), making this approach more complicated than Kimballs. The downside is that you need skillful people to design this data model and to integrate all of the subject areas into it. Compared to Kimballs, it takes more time to get it running, but it’s way easier to maintain, and it’s more of an enterprise-grade approach. Inmon Flow: To compare these to it all comes down to the company/business area data, we want to model. From my personal experience, the first project I had was from a retail client. We did a model on Kimball with a Star schema, because we knew requirements and their data questions. If it would want some more integrations to their DWH, like join the employees who were working those days, inventory management, it would be more appropriate with the Inmon approach. In my opinion, if a company is small and they want to track and improve only particular elements — usually it’s easier and faster to go with Kimballs’ approach. Moving from small company to a big corporation had it’s toll on me as well. I got a better understanding that sometimes we need a better and more simplified EDW. That’s when I started to work with Data Vault. As far as I see it — it’s a combination of Kimballs Star Schema and Inmons methodology. Best of both worlds. If you want a more detailed view check out Dan Linsteds  site , he’s the creator of this approach. I’m going to cover several most essential components of Data Vault. Hub is a collection of all of the distinct entities, i.e., for account hub, we’d have an  account, account_ID, load_date, src_name . So we can track from where the record originally came from when it was loaded and if we need a surrogate key generated from the business key. Not to sound funny, but the link is a link between different hubs. I.e., we have employees, and they belong to teams. Teams and employees have different hubs, so we can have team_employee_link, which would have  team_employee_link, team_id, employee_id, load_date, src_name . Slowly changing dimension of particular entity attributes. I.e., we have a product as an entity. We have multiple product information columns, name, price. So we load this information as a slowly changing dimension with information  product_id, open_date, close_date, is_deleted, product_name, product_price . Capturing all the changes lets us re-create snapshots of data and see how it evolved. As well as these most basic entities, we have Satellite Links, Transactional Links. I won’t go into details for these; if you want to know more about it — check Dan Linstedts site or Wikipedia. Coming from a simple data background, I came across a Data lake term. Which stores all the information (structured and non-structured) we have. Nowadays, terms like  Big data ,  Data lake  are getting tons of attention. I didn’t understand why we needed to store that much data of different types until I started working with vast amounts of it. Data is today’s and future’s gold. In my experience for all data-driven companies, data lakes are almost a must. Store as much as you can then do analysis, look for insights. Storing data from multiple sources in raw formats comes with its own cost. If you won’t keep tabs and manage your data lake properly — it might become a data swamp. From my point of view, it’s an additional layer before creating your EDW. You have data engineers working on bringing raw data to the data lake and building an EDW on top of it. Analysts can work and rely on the pre-processed and cleansed data. Databricks company  introduced this term at the end of January 2020 . The idea here that we do everything straight on the source data. Most of the ML/AI tools are more designed for unstructured data (texts, images, videos, sound). Processing them to DWH or Data Lake will take some time and, for sure, will not be near real-time. I’m a bit skeptical about this approach. It’s like creating a vast data swamp and letting people drown in it. Too much data that is not managed, cleansed might lead to false assumptions and will not be one source of truth in a big company. At least now, while streaming is not that big of a thing, I think it’s worth waiting for this methodology to mature more. Unless you’re some tech startup that wants to be way ahead of your competition and use bleeding-edge technology, but it’s just my assumptions. In my opinion, all of these methodologies will co-exist for a long time. It all depends on the company and its use case! If it’s just a small or medium business, and if the old school Data Warehousing approach satisfies your needs, why use something which might not bring more profit for you? If it’s a large enterprise — the data lake probably is a must if you want to stay competitive and provide excellent service to your customers. In my opinion, you still will have to create a pre-processed layer, which would be some sort of one source of truth for reports, with more aggregated/cleansed data. The best fit here (in my opinion) is Star or Snowflake schemas. It will enable us to look for general patterns and trends more quickly. In case we need to do a deep dive, and DWH/Datamart is too aggregated — we could always go to the data lake and check raw data. Or maybe your employees are tech-savvy, and you’re a next-gen tech startup, and you want to beat your competition by giving insights while using bleeding-edge technologies — perhaps you need a lakehouse? Unfortunately, I haven’t seen real use cases, can’t think of how it can work with the old school methods for cleaner and smoother analysis. [1]  https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/books/data-warehouse-dw-toolkit/ [2]  https://en.wikipedia.org/wiki/Star_schema [3]  https://en.wikipedia.org/wiki/Snowflake_schema [4]  https://danlinstedt.com/solutions-2/data-vault-basics/ [5]  https://en.wikipedia.org/wiki/Data_vault_modeling [6]  https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html"
Is it the end of Data Warehousing?,s it the end of Data Warehousing,"This is a personal medium post. The opinions expressed here represent my own and not those of my current or any previous employers. T he Data warehouse architecture has been in IT industry for at least three decades from now. The history of data warehousing started with helping business leaders get analytical insights by collecting data from operational databases (transactional systems — historical data) into centralized warehouses, which then could be used for decision support and  business intelligence  (BI). This approach has been well known and refers to the first generation data analytics platforms. In the picture below, we can see what it looked like. In the meantime, this kind of architecture has been applied and worked for several years, the data requirements in the path start to change dramatically. We are talking about a tremendous volume of data generated by: IoT devices, social medias, mobile phones, etc. In addition, we also had to consider differents unstructured datasets such as video, audio, documents. As a matter of fact, traditional data warehouses architectures could not address this kind of requirements anymore. It emerges the second generation of data analytics platforms, which was built on top of new technologies, and a these technologies we put all together and call it for BigData for simplicity. Without enter in the specifics of this approach, we have to understand that this architecture tries to solve new problems, using new technologies, mixed with old technologies. Data lake has been introduced as a cheaper way to store data independent of the structure of data, but the necessity of analytical insights are still addressed by Data Warehouses and BI tools. In the picture below, we can see what it looked like. As we can noticed in the picture above, it is extremely hard to maintain the second generation architecture for a couple of reasons. In this article, it is not my intention to list all challenges in this approach, but a group of academics at Berkeley (University of California) noticed that there is a opportunity in the bigdata analytics world. After years of research, creating and collaborating with open-source data technologies — the most famous is Apache Spark — they created a new company called Databricks, which has a mission to provide bigdata analytics and AI tools to business using a more simpliest and integrated data analytics plataform. At Databricks, a new architecture pattern has emerged and promise to address all issues that we are seeing with old architectures. That is a Lakehouse architecture. What it can see in the picture below. In a Lakehouse architecture, the data warehouse will not be necessary any more, since that classical BI and reports could be generated accessing the  datalake,  and new advanced techniques such as: Machine learning and data science could be generated accessing the same layer. In the next blog posts we will go deep of this approach and technologies behind. If you want to know more, I will give you some links to study: If you like or dislike the article, or would like to give your opinion feel free. What do you think? Is it the end of data warehousing?"
What Are The Benefits Of Cloud Data Warehousing?,hat Are The Benefits Of Cloud Data Warehousing,"Data warehousing was introduced in 1988 by IBM researchers Barry Devlin and Paul Murphy. Since then the concept has evolved and taken on a life of its own. Increasing challenges and complexities of business have forced data warehousing to become a distinct discipline. Over the years this has led to best business practices, improved technologies, and hundreds of books being published on the topic."
Data Engineering Explained: Data Warehousing and OLAP,ata Engineering Explained: Data Warehousing and OLA,"I am starting a new series of articles entitled “Data Engineering explained” in which, I will share with you the following:"
Data Warehousing Schemas,ata Warehousing Schema,"Much like an OLTP system(database), an OLAP system(data warehouse) is also required to maintain a schema. A database uses an entity relational model, while a data warehouse uses a dimensional model. In this blog, we will see the three major schemas that are used to model a data warehouse."
Amazon Redshift: Data Warehousing for the Masses,mazon Redshift: Data Warehousing for the Masse,"Amazon Redshift has gotten a lot of news in the past few months. Larry Ellison  slammed Redshift  at OpenWorld in October 2017, the 2nd time after he had already  thrown darts  in 2016. You know you’re doing something right when Larry calls you out on stage. We’ve not been able to discover the reason why Amazon chose the name Redshift for its data warehousing solution, but we suspect it had something to do with hyper expansion. Like the universe’s rapid expansion since the Big Bang, enterprise data has also expanded at a rapid rate. And, as with the universe, there’s no end to its growth in sight! In this post, we’ll cover the basic concepts behind Amazon Redshift, and why it’s seen so much adoption. Let’s take the expanding universe analogy a bit further. Much of the universe lies beyond our current observational horizon. In a similar way, lots of enterprise data also lies in the dark — untouched and unanalyzed. One motivation for Amazon building Redshift was to ease the path to analyzing this “ dark data .” And do it in a way that allows anybody to deploy a data warehouse solution without large setup costs or deep technical expertise. Amazon offers a  free 60 day trial  for data analysts to get their feet wet with Redshift. A time-to-first-report is around 15 minutes. That’s enough time for analysts to quickly gain some initial business insights into their data and see if it is worthwhile before committing further resources. Should they decide to go with Redshift, there’s no upfront expense. The  entry price point for the smallest data set is $0.25/node/hour . That’s less than $1,000/TB/year, an order of magnitude cheaper than current warehouse solutions. With cost no longer a factor in preventing the democratization of data warehousing, let’s probe what Redshift is, how it works and what the future holds. Amazon Redshift is a relational database system built on PostgreSQL principles. It’s optimized for performing online analytical processing (“OLAP”) queries efficiently over petabytes of data. The query handling efficiency is achieved through the combination of: Each of these merits its own post. In what follows, we’ll provide the highlights of the design. The heart of each Redshift deployment is a  cluster . Each cluster has a  leader node  and one or more  compute node s, all connected by high speed links. There are  two types of compute nodes , each with different CPU, RAM and storage characteristics. The  Dense Compute (DC) nodes  are meant for speed of query execution, with less storage, and is best for high performance activities. It is implemented with SSD drives. The  Dense Storage (DS) nodes  are for storing and querying big data, using typical hard disk drives. You choose the node type depending on what you wish to achieve. For instance, you may want to store large amounts of data and run complex queries against it, or increase speed of execution for near-real time analytics, etc. The number of nodes you select depends on the size of your data and the query handling performance you seek. There’s a difference in pricing for these options, of course. And you can adjust these as your needs change. A leader node is the interface to your business intelligence application. It offers  standard PostgreSQL JDBC/ODBC interfaces  for queries and responses. It serves as the traffic cop directing queries from customer applications to the appropriate compute nodes, and manages the results returned. It also distributes ingested data into the compute nodes to build your databases. Each compute node has its storage divided into at least two slices, with each slice performing a portion of the node’s workload in parallel. A user-specified  data distribution plan  places ingested data across the various compute nodes and their slices. Choose this plan with care so that nodes are not under-utilized and reduce data movement between nodes during query execution. There is also an art to  selecting the appropriate sort key  to make query handling more efficient. We’ll write more on best practices around these topics in future blog posts. Redshift clusters run on the  Amazon Elastic Compute Cloud (EC2) platform . These clusters can either be on a space shared with other AWS users (the so-called “classic” platform) or cordoned off by your own  Virtual Private Cloud  (VPC). In either case, you assign IP addresses to allow external applications to access you clusters. Also, you can decide if you want your external data sources to traverse the VPC to populate your clusters. Finally, you set up your clusters in what’s called an  availability zone , which is an isolated “location” within an Amazon EC2 region. Redshift data is also replicated in the Amazon  Simple Storage Service  (S3). Customers can also choose to have another S3 backup in a different region. You can ingest data into Redshift data from multiple sources, including using Amazon  S3 , Amazon  DynamoDB , or Amazon  EMR  as staging entities. Or, it can come  directly from any other data source  such as an EC2 instance or a host that supports the  Secure Shell  (SSH) protocol. Redshift’s major advantage is that the other Amazon cloud services — S3 for backup, EC2 for instances, VPC for network isolation , and may other services in the AWS ecosystem — are built into the warehouse deployment, and you don’t pay for these separately, or need to manage their setup. It’s all done under the hood for you, through a  simple configuration console  when setting up your database and loading your clusters. Thus, you get the benefits of resilience, load balancing, monitoring, logging and all other such goodness in one managed package. A key advantage of Redshift that we think a lot of people are not aware of is simplicity. It used to take months if not quarters to get a data warehouse up and running. And you’d need the help of an Accenture or IBM. None of that anymore. You can spin up a Redshift cluster in less than 15 minutes, and build a whole business intelligence stack in a weekend. A report from June 2017 by Forrester mentions  that Redshift has over 5,000 deployments, with plenty of  success stories . That’s quite a remarkable adoption, considering that Redshift has only been available since November 2012. The size of the data warehouses range from a few terabytes to petabytes. Looking at its  success stories , there are  four basic types of uses cases  for Redshift. We cover more details on the use cases in our Quora answer  What are some good real world examples of using Amazon Redshift? DB-Engines provides a  ranking  for different databases. Benchmarking Redshift vs. legacy vendor Teradata, it becomes clear how much traction has seen in just 5 years of its existence. The latest addition to Amazon’s data warehouse toolkit is  Redshift Spectrum . It is a pay-as-you-use way to run queries against data you’ve stored in your S3 “data lake.” It uses the same business intelligence applications as well as the same queries that you already use with Redshift. Spectrum is useful when analyzing older or legacy data kept in S3 (incurring only normal S3 storage charges), while newer data is in Redshift. Redshift mediates the query towards data in S3, taking care of the hard stuff such as normalizing the data formats. It will be a bit slower, but older data is not “dark” any longer. (Some enterprises may well have exabytes of such dark data.) You only pay $5 per terabyte of data pulled from S3 during the query processing. There’s more to Redshift Spectrum, of course, and we will write more about it in future posts. For now, it’s good to keep the following points about Amazon Redshift in mind: AWS Re:Invent 2017 is coming up, and we can expect more announcements on features and use cases that should further drive up adoption."
The Extinction of Enterprise Data Warehousing,he Extinction of Enterprise Data Warehousin,"Data warehousing  and  business intelligence  play an important role in many, if not, all of the large sized organizations working on turning data into insights that drive meaningful business value. Data warehousing is process for collecting and managing data from varied sources to provide meaningful business insights. Business intelligence comprises the strategies and technologies used for providing these insights. Both areas can be considered part of data management. They are heavily intertwined with the other data management areas and depend a lot on data integration. Data warehousing became popular during the nineties and started as a common practice to collect and integrate data into a harmonized form with the objective to create a consistent version of the truth for the organization. This consistent version of the truth became an important source for business decision-making within the company. Such decision-making was supported by another trend, business intelligence. Business intelligence, as defined by  Gartner  is “an umbrella term that includes the applications, infrastructure and tools, and best practices that enable access to and analysis of information to improve and optimize decisions and performance”. Business intelligence started with producing data and simple reports in a nice presentable way, but at a later stage  Self-Service  was introduced, with more advanced capabilities, such as in memory processing and predictable functions. Many enterprise organizations heavily rely on data warehouses. A lot of these have been developed during the last decade. They are considered to be a critical assets, used in many daily processes and fulfill an important role of the data consumption and distribution within the company. In this post I will explain some of the popular approaches and common characteristics and unravel the enterprise data warehouse concept, which differs in scale. WARNING:  I need to give you a warning as we continue. It is my strong belief that the  enterprise  data warehouse (EDW) will soon to become extinct. Not the underlying concepts of data warehousing themselves, because the need for data harmonization, bringing amounts of data into a particular context always remains. But what will become extinct is using a data warehouse for enterprise wide data integration, consumption and distribution. I’ll explain why in this post. Many companies are still investing heavily in data warehouses. The size and scale in which data warehousing architectures are used however can differ significantly. Before I tell you my viewpoint on this architecture let’s step back and start from the beginning. What is a data warehouse? The core concept of data warehousing was introduced in 1988 by IBM researchers  Barry Devlin and Paul Murphy . As computer systems became more complex and the amounts of data increased, an architectural model was required for the flow of data from operational systems to support decision environments. Bill Inmon, a few years later, published a book and gave a more concrete definition about what data warehouses are meant for: “a data warehouse is a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management’s decision making process.” As data warehouses gain popularity, and variations of the model with different definitions started to emerge, I like to define a data warehouse as  ”a system which harmonizes (historical) data from different sources into a central repository and manages it, with the main objective to support the decision-making process”.  From this sentence, we can extract a number of important characteristics. From the word  central  you can conclude that in a data warehouse data is brought together and centrally stored. In many of the cases this is indeed true, but some technology trends allow a data warehouse to be developed in other ways, such as data virtualization. From the word  harmonize  we can derive that data is integrated, unified and made consistent. This means additional components are needed to collect, cleanse, fix, transform, and store data. By stating  different sources  we make explicit the number is plural, so a data warehouse isn’t or won’t be used to only collect data from a single source. Bill Inmon once  said  that  “one of the great appeals of the data warehousing concept is there is a single version of the truth” . This  single version of the truth  means that there’s a single vocabulary used and all users must agree with the meaning and definitions for all data used within the data warehouse. This single vocabulary aspect we cover in more detail later. This sounds comprehensive, but why do you want a data warehouse in the first place? What are the benefits? In order to understand better why data warehouses are useful we need to step back, explain some of the key drivers and first look more closely how transactional and analytical systems work. The space of applications and databases traditionally has been divided into two worlds. Many of the applications started as transactional or operational as the world of computers begun with the requirement to handle transactions and storing records. These systems made processes more efficient as they were capable to replace the traditional card catalogs and many of the manual processes. Banking systems, airline ticket booking systems, order management systems, customer systems, telecom systems, healthcare systems, shopping systems and so on. These applications are called  online transaction processing (OLTP)  systems given their crucial operational role. What’s important for OLTP systems is that both consistency and stability must be guaranteed. Imagine you run a large regional telecom company and the telecom system goes down. The impact can be tremendous. Customers can no longer make phone calls, answer their phones, receive text messages, etc. If the system stays down for too long, the business model will be impacted. Customers lose trust and walk away. OLTP systems, therefore, are designed for data integrity, system stability and availability. The large majority are ACID compliant. An observation we can make from OLTP systems, is that the (operational) workloads are usually quite predictable. We know how OLTP systems are used and what typical loads are expected. Queries are relatively straightforward and and retrieved data is relatively low in volume: read a record, update a record, delete a record, etc. The underlying physical data model is designed based on these predictable queries. Tables in the OLTP systems are usually  normalized . Ideally every single attribute is only stored once. This normalized design comes with some drawbacks: operational systems are not designed to provide a representable comprehensive view of what is happening in the domain or business. Pulling out data from heavily normalized data models for complex questions is often difficult and performance intensive. Complex questions require more data and combinations of data. Writing such a query would require many tables to be joined, or grouped together. These types of queries are typically fairly performance intensive, so executing too many of these introduces the risk of reaching performance limitations. If so, a system become unpredictable and that is the last thing we want to see happening with OLTP systems, given the trust we need to provide. The consequence of having high integrity and high performance for availability is that OLTP systems are expensive. They aren’t in general positioned to retain large amounts of data, so data life cycle management is important to keep them healthy and fit for purpose. Typically unused data is either removed or moved to a secondary location. From this secondary location data always can be placed back, if it becomes relevant again. OLTP systems are one part of how databases traditionally have been divided. The other part are OLAP systems, which will be discussed in the next section. Business users want to analyze data for their business decisions by examining large and more complex collections of data. Since OLTP systems are expensive and fulfill different purposes, the common best practice always has been to take data out, to bring it over to another environment. This other environment (different systems and databases) will be used for  online analytical processing (OLAP) : complex and analytical processing. Since offline analyses are usually less business critical, the integrity and availability requirements can be less stringent. Considering this, you could say that OLAP systems are usually less expensive than OLTP systems. Data, in OLTP systems, is stored and optimized for integrity and redundancy, but in OLAP we optimize for analytical performance. As we do mainly repeated reads and barely any writes, it is common to optimize for reading the data more intensively. Data can be duplicated to facilitate different read patterns for various analytical scenarios. Tables in OLAP database are usually not that heavily normalized, but preprocessed and in structures that are  denormalized : tables are flattened, sparse, and contain more redundant copies. Data is repeatedly stored. This sounds ineffective, but when data is logically grouped and physically stored together, it is easier for systems to process data and deliver results quicker. Since OLAP systems are less expensive we can afford using more data storage. Note:  Some systems combine OLTP and OLAP. These systems are called  Hybrid transactional/analytical processing , a term created by Gartner. Although these systems look like emerging architectures on the outside, on the inside there are, in general, still two databases. One database is designed for many small transactions with a high fraction of updates. The other (in-memory) database handles the complex queries for the analytical workloads. This approach of separating the command from the queries is quite similar to  CQRS . OLAP systems typically are also used to facilitate data life cycle management by storing the unused or less frequently used data from the OLTP systems. This is done for two reasons. The first reason is that historical data is of value, because for analysis we often look to the past. Performance is the second reason. After OLTP systems have copied their data to OLAP systems, they clean up by removing the redundant copies. This maintenance activity makes OLTP systems more effective, because if tables contain less data, the lookups and queries will run faster. In OLAP systems data from multiple OLTP systems is often brought together, because business users or data analysts often want to make combinations of data. This requires a form of harmonization or unification, because, systems use different contexts and have different data structures. What are Operational Data Stores?  Operational Data Stores (ODS) are, in general, used for operational- analysis and reporting. ODSs carry some characteristics from both OLTP and OLAP systems. The word “operational” positions ODSs closer to OLTP systems, because their purpose is to get insight in operational performance and activities. They mainly inherit the context from OLTP systems. The characteristics that come from the OLAP systems are that ODSs are aimed to take away the analytical workloads, which would be caused by ad-hoc, less predictive analysis and reporting. Another characteristic is that ODSs in general retain more historical data than OLTP systems. One more characteristic is that ODSs can integrate small proportions of data from other systems. This means there could also be an integration or harmonization aspect involved when designing an ODS. In general however ODSs stay closer to the design of the  primary  OLTP system they are positioned to work with. Table structures are often similar. ODSs, as a result, differentiate from data warehouses, because they typically use data only from one  single  OLTP system, unlike data warehouses, which house data from  multiple  sources. The harmonization of multiple source systems is a nice bridge to  data warehouses , because they also take care of the historical requirements, bring data together into a harmonized form and facilitate the analytical activities. Data warehouses therefore can be classified as OLAP as well. Now we know that data warehouses harmonize data from multiple sources into a integrated data model (single version of the truth), it is time to go a little more in depth. In the next sections I want to discuss some of the common characteristics and styles of building and operating data warehouses. The most popular styles were developed by industry leaders Bill Inmon and Ralph Kimball. Bill Inmon’s view is that any piece of data that could possible useful should be stored in a single universal data model that would be a “single source of truth” for the enterprise. This source of truth uses an integer and efficient model for storage, typically a 3NF structure. From this single source of the truth, selections (subsets) are made for specific projects, use cases or departments. This selected subset, which is optimized for the read performance of the use case, is called a  data mart . Ralph Kimball’s view is that a data warehouse must be a conglomerate or collection of dimensional data, which are copies of the transaction data from the source systems. Because a data warehouse is used for various use cases, Kimball has the concept of ‘conformed dimensions’, which are the key dimensions that are shared across and used by different consumers. In the next sections we will look more closely at the two styles, together with some generic components that are always needed when designing and building data warehouses. We’ll start with the staging layer, and move on to cover the remaining parts of a data warehouse. To engineer a data warehouse a number of additional components are required. Within both the Inmon and Kimball approaches data must be extracted and stored (staged) on an intermediate storage area first, before processing can take place. This environment for letting the data “land” first, is called a  Staging Area  and has been visualized in the image below. Staging areas always sit between the data sources and the data targets, which are often data warehouses. They are used to decouple systems and play an important role during the extract, transform and load (ETL) processes. Staging areas can be implemented in different ways, varying from relational databases and file stores. Also the implementation of how data is captured can vary: pushing out data, pulling data or using  CDC . Data is typically delivered in the raw operational format, although here there can be variations as well to this model. I will come back to this later. Staging areas are also typically used to retain historical copies. This is useful for reprocessing scenarios, in cases where the data warehouse gets corrupted and needs to be rebuilt up from a longer period. The number of older data deliveries (historical copies) can vary between staging areas. I have seen use cases where all the data deliveries, including corrections, had to be kept for audits for several years. In other use cases I have seen the staging area emptied (non-persistent) after successful processing or after a fixed period of time. Cleaning up saves storage and costs. Staging areas play an important role within the area of data quality as well. Before data ingestion into the data warehouse starts, it is recommended to first validate all data. If any of sources are missing or data is invalid, the processing can be temporarily stopped or paused. Sources can be asked to redeliver again or corrections on the data can be made. Once the all acceptance criteria are met, processing and ingested the data into the integration layer of the data warehouse can truly start. After all data has been successfully inspected and all acceptance criteria of the staging layer are met, data is ready to be integrated into the  Integration Layer . This is where all cleansed, corrected, enriched and transformed data is stored together into an integrated and common model, using a unified context. It is harmonized, which means unification has been applied to formats, types, naming, structures and relations. Also the historical data is expected to be in this layer, although the difference with the staging area is that data is transformed and no longer raw. The integration layer and how data is stored and structured is also where the Inmon and Kimball styles differ. In the Inmon style all data first lands into a (central) integration layer, which has a normalized relational model. It means that data is optimized for redundancy. You could argue that this layer is similar to the data models we discussed in OLTP and you’d be right. It represents the transactional systems, but in an harmonized way, which means that unifications are applied on formats, field names, data types, relations and so on. This implementation requires a big upfront  data modelling  exercise to align all the different source system structures and transform them into a harmonized and unified data model. The step from staging to the integration layer usually leads to specific and complex ETL logic. The Inmon style also introduces  data marts . After data has been stored in the integration layer, additional layers are created for the consumers. These (presentation) layers are called data marts or dimensional data marts. They typically only contain a subset of the integration layer’s data. They are also specific for a use case, group or set of users. Data in data marts typically is organized in dimensional structures, because it has been optimized for reading performance. Data marts are often implemented using cubes, although they can also be implemented using relational databases (using  star  and  snowflake  schemas). The additional layer of a data mart has a benefit: data marts are decoupled from the integration layer. Changes to the integration layer, if made carefully, shouldn’t impact the data marts and thus the data consumers. The style of Inmon has several drawbacks. The first drawback is that the development time is typically long. Designing an integer and normalized model takes time, because all the different source systems need to be mapped carefully. Integrity and consistency are important drivers. As the size and number of sources start increasing, the number of relationships with dependencies increases heavily. This could lead to cascading effects within the design: endless parent-child complexities or structures. Another drawback is that if new data has to be added to a data mart, data always has to be added to the integration layer first. Since the development takes time and changes to the integration layer have to be made carefully, users requiring new data have to wait (long). The list below summarizes the pros and cons of the Inmon style. Benefits Downsides The Inmon style differs from the Kimball style mainly on the integration layer. In the Kimball style, data in the integration layer differs from the way data is stored in the transactional systems. When data is copied into the integration layer, it is already optimized for read performance. The data is more flattened, sparse and thus has similarities with data mart structures from the Inmon style. The difference here, compared to Inmon, is that the integration layer is conglomerate of dimensional tables, which are the ingredients for the data marts. A data mart is just a logical distinction or selection of the different tables with some additional tables to link everything together. The data warehouse, in this approach, is the combination of all individual data marts. The benefit is that these dimensional tables are better understood, because the overwhelming number of parent-child structures have been reduced. An important aspect of Kimball style is data is logically grouped into what is called  facts  and  dimensions . Facts are quantitative numbers: sales figures, amounts, counts, etc. The dimensions represent the business entities, so customers, products, employees, contracts, etc. By linking facts and dimensions together, a  star schema  is created. Conceptually these are the same as data marts, with the difference that they sit in the same integration layer, and share the same underlying database technology. The benefit of the Kimball model is that this style has a higher short term agility than Inmon. Typically users don’t need to wait for months or years, before consumption can start. Although the data is organized subject area and more easy to understand for the users, the Kimball style also has some drawbacks. Since each data mart is created for a specific user group typically many additional tables are needed to create more specific star schema’s. As users will have more conflicting requirements more additional “helper” tables will be needed. Within Kimball there is also a higher chance of more coupling: users can start reusing integration logic or helper tables from other use cases or users. So, as size grows and the number of users increases, then the complexity grows as well. Consequently, even small changes to structures can have high-impact on all the users. Another implication is that when loading the data, issues can occur with transactions (facts) appearing before dimensions are loaded. Lastly is that updates and deletes can be performance intensive due to high level of denormalization applied on the tables. In the table below I have listed all high-level benefits and downsides from the Kimball style. Benefits Downsides Based on advantages and disadvantages of both styles, engineers started to experiment and combine different approaches. An example can be a core data warehouse model, which is Inmon based, but extended with dimensional tables, which are Kimball based. Complementary to these alternatives approaches, is the technique of Data Vault. Data Vault  was originally created by Dan Linstedt and is especially popular in Northern Europe. Linstedt defines a data vault as a detail oriented, historical tracking and uniquely linked set of normalized tables that support one or more functional areas of business. Absent from this definition is the attempt to use data vault for cleansed or integrated data. No effort is made to reconcile data model differences across source systems. Instead, data vault is depending on downstream architectural components for presenting analysis-ready data. Data vault addresses the inflexibilities of both Inmon and Kimball via hubs, satellites and link tables. Via separated tables, data vault is more decoupled, allowing for parallel data loads and more flexibility when carrying out changes. Relationships can easily be dropped and recreated on-the-fly, and new systems can be integrated into the existing model without any disruption. These flexibility improvements also come with a downside: Data vault itself is not directly suited for ad-hoc queries and analysis, because querying all the hubs, satellites and links, joining and them together, is performance intensive. In most cases it requires an additional presentation or data mart layer. The number of tables within Data Vault is significantly higher than 3NF modelling. It requires that a higher number of ETL jobs are needed to ensure the segregations and referential integrity between the different tables. This drives up the complexity and makes it more difficult for engineers to understand how the data warehouse is designed. Companies, like  Wherescape,  can help to address these problems, by generating the ETL jobs via metadata. One last aspect is that there’s no strict or formal standard. Data vault has been described conceptually well, but people use implementation variations.  Dan Linstedt ,  Kent Graziano  and  Hans Hultgren , all experts in this field, advocate data vault dissimilarity, which introduces a risk of having conflicting style applied in larger projects. The list below contains the high-level benefits and downsides from the data vault. Benefits Downsides Data warehouses distinguish themselves from other applications because of their integration layers and complex data models. They consume, combine, harmonize and integrated data. These integration techniques require a data-modeling expertise, which is typically different from designing and developing regular applications. Data warehouses cannot be created without pulling data from different sources. In the previous sections we learned about the role of staging areas to facilitate data to come together in different formats. What we haven’t discussed are the formats or structures in which these data sources are coming in. A commonly seen format of delivering the data is to use the raw (identical) format of the source systems. Typically full collections of data are delivered, so what you end up with are complete raw extracts of all tables in the form of flat files. A drawback of a this delivery style is that there’s high coupling with the inner data structures from the source system(s) as they are the same representations. If a source system changes its structure, processing the data might no longer work. An alternative method of delivering data towards data warehouses is to agree on some form of abstraction that reduces the coupling risks. Structures can be agreed to be more simple, they can have a fixed format. These changes at least take away that processes break whenever source systems change their structure. A more extreme form of abstraction is asking source systems to conform their data deliveries to the format of the integration layer. The files from the source systems in this method must be delivered in the exact same format of the data warehouse. I consider this form of abstraction as high coupling as well, because it requires all sources to change simultaneously when the integration layer changes. Note:  Most commercial vendors (Oracle, SAP, Siebel, etc.) use highly specialized and normalized data models for their operational systems, which change with every release cycle. These data models are difficult to interpret. People make a decent living knowing how to interpret and map these data models into data warehouses. To make data integration easier typically a standardized format is chosen. Comma-separated values (CSV) is one of the most popular choices, because the majority of all databases and ETL tools can work with this format. XML and JSON can also be used, but these formats in general produce more overhead, since a lot of metadata is included. An alternative to using full extracts is requesting or pulling out data based on data changes or deltas. Only changed data is delivered in this approach. A popular design pattern is to use CDC. CDC is well supported by the different vendors and can work with various mechanisms: timestamp-based, version-based, indicator-based, and so on. The next step, after all data has been collected, is to integrate data into the integration layer. Data needs to be cleansed, mapped and transformed into the unified data model of the data warehouse. This requires ETL tooling and can be done via several approaches. The most common way to pick up data from staging areas, transform and load into the integration layer using a commercial ETL tool. Informatica, SQL Server Integrated Services, Ab Initio, Infosphere Information Server, Talend, Pentaho and Oracle Data Integrator are popular choices and seen within many large enterprises. These tools have a wide number of features, large number of database connectors, debugging and testing capabilities and often an intuitive look and feel. They can extract the data into their own local environment or delegate the work to the target database. A common heard drawback of these commercial tools is tight coupling. Once you choose an ETL vendor for your data warehouse, you’re married with that vendor forever. The alternative for ETL tools is to build an ETL framework yourself. I have seen a number of companies, who generate ETL code (Java, Oracle’s PL/SQL, Teradata’s SQL, etc.) from metadata repositories. Coding and storing all the integration logic in a source code repository, such as  git,  is also not uncommon. Some companies try to achieve what commercial vendors are providing, including intuitive web-based frontends, debugging capabilities, track and trace of data, lineage, etc. A benefit of building an ETL tool yourself is looser coupling. As long as you adhere to the commonly used open standard ANSI SQL you could potentially replace the database technology of the data warehouse at any time. The “do it yourself” scenario requires highly skilled engineers for maintenance and development. There is a potential risk of seeing engineers leave over time. Resources with ETL knowledge of the commercial vendors is in general more easy to attract. ETL processing within a data warehouse ecosystem typically doesn’t allow for a lot of variations. In general a single product or framework is used. It is also high likely, due to complexity, ETL tools are scheduled to run in fixed time windows. Processing times are typically long, because of performance reasons. Another reason why the ecosystem is static, is that many enterprises are terrified to make big changes to ETL: code is often large and enormously complex, because it has been evolved through years of development. After the introduction of this blogpost I mentioned that some technology trends changed the way data warehouses can be built. One of these technologies is  Data Virtualization : data remains in place, and sources can accessed and viewed as a single database, without the batch-oriented need for physically extracting and duplicating data. In detail there is a data virtualization layer, which presents itself as an integration layer that caches data, by extracting, transforming and integrating data virtually and at runtime. This technology comes with an application component: a virtualization engine that handles all the incoming queries, determines the best strategy for caching, knows how to access different data store technologies, knows how to integrate data, and so on. Under the hood there is a lot of metadata for building the abstraction. I have visualized this technique in the image below. Although this technique looks attractive it has potential drawbacks: Note:  Some database vendors provide a database  (virtual) query layer , which is also called a data virtualization layer. This layer abstracts the database and optimizes the data for better read performance. Another reason to abstract is to intercept queries for better security. An example is  Amazon Athena . Despite the drawbacks data virtualization can help in migration scenario’s. Making the database virtual and part of the new capability can lighten the pain. Data virtualization is also real-time, thus faster for relatively small amounts of data, compared to ETL tooling, which needs to process and persist all data, before consumption can take place. The last capability we didn’t discuss in too much detail yet are  Business Intelligence (BI)  tools, which are used to generate insights and present results for smooth business decisions. They became quite popular as data warehouses started to emerge with more and more data. They also expanded their capabilities throughout the years. First versions only included simple reporting and dashboarding functions, but later generations offer strong visualization capabilities, support self-service, intelligence caching techniques and predictive analytics. Business intelligence thus doesn’t exclude (predictive and prescriptive) advanced analytics: Both disciplines are complimentary and can strengthen each other. Modern business intelligence tools are also quite flexible with the number of sources they can work with. Traditionally BI tools were more closely coupled to data warehouses, but later generations are capable of pulling out and combining data from multiple and a higher variety of source systems. BI tools can also pull only out the changes and persist data longer into what are called  Cubes . These cubes have some similarities with data virtualization, because they also abstract the underlying sources and can pull data our directly or cache it. What are the differences between OLAP, ROLAP, MOLAP, and HOLAP?  Cubes, also known as OLAP cubes, are pre-processed and pre-summarized collections of data to drastically improve query time. OLAP (Online Analytical Processing) refers to specialized systems or tools that make data easily accessible for analytical decision making. OLAP cubes are logical structures as defined by the metadata. Multidimensional expressions, or MDX, is a popular metadata-based query language that supports you to query these OLAP cubes. Vendors offer a variety of OLAP products that you can group into three categories: ROLAP (Relational Online Analytical Processing):  ROLAP products work closely together with the relational databases to support OLAP. Often a star schema structure is used to extend and adapt an underlying relational database structure to present itself as an OLAP server. MOLAP (Multidimensional Online Analytical Processing):  MOLAP products provide multi-dimensional analyses of data by putting it in a cube structure. Data in this model is highly optimized to maximize query performance. HOLAP (Hybrid Online Analytical Processing):  HOLAP products combine MOLAP and ROLAP, by using a relational database for most of the data, and a separate multi-dimensional database for the most dense data, which is typically a small proportion of the data. Modern BI tools also can keep the data close with them, by allowing you to integrate (ETL) and persist data inside their “storage layer”, rather than integrating and storing it inside the data warehouses. My personal recommendation is to store data in only in the data warehouse, when it is subject to reuse. For shorter term and one off integration exercises integrating inside the business intelligence solution is more recommended. You have learned a lot about how data warehouses work, the integration layers are organized and how data is ingested and consumed. In the next section we will discuss the data lake, which some people call the second generation data repositories. As data volumes and the need for faster insights grow, engineers started to work on other concepts.  Data Lakes  started to emerge as an alternative for access to raw and higher volumes of data. By providing data as-is, without having to first structure the data, any consumer can decide how to use it, and how to transform and integrate it. Data lakes and data warehouses are both considered centralized (monolithic) data repositories, but they differ because data lakes store data before it has been transformed, cleansed and structured. Schemas therefore are often determined when reading data, rather than loading data in a fixed structure. They also support a wider variety of formats: structured, semi- structured, and unstructured. A bigger difference between data warehouses and data lakes are the underlying technology used. Data warehouses are usually engineered with relational database systems, while data lakes are typically engineered with distributed databases or NoSQL systems. Hadoop is a popular choice, because it runs on commodity hardware and therefore can be cost effectiveness. It is also highly scalability, open-source, and modular with a large variety of database types and analytical frameworks. Using public cloud services is also a popular choice for building data lakes. Recently distributed and fully managed cloud-scale databases, on top of  container  infrastructure have simplified the task of managing centralized data repositories at scale, while adding advantages in elasticity and cost. Many of the data lakes, as pictured in the image above, collect pure, unmodified and raw data from the original source systems. Dumping in a raw —  exact copies of  — data is fast and enables data analysts and scientists to have quick access. However the complexity with raw data is that use cases always require to rework the data. Data quality problems have to be sorted out, aggregations are required and enrichments with other data are needed to bring the data into new context. This introduces a lot of repeatable work and is also why data lakes are typically combined with data warehouses. Data warehouses, in this data lake combination, act as a high quality repositories of cleansed and harmonized data, while data lakes, act as analytical environments, holding a large variety of raw data to facilitate analysis, such as data discovery, data science and processing unstructured data. When combined, outcomes or insights flow back into the data warehouse, while the data lake is built for raw data and (manually) operated. By knowing what a data lake is we can move to the next section, in which I want to discuss the scale in which data warehouses and data lakes can be applied within organizations. As people started to see the benefits of data warehouses, the initiatives and scopes became larger. Companies instructed people to go to training facilities and prepare to start building something massive. Business users were asked to deliver all their requirements to the central data warehousing engineering teams. Large programs were initiated to deliver  Enterprise  Data Warehouses (EDWs). An EDW is a central data warehouse with the objectives to create a single version of the truth and service all reporting and data analysis needs of the  entire  organization. Although the image below is a high-level representation of an enterprise data warehouse, you can see the complexity of many integration points, steps of integration and dependencies. Once enterprise data warehouses start growing, their complexity grows exponentially. Before you know you end up in an uncomfortable situation where changes are seen as bottlenecks. What I want to do in the next sections is to discuss a number of widespread observations and  failure modes . Before we continue, I would like to ask you to take a deep breath and put your biases aside. The need for data harmonization, bringing amounts of data into a particular context, always remains, but something we have to consider is the  scale  in which we want to apply this discipline. Is it really the best way to bring all data centrally before it can be consumed by any user or application? EDWs require data providers and data consumers to agree on a single (enterprise) canonical data model. This single version of the truth is, to my mind, the biggest problem why companies can’t scale. In large ecosystems many different contexts exist. Building an integration layer that accommodates everybody’s needs is a big challenge, because it requires everybody to agree. The larger the company, the more conflicts you will see, the longer it takes to agree and align. My second biggest problem with the harmonization at scale is that value is lost. Engineers are required to make interpretations when building up data warehouses. For example, what is a  start date  of a contract? There are quite a number of possible answers. The  start date  can be the legal signing date of a contract, it can be the system’s booking data. The start date can also be the approval date given by the legal department. It can be a date of when the usage started or the starting date of when the first payment was made. Chances are relatively high that the meaning of these definitions differ across different departments and are implemented specifically within systems. We either end up creating  many variations  or accept the differences and  inconsistencies . The more data we add, and the more conflicts and inconsistencies in definitions arise, the more difficult it will be to harmonize. Chances are that you end up with a unified context that is meaningless to everybody. Another problem is that data, and thus context, is thrown away during integration process. Very specific value ranges from one domain are mapped to ranges with fewer details in favor of another domain, aggregations are made out of details or fields are left out. For advanced analytics, such as machine learning, leaving these details out is a big problem. Analytical models, such as machine learning, work more precisely with detailed data. A second concern, when using data warehouses at a large scale, is the need for an additional transformation step. Data always has to be integrated first into the data warehouse, before it can be used. This step means a lot of waiting time, because in a data warehouse there is inheritance coupling between almost all components. Tables have references to other tables, and tables have dependencies to ETL jobs. A change in one table usually forces a ripple effect of changes in other tables and ETL jobs. This complexity requires a lot of work and increase waiting time, which starts to make people creative. What is a Big ball of mud?  A “ big ball of mud ” is a haphazardly structured, sprawling, sloppy, duct-tape-and-baling-wire, spaghetti-code jungle. It is a popularized term that has been first coined by Brian Foote and Joseph Yoder. A big ball of mud describes a system architecture that is monolithic, difficult to understand, hard to maintain, and tightly coupled because of its many dependencies. The image below shows a  dependency diagram  that illustrates this. Each line represents a relationship between two software components. Data warehouses with its integration layer, countless tables, relationships, scripts, ETL jobs and scheduling flows, often end in a chaotic web of dependencies. These complexities are such that you often end up, after a while, with what they call a big ball of mud. An engineer might propose for the sake of time to bypass the integration layer, by directly mapping data from a source system to a data mart. Another might suggest building a data mart on top of two different, already existing, data marts, since changing the integration layer takes too much time. This  technical debt  (future rework) will cause problems later. The architecture becomes more complex and people loose insight in all the creativity and shortcuts created in order to deliver on time. Data warehouses are characterized by their exponential complexity in terms of the number of sources and consumers. The more sources you add, the more difficult it becomes to make changes. This same principle applies for the number of data consumer entry points. If you have many data marts sitting on top of the integration layer, making changes to the integration layer can be quite impactful, because its changes have impact on data marts, which need to be coordinated closely with all data consumers. What I also have seen is that there is a ’ tipping point ’: once you reach a certain number of sources and data marts, the agility drops to almost zero. Integrating a huge number of sources requires tremendous coordination and unification. Release cycles of several months aren’t exceptions for complex data warehouse systems. Because of the tight coupling and long release cycles for changes, stakeholders are queued up, and have to agree on the priorities. I have seen long discussions between engineers and business users to agree on the priorities of new data sourced into the data warehouse or changes to be made to the integration layer. These discussions don’t add any direct business value. Agility problems of monolithic platforms introduce a consumer habit of impatience. Adding data to EDWs and data lakes can take long, so users start to ask for  all  data to be consumed. Having access to all data takes away the risks of waiting any longer. I have seen data marts that contain almost all data from the integration layer. The storage space sum of all data marts becomes a multiplier of all storage required by the integration layer. This doesn’t make the architecture cost effective. I have also seen business and IT users accessing both data marts and the integration layer at the same time. Users building views on top of data marts and integration layers to get their needs quickly fulfilled. These views were exposed again to other users. Eventually an endless cascading layering effect is seen. Users are pulling out data without having any clue where data is exactly coming from. Often no requirements are set by consumers. Data is provided because business users are demanding, otherwise escalations will follow. If timelines aren’t met, often ad-hoc interfaces are created. Another problem of operating data warehouses and data lakes at a large scale is that engineers and IT users are required to coordinate data deliveries. Sources might change and interfaces get broken, which requires escalations to get things fixed. I have also seen situations where engineers were taking care of the problems themselves. For example, fixing data in the staging layer in order to have data properly loaded into the data warehouse. These fixes became permanent and over time hundreds of additional scripts had to be applied before data processing can start. These scripts aren’t part of trustworthy ETL process and can’t be tracked back. It is expected this has a significant effect on the lineage and transparency. Data governance is also typically a problem, because who owns the data in data warehouses? Who is responsible if source systems mess up and data gets corrupted? Many of the source system engineers point to the data warehouse engineers when something goes wrong, because they transform data to something system owners do not know. Visa versa, data warehousing engineers blame source system owners for delivering incorrect data. Data quality is often a point of discussion as well. Because after the data has been transformed it is unrecognizable for the source systems owners. They blame the engineers for bad data quality. In many of the implementations I have seen the data warehouse engineers taking care of data quality. Data is fixed before or after loading it into the data warehouse. Especially for integrity reasons it is important to have the relationships between the tables setup properly. But what data is the truth? Users start comparing figures and results from operational systems with data warehouse figures. Nobody knows what the truth exactly is. Data quality between operational systems and data warehouses might diverge so badly that nobody has trust in data. Data life cycle management of historical data is also an issue. Because EDWs are seen as the archive of truth, operational systems cleanup irrelevant data knowing data will be retained in the warehouse. What if the operational systems need to make an analytical reports based on their own historical data, what happens then? In many cases ODSs would fulfill this need, but I have also seen companies using EDWs for this purpose. This brings up an interesting pattern. First, operational data is moved and transformed into a harmonized model. Second, that same data is transferred and transformed back (reverse engineered) into its original context. For operational advanced analytics historical data often is a requirement. It is expected to be fast available, because windows of opportunity are often fleeting. Using a data warehouse for this purpose is often a problem, given that they have to process data typically for many hours. Keeping and managing data centrally is a problem, because the organizational needs are very diverse and have a high variety: different types of solutions, teams with different sizes, different types of knowledge, different requirements varying from defensive to offensive, and so on. Keeping everything together is a big problem, because of proliferation. EDWs and data lakes are tightly coupled with the underlying chosen solution or technology, meaning consumers requiring different read patterns are always required to export data to other environments. As the vendor landscape changes and new types of databases pop up, these monolithic platforms are getting more scattered, always forced to export data. This trend undermines the concept of efficiently using a ‘single central repository’. Consequently point solutions are created and the underlying hardware of the data warehouse is only used ETL processing and data persistency. This is a true waste of money, because date warehouse systems use in general very expensive hardware, optimized for intensive querying. After data has been carried, and is enriched, it is also typically distributed further. This means data consumers start acting as data providers. For data consumers in a large scale environments this situation might be quite confusing, because where did data originate? Some consumers start consuming without having the awareness that data is also being consumed indirectly. This makes the chain longer, and therefore more fragile. This further distribution and proliferation of data also introduces another problem. Many data consumers don’t know where to find the right data, because data distribution is scattered throughout the organization. Is the operational system the right place to look for? The enterprise data warehouse? The data lake? Or perhaps another data consumer, who may have already slightly better prepared the data? Users use the quickest route, the one with the shortest distance and least resistance, to the data. EDWs and data lakes grow into a  spaghetti  architecture. Others call it a big ball of mud. Uncoordinated changes, unregulated growth, and distribution of data harm the overall enterprise data architecture. Consistency, insights, and most important agility, are lost. EDWs and data lakes often lack insight in ad-hoc consumption and further distribution, especially when data is carried out of the DWH ecosystem. With new regulation, such as GDPR or CCPA, insight into the consumption and distribution is important, because you want to explain what personal data has been consumed by whom and for what purpose. Logically where the creation and origination of the data starts, responsibilities should start. The problem with enterprise data warehouses is that data providers have no control and limited insight. There is no way to control data consumption, nor there’s insight in further distribution of the data or the purpose where data is being used for. Within many organizations the creation of APIs for service orientation is handled via an enterprise canonical model by a separated team. This is strange because both service orientation and EDW use and rely on the same operational systems. A separate implementation creates two different versions of the truth. This can result in two camps in the organization and two different vocabularies with different meanings of data. The original sources and their context however, are still the same. This may seem like a bit of an exaggeration, but enterprise data warehouses also have evangelists. Most data initiatives started with the best intentions, but when times were less busy, engineers continued mapping data. Integrating data was no longer done for the sake of data consumption, but only for its own sake. Data integration became a hobby. The side effect is that various data warehouse became unnecessary complex and too difficult to maintain. Some of these evangelists were only discussing the modelling techniques, rather than paying attention to business users and their requirements. Business users felt not fully understood, so lack of trust was created. Another problem is how the enterprise data warehouse and data lake teams are assembled with traditional software engineers. Many come with a database administrator (DBA) background and are used to design monoliths. They are trapped in the past and lack today’s modern skillsets like domain-driven design, distributed and evolutionary design thinking, data versioning,  CI/CD  and automated-testing experience, and so on. Scaling up and fulfilling today’s modern requirements is consequently a challenge. Many EDWs are often build upon ‘prefabricated’, off-the-shelf, industry-specific data models. IBM, Teradata, Oracle and Microsoft, for example, provide these data models for different industries. The problem I have with these industry-specific data models is that they are the  vendor’s  version of the truth, not yours. Many of these models are too detailed and full with assumptions. These details require you to fill up many empty tables with only keys. Consequently the integration layer becomes extremely large and complex. There is also an approach of combining EDWs, data lakes, with data virtualization. They are “complemented” with an additional virtual layer of abstraction. Underlying integration complexity is abstracted and new integrated views of data are created. These can span across operational systems and many other applications as well. Henry Cook, research director in Gartner, calls this design pattern a  Logical Data Warehouse . Although not having to add new data to the enterprise data warehouse gives short terms benefits in terms of agility, longer term it will be a catastrophic failure. First, it is tempting to substitute complex integration logic by abstracting, but all the underlying complexity is still there, including all the operational and technology overhead. Second, it expands the data platform with direct bypasses to operational systems and ad-hoc data sources, while directly reusing parts of the data platform. Many of these operational systems, as you have learned, aren’t optimized for intensive read access and can’t retain larger amounts of historical data. Third, there is tight coupling with both operational systems, the data platform and a virtualization product. Changes to the underlying physical structures of data platforms or operational systems, that are not carefully coordinated, will immediately break the logical layer. Lastly, all data integration is still “funneled”, because data virtualization is just another ETL tool for maintaining the enterprise data model. Although technology has changed, the whole concept around combining data virtualization with data lakes, enterprise data warehouses, and funneling all ETL is the root of the problem. Data is moved to a monolith, requiring everybody to wait for, all changes to be coordinated precisely, without allowing use cases to optimize and choosing a fit for purpose technology that suits the use case requirements best. Many companies have put all their hope in big data ecosystems, which pulls together raw and unstructured data from various source systems, with probably EDWs attached to it as well. Analytical use cases are expected to run in the lake and consolidated views are expected to land in the EDWs. This approach carries a number of challenges: The challenges I summed up are just a few reasons why the failure rate of big data projects is so high. Management resistance, internal politics, lack of expertise, and security and governance challenges are a few of the other reasons why analytics never made it to production. One of the biggest problems with building a centralized data platform, like EDWs or data lakes, is how the teams are organized: people with data engineering skills, are separated from the people with domain and business knowledge. Those people are, in many cases, also the people producing and knowing the data. By siloing all the data professionals, one stovepipe is created. All the knowledge is siloed, allowing no other teams to leverage from data. Data life cycles and heart beats have to be adjusted to meet what centrally is dictated. One team owns all infrastructure, which is only available to that central team. One single team has to change everything, run everything, maintain everything, fix everything and so on. It is this stove pipe that hampers organizations from scaling up. Data warehouses are here to stay, because the needs to harmonize data from different sources within a particular context will always remain. Patterns of decoupling by staging data won’t disappear. Just like the steps of cleansing, fixing and transforming schemas. Any style of Inmon or Kimball can be well applied, depending on the needs of the use case. Technology trends, like metadata-driven ELT, data virtualization, cloud, distributed processing, machine learning for enrichments will change data warehousing, but not in a negative way. The way we want to manage our data is something we have to consider. Big silos, like  enterprise  data warehouses will become extinct, because they are unable to scale. Tightly coupled integration layers, loss of context and intenser data consumption will force companies to look for alternatives. Data lake architectures, which pull in data raw are the other extreme. Raw, polluted data, which can change any time, will force experiments and use cases to never make it into production. Raw on itself carries a lot of repeatable work with it. Also the siloed and monolithic thinking is a problem, which doesn’t allow companies scaling up. Siloing data professionals creates stovepipes. Separating from the people with domain and business knowledge results into coordination problems. What is needed is a balanced and well governed data management environment, which allows for variation by using a mix of technologies. It must give domains the insight and flexibility to adapt and distribute data, while staying decoupled at the same time. How the modern trends are effecting data integration and what type of architecture you need for scalability is something that I’ve outlined in a series of articles and in the book:  Data Management at Scale . towardsdatascience.com towardsdatascience.com towardsdatascience.com towardsdatascience.com piethein.medium.com piethein.medium.com"
Data Modeling in AWS DynamoDB,ata Modeling in AWS DynamoD,"Nowadays, storage is cheap and computational power is expensive. NoSQL leverages this fact and sacrifices some storage space to allow for computationally easier queries. Essentially, what this means is that when designing your NoSQL data…"
DynamoDB: Data Modeling,ynamoDB: Data Modelin,"This is post #3 of the series aimed at exploring DynamoDB in detail. If you haven’t read the earlier posts, you can find the links below. This post will aim to present some efficient data modeling techniques and some key considerations to be noted while creating and accessing DynamoDB tables. If you are not interested in reading through the entire blog and want to jump to the summary straight away, click  here A few Q&A to start off with. In DynamoDB terms, A good Data Model is key to the performance, and consequently the scalability, of your DynamoDB table. For the purposes of illustration, let’s consider a table named “ Landmarks ” with the following data structure. The table lists the hotels in key cities in the UK, along with key landmarks around the hotels. Assume, there are 60000 items in the table and every 10000 records constitute a data volume of 10 GB. Total size of the table = 60 GB. When you create a DynamoDB table, it is mandatory that you choose an attribute as the Partition Key of the table. This allows DynamoDB to split the entire table data into smaller partitions, based on the Partition Key. So, the table shown above will be split into partitions like shown below, if  Hotel_ID  is chosen as the table’s Partition Key. Now, assume you have a read operation like the one below. The arguments for get-item are stored in a JSON file named “key.json”. Here are the contents of that file: The read path for this operation will look like the one shown below. As can be seen above, DynamoDB routes the request to the exact partition that contains Hotel_ID 1 (Partition-1, in this case). How does it do this?  As mentioned  above , DynamoDB splits table data into partitions based on the Partition Key. This happens in several steps: When there is a read request, the following happens: This is the ideal scenario and every request should target hitting no more than one Partition. The ideal scenario is to have every request hitting no more than one Partition. So, it does look like our choice of Partition Key is a good one. But, let’s consider another case. Assume, “ London ” is the favorite destination for the users of your service. Say, requests for hotels in London contribute to approx. 60% of your total traffic and the rest of the destinations have more or less even load. Below is how your Partition usage might look like. This is called “ Hotspotting ”*. Even though you have data distributed evenly across Partitions, the workload on the table focusses most of the traffic on only one Partition while the other Partitions are relatively unused. This is not desirable. Why?  DynamoDB model mandates that you provision throughput at the table level. Say for example, when you create your table, based on the expected workload you provision 6000 RCU and 2000 WCU for the table**. Now, if your table contains 2 Partitions, then this throughput will be split evenly among the two Partitions, as shown below. **  This illustration and the rest of the sections in this article are with respect to the provisioned capacity mode only and do not consider the new  on-demand capacity mode . Please refer to “ on-demand capacity mode ” section for key considerations related to this mode Assuming a total workload is 6000 4KB reads/sec, if the workload is evenly distributed, each partition will serve 3000 reads/sec without any throttling. But, if the workload is not evenly distributed (hotspotting), then one partition can get more requests than the other. Say, for example, one partition (P1) gets 3500 reads/sec and the other (P2) gets 2500 reads/sec. Requests to P2 will be served without any issues as they are well under the provisioned throughput allocated to that partition. But, requests to P1 exceed the throughput allocated and hence will be throttled. Note:  DynamoDB has “ adaptive capacity ” enabled by default which can reassign unused provisioned throughput in P2 to P1 but that will not save the day in this case because of 2 reasons: The choice of Partition Key can lead to “hotspotting” and throttling of requests This is exactly why a proper Data Model is key to the performance and scalability of your services. *  Refer to  avoid hotspotting  for possible solutions to the hotspotting problem One of the fundamental differences between a relational database model and a NoSQL model is that, while the key goal of relational model is to offer the ability to make any query that can dynamically bring out the relationship between entities, the goal of NoSQL model is to serve a set of your most important predefined queries, as fast as possible. So, the fundamental requirement to a good Data Model (with DynamoDB) is to identify the queries that are most important to your service (i.e.) your service’s “access pattern” Let’s consider the example of the same “Landmarks” table specified earlier in this post and we’ll look at how the table’s Data Model should change based on your service’s access pattern. Assume, your access pattern is to get all the attributes of an item, given a hotel id and a landmark’s description. For this access pattern, you can design your data model in either of the two ways, shown below. You can have “Hotel_ID” as your Hash Key and “Description” as your Range Key or vice versa. So, which one would you choose? Never choose an attribute with low cardinality. As can be seen above, when you partition by “Hotel_ID”, a query to retrieve all landmarks around a given Hotel_ID has to go only to one Partition. On the contrary, if the table is partitioned by “Description”, then the same query has to go to multiple partitions in order to get the same results. The scatter-gather effect observed with the latter is detrimental to the performance of queries and hence will not be recommended, for the given access pattern. One of the ways to avoid “hotspotting”, is to not choose a low cardinality attribute as your table’s Partition Key. Despite that, due to the nature of your workload, if you still face “hotspotting” in your dataset then “Write Sharding” might be a good option for you. Let’s continue with the same access pattern, as mentioned above — get all the attributes of an item, given a hotel id and a landmark’s description. But this time, assume hotels in London receive 60% of the total traffic. This leads to “hotspotting”. In this case, “Write Sharding” might be a good option, if: You can add a random number to the partition key values to distribute the items among partitions (example: 1_12345, 1_23456, etc., for the same Hotel_ID 1), or you can use a number that is calculated based on something that you are querying on (example: 1_nationalgallery, 1_buckinghampalace, etc.,). Add a salt value to the partition key values to distribute the items among partitions Below is the data model and the corresponding partition view of the “Landmarks” table with “Write Sharding”. Please note, while this might be a good option if you are reading/writing one item per request, this might not be a most efficient option if you need to retrieve all the items for a given partition key. As you can see above, given that data belonging to a single partition key is distributed across multiple partitions, read requests have to be executed against different partitions and the results merged together in the end, to send a compiled response to the client. This is not very efficient. The cost of a DynamoDB table can be split into 3 major components: As your table grows in size over time, the storage costs keep increasing. In addition, as the volume of data in the table grows, the table is split into smaller partitions, with each partition getting a portion of the table’s provisioned RCU/WCU. So, if you have a table that stores, say time-series data for example and reads/writes are done on only the most recent day/month/quarter/year, then it is advisable to delete/archive older (unused) data, for the following reasons. The below diagram illustrates this effect on a sample table containing time-series data. As can be seen above, if you have large volumes of outdated/unused data in your table, you could be paying several times the cost of your actual usage. If you have large volumes of outdated/unused data in your table, you could be paying several times the cost of your actual usage. So, when you’re designing your data model, if you know you’re going to have data that will not be actively used after a period of time, it is recommended to do one or more of the following: By default, all DynamoDB tables are local to the region in which it is hosted. But, if you have a use case where multiple systems in different regions are writing to the same Table, then Global tables might be a good option for you. Two things to keep in mind when creating Global tables: As far as the reads and writes are concerned, there is no difference in the performance of reads and writes to a Global table as against a standard table. Services in a given region write to the Global table replica in that particular region only and this is eventually asynchronously replicated to the other replicas in the other regions. Within a given region, the Global table read and write performance should not be different from the performance of a standard table. But for multi-region requests, Global tables would be much faster because they read from/write to replicas in the local region. Global table read and write performance should not be different from the performance of a standard table. A comparison of the performance of a standard table, as against a Global table in a multi-region setup is illustrated in the below figure. As can be seen above, for multi-region requests, Global tables can make reads/writes much faster as compared to Standard tables. Please note, Global tables are more expensive than standard tables. The additional cost comes from the following factors. If you refer to the above figure(s) for example, for a single write into a Standard table, you would require a provisioned capacity of 1 WCU. In the case of Global tables, the number of rWCUs required for a single write = 1 rWCU (for write) + 2 rWCU (for replication) = 3 rWCUs. Accommodating for conflict resolution, you would require 3 * 2 = 6 rWCUs for a single write. This can quickly add up and make global tables several times more expensive than a standard table Amazon has recently ( Nov 2018 ) announced a new “On-demand Capacity Mode” for unpredictable workloads. As per the announcement: It promises to help serve thousands of requests/second without any capacity planning. How is this possible and how different is it from the current “Provisioned Capacity mode”? To illustrate the differences between these two modes, consider the below example of an e-commerce website. On Black Friday, assume the website suddenly receives an unexpectedly large volume of traffic, say 2000 RCUs and 500 WCUs. In this case: This kind of scenario can put the website availability in some serious trouble. In cases such as above, On-demand capacity mode can come in handy. With this mode: So, in the Black Friday example above, DynamoDB will automatically increase throughput provisioned as the workload increases and you would not expect any downtimes. That’s fantastic, isn’t it?! As per Amazon’s  documentation , below are the key differences between this new mode and the current Provisioned capacity mode. As you can see above, the recommendation is to use On-demand mode only for unpredictable workloads. If On-demand is so good at dynamically handling workload increases, you might ask, why use Provisioned capacity mode at all? The answer to that question is  Cost! The above table illustrates the differences in cost between On-demand and Provisioned capacity modes for a sample write workload. As can be seen above, the cost of your writes in the On-demand mode can increase to nearly ~7 times (3240/468 ~= 6.9) your cost in the Provisioned capacity mode. The cost of your writes in the On-demand mode can increase to nearly ~7 times your cost in the Provisioned capacity mode So, while On-demand capacity remains an extremely useful option for unpredictable workloads in the short term, it might not be the best mode of operation for all workloads. Poorly designed data models can still result in throttling even in On-demand capacity mode. Please note, while On-demand capacity mode rapidly accommodates for changing workloads, it still does not guarantee zero throttling. Poorly designed data models can still result in throttling even in On-demand capacity mode. Below is one example that illustrates this issue. Currently, you cannot use both modes on a table - it is either Provisioned or On-demand Capacity Mode. However you can switch between the two - so, in the example above you would use Provisioned capacity normally, but on expected spikes like Black Friday, you could switch to On-demand mode and then back again after the bursty period has ended. I hope this article gave you a reasonable insight into designing data models for DynamoDB tables. The next article —  part 4  of this series will focus on the guidelines to be followed for faster reads/writes in DynamoDB."
The rise of SQL-based data modeling and DataOps,he rise of SQL-based data modeling and DataOp,"Update: Confused about the complex analytics landscape? Check out our book:  The Analytics Setup Guidebook . Originally published by Thanh, our Chief Engineer at  https://www.holistics.io  on November 15, 2019. If there is one analytics trend we can all agree on, it is that we’ve seen a vast increase in the amount of business data collected in the past decade. A typical business today uses dozens if not hundreds of cloud-based, subscription-as-a-service software, for all sorts of purposes. Together with more traditional in-house enterprise software, the average business produces a mountain of data today, compared to just a few years ago. The savvier businesses are the ones who have found great value in analyzing their data, in order to make informed decisions, causing a tremendous uptick in both the demand and the supply of data management systems. Up till a few years ago, the traditional way of managing data (in SQL-based RDBMSs) was considered a relic of the past, as these systems couldn’t scale to cope with such a huge amount of data. Instead, NoSQL systems like HBase, Cassandra, and MongoDB became fashionable as they marketed themselves as being scalable and easier to use. Today, there seems to be a resurgence of SQL-based database systems Yet today, there seems to be a resurgence of SQL-based database systems. According to Amazon’s CTO, its PostgreSQL and MySQL-compatible Aurora database product has been the “fastest-growing service in the history of AWS”. SQL-based MPP (massively parallel processing) data warehouses, which used to be expensive and could only be purchased by big enterprises, have now moved to the cloud and are increasingly commoditized. All the major cloud providers today have their own MPP data warehouses: Google has BigQuery, Amazon has Redshift and Microsoft has Azure Data Warehouse … and that’s not to mention Snowflake — an independent data warehouse vendor that was recently valued at $3.9 billion. Everyone can now spin up their own data warehouse … and pay by the minute, which was unheard of just a decade ago. These developments have brought tremendous power and unprecedented access to data analysts, for they may now execute complex analytical queries and get the results in seconds or minutes instead of hours like with traditional data warehouses. For data analytics, the reasons to this resurgence are simple to understand: SQL, on the other hand, is ubiquitous and standardized. Almost every data analyst is familiar with it. In Google’s own words about their massively scalable in-house data management system, Google Spanner: The original API of Spanner provided NoSQL methods for point lookups and range scans of individual and interleaved tables. While NoSQL methods provided a simple path to launching Spanner, and continue to be useful in simple retrieval scenarios, SQL has provided significant additional value in expressing more complex data access patterns and pushing computation to the data. Spanner’s SQL engine shares a common SQL dialect, called “Standard SQL”, with several other systems at Google including internal systems such as F1 and Dremel (among others), and external systems such as BigQuery… For users within Google, this lowers the barrier of working across the systems. A developer or data analyst who writes SQL against a Spanner database can transfer their understanding of the language to Dremel without concern over subtle differences in syntax, NULL handling, etc. Together with the rise of cloud-based MPP data warehouses, SQL has now become a very attractive way to do data analytics. Capitalizing on this trend, startups like Mode Analytics, Periscope Data and open source tools like Redash became more popular. For analysts which are proficient with SQL, they can now utilize the power of their brand new cloud-based data warehouses to produce beautiful charts and dashboards without the need to learn any proprietary language or tool. Furthermore, SQL code is just text and thus can be stored in a version control system, making it trivial to manage. However, using SQL for data analytics is not all sunshine and roses. Any tool that relies heavily on SQL is off-limits to business users who want more than viewing static charts and dashboards but don’t know the language. It forces business users to fall back to old-school spreadsheets, where they export raw data to Excel and perform manual calculations on their own, causing problems around accuracy and data consistency down the line. SQL is also too low level, and queries quickly become convoluted when more complex analysis is required. Most SQL queries read just fine when the analysis requires a couple of tables. But when the number of related tables increases, data analysts are forced to keep track of multiple types of joins and choose the right one for the right situation every time they write a query. This implies that SQL is not reusable, causing similar code with slightly different logic to be repeated all over the place. For example, one cannot easily write a SQL ‘library’ for accounting purposes and distribute it to the rest of your team for reuse whenever accounting-related analysis is required. The traditional way of providing BI to non-technical users is to give them a drag-and-drop interface … after an analyst prepares a data set via a  data modeling process . The process works by allowing the analyst to set up mappings between business logic and the raw underlying data. By exposing the high level business concepts to end users via the software’s interface, it enables them to do data exploration on their own without learning query languages like SQL. This process started a few decades ago with vendors such as Cognos, SAP BI, and so on. The modern versions of this include vendors like Sisense, Tableau and PowerBI. However, regardless of old or new, these tools suffer from the same few problems. Legacy tools weren’t designed to take advantage of modern data warehouses. First, data is loaded into each vendor’s proprietary data store before becoming useful. This means data is now duplicated and that modern powerful MPP data warehouses becomes nothing more than a naive data store. The reason for this is simply because these tools weren’t designed to take advantage of modern data warehouses. They were designed at a time where data warehouses were expensive, and when it was natural for each BI tool to build their own datastore engines. Second, the data modeling process is mostly GUI-based, meaning it is not easy to reuse and can’t leverage powerful version control systems like git to track changes. The advantages of both approaches naturally point us to a new direction: what if we can combine the power of SQL and the ease of use of data modeling? The answer is that we absolutely can! Looker, which was recently acquired by Google for $2.6 billion, was a pioneer in using this approach. It provided a data modeling language called LookML which serves as an abstraction layer on top of a customer’s databases. The software does not load the data into its own data store or use a proprietary query engine. Instead, when a non-technical user uses Looker to explore data, the software translates the modeling input into a SQL query, and then sends this query to the customer’s database. This means the user doesn’t need to know SQL at all but still can utilize the power of modern MPP data warehouses! Another advantage of using a text-based modeling language is that now the whole modeling layer can be stored in a powerful version control system, making it trivial to track changes and to revert when there are issues. The SQL-based data model approach is just the tip of the iceberg. Take ETL, for instance. In order for our business data to be present in a data warehouse and ready to be modeled, one needs to import the data from multiple sources, and transform that data into a format that’s suitable for analysis. Traditionally, these tasks belong to a category of tools called ETL tools. The process includes three steps: With the advent of powerful MPP data warehouses, more and more data teams are switching to an ELT approach, as opposed to an ETL approach. This process extracts raw data from sources and loads them directly into data warehouses with no transformations involved. Once stored, analysts can then use their favourite tool — SQL — to transform the data to a final format suitable for consumption. Because data transformations are now written in SQL, it follows naturally that we can apply the same SQL-based data modeling approach to it! Tools like Holistics take Looker’s SQL-based data modeling approach one step further by extending the modeling further into the L (load) and T (transform) layers, covering the entire analytics pipeline from end to end. This approach of using a unified SQL-based modeling approach to import, transform and explore data uncover multiple advantages that weren’t available before: When you start to think about data modeling as just another type of logical coding using SQL, the similarities between data analytics and software engineering became more and more apparent. The goal of software engineering is to deliver value by shipping software, while the goal of analytics is to deliver value by shipping explorable datasets and visualizations. Both of these processes include the need to write logical code (programming languages for the former and SQL modeling languages for the latter) then deploy the code to “production”. The software development world has seen multiple decades of process experimentation. One relatively modern movement that has been instrumental in improving software development is the DevOps movement. This movement incorporates automation and continuous delivery in order to help countless organizations speed up software delivery, while increasing reliability and quality of their software. My thesis is that we can significantly improve the data analytics process if we copy the ideas expressed in DevOps. To name a few examples: SQL, data modeling and DevOps are three age-old concepts with little to do with one another. Yet when combined, we have an entirely new paradigm for thinking about analytics, one that’s never been seen before in our 60-year old industry. We call this the DataOps paradigm. It is the guiding paradigm behind Holistics, a unified analytics platform that we’ve been working on for the past two years. And we’re incredibly excited about its future."
Learn from Silicon Valley’s Data Engineers: Dimensional Data Modeling is Dead,earn from Silicon Valley’s Data Engineers: Dimensional Data Modeling is Dea,"My first day in Silicon Valley was in 2019, and one of my biggest surprises was that I didn’t find any dimensional data marts. I was used to joining facts to dimensions and could rattle off normal forms and preach best practices of data modeling. I considered myself an expert on slowly-changing dimensions and how to apply them. Dimensional data modeling, popularized by Ralph Kimball with his 1996 book, is a method of organizing data within a data warehouse. While many benefits are preached, I believe that it exists for 3 primary reasons; optimizing compute, organizing data by topic, and optimizing storage. These foundational purposes driving the advent of dimensional modeling have evolved over time. Let’s take this moment in history to revisit why dimensional modeling exists, and how its roots meet our needs today. In early days of computing, storage cost a premium; as much as $90,000 in 1985. At this rate, data needed to be organized in such a way that minimized the number of times the same value would be stored. To do this, pointers were used in the form of database keys, to point a unique identifier to many records of the same value. Database normalization came about to describe how de-duplicated and storage-optimized a database had become. Instead of storing a long descriptive value every time a dimension existed in the table, we could store this description just once, and connect it to the record it is meant to describe. With the fastest computer putting out 1.9 gigaflops of computational power in 1985 for a whopping $32M, compute-optimization was equally as important. For reference, some of the fastest computers today put out over 400 petaflops of compute, or 20,000 times more flops. In the early days of databases, reducing the number of operations and the average complexity of each operation could save companies millions. For one example, instead of needing to analyze long complex string values to relate records, we could assign a single integer to unique instances, and relate these integers instead of strings. In order to accomplish these optimizations, data was organized into topic-oriented data models. One common topic-oriented model was the star schema. The benefit of this model was that the center of the star; the fact table, contained values that were physically indexed and easily retrievable. The more costly values to retrieve were stored on dimension tables and could be selectively retrieved, saving on processing cost. If new dimensions were to arise relating to the fact, new dimension tables would have to be put in place, key relationships would need to be enforced, and the process of normalization compliance would be maintained. In successful dimensional modeling, the source data tables were torn apart, distributed among many tables, and if done correctly, could be re-assembled back to the source table if necessary. Database normalization is showing its age. The cost of 1gb of AWS Cloud storage per month is  just 2 cents . The returns for breaking a long and/or wide table down into a star or snowflake schema are poor. While storage could be reduced by perhaps 95% of the total space required to store the table, this cost isn’t a consideration factor when valued at pennies per year. This holds true for tables of nearly all sizes, and companies of all sizes. While the cost of a 10-fold reduction in compute for a data model could save hundreds of thousands, or even millions of dollars in yesteryear, these savings are no longer cost-justified. The speed can also be a factor to consider, but the time required to perform all necessary operations and return a result set is miniscule. With the advent of cloud architectures, compute is easily scalable and requesting additional resources to apply to a long-running query has become easy. For our average data consumers, such as the business that operates based on conclusions drawn from the data surfaced in these models, data models aren’t intuitive. We as Data Engineers may look at data models and intuitively understand them, but the users who ultimately benefit from the data systems we maintain almost universally prefer to view data in a format they’re familiar with; tables like spreadsheets. In my experience, it’s much easier to teach simple ‘SELECT’, ‘FROM’, ‘WHERE’ than to describe dimensional models, why they exist, and how to join tables to retrieve values. New columns being added to source-systems don’t consume any integration resources to maintain, while integrating new columns or source systems into data models is relatively time consuming. While new data modeling tools have made integration easier in recent years, if the data model isn’t adjusted each time new columns are added to source tables, new columns often aren’t usable by end-users. The benefits of dimensional data modeling are showing their age. Just as cubes came to fame and soon faded from popularity, star schemas have also had their day in the spotlight. In the coming years, data lakes and data lakehouses will take the spotlight. Data lakes provide a better end-user experience, are inexpensive to maintain, and require no additional engineering resources to construct. The primary benefit of data lakes is usability for the business. The middle layer of the data ingestion machine; Analysts or Business Intelligence Engineers, were once required to interpret complex data models to deliver value to the business, the data can now be connected directly from source to end-user. Analysts and Business Intelligence Engineers can now focus on solving more valuable problems, such as engineering features to build predictive pipelines. The recent success of data lakes show that compute and storage resources that no longer benefit from being marginally reduced, and increased usability has been re-discovered as a major overall uplift to the data ecosystem. The maintenance cost associated with dimensional models could instead be dedicated to creating rapid-value for the business. What about dimensional modeling? Dimensional modeling has its time in history, and much like the cube, I believe it will drift into obscurity. There are many companies today that are deeply committed to dimensional modeling, so I don’t believe the skill will die out for many years. As new teams begin evaluating the cost of data lakes and dimensional models, fewer and fewer dimensional models will come to exist."
From RDBMS to Key-Value Store: Data Modeling Techniques,rom RDBMS to Key-Value Store: Data Modeling Technique,"In the  previous blog , we found out what are the basic concepts and operations of LevelDB. In this blog, we are going to find out some techniques to model an existing RDBMS to a Key-Value schema. Before moving on let’s have a quick recap on RDBMS and its concepts, just in case you forgot them already. 😃"
The lost Art of Data Modeling,he lost Art of Data Modelin,Data Modeling seems to have become a lost art amongst data engineers. What was once the primal part of the job of a data engineer seems to have been relegated to a secondary rank. Shaping the data by developing an understanding of the underlying data and the business process going along with it doesn’t seem nearly as important these days as the ability to move data around.
[big] Data Modeling,big] Data Modelin,"Dimensional data modeling has been around for a long time but it wasn’t until  Ralph Kimball  popularized the concept in the late 1980s and early 1990s that it really picked up steam. Many aspects of dimensional data modeling assumed the data warehouse would be confined to a single-server database (either RDBMS or OLAP). However, with the popularity of distributed computing and storage systems in the last 15 years, many of these assumptions no longer apply. The methods and techniques of dimensional data modeling popularized by Ralph Kimball 25 years ago require some revision. Many seasoned data engineers will probably remember the old days when Ralph Kimball and Bill Inmon battled it out in the data warehouse wars. For those not familiar, these were two very different approaches for building a data warehouse which is beyond the scope of this article. While Kimball had a larger following, many of Inmon’s ideas still survive to this day and the majority of modern data warehouses are based upon ideas from both camps. That being said, I believe it was Kimball’s use of dimensional data modeling that had the biggest impact on how data warehouses are designed and operate today. The overarching theme of dimensional data modeling is simple: organize data in such a way that it is quick & easy to understand & query for analysis & reporting. To this day, this theme (mostly) still applies; the only thing that has changed is that today’s data warehouse has far more uses than just analysis and reporting. Data science, machine learning, and algorithm engineering are just a few of the emerging uses for big data stored in modern data warehouses. Of course, this revision doesn’t require us to come up with an entirely new way of modeling data; it simply means that — with some minor tweaks — dimensional data modeling can satisfy the (big) data needs of today’s much wider audience. I’m proposing a list of revisions and updates to bring dimensional data modeling into the world of big data. We may need to add/revise this list in the future but this is probably a pretty good start: From a data modeling point of view, snowflake schemas have basically won; but that doesn’t mean star schemas just gave up. All it means is that we should have different tables for many of the different levels of granularity a dimension might hold (which is the approach of a snowflake schema). However, we should still de-normalize lower-granularity attributes into parent dimensions (which is the approach of a star schema) to reduce the number of shuffle stages that a query will require to execute in a distributed computing environment. Consider a scenario where we have product_type_d and product_subtype_d dimensions. We’ll want all of the attributes of product_subtype_d included (via de-normalization) in product_type_d. Of course, this begs the question: if we have all of the information in product_type_d, then why do we need the product_subtype_d table? The answer is that we still need a dimension at the granularity of product subtype so that fact tables with a product_subtype_id foreign key can still perform a simple join for dimension attributes. Basically, merging the two techniques yields us the best of both worlds. The only cost is in storage. So while you may want to be careful doing this for extremely large dimensions; it is well worth the cost for the majority of use cases. As cool as I thought surrogate keys were when I first learned about them, they have turned out to be nothing but trouble and can be extremely difficult to maintain in today’s environment. Using natural keys is perfectly fine. If your dimension has a composite primary key and you want to simplify querying (so that the join doesn’t need to be made with two columns); just concatenate them together with a reasonable delimiter to produce a single key based upon multiple natural keys. Surrogate keys can make re-stating data a total nightmare and my next point eliminates the only good reason they were ever really needed in the first place. The vast majority of the time, a Type-0 or Type-1 SCD will do the trick. Unless there is an exceptionally critical reason, I avoid using Type-2 SCD’s. In over 15 years of data modeling, I have only come across a handful of real use-cases for Type-2 SCDs. While they can be a very powerful feature in a data warehouse, they are very seldom actually used (even when data consumers swear that they need it). If you need to implement a Type-2 SCD, do not use surrogate keys. Instead, use the natural key in combination with date/timestamp fields representing effective and expiration dates in the SCD. This is only slightly more complicated to query but is far more flexible, much easier to implement, and avoids the need for surrogate keys. This is a somewhat new concept that I think needs to be added to the data modeling toolbox. Snapshot dimensions wouldn’t have made sense in an RDBMS but make perfect sense in a big data environment. What are they? Take a dimension (for example: customer_d) and add a partition column (for example: as_of_date). Each day that your ETL runs all you need to do is pick up data in the previous as_of_date, read event data about inserts/updates/deletes to customers, process the combined dataset and write the result to a new as_of_date partition. Some of the benefits to this approach are: Is this a waste of space? Sure. But the extra space this requires is probably a lot cheaper than having a customer dimension with corrupted data and no quick and easy way to fix it. Think of it less as a waste of space and more as an insurance policy. Additionally, it is relatively simple to create a clean-up job that drops old partitions that are no longer needed. Denormalizing some attributes into fact tables isn’t as taboo as it once was. Part of the reason why this was frowned upon in the past was because of how an RDBMS would store data in tables. With the advent of columnar data storage formats like Parquet and ORC; this is no longer a huge concern. For a field to be denormalized, it should meet one of the following requirements: If one of these requirements are met, then I consider the advantages and disadvantages to denormalizing an attribute into a fact table: It is important to understand that this list of advantages and disadvantages is just a starting point and that each point may carry a different weight. In the end, this is one of those situations where the artistic side of data modeling becomes just as important as the scientific side. Like surrogate keys, I thought these were cool when I first learned about them; but, in practice, I never really found a lot of use for them. In addition, by their very nature, they required you to perform UPDATE operations to a large number of random rows in a fact table. Random UPDATE operations and big data are sworn enemies — and their battleground is your cluster. Unless you need the heat from your cluster to make it through the winter, find some other way to model your fact tables. While complex data types violate the most important rule of normal forms (atomic columns); much of the traditional teachings of dimensional data modeling broke several of these rules as anyway. With a bit of common sense and caution; complex data types can be extremely powerful in a modern data warehouse. Here are some examples of powerful use-cases where complex types should probably be used. Of course, with great power comes great responsibility, so here are some points of caution to consider about complex types: Much can be learned by reviewing Ralph Kimball’s  dimensional data modeling techniques . While the technology may have changed (which inspired me to write this article), the core idea remains the same: a data warehouse should be designed with the data consumer’s requirements as a top priority."
Firebase Data Modeling,irebase Data Modelin,"Firebase provides very little guidance on how to structure your unstructured JSON data. Firebase provides push keys and dis-incentivizes us from using numbered list keys… but that’s it. The rest of your data model is up to you. Let’s review a few best practices that will make your Firebase experience fun and fresh. Most JSON data structures are completely denormalized, meaning that we don’t tend to use references with JSON. That tendency is easy to carry over to Firebase, but that’s a mistake! Firebase is happiest when you keep your data structures shallow, and you’ll need to normalize your data to achieve that. Let’s review two example data structures. First, the slow and inefficient… deeply-nested data. Note:  The following examples use keys such as  product1  or  transaction2  for convenience in creating and reading example data structures. In a production application we would use push keys generated by  someRef.push() . For example, a common product or transaction id would look like  +M0H4sFUOPe1vgQSXkWqdA==  rather than  product1  or  transaction2 . Deep Data <anti-pattern alert!!!> Notice in the previous data structure how every user attribute contains all of it’s children. It has an email address and a collection of transactions. This model is inefficient, because I can’t loop through all of my users and just pull the email addresses. I can pull an individual user’s email address efficiently with Firebase, but I can’t pull  just  email addresses for a group of users. If I needed to loop through 1000 users, I would have to request all of those users’ transactions along with their email addresses. Now let’s look at the happier, shallow data structure: Shallow Data Notice how the  /users/user1  attribute has only one child node, the user’s email address. The user’s transactions are still accessible via  transactions/user1 , but I can efficiently loop through my users’ email addresses without pulling down excess data. The downside to shallow data structures is that I occasionally need to create a second ref to pull in transactions… they’re not available on my  users/{userId}  ref… Using two refs to join data We have to constantly balance normalization (shallow structures) vs denormalization (deep structure) based on how we want to use our data. If we find that we’re regularly pulling email addresses along with transactions, we may need to duplicate the users’ email addresses in the transactions like this: Don’t be afraid of duplicating data to speed up your reads. Yes, duplicating data can slow your writes a bit and can be obnoxious to manage, but duplicate data will enable your apps to scale effortlessly to millions of reads. Modeling your data as streams provides great scalability and prevents large queries that slow down your Firebase. Consider a data structure for a chat application: The structured chat data above is too deeply nested. You’ll have difficulty querying this data, because Firebase can only query on one child node at a time, and it can’t be a “grandchild” node… it must be a direct child of the list’s top level. In this case, you can’t query the  userChats  node because none of it’s direct children are values, they’re all nested nodes. Now let’s consider a flatter structure: Stream Chat Data In this case we’ve named the top node “chats”, and we’ve duplicated the user ids and usernames for each chat. We can now query the  chats/  node on the  user  like so: We can also listen to the  child_added  event to add chats to our UI: Make sure to structure your data as streams whenever possible, meaning long, shallow lists of data. Don’t nest any more than is necessary for your needs. Also, do not be afraid to duplicate data such as usernames, user ids, object titles, etc. Try to match your data structure to your UI. In the previous example, each “chat” object must have the username attached to it, because attempting to join usernames to chats would be incredibly expensive. Firebase provides two primary event types for retrieving your data,  value  and  child_added . The  value  event returns all child nodes in an unsorted JSON object and then returns all nodes every time there’s any change to any of the child nodes. The  child_added  event fires once for each existing child and then fires again every time a child is added. Since  child_added  fires once for every child, it can respect query  orderBy*  parameters. Most beginning Firebase users initially prefer the  value  event because it’s so easy to reason about; however, more sophisticated users tend to use  child_added  wherever possible, because  child_added  places less load on the server running your Firebase, so it scales better. Also, since  child_added  respects sort order, you don’t have to manually sort the data on your client. We tend to think about Firebase as a front-end, client-side technology, but it provides a great architecture for highly-scalable server processes: Queues! Firebase integrates with Google Cloud Functions to create lightweight Node.js tasks that are fired off by adding items to a Firebase list. Users can add jobs to a queue and your Cloud Functions can listen to that queue, process the job, remove the job from the queue and even add another job to a different queue for further processing. The following example illustrates a simple queue data structure that takes proposed username changes and proposed shopping cart checkouts from users. In this example user1 has requested a username change and user2 has requested a shopping cart checkout. The server has already approved a username change for user3 and has added it to the  serverQueues/updateUsername/  node for further processing. The server has also approved a  userQueues/cartCheckout  job for user4 and has added user4’s credit card to the  serverQueues/chargeCard  node for payment processing. Queues Example Notice how the  userQueues/changeUsername/$user  node and the  userQueues/cartCheckout/$user  node use each user’s id as child keys? We would typically use fresh, new push keys for a list like this, but these nodes have to be user-writeable so that our clients can add jobs to the queues. By using the user id as the child key, we can write a security rule to enforce that users must be authenticated and can only queue one job at a time: The security rules statement above grants write privileges to any user whose auth uid matches the user id for  usersQueues/$queueName/$userId . Security rules default both read and write privileges to false. Security rules match by node name, but also allow wildcard node names that begin with  $ . So in this case, we’re adding a rule to  userQueues  followed by a wildcard  $queueName  and a wildcard  $userId . The rule grants write access to the  userQueues/$queueName/$userId  node if the user is authenticated and the user’s authentication uid matches the node name. So if your auth uid is  user6 , you  can  write to  usersQueues/changeUsername/user6  or  usersQueues/cartCheckout/user6  or  usersQueues/anyOtherQueueName/user6 . However,  user6  cannot write to  userQueues/changeUsername/user7 , because the  user7  part of the path does not match  user6 ’s uid:  user6 . In practice, these uids are much longer than the ids we’ve used in this example:  user1  and  user2 . These keys are determined programmatically by Firebase Authentication and look like long, encoded strings such as  WQ3mVT7f8pRbBmry6eZju1Z4lPi1 . All nodes in this data structure are available to the server with full read/write privileges, which has admin privileges through it’s  /service-account.json  api key. So users can add one job at a time to their  userQueues/$queueName/$userId  nodes, but only the server can add jobs to the  serverQueues/  data tree. Review the docs on  data structure  and answer the following questions."
Dimensional Data Modeling,imensional Data Modelin,"Dimensional modeling  (DM) is part of the  Business Dimensional Lifecycle  methodology developed by  Ralph Kimball  which includes a set of methods, techniques and concepts for use in  data warehouse  design. The approach focuses on identifying the key business processes within a business and modelling and implementing these first before adding additional business processes, a bottom-up approach. [1] The objectives, laid out by Ross and Kimball, are straightforward: If you worked on or used an ETL system, you would have noticed that information consistency is achieved by conforming measures, timeliness provided by ETL cycles and adaptability also depends largely on the ETL design. As a data engineer, you know SQL very well and can probably write SQL queries for all day long. But you cannot assume that the typical end-user will be an expert on writing SQL queries. So, our objective is to build a DW so easy for analysts to write analysis queries quickly and effectively. Note, the above points are not the easy ones and that your system should be scalable enough for all these kinds of queries. The OLTP databases are transformed into facts and dimensions due to the aforementioned objectives. Most businesses measure their success and efficiency by measuring certain types of data. This data captures real business activities and progress. This data is called as  facts . The OLTP oriented databases record transactions at a time, kind of like event streaming but centralized around transactions. The DW is different. DW does not need to record details at the transactional level. DW needs to have facts across different criteria of your business. DW needs to aggregate(or let analysts aggregate) the information required to improve business. And so, redundancy is an unforgivable sin in DW. In a data warehouse, a measure is a property on which calculations can be made.[2] The facts we derive from the operational data stores come with some additional data which is typically summed in our analysis. These are the aspects of a fact that allow the analyst, or the executive viewing the analysis, to see the value in the fact. So that you can ensure that your system can legitimately correlate and aggregate across facts. But its not always possible to have data at atomic levels. So, to bridge this gap, there are two methods: As the name suggests, they are collected at regular time intervals. Consumption of gas, audit, and inspections are some instances of data collection that have periodic snapshots enabled for them. When a business performance indicator is a rate of completing a multi-step business process, you may want to capture at the grain of the  entirety  of the process and record the start, the completion, and the steps in between. This may be transaction-grained, but it has a lot of measures in between. So, you use accumulating snapshot fact tables to answer complex questions in business intelligence where there is the passing of time between facts. One good example would be a fact table row of your ordering a chicken sandwich and the fact table row of the bag being handed through the window of the car at McDonald’s drive-thru. You and your analysts need to know how to query and filter facts to derive business intelligence from them. This purpose is served by dimensions. Dimensions are almost always created with a surrogate key; the surrogate key, naturally, is referenced by the foreign key (or  keys ) in the fact table. We search the table by searching the dimensions in which we are interested. All of the other data describing our facts, such as timestamps, customer agents, store location, product, and customer are what we turn into dimensions. The beauty of dimensional modeling is that facts are not defined by the primary keys or any sort of unique identifier, instead, they are defined by the combination of dimensions. This gives rise to Star Schema. It is very important that we have a uniqueness in our dimensions. When we get to queries across facts, duplicates among dimension combinations will turn into a disaster. If you can’t, then add or aggregate dimensions to make ’em unique. Consider the following two images. An analyst will have an easy life if you setup the second dimension table for him/her. So, with the second table, you have the following hierarchy: There are all kinds of hierarchies — multiple hierarchies, single hierarchy, etc. I am not addressing them in this blog post. One thing I would like to point out is that the time dimension is a real pain in the neck. You have to take care of the magic days, the fiscal calendar, the time zones, the cycles(profit over quarter types). And don’t be lousy or over-confident in this, even time-series databases won’t help you in hierarchies if your ETL is messed up. You might want to take a look at Outrigger dimensions. There are also occasions in which one dimension is naturally dependent on another. In such a case, designers may put a foreign key from the one to the other. This is what constitutes an “outrigger dimension. In calendar dimensions, this is very common. You cannot use a date at a different grain in an outrigger from the dates you use in the fact table. You cannot allow aggregation over outrigger dimensions. If need be, mask numeric values in the outrigger with textual prefixes or suffixes to obstruct this. [4] As much as I would like to write about it, I still think that it’s better for my readers to understand this concept thoroughly from  here . I am not discussing Snowflake dimensions but just to point out, they are still in use with OLAP databases. You will treat your tabular Big Data as having been acquired through one of your standard Extract phases. Thus, you will apply to it the same steps you did in transform: I wanted to understand the theoretical aspects of database design which led me to read the book, Ross and Kimball. I then became curious to draw differences and analogies in their methods and that of today’s leading data-driven companies like Netflix, Airbnb, Uber, etc. In this quest, I can fairly say that the structured format of dimensional modeling is preferred over just a hardcore ETL. Because in this way, you remove the dependency on you, your BI team doesn’t ring you up on Slack to create a new DAG for every other insight, instead, with correct modeling, you enable them to act and explore freely without your need. Please leave feedback on how I could improve, I am sure this was not your best read. Thank you for your time. [1]  https://en.wikipedia.org/wiki/Dimensional_modeling [2]  https://en.wikipedia.org/wiki/Measure_(data_warehouse) [3] Ross and Kimball, ch 2 and ch 18 [4] Kimball/Ross pp103–109"
"Data Lake — Design For Better Architecture, Storage, Security & Data Governance","ata Lake — Design For Better Architecture, Storage, Security & Data Governanc",I started writing this post with the intention to demonstrate a practical approach on how to build a data lake on AWS and Azure. However realized its important to first discuss some of the best architecture patterns before going deep with hand-on. I will be writing a few more posts on demonstrating a practical…
R.I.P. data governance: Data enablement is the clear path forward,.I.P. data governance: Data enablement is the clear path forwar,"Let’s be clear — we do not intend to bury data governance, but rather to place it within the context of a more comprehensive approach: data enablement. The goal of data governance — ensuring the quality of an organization’s data across the data lifecycle — is noble enough. But the processes involved often end up stymieing transformative progress and success. At their worst, these processes are black holes that drain resources from the organization and yield no tangible benefits."
We need to talk about data governance (and introduce Alvin),e need to talk about data governance (and introduce Alvin,"It’s easy to understand why data driven companies are embracing self-serve analytics; when individuals have autonomous access to insights, better decisions can be made faster — theoretically at least. It falls on BI teams to act as the mediators between raw data and the insights generated from it. From our conversations with these teams, this comes with some pretty significant challenges. Organizations are now packed full of demanding data ‘customers’, who have come to rely on data to do their jobs. Keeping them happy whilst balancing other considerations such as privacy and security can prove tricky, particularly in the face of tightening privacy regulation (GDPR in Europe, CCPA in California) and an increasing threat from cyber attacks. “Moving fast and breaking things” isn’t all that cool when it comes to leaking sensitive data, or misreporting numbers to the board. The answer? Companies of all sizes, including startups and scaleups, are looking to implement some form of data governance. Data Governance : maximizing access to the high-quality data stakeholders need to drive business value, whilst minimizing risks. Currently, tools that try to address these pains trend towards the enterprise; bundled features, a high price point, lengthy setup and plenty of maintenance. These tools are not really fit for purpose in rapidly scaling, time poor companies, and are often over-kill when it comes to solving the real ‘hair on fire’ problems. Internally, we define data governance as  maximizing access to the high-quality data stakeholders need to drive business value, whilst minimizing risks . This encompasses very specific challenges, with different personas and use cases. It is our belief that these need to be met with tools that respect the workflows of today’s data professionals, with use case driven features powered by automation. Data democracy without data governance — data anarchy? The shift towards self-serve analytics it’s often referred to as ‘democratizing data’. Some companies have adopted data catalogs to promote this culture, making data assets searchable across teams and departments, and providing additional context such as sensitivity and quality. Enterprise data catalogs have been around for a while, but we’ve also seen data-driven companies such as Lyft (Amudsen), LinkedIn (DataHub) and Netflix (Metacat) build their own open-source solutions, having found nothing on the market to meet their needs. So what is the relationship between data governance and data democracy? To stretch the analogy, in any democracy, it is the role of the government to put in place the rules and structures needed to ensure it survives, and you could say the same about data governance. With no governance you get ‘data anarchy’; a free for all with little to no control. Even with a simple setup consisting of a data warehouse (e.g. Snowflake) and a data visualization tool (e.g. Looker), without good governance things can get out of hand quickly. Unused and similar versions of tables and dashboards can clutter the environment, causing a lack of trust, with no single source of truth — as well as driving storage and compute cost. Keeping track of what columns and dashboards contain sensitive data, and who has access to it, becomes a major challenge. When changes cause an important dashboard to break, BI engineers can expect a flood of messages from stressed out colleagues. Where to start in automating data governance? Know the flows. For BI teams to work effectively, they need to understand how data flows. For instance, they may want to delete a column that is no longer relevant. However, this has the potential to break an ETL pipeline or a C-level dashboard. Without a tool that can perform impact analysis, BI teams have to choose between painstakingly mapping out data flows manually, making the change and hoping for the best, or simply avoiding changes altogether! In most cases, the BI team doesn’t need a company-wide enterprise data catalog with all bells and whistles — they need to understand how data is flowing, where it is flowing and who is using it. In enterprise data catalogs, essential features like cross-system and column-level data lineage either don’t exist or are only included in the top-tier pricing. Open-source data catalogs are looking towards it, but it is a technically challenging problem (as we know from experience!). There is, of course, lots to learn from data catalogs and the open-source community— in fact, any useful metadata product needs a catalog at its core. As data flows can be seen as a graph with edges between nodes, it is evident that you cannot talk about flows without an index of data entities such as tables, columns, and dashboards. The flow of data occurs on a timeline as statements or jobs are issued causing data to move between entities. This becomes truly powerful when data entities have been tagged, for example with location or sensitivity. Suddenly we can start tracking where personal data is trickling into dashboards and ad-hoc queries automatically. To truly understand data flows, usage data must be treated as a first-class citizen. Queries, jobs, and dashboards must be parsed and structured. Usage data is what makes the product opinionated. This is analogous to how Amazon gives you suggestions based on your search and purchase history. What is Alvin — and what is our mission? In our past lives at data focused startups, we experienced first hand some of the big challenges facing big data.  Alvin  was started with the belief that companies are no longer competing on how much data they collect and store, but on its quality and how they leverage it. We’re currently two founders ( Martin  and  Dan ), recently backed by a top nordic VC, and are now growing the team (shameless hiring plug!). Alvin automates key aspects of data governance — freeing up time and head-space for BI professionals to focus on the true value-creating activities. Our core technology analyzes data flows and interactions between systems and people. This will power a range of modules, each laser focused on a specific challenge, including data lineage, access management, privacy and financial compliance, cost control and logging. Our first module delivers plug and play data lineage to BI teams, answering questions such as: We have spent the last year or so fully immersed in the space, building our core tech, talking to BI teams and conducting pilots. We’ve seen how easy it is for highly skilled BI professionals to get stuck doing tedious tasks that could be automated, as much as 40% of their time in some cases. With so many competing priorities in scaling companies, implementing a fully fledged enterprise solution is unlikely to make it to the top of the list, so BI teams continue to operate below their true capacity. By building Alvin to be zero setup and modular, we aim to break this cycle and take data governance beyond the enterprise. With a BI team focused on innovating and inventing, scaling companies give themselves the best chance of data-driven success. Alvin is helping data teams of all sizes meet the challenges of data quality, reliability, cost and governance.  Get started for free here  and unleash the power of highly accurate lineage and usage data."
6 Reasons Why Companies Fail at Data Governance, Reasons Why Companies Fail at Data Governanc,"Everywhere we go in the cloud data space today, we’re hearing one message loud and clear: “you should be thinking about data governance”. It’s a sentiment that we wholeheartedly endorse, but we like to take it a little bit further — you should be thinking about data governance  differently . In this article, I’ll share six of the top reasons that we have seen data governance initiatives fail for even some of the best data teams — and how you can avoid falling into the same traps. You Think Data Governance is About Right and Wrong At a major ridesharing company, two teams had been struggling for months to reconcile their reporting. No matter what they did, they were never able to achieve the same results, even when they insisted that they were defining their metrics in the same way, having queried from the same tables and walked through the code line by line. DAS42 accepted the challenge, and what we found became one of our favorite examples of the criticality of data governance. One team was using a filter on “mega-region = ‘US and Canada’”. The other team was using a filter on “country-code = 1 (US) OR 32 (Canada)”. These, we were told, were the same thing. But sure enough, we looked at the mapping table and found that country-code 1 included Puerto Rico, but the mega-region ‘US and Canada’ did not. It wasn’t that one team was right and the other was wrong. It was that  neither of them completely understood what was being included in or excluded from their metric . This is one of the most common data governance errors we see, and it’s led to some of the most pernicious and disheartening data woes for our clients. It is not enough to point at a query or output and say “this is the source of truth” — to say “this number is right, and any other number is wrong”. This is because fundamentally,  data governance is not about right and wrong . Both of the teams mentioned above had numbers that would be valid, depending on the use case — depending on  what the stakeholder meant  when they said “we want these numbers for US and Canada”. That’s not to say there’s no such thing as a “wrong” number. Of course, we’ve also seen analysts try to exclude test accounts with a clause like “last name does not contain ‘test’”, which is a terrible shame for real human beings named with surnames like Battesten or Contestanza. (We have yet to encounter a real person named Testy McTesterson, but stranger things have happened.) But it is not useful to enshrine a number — a metric definition, for example — as  right  if you are not able to explain what differentiates it from the others. Shift your mindset from “we need to know which of these numbers are right and wrong” to “we need to understand these numbers, what goes into them, what makes them different, and in what contexts it would be appropriate to use them”. You Over-Emphasize Executive Buy-In Don’t get us wrong — of course having respected senior team members as champions for your data governance initiatives is a powerful tool in your toolbox. But it’s also absolutely essential to have buy-in from the people handling the data on a day-to-day basis as well — your front-line staff, your engineers, your project managers — the people who will have to make changes to their processes in order for you to fully implement a governed framework. Before you start trying to secure leadership and stakeholder buy-in, it’s important to  be transparent  about the current state of your data governance strategy. Consider how you might answer the following questions: In the same way that having visibility into your data pipelines makes it easy to ensure high data quality, transparency into both your data governance strategy and its incremental progress will be critical when it comes to keeping everyone informed and accountable on your leadership team. You Think of Implementing Data Governance as a Project There are two primary mistakes data leaders make when it comes to implementing a data governance framework. The first is the “set it and forget it” mentality. They think of data governance as an initiative to be completed, and that once it’s done, they’ll be on to the next. The second mistake is similar, which is the inclination to over-govern. While understandable, both approaches miss the heart of what data governance is intended to be. Just like data ingestion or quality assurance, data governance is a process. It may require more intentional effort on the front end, but data governance isn’t a project that’s ever truly completed. As your company grows and evolves, your metric definitions will evolve along with it. The problem with the approaches mentioned above is that they don’t leave room for change. Remember that data governance is less about the right and wrong of your metrics and more about changing your company’s cultural approach to those metrics. Don’t think of data governance as something to be completed — think of data governance as something to be adopted. You Think A Tool Will Do All the Heavy Lifting The data landscape is crowded with tools, managed services, methodologies, and frameworks, and we don’t deny that many of them really can help you take your data to the next level. Data observability is one of those. But, as highlighted above, data governance isn’t a single tool, or a workflow you set up once and all your problems are solved — it’s an ongoing process that involves judgment, decision-making, and differentiation. While data catalogs and other governance solutions often market themselves as the answer to all of your company’s data problems, many data leaders find these tools lacking in even the most rudimentary aspects when it comes to manual requirements. There are tools and companies that can make things simpler, automate processes, and help you to step outside of your assumptions. But for all the innovation we’ve seen across the data landscape over the last decade, there’s still no technological replacement for the difficult (and immensely gratifying) work of  talking  about your company, your processes, your definitions, your measurements, and your goals. You Think You Can Focus on Data Governance Alone Some of the most earnest stakeholders we’ve seen profess their commitment to implementing data governance by putting it at the top of their priority list — and putting everything else on hold. They want to govern  everything , and they want to do it  now . This can cause several problems. First, it can lead to frustration and disenchantment from stakeholders as they fail to see material gains in the form of new deliverables. (Of course, some of us think that data governance is tremendously exciting, but probably not everyone at your company is this  particular  variety of nerd.) Second, if you fail to take an iterative approach, practical lessons might come too late. We’ve seen companies “govern everything” and then, a couple weeks after they’ve finished and everything is implemented, see something major that they missed… and they missed it across the board. We mentioned above the criticality of getting buy-in from stakeholders at all levels. As you start on a data governance journey, choose a few key examples. These might be fundamental to your business — you may need to define your categories or product hierarchies, to determine your net operational revenue or availability rate, or you might need to define what you mean when you use the word “customer”. (How  do  you exclude those test accounts, anyway?) Choose two or three high-impact areas to govern. Build your governance muscles, and make it sustainable — something that you can do alongside your day-to-day work and “business-as-usual” deliverables. And give yourself time to learn lessons as you move on to the next, and the next. You Don’t Know What Data Actually Matters Once you’ve identified the first few domains you’d like to focus your energies on, the next step is to ensure that the data you’re governing is actually worth being governed. Not all data is created equal, and in today’s economic climate, it is not inaccurate to say that some data is worth more than others. For instance, data forecasting your company’s revenue next quarter is probably more worthy of your attention than a duplicate table sitting in a dusty corner of your Snowflake warehouse. Before you roll out your governance strategy, identify what data actually matters most to your business and prioritize accordingly. Having visibility into your most critical assets — no matter what stage of the pipeline they’re in — can ensure that your team is a) spending time building a data governance program for data the business is actually using and b) tell you if the data is available, fresh, and, most importantly, accurate. One way to do this? Set service-level agreements (SLAs) and service-level indicators (SLIs) for data assets with the most eyeballs, for instance, the table feeding your CFO’s quarterly metrics dashboard or that Salesforce data informing your ad campaigns. Setting data reliability SLAs helps build trust and strengthen relationships between your data, your data team, and downstream consumers — whether that’s your customers or cross-functional teams at your company. Without these clearly defined metrics, consumers may make flawed assumptions or rely on anecdotal evidence about the reliability and trustworthiness of your data platform. In other words, data SLAs help your organization be more “data-driven” about data — and in turn, data governance. For most organizations, data governance is relegated to a few lone wolves responsible for convincing an entire team of “numbers people” to care about something that is inherently difficult to quantify. If it sounds like a trap, it’s because it is one — but it doesn’t have to be. At the end of the day, the goal of your data governance strategy will be to ensure that teams across the entire company feel empowered to use data, and the only way to empower is to build trust and educate. Our biggest suggestion:  start your governance initiative on a few key functional areas (land and expand), tracking for a few key SLAs, and across a handful of critical data assets. In a world where bigger (data) is always better, sometimes it pays to start small. +++ This article was written with  Teresa Kovich , Principal Consultant at Das42. Interested in sharing your data governance horror stories? Reach out to  Barr ."
The Rise of DataOps (from the ashes of Data Governance),he Rise of DataOps (from the ashes of Data Governance,"These days, executives are interested in data governance because of articles like these: On the other hand, the vast majority of data governance initiatives fail to move the needle, with Gartner also categorizing  84% of companies as low maturity in data governance . Despite the fact that  nearly every organization recognizes the need  for data governance, many companies are  not even starting data governance programs  due to the strong negative connotation of the term within the executive ranks. In my experience, the reason for the lack of progress is that  we have been doing data governance the wrong way, making it dead on arrival . Stan Christiaens got this right in  his Forbes article , despite the fact that it was essentially an ad ( a very effective one ) for his company. I agree with him that the primary reason governance has failed in the past is because the technology just wasn’t ready, and organizations couldn’t find ways to motivate people to follow the processes that filled the technology gaps. However, I disagree that modern data catalog tools provide the complete technology answer we need to be successful (although they are a step in the right direction). Recent advances in data lake tooling (specifically the ability to version data at scale) have put us at a tipping point where we can reimagine the way we govern data (i.e. the culture, structures, and processes in place to achieve the risk mitigation and cost take out from governance). At the end of the transformation, data governance will look a lot more like DevOps, with data stewards, scientists, and engineers working closely together to codify the governance policies throughout the data analytics lifecycle. Companies who adopt these changes early on will create a huge competitive advantage. To understand how I came to that conclusion we will have to go back through some of the history of Software Engineering, where 2 core technical innovations enabled process and eventually cultural changes that transformed coding from a hobby to a world-eating revolution. We’ll then see how similar innovations were the primary enablers of the DevOps movement, which has similarly transformed IT infrastructure in the cloud era. Finally, we’ll see how these innovations are poised to drive similar process and cultural changes to data governance. It’ll take a little while to build the case, but I haven’t found a better way to get the point across yet, so please stick with me. The core innovations that created the discipline of software engineering are: Before these systems, back in the 1960s, software development was a craft, where a single craftsman had to deliver an entire working system. These innovations enabled new organizational structures and processes to be applied to the creation of software, and programming became an engineering discipline. This is not to say that the art of programming is not extremely important, it’s just not the topic of this article. The first step to moving from craft to engineering was the ability to express programs in higher level languages through compilers. This made the programs easier to understand to the people who were writing them, and easier to share across multiple people on a team, because the program could be broken down into multiple files. Additionally, as the compilers got more advanced, they added automated improvements to the code by passing it through many intermediate representations. By adding a consistent version system across all of the changes made to the code that ended up producing the system, the art of coding became “measurable“ over time (in the sense of Peter Drucker‘s famous quote: “you cannot manage what you cannot measure“). From there, all sorts of incremental innovations, like automated tests, static analysis for code quality, refactoring, continuous integration, and many others were added to define additional measures. Most importantly, teams could file and track bugs against specific versions of code and  make guarantees  about specific aspects of the software they were delivering. Obviously there have been many other innovations to improve software development, but it is hard to think of ones that aren’t dependent in some way on compilers and version control. In recent years, these core innovations have been applied to new areas, leading to a movement aptly titled  everything-as-code . While I wasn’t personally there, I can only assume that software developers met the first versions of SVN back in the 70s with a skeptical eye. In much the same way, many new areas consumed by the everything-as-code movement have garnered similar skepticism, some even claiming that their discipline could never be reduced to code. Then, within a few years, everything within the discipline has been reduced to code, and this has led to many-fold improvements over the “legacy” way of doing things. The first area of expansion was infrastructure provisioning. In this example, the code is a set of config files and scripts specifying the infrastructure configuration across environments, and the compilation happens within a cloud platform, where the config is read and executed alongside scripts against the cloud service APIs to create and configure virtual infrastructure. While it may seem like the Infrastructure as Code movement swept through all infrastructure teams overnight, a ton of amazing innovations (Virtual machines, software defined networks, resource management APIs, etc.) went into making the “compilation” step possible. This likely started with proprietary solutions from firms like VMWare and Chef, but it became widely adopted when public cloud providers made the core functionality free to use on their platforms. Before this shift, infrastructure teams  managed  their environments to ensure consistency and quality because they were hard to recreate. This led to layers of governance, designed to apply control at various checkpoints in the development process. Today, DevOps teams e ngineer  their environments, and the controls can be built into the “compiler”. This has created an orders of magnitude improvement in the ability to deploy changes, going from months or weeks to hours or minutes. This enables a complete rethink of the possibilities for improving infrastructure. Teams started to codify each of the stages for creating their system from scratch, making the compilation, unit testing, analysis, infra setup, deployment, functional and load testing a fully automated process (Continuous Delivery). Additionally, teams started testing that the system was secure both before and after deployment (DevSecOps). As each new component moves into version control, the evolution of that component becomes measurable over time, which will inevitably lead to continuous improvement because we can now  make guarantees  about specific aspects of the environments we deliver. The next field to be consumed by this phenomenon will be data governance / data management. I’m not sure what the name will be (DataOps, Data as Code, and DevDataOps all seem a bit off), but its effects will likely be even more impactful than DevOps/infrastructure as code. “With Machine Learning, your data writes the code.” — Kris Skrinak, ML Segment Lead at AWS The rapid rise of Machine Learning has provided a new way to build complex software (typically for classifying or predicting things, but it’s going to do more over time). This mindset shift to thinking of the data as the code will be a key first step to converting data governance to an engineering discipline. Said another way: “Data pipelines are simply compilers that use data as the source code.” There are 3 things that are different, but also more complex, about these “data compilers” compared to those for software or infrastructure: 3. We still don’t really understand how data writes code. This is why we have  data   scientists  experiment to figure out the logic of the compilers and then  data engineers  come in later to build the optimizers. The current set of data management technology platforms (Collibra, Waterline, Tamr, etc.) are built to enable this workflow, and they’re doing a pretty good job. However, the workflow they support still makes the definition of data governance a manual process handled in review meetings, which holds back the type of improvements we saw after the advent of DevOps & Infrastructure as Code. Because data is generated “in the real world,” not by the data team, data teams have focused on controlling the metadata that describes it. This is why we draw the line between data governance (trying to manage to something you can’t directly control) and data engineering (where we are actually engineering the data compilers rather than the data itself). Currently, data governance teams attempt to apply manual control at various points to control the consistency and quality of the data. The introduction of version tracking to the data would allow data governance and engineering teams to  engineer  the data together, filing bugs against data versions, applying quality control checks to the data compilers, etc. This will allow data teams to  make guarantees  about the system components that the data delivers, which history has shown will inevitably lead to orders of magnitude improvement in the reliability and efficiency of data driven systems. Platforms like Palantir Foundry already treat the management of data in much the same way as developers treat the versioning of code. Within these platforms, datasets can be versioned, branched, acted upon by versioned code to create new data sets. This enables data driven testing, where the data itself is tested in much the same way as that the code that modifies it might be tested by a unit test. As data flows through the system in this way, the lineage of the data is tracked automatically by the system as are the data products that are produced at each stage of each data pipeline. Each of these transformations can be considered a compile step, converting the input data into an intermediate representation, before machine learning algorithms convert the final Intermediate Representation (which data teams usually call the Feature Engineered dataset) into an executable form to make predictions. If you have $10M-$40M laying around who are willing to go all in with a vendor, the integration of all of this in Foundry is pretty impressive (disclaimer: I don’t have a ton of hands on experience with Foundry; these statements were based on demos I’ve seen of real implementations at clients). For the rest of us, there are now open source alternatives. The  Data Version Control project  is one option that is focused on data scientist users. For big data workloads, DataBricks has taken the first step in open sourcing a true version control system for data lakes with the release of their  open source Delta Lake project . These projects are brand new, so branching, tagging, lineage tracking, bug filing, etc. haven’t been added yet, but I’m pretty sure the community will add them over the next year or so. The arrival of technology for versioning & compiling data puts the impetus on data teams to start rethinking how their processes can take advantage of this new capability. Those who can actively leverage the capability to make guarantees will likely create a massive competitive advantage for their organization. The first step will be killing off the checkpoint based governance process. Instead the data governance, science, and engineering teams will work closely together to enable continuous governance of data as it is compiled by data pipelines into something executable. Somewhere behind that will be the integration of the components compiled from data alongside the pure software and infrastructure as a single unit; although I don’t think the technology to enable this exists. The rest will emerge over time ( and   in   other   posts ), enabling a culture of governance that reduces major issues while accelerating the time to value for machine learning initiatives. I know it sounds crazy to say, but this is an exciting time to be in data governance. If you’ve read this far, you likely are interested in data governance, so please leave a comment / draft a response with what you’re thinking."
Band Protocol — A Protocol for Decentralized Data Governance,and Protocol — A Protocol for Decentralized Data Governanc,"It has been exactly one year since we first introduce  Band Protocol  in our original blog post. Since then, we have been putting our heads down to design and launch our protocol for real-world adoption. We have also received many valuable feedback from both decentralized communities and centralized enterprises who seek to utilize our solution. Aiming to make our protocol as robust and useful as possible, we incorporate these feedback and grow Band Protocol to meet the requirement of end users. Our core value proposition is still the same:  providing trusted and reliable data to the open internet . What have changed is the design of our protocol to add more curation method, beyond simple token-curated registry, to make sure we can tackle all sort of data including both objective and subjective data. We have recently launched our testnet and believe this is a good time to recap what problem we are solving and the path going forward. Band is an open protocol that facilitates the governance of data used in decentralized blockchain systems. The protocol functions as an open standard for data handling and management for any decentralized program or application that requires trusted and reliable data. All blockchain platforms in existence that operate and execute code ‘trustlessly’ in smart contracts suffer from the same centralizing issues that arise when needing to use external data points. Many decentralized systems rely on being able to perform basic tasks and computations that require external data feed points such as price feeds, inter-chain communications, real-world events and external web API interactions. Essentially,  smart contracts cannot access data by themselves  — there is no simple and intuitive query interface for decentralized applications to receive real-world data. Until decentralized applications can interface real-world external data inputs into simple function calls there will be significant barriers in adoption of the technology and the accessibility for developers to realize new applications. Existing data availability solutions for blockchain smart contracts either depend on highly critical central points of failure or are subject to asynchronous interactions which causes delays and complicates the smart contract logic execution. In the permissionless environments of decentralized systems the economic incentive and temptation to corrupt and attack critical data sources of a systems operation can be significant. Without strong incentive mechanisms to ensure high quality and reliable data provision, decentralized systems and applications will persistently suffer from these security risks. For example, if an external data source provided by an ‘oracle’ controls the data inputs to a smart contract, then it has the ability to determine the response and behavior of that smart contract. Essentially the data source controls that smart contract — if the oracle is compromised then so is the smart contract and all the systems depending on it, creating a significantly weak point in the security and censorship resistance characteristics of blockchains. For decentralized applications to continue to be able to become increasingly sophisticated and useful, they must be able to use and replicate equivalent tools used in centralized settings for their decentralized system counterparts. Ultimately enabling developers to build the decentralized applications of tomorrow that will improve people’s lives. Band Protocol is a Web 3.0 component layer solution for managing data that resolves the data availability and reliability problem for blockchains. DApps using Band Protocol consume data via BAND public smart contract data points rather than through oracles that are external to the blockchain the dApp is built on.  BAND data feeds are community-curated data sources,  providing a framework for dApp users and developers alike to self moderate, curate and manage the data sources   such that they can be trusted and reliable for their intended purposes. By creating a standard framework for the community governance of data, Band can create a socially scalable method for the widespread adoption and integration of trusted data that all dApps can utilize. Band data interfaces are source and application agnostic, meaning that can be applied for any purpose that the community curating the data deems fit. Sources can be aggregated using mean, median or mode and can be sourced from multiple sources such as centralized external feeds or aggregated smart contract data. Most importantly  Band does not define how the data is treated, rather it provides the means for a community to collectively decide how it will be used and curated.  No assumptions are made as to how the data should be curated and treated by Band, this power is placed entirely in the hands of the community that wishes to use that data for their decentralized application, creating optimally incentivized participants that align with a common goal of creating a reliable data source for their decentralized applications. Band Protocol provides solutions and products for decentralized data governance in four key areas: The tokenization of datasets is necessary for the correct incentive mechanisms to be realized for effective protocol operation. In the permission-less environment of Web3.0, participants performing work must receive direct economic benefits (such as token payments) in order for the system to operate effectively and for participants to not act maliciously. Dataset tokens are issued and valued for each associated dataset and give the ability for an incentive mechanism to be tied to the specific mechanics of that particular dataset. It also gives the opportunity for new business models to emerge where users retain control and sovereignty over their data. Bonding curves  (similar to the structure of UniSwap and Bancor) are used to issue the Dataset tokens such that there will always be liquidity and availability for use of these tokens and hence maintain continuous effective operation of the protocol. DApps cannot directly access trusted data for their dApp to work. Without access to external data and APIs the use cases for DApps are very limited. Existing data feed solutions such as oracles are either very centralized with critical single points of failure or are very nascent, inefficient and developer unfriendly such as prediction markets which presently are too illiquid to be practical to meet a DApp developers needs. For example many decentralized finance and betting applications suffer in their security models due to their need to access to price feed and external event data. Band Protocol provides a standard framework for users and developers of these DApps to manage the access to data sources and incentivise their correct usage resulting in more robust and usable DApps. In this framework the responsibility of providing and curating data is delegated to elected top data providers. These top data providers are governed by the Band Token holders, allowing them to remove underperforming providers to ensure optimal operation. An elected data provider stakes dataset tokens along with other token holders who stake on their behalf, and collects rewards in the form of the fees paid by the dApps that consume the provided data. Centralized information providers are more susceptible to corruption and malicious behaviour without appropriate checks and balances. Direct curation through  token curated registries  provides a more reliable and transparent way for data to be organized and managed for a particular group wishing to use that particular data source, with incentives to ensure their correct management. Band Protocol will be building community-curated blockchain content platform, CoinHatcher, that focuses on providing accurate cryptocurrency price, volume, news and research data. A major factor limiting enterprise participation in the public blockchain ecosystem is the lack of a means for companies to share data securely and privately in a decentralized system. This results in data remaining highly siloed and forces many to resort to more permissioned blockchain solutions. Band will build a private information sharing solution for decentralized technology with a focus on identity management, reputation systems, credit scoring and supply chain. Band Protocol design now encompasses more robust method to govern, manage and curate trusted data, which can accommodate more use cases as we move toward mass adoption. In future blog posts, we will elaborate more on the above mechanics, their specific use cases, and their limitations. For more technical detail, check out our new  whitepaper . Currently there is  documentation  and  tools  available now for developers to build and deploy their own token curated dataset on testnet. We also have sample applications:  data feed  and  CoinHatcher  beta version available on testnet. We are excited to keep you up to date on our development and will be providing further updates on our products and data feed soon. We ask you to join us in building the data governance framework for Web 3.0 as we strive for a decentralized future and if you have any questions please feel free to ask us! Onwards, Soravis Srinawakoon, CEO Band Protocol Band Protocol is a protocol for decentralized data governance. It operates as an open-source standard and framework for the decentralized management of data. Build and manage off-chain oracles, reputation scores, identity management systems, token issuances and token curated registries. Website  |  Whitepaper  |  Telegram   |  Medium  |  Twitter  |  Reddit  |  Github"
"Releasing lookml-tools: better Looker code, user experience, and data governance","eleasing lookml-tools: better Looker code, user experience, and data governanc","At WW, we leverage  Looker  as our primary business intelligence tool. This helps us understand our business and enhance our member experience. A core feature of Looker is LookML. This is a flexible markup language that defines the sources of data and how it is transformed and joined while also defining the  dimensions  (the data filters, such as country or time) and the  measures  (the aggregates, such as totals and averages) that the user is exposed to. In this post, we are pleased to announce  lookml-tools , a new toolkit that our customer intelligence engineering team is open-sourcing to help the Looker community — especially LookML developers — write cleaner, more consistent code, deliver a better end user experience, and enhance data quality and governance. The library contains three tools: Let’s dig into each of these components. To provide a consistent Looker user experience, we developed a LookML style guide. This provides rules for user-facing aspects such as naming conventions and how to ensure that users can navigate and explore the data effectively, as well as best practices under the hood. This is good for both end users as well as the engineering and analytics staff who develop the LookML models, views, and dashboards. How do we evaluate LookML and flag any infractions? We developed a sweet and simple Python linter that runs these checks periodically and writes its finding — a report of which LookML dimensions, measures, or files passed or failed the rules — back into Looker itself, where they can be measured, dashboarded, and alerted. While we implemented 11 rules into the style guide (the declutter rule is difficult to define and alert against), we also made sure that the framework is very easy to extend if we want to create new rules. In fact, most rules are just 3–5 lines of code that evaluate whether a chunk of LookML is relevant for this rule and whether it passed. As such, we hope that the community can contribute to a larger suite of rules that we can all use. As developers write LookML code, they create different, interrelated files:  models  that define data sources,  views  that expose the dimensions and measures for some data source, and  explores  that group views into a logical unit, often around a data source. For instance, perhaps the model defines a Google Analytics data source, and one view focuses on session data while another covers referrals, and the explore file groups sessions and referral views into a single GA-focussed unit. When you have multiple developers building out the LookML, especially if they are new to a data source or to LookML development, you might test out ideas and start to lose the forest for the trees: it becomes hard to keep track of the bigger picture and relationships among these files. This is where the grapher comes in. This tool parses the LookML files and produces a network diagram to show the relationships. In the example below, we can see the relationships between the models (blue), the explores (green), and the views (purple) as well as code reuse. Moreover, you can spot a single orange dot at the top. This is an orphan file, a view that is not referenced by any explore and represents dead code that can be removed from the repository. Data is higher quality if it is defined. When Looker users mouse over a dimension or measure, a description of that term will pop up and help them understand what the metric means  but only if a description is included in the LookML . We can check for that with our linter. However, where should those definitions originate? While developers could add those into the LookML directly and manually themselves, they will inevitably get out of sync with other systems. Data is higher quality if it is consistently defined across systems, and many companies have multiple business intelligence tools. Thus, we wanted to create a system that took definitions from a master source and injected or updated them in the LookML automatically, creating a pull request for the LookML repository admins to review. This is the updater component of  lookml-tool s. As such, it solves both the syncing and consistency problem, enhancing data governance. In this case, we use a data catalog as our master source, and the updater queries from that master database to obtain the list. We maintain a mapping table from those master definitions to individual dimensions and measures, and then the updater parses the LookML repo. You don’t have to use a data catalog, you can just maintain a list of master definitions as a CSV file and  lookml-tools  can use that. The key is that you are freeing developers of a tedious but important task and delegating it to a machine: the code runs periodically in Docker. We are excited to release lookml-tools: We hope that others find this useful, and we would love feedback, suggestions, and hopefully contributions. Create a pull request!"
Data Governance Framework — Implementation Guide,ata Governance Framework — Implementation Guid,"Collecting, storing, and interpreting data has turned information into valuable assets that enable organizations to be more agile and competitive in a global business environment. However, you can only use these data assets for  digital transformation  if you can manage them effectively. However, organizations need more than just data management; they need a management system that sets the rules for every activity. The system must break down data silos across different departments, answer questions about data ownership, and provide solutions to the growing need for  big data  and its various benefits. Now is the time if you have decided to create a data governance framework. This structure should oversee data standards and delegate the necessary roles and responsibilities within your organization to the business ecosystem in which your company operates. But you need to choose the one that suits your organization and your future business goals, models, and strategies. Let’s look at data governance, its structure, and how you can implement a data governance framework in your company. Virtually every organization has implemented data governance framework elements to support its transformational initiatives. The data management market will be worth  $136.4  billion by 2026, up from $1.81 billion in 2020. A 2021 survey of Fortune 1000 companies shows that  48.5%  of respondents use data as the basis for innovation. In addition, data-driven businesses are streamlining processes and growth plans by using structures to manage data. It is an important practice for any company looking to gain a competitive edge in today’s business environment. Implementing a data governance framework means getting consistent, compact, and clean data. It ensures better analytics, better  security  and compliance, better business decisions, and better results. The lack of a data governance structure results in managers, team leaders, or line employees being forced to make crucial decisions based on limited, outdated, or even inaccurate data. A  McKinsey survey  found that about 30% of all time in an organization is spent on non-value-adding tasks due to data’s poor quality and availability. Without quality governance, companies are not only missing out on data-driven opportunities; they are wasting resources. The problem is that most management programs today are ineffective. Top management is often unaware of the potential for value creation in data governance. As a result, the program becomes a set of policies and guidelines relegated to a support function performed by the IT department and not universally applied, making data-driven initiatives equally ineffective. In other cases, organizations are trying to use technology to solve a problem. While technology solutions such as  data lakes  and data management platforms can help, they are not a magic bullet. Although the terms “data governance” and “data management” are often used interchangeably, there is a difference. Both disciplines ensure that enterprise data remains a valuable business asset. It is especially important as many companies migrate their core data to the cloud, where information management and skilled data governance are critical to a successful transition. According to the  Data Governance Institute , this is a practical and actionable framework that helps data stakeholders in any organization identify and meet their information needs. It is a specific set of principles and processes that govern how data is collected, stored, and used. At a minimum, the data governance framework should establish the following policies for each data asset in the organization: Structure:  determines how data will be organized, retrieved, and stored; Access:  appoints who of the employees and third parties will have access to data; Usage:  determine parameters and restrictions on use to minimize legal risks, ensure consumer data privacy, and comply with regulations; Classification:  categorizes data into levels of sensitivity such as internal vs. public or classified vs. restricted; Integrity and interoperability:  establishes standards to ensure accuracy, validity, and reliability so data can be trusted for decision making. A data governance framework allows an organization to have complete access to data and manage it, derive value from it, minimize cost and complexity, manage risk, and ensure that the organization can meet the ever-increasing demand. It will drive business transformation at many levels within the organization: Management:  oversight of corporate data assets, their value, and their impact on changing business operations and market opportunities; Finance:  ensure consistent and accurate reporting for finance departments; Sales and marketing:  provide reliable information about customer preferences and behavior; Procurement:  strengthen cost-reduction and operational efficiency initiatives based on data and collaboration across the business ecosystem; Manufacturing:  this will be important when deploying automation; Legal Aspects:  from legal and compliance standpoints, this will be the only way to meet growing regulatory requirements The data governance framework consists of the processes, policies, rules, organizational structures, and technologies implemented within the governance program. It also includes the program’s mission statement, its goals and measures of success, and decision-making responsibility and accountability for different functions that will be part of the program. The organization’s governance structure should be documented and published so that all participants know how the program will work. As an example, we will give the data governance framework example of the Data Governance Institute. It consists of 10 components: Keep the following data governance principles in mind to ensure a successful implementation. Any successful data governance approach requires a high level of accountability. Data governance in an organization will be pointless, dumb, and useless if no one takes responsibility. Throughout the organization, you must implement ownership and accountability. If only one department, such as IT, takes charge, other business leaders may begin to view the company-wide problem as something that only IT needs to understand. Tip:  Create a data governance board that includes representatives from all departments, including senior executives, to ensure cross-organizational accountability. The newly formed data governance board should develop data processes and policies that will be followed throughout the organization and strictly enforced. Every successful data governance process will need to develop and comply with uniform rules and regulations to protect data and ensure all applicable external laws process it. These standardized rules and regulations, created at the data board level and implemented by the data controller, will provide criteria for all aspects of data use. Tip:  Comply with regulations and avoid penalties by documenting the origin of data assets and access controls associated with data. Firms that underinvest in governance expose their organizations to real regulatory risk, which can be costly. Responsibility follows accountability for effective data governance in an organization. A dedicated data administrator is key to ensuring proper accountability for your company’s data. This person reports to the data council, enforces data rules and regulations and follows them regularly. Tip:  Optimize staff performance by providing data assets that meet the desired quality thresholds. People need to use high-quality and reliable  data  to make informed and productive business decisions. However, you would be surprised how few companies can fully trust their data. Tip:  Create a common set of data quality standards to improve the quality of your business data. These standards will ensure that data quality governance is assessed and documented. In addition, it helps to ensure that the entire organization knows and follows these guidelines. All data governance processes established in an organization should be as transparent as possible. By keeping a consistent record of all relevant data governance activities and procedures, any third-party audit to see exactly how sensitive data is handled, what you used it for, and why you used it the way you did. This openness will protect the company from a data breach and allow you to understand better how the data is being used. Tip:  Improve data security by establishing data ownership and responsibilities. In a survey conducted by McKinsey,  87%  of respondents said they would not work with a company if they had concerns about its security practices, and 71% said they would stop doing business with a company if it disclosed confidential information without permission. By implementing a data governance framework, you can protect your customers’ data from potential harm. These principles are the keystone of any effective data governance policy, allowing key personnel in the organization to assist in the implementation and regularly review compliance. What is a governance model? It’s a system with a basic structure that serves as the foundation and support for its many parts. A governance model represents the many feasible ways a system could work if you changed one or more elements. There are several data governance models that you can use depending on your business needs and the types of data governance you use. We will describe three of them in more detail. In the centralized data governance model, one person is usually designated as the data manager responsible for decision-making and program management. This person is also answerable for managing the master data and distributing it to users as needed or requested. This model is ideal for the individual business owner who manages and maintains all their data. It provides: This model also has its disadvantages. Being a top-down approach, centralized models feed bureaucracy and often fail to meet the unique needs of each business area. A committee typically develops and manages an enterprise data governance strategy in a decentralized model. However, business functional areas create and manage their datasets and distribute information to users. The advantages of this model are: In terms of risks, inconsistencies and duplication of key data are possible in a decentralized model, affecting consensus, reporting, and decision making. A centralized data governance framework provides the structure, technology, and best practices to follow in a hybrid model while teams work autonomously. Each business area owns its data and metadata and is free to develop standards, policies, and procedures that best suit its business needs. This federated model provides: If we talk about the shortcomings, then for such autonomy, deep knowledge in each area of ​​business is required. And you’ll have to take steps to ensure data consistency and coordination across teams and the organization. Measuring your company up against a data governance  maturity model  can be useful in building the roadmap and communicating the as-is and to-be part of the data governance initiative and the circumstance for deploying a data governance framework. One example of a maturity model offers Gartner, an analyst firm. Most organizations that start creating a data governance program find themselves in the lower phases of this model. Data governance is an ongoing program and not a technology solution, but some standardized management tools can help support data governance standards. They are not a required component of the platform, but support collaboration, policy development, process documentation, schedule and workflow management, data cataloging, and more. Companies can also use data quality management, metadata, and master data management tools. The best data management tools offer  IBM ,  Informatica ,  Information Builders , and data governance specialists  Adaptive ,  ASG Technologies ,  Ataccama ,  Collibra ,  Erwin ,  Infogix , and  Talend . In most cases, governance tools are a part of larger suites, including metadata management and data origin functionality. While every organization is unique, you don’t have to recreate the wheel and re-invent your structure. These data governance guidelines will help you create a roadmap. Create a data strategy by bringing together existing processes, people, and workflows. The core of a strong data governance program is getting people to take accountability. It assigns responsibilities and makes specific folks accountable for particular data domains. Outcome:  A data governance strategy helps prevent your organization from having “bad data” and poor decisions based on them. Implementing a solid data governance framework takes time. And the first thing we recommend is to find an approach that meets the needs of the business and is useful for data consumers. Outcome:  by starting with improving the quality of data in several key business processes, you will demonstrate the value of implementing corporate governance. Working with relevant stakeholders to develop standards and rules for data governance is the first step toward implementing successful data governance practices. Outcome:  informing key stakeholders about the business benefits of data governance is essential to gaining support. Here are the traditional data governance roles in setting up early in the process: Outcome:  Assigning business outcomes and ownership levels provides fundamental support for this massive change for the organization. Reaping the benefits of an enterprise-wide data management program requires dedication and investment of time and resources. It cannot be easy to convince stakeholders to take on a challenge unless you have a compelling business case. So you can start by recognizing the core data elements and the critical business processes they support. Then, describe the costs associated with managing, integrating, and validating these elements with ongoing manual processes. Outcome:  By highlighting the potential business impact of manual process failures, you can prove the value of adopting a data governance strategy. Demonstrating business value on an ongoing basis requires the development of quantitative metrics, especially concerning data quality improvement. It may include the number of quarterly data errors, resulting in revenue or cost savings. Other common data quality metrics measure datasets’ accuracy, error rate, and related attributes such as data completeness and consistency. Outcome:  The information gained from consistently tracking and collecting metrics helps maintain engagement and demonstrates the success of a data management program. Metrics display the effectiveness of processes and policies and identify areas for improvement. Data governance is critical to delivering value through analytics, digitalization, and other transformational opportunities. You will succeed much faster by transforming your thinking about data governance as a foundation and policy and incorporating it strategically into your day-to-day operations. Ready to bring industry-leading data governance to your organization? Take the next step by hiring  Jelvix  to develop an overall data governance strategy and framework. We use a people-centric approach and can organically launch governance based on what people are already doing. Originally published at  https://jelvix.com ."
What We Got Wrong About Data Governance,hat We Got Wrong About Data Governanc,"Data governance is top of mind  for many of my customers, particularly in light of GDPR, CCPA, COVID-19, and any number of other acronyms that speak to the increasing importance of data management when it comes to protecting user data. Over the past several years,  data catalogs have emerged as a powerful tool for data governance , and I couldn’t be happier. As companies digitize and their data operations democratize, it’s important for all elements of the data stack, from warehouses to business intelligence platforms, and now, catalogs, to participate in compliance best practices. But are data catalogs all we need to build a robust data governance program? Analogous to a physical library catalog,  data catalogs  serve as an inventory of metadata and give investors the information necessary to evaluate data accessibility, health, and location. Companies like Alation, Collibra, and Informatica tout solutions that not only keep tabs on your data, but also integrate with machine learning and automation to make data more discoverable, collaborative, and now, in compliance with organizational, industry-wide, or even government regulations. Since data catalogs provide a single source of truth about a company’s data sources, it’s very easy to leverage data catalogs to manage the data in your pipelines. Data catalogs can be used to store metadata that gives stakeholders a better understanding of a specific source’s lineage, thereby instilling greater trust in the data itself. Additionally, data catalogs make it easy to keep track of where personally identifiable information (PII) can both be housed and sprawl downstream, as well as who in the organization has the permission to access it across the pipeline. So, what type of data catalog makes the most sense for your organization? To make your life a little easier, I spoke with data teams in the field to learn about their data catalog solutions, breaking them down into three distinct categories: in-house, third-party, and open source. Some B2C companies — I’m talking the  Airbnbs ,  Netflixs , and  Ubers  of the world — build their own data catalogs to ensure data compliance with state, country, and even economic union (I’m looking at you GDPR) level regulations. The biggest perk of in-house solutions is the ability to quickly spin up customizable dashboards, pulling out fields your team needs the most. While in-house tools make for quick customization, over time, such hacks can lead to a lack of visibility and collaboration, particularly when it comes to understanding data lineage. In fact, one data leader I spoke with at a food delivery startup noted that what was clearly missing from her in-house data catalog was a “single pane of glass.” If she had a single source of truth that could provide insight into how her team’s tables were being leveraged by other parts of the business, ensuring compliance would be easy. On top of these tactical considerations, spending engineering time and resources building a multi-million dollar data catalog just doesn’t make sense for the vast majority of companies. Since their founding in 2012,  Alation  has largely paved the way for the rise of the automated data catalog. Now, there are a whole host of ML-powered data catalogs on the market, including  Collibra ,  Informatica , and others, many with pay-for-play workflow and repository-oriented compliance management integrations. Some cloud providers, like Google, AWS, and Azure, also offer data governance tooling integration at an additional cost. In my conversations with data leaders, one downside of these solutions came up time and again: usability. While nearly all of these tools have strong collaboration features, one Data Engineering VP I spoke with specifically called out his third-party catalog’s unintuitive UI. If data tools aren’t easy to use, how can we expect users to understand or even care whether they’re compliant? In 2017, Lyft became an industry leader by open sourcing their data discovery and metadata engine,  Amundsen , named after the famed Antarctic explorer. Other open source tools, such as  Apache Atlas ,  Magda  and  CKAN , provide similar functionalities, and all three make it easy for development-savvy teams to fork an instance of the software and get started. While some of these tools  allow teams to tag metadata  within to control user access, this is an intensive and often manual process that most teams just don’t have the time to tackle. In fact, a product manager at a leading transportation company shared that his team specifically chose not to use an open source data catalog because they didn’t have off-the-shelf support for all the data sources and data management tooling in their stack, making data governance extra challenging. In short, open source solutions just weren’t comprehensive enough. Still, there’s something critical to compliance that even the most advanced catalog can’t account for:  data downtime . Recently, I developed  a simple metric  for a customer that helps measure  data downtime , in other words, periods of time when your data is partial, erroneous, missing, or otherwise inaccurate. When applied to data governance, data downtime gives you a holistic picture of your organization’s data reliability. Without data reliability to power full discoverability, it’s impossible to know whether or not your data is fully compliant and usable. Data catalogs solve some, but not all, of your data governance problems. To start, mitigating governance gaps is a monumental undertaking, and it’s impossible to prioritize these without a full understanding of which data assets are actually being accessed by your company. Data reliability fills this gap and allows you to unlock your data ecosystem’s full potential. Additionally, without real-time lineage, it’s impossible to know how PII or other regulated data sprawls. Think about it for a second: even if you’re using the fanciest data catalog on the market, your governance is only as good as your knowledge about where that data goes. If your pipelines aren’t reliable, neither is your data catalog. Owing to their complementary features,  data catalogs  and  data reliability solutions  work hand-in-hand to provide an engineering approach to data governance, no matter the acronyms you need to meet . Personally, I’m excited for what the next wave of data catalogs have in store. And trust me: it’s more than just data. If you want to learn more, reach out to  Barr Moses ."
The New Face of Data Governance,he New Face of Data Governanc,"This article has been co-written with  Molly Vorwerck , Head of content at Monte Carlo. Data governance doesn’t need to be a hassle. It just needs to meet data teams where they are: in distributed organizations. Here’s why our longstanding approach to data governance isn’t a fit for the modern data stack and what some of the best data teams are already doing about it. Data governance is top of mind for many data leaders, particularly in light of GDPR, CCPA,  IPOs , COVID-19, and any number of other acronyms that speak to the increasing importance of compliance and privacy when it comes to managing your company’s data. Traditionally, data governance refers to the process of maintaining the availability, usability, provenance, and security of data, and, as a data leader once told us, is the “keep your CFO out of jail card.” Still, Gartner suggests that  more than 80 percent  of data governance initiatives will fail in 2022. Frankly, we’re not surprised. While  data governance  is widely accepted as a must-have feature of a healthy data strategy, it’s hard to achieve in practice, particularly given the demands of  the modern data stack . We owe this shift in how we approach governance to one key catalyst:  the cloud . Before the rise of the cloud data warehouse (thanks Snowflake!), analytics was primarily handled by siloed teams. Cloud architectures and the tools that powered them made it cheaper and easier than ever before to spin up analytics dashboards and reports. Suddenly, Joan from Marketing and Robert from Finance could glean near real-time insights about the business without having to ping the Data Science team every time they had to present to their board. Data was *actually* becoming democratized. Sort of. While the technologies enabling data to become more accessible and easier to use were innovating at an unprecedented pace, the tools and processes in place to ensure that data was easily discoverable and reliable (in other words, data governance) are falling short of our needs. Here’s why. A manual approach to governance is no longer cutting it. While we’ve made great advancements in areas such as self-service analytics, cloud computing, and data visualization, we’re not there yet when it comes to governance. Many companies continue to enforce data governance through manual, outdated, and ad hoc tooling. Data teams spend days manually vetting reports, setting up custom rules, and comparing numbers side by side. As the amount of data sources increase and tech stacks become more complex, this approach is neither scalable nor efficient. While data catalogs often market themselves as the answer to data governance, many data leaders find their catalogs lacking in even the most rudimentary aspects when it comes to manual requirements. In some organizations, these operations take up a significant amount of time in manually mapping upstream and downstream dependencies — not to mention the maintenance work required to keep this up to date. Data is everywhere; data governance is not. For many companies, increasing the speed of data innovation is critical to survival. While data infrastructure and business intelligence tools have advanced to support this innovation over the past several years, DataOps has lagged behind with most DataOps solutions like data quality alerts and lineage tracking being manual, one-dimensional, and unscalable. One of the ways in which DataOps and solutions can catch up is by drawing on concepts from software engineering. Many of the problems we face in data are in fact problems that have been solved in engineering, security, and other industries. As companies migrate to more distributed architectures (i.e.,  the data mesh ), the need for ubiquitous and end-to-end governance has never been greater. The good news? We don’t have to accept the status quo. In the same way that software engineering teams lean on automation, self-healing processes, and self-service tooling, data teams need to take a similar approach by embracing the new face of data governance. Here’s why — and how. Many things have changed since the B.C era. Before painting the new face of data governance, it’s key to understand the structural changes which brought about the need for renewal. This part is dedicated to exploring the catalysts to new data governance practices. In the past few decades, storage and computation costs have plummeted by a factor of millions, with bandwidth costs shrinking by a factor of thousands. This has led to the exponential growth of the cloud, and the arrival of  cloud data warehouses  such as Amazon Redshift or Google BigQuery. The peculiarity of cloud data warehouses is that they are  infinitely more scalable  than traditional data warehouses, with a capacity to accommodate virtually any amount of data. These developments allowed organizations from all horizons to collect and store tremendous amounts of data. Data is now far more disparate and distributed than it used to be and it can be characterized by the “  Three Vs  “ of big data: soaring volume, variety and velocity. The very nature of data has changed, which should prompt a change in data governance models that are meant to manage this data. Until 15–20 years ago, data pipelines were rather basic, serving stable requirements for business analytics. Business intelligence teams required historical measures of their financial position, inventory levels, sales pipelines, and other operational metrics. Data engineers used  ETL (Extract, Load, Transform) tools  to transform the data for specific use-cases and load it in the data warehouse. From this, data analysts created dashboards and reports using business intelligence software. Data pipelines now run with  a combination of complex tools  (Apache Spark, Kubernetes, and Apache Airflow, to name a few), and as the number of interconnected parts increases, so does the risk of  pipeline failure . The diversity of tools is necessary, as it allows data teams to choose the best platforms at each layer of their data stack. But the combination of all these engines makes it practically impossible to gain  visibility  into the different parts of the pipelines. Modern data pipelines are not only complex, but they also have  blackbox features . You know what goes in, you know what comes out, but you have no clue of what happened in between. It’s fine as long as the desired outcome comes out. But when it doesn’t, it’s highly frustrating. When data sets come out of the pipeline, you are often left with strange values, missing columns, letters in fields that were meant to be numeric, and so on. As a result, data engineers spend hours scratching their heads about what on earth went wrong, where, and how to fix it. Forrester estimates that data teams spend upwards of  40% of their time on data quality issues  instead of working on value-generating activities for the business. Traditional data governance models are not suited to these highly complex systems, causing data to break for obscure and unexplainable reasons. We need to shift towards a new model which can deal with this new level of complexity. Data is not only more voluminous, it is also more regulated and scrutinized. Although the first regulations related to data privacy came into force in the 1990s, they only became a worldwide issue in the 2010s with the appearance of  GDPR , HIPAA, etc.. This led to the emergence of data governance tools, which helped enterprise-level organizations comply with these tight requirements. At the time, only enterprise-level organizations could afford the infrastructure necessary to collect and store data. Hence, they were the only ones who faced data compliance issues. But as we’ve seen in the above section, the 2010s also marked the exponential growth of data volumes in organizations thanks to cloud storage. This cheap storage alternative allowed startups and SMBs to collect and store considerable amounts of data. The problem was, GDPR, HIPAA, and other regulations leave  no small business exemptions  and demand that businesses from all sizes follow the law and take their responsibilities for handling personal data. Companies still need to comply with GDPR, even if they have less than 250 employees. This gave rise to the challenge of  data governance for small businesses,  as startups and SMBs started to face enterprise-level problems. Existing data governance tools are out of the question for SMBs, as their pricing model reflects their enterprise focus (i.e they’re super expensive). The cost of collecting and storing data has drastically fallen in the past decades, it’s time for the cost of managing the integrity and security of this data to follow the same trend. New data governance models need to take into account that compliance is not just for big actors, but that everyone has to be serious about it. With only a few specialized people using data, it’s easy to control access to data and enforce some kind of data traceability. The issue is, data is not reserved for a small group of specialists — it’s now used by everyone. Today, companies increasingly engage in  operational analytics  — an approach consisting in making data accessible to “operational” teams, for operational use cases (sales, marketing, ..). We distinguish it from the more classical approach of using data only for reporting and business intelligence. Instead of using data to influence long-term strategy, operational analytics informs strategy for the  day-to-day operations  of the business. Trends such as code-less BI make operational analytics possible by empowering operational teams to manipulate data. Organizations are thus increasingly aiming at democratizing data, ensuring everyone can access the data they need, whenever they need it. Although this brings about many great things, it also creates two major problems: This “data anarchy” cause traditional data governance models to fail, prompting the need for new, more adapted models. Data governance was born as a response to the tight regulations around data that emerged in the past decades. It used to be perceived as this boring, uninteresting task which you have to complete in order to escape hefty fines. Today, things are different. Data governance does not only allow organizations to escape fines, but it also drives business performance. How so? From a short-term viewpoint, a strong data governance program ensures data traceability and data quality. It makes data users more efficient at finding and understanding data. It also introduces the ability to reuse data and data processes, thus reducing repeated work in organizations. A more productive data team maximizes the income generation of potential data. In the long term, good data governance increases consistency and confidence in decision-making. It’s much easier to make a decision when numbers and dashboards from all departments agree than when they go in opposite directions. Disagreement between numbers leads to trust erosion, making the huge investments in data collection largely ineffective. To recapitulate this part, we went from a world in which a few people needed access to limited amounts of unregulated data, to a world in which everyone needs access to vast amounts of highly regulated data. And it’s no surprise that these two worlds should be governed in a different manner. In light of the changes in the nature of data, the level of data regulation, and the data democratization trend, it’s safe to say that the traditional, old, boring, data governance is dead. We can’t let it in the grave, as we need data governance more than ever today. Our job is thus to resurrect it, and give it a new face. This section is dedicated to understanding what this new face should look like for modern data governance models to work. A good data governance program should ensure flawless data compliance. Data compliance refers to the practice of ensuring that all sensitive data is managed and organized in a way that enables organizations to meet legal and governmental regulations. It pertains to the privacy of information considered as personal, and how businesses manage this sensitive data. New data governance models should have you be confident that your organization is adhering to regulations. This should be the building block of data governance programs, as regulations will only tighten in the future, with fines becoming more important. Data governance should embrace the trends of operational analytics and data democratization, and ensure that anybody can use data at any time to make decisions with no barrier to access or understanding. Data democratization means that there are no gatekeepers creating a bottleneck at the gateway to data. This is worth mentioning, as the need for data governance to be secure and compliant often leads programs to create bottlenecks at the gateway to data, as the IT team is usually put in charge of granting access to data. Operational people can end up waiting hours until they manage to get access to a dataset. By then, they have already given up on their analysis. It’s important to have security and control, but not at the expense of the agility that data offers. Traditional data governance programs have organizations manage data through the  data steward or a centralized IT team . Given the change in data volumes, this way of doing things can’t do the trick anymore. In fact, it becomes virtually impossible for a single department to keep track of all the organization-wide data. For this reason, data governance must shift towards a distributed model. By distributed, we refer to an organizational framework for  delegating data management responsibilities  throughout the enterprise. It means the responsibility of data management is split between data users in the organization. It works great, as data users have grown in number thanks to  data democratization,  which makes it easier to share the burden of data governance. The goal of a distributed data governance model is to allow teams that are closest to the data to manage access and permissions while eliminating the bottleneck that currently exists with centralized IT. This system accomplishes a lot, as it allows data governance to be  both secure & compliant while making data accessible to al l. We thus need to find a framework or tools which can orchestrate the collaboration of data users to support the data governance effort. Of course, that’s no easy task. If you’re interested in how to achieve distributed data stewardship, check out  this article . Data governance processes used to be conducted manually. Yet, data is alive and processes change every hour. What’s more, data volumes managed by organizations make it practically impossible to track data assets manually. It would mean maintaining metadata for 10+ fields for thousands of tables manually. With current data volumes, it would mean hiring a full team just to work on data governance issues. For this reason, it’s time to turn towards automated ways of orchestrating data governance. Automated data governance tools take 10 min to set up on your cloud data warehouse (compared with 6 months for non-automated tools), and minimizes the fields that have to be maintained manually. Data teams serious about governance need to embrace technologies that lean into the distributed, scalable nature of the cloud AND the distributed nature of modern data teams. To get there, we need to reframe our approach across three different pillars of governance: observability, discovery, and security. Observability Instead of putting together a holistic approach to address unreliable or inaccurate data, teams often tackle data quality on an ad hoc basis. Much in the same way DevOps applies observability to software, data observability platforms give data teams the ability to monitor, alert for, root cause, fix, and even prevent data issues in real-time. Data observability refers to an organization’s ability to fully understand the health of the data in their system, and supplements data discovery by ensuring that the data you’re surfacing is trustworthy at all stages of its life cycle. Like its DevOps counterpart, data observability uses automated monitoring, alerting, and triaging to identify and evaluate data quality and discoverability issues, leading to healthier pipelines, more productive teams, and happier customers. Some of the best solutions will also offer the ability to create custom rules and  circuit breakers . Data observability is broken down into its own five pillars: freshness, distribution, volume, schema, and lineage. Together, these components provide valuable insight into the quality and reliability of your data. Freshness:  Freshness seeks to understand how up-to-date your data tables are, as well as the cadence at which your tables are updated. Freshness is particularly important when it comes to decision making; after all, stale data is basically synonymous with wasted time and money. Distribution:  Distribution, in other words, a function of your data’s possible values, tells you if your data is within an accepted range. Data distribution gives you insight into whether or not your tables can be trusted based on what can be expected from your data. Volume:  Volume refers to the completeness of your data tables and offers insights on the health of your data sources. If 200 million rows suddenly turns into 5 million, you should know. Schema:  Changes in the organization of your data, in other words, schema, often indicates broken data. Monitoring who makes changes to these tables and when is foundational to understanding the health of your data ecosystem. Lineage:  When data breaks, the first question is always “where?” Data lineage provides the answer by telling you which upstream sources and downstream ingestors were impacted, as well as which teams are generating the data and who is accessing it. Good lineage also collects information about the data (also referred to as metadata) that speaks to governance, business, and technical guidelines associated with specific data tables, serving as a single source of truth for all consumers. With data observability, you can monitor changes in the provenance, integrity, and availability of your organization’s data, leading to more collaborative teams and happier stakeholders. Data discovery posits that different data owners are held accountable for their data as products, as well as for facilitating communication between distributed data across different locations. Once data has been served to and transformed by a given domain, the domain data owners can leverage the data for their operational or analytic needs. Data discovery replaces the need for a traditional data governance platform by providing a domain-specific, dynamic understanding of your data based on how it’s being ingested, stored, aggregated, and used by a set of specific consumers. Governance standards and tooling are federated across these domains (allowing for greater accessibility and interoperability), a real-time understanding of the data’s current (as opposed to ideal) state is made easily available. Data discovery can answer these questions not just for the data’s ideal state but for the current state of the data across each domain: While modern data stack tools open up additional capabilities for working with data, they must also be protected and appropriately managed to ensure that only the people that should have access to data are using it. To start the New Year on a more secure foot, we suggest data engineering teams partner with their security and legal counterparts to conduct a cross-organizational “data audit.” According to  Atul Gupte , former Data Product Manager at Uber, the first job for any data leader is to audit the data that you’re collecting and storing, as well as who has access to this data (they referred to it as their “Data Blast Radius”). In 2022, most businesses will rely on a combination of enterprise-level transactional data residing in traditional data warehouse systems, event streaming, and other data platform capabilities, including your company’s operations strategy. Both requirements emphasize the need for  a robust and automated  approach to policy enforcement that prioritizes PII identification and access control to capture the meaning, location, usage patterns, and owners of your data. At the end of the day, solving for data governance extends beyond implementing the right technologies. Achieving equitable data access and end-to-end trust relies on building better processes, too. After all, it doesn’t matter what’s in your data stack if the data itself can’t be found, trusted, or secured. It’s time we adopted a new culture that prioritizes these three sides of data governance’s new face — and in the process, gives data teams the ability to empower data democratization and better decision making across the business. Data governance doesn’t need to be a hassle — it just needs to meet data teams where they are. Originally published at  https://www.castordoc.com ."
Getting started with Puppeteer and Chrome Headless for Web Scraping,etting started with Puppeteer and Chrome Headless for Web Scrapin,"[Update]: You can read Chinese version of this article  here . For sure, Chrome being the market leader in web browsing, Chrome Headless is going to be industry leader in Automated Testing of web applications. So, I have put together this starter guide on how to get started with  Web Scraping  in Chrome Headless. Puppeteer  is the official tool for Chrome Headless by Google Chrome team. Since the official announcement of Chrome Headless, many of the industry standard libraries for automated testing have been discontinued by their maintainers. Including PhantomJS. Selenium IDE for Firefox has been  discontinued  due to lack of maintainers. Puppeteer’s team had concerns about the usage of  jsdom  when  page.evaluate  method could be used for DOM manipulation. So, I have updated the code snippets as well as the repository to rely on  evaluate  method for text extraction. In this guide we will scrape GitHub, login to it, extract and save emails of users using  Chrome Headless ,  Puppeteer ,  Node  and  MongoDB . Don't worry GitHub have rate limiting mechanism in place to keep you under control but this post will give you good idea on Scraping with Chrome Headless and Node. Also, stay updated with the  documentation  because  Puppeteer  is under development and APIs are prone to changes. Here is the accompanying  GitHub repository . Before we start, we need following tools installed. Head over to their websites and install them. Start off by making the project directory Initiate NPM. And put in the necessary details. Install  Puppeteer . Its not stable and repository is updated daily. If you want to avail the latest functionality you can install it directly from its GitHub repository. Puppeteer includes its own chrome / chromium, that is guaranteed to work headless. So each time you install / update puppeteer, it will download its specific chrome version. We will start by taking a screenshot of the page. This is code from their documentation. If its your first time using  Node  7 or 8, you might be unfamiliar with  async  and  await  keywords. To put  async/await  in really simple words, an async function returns a Promise. The promise when resolves might return the result that you asked for. But to do this in a single line, you tie the call to async function with  await . Save this in  index.js  inside project directory. Also create the screenshots directory. Run the code with The screenshot is now saved inside  screenshots/  dir. If you go to GitHub and search for  john , then click the users tab. You will see list of all users with names. Some of them have made their emails publicly visible and some have chosen not to. But the thing is you can’t see these emails without logging in. So, lets login. We will make heavy use of  Puppeteer documentation . Add a file  creds.js  in project root. I highly recommend signing up for new account with a new dummy email because you might end up getting your account blocked. Add another file  .gitignore  and put following content inside it: For visual debugging, make chrome launch with GUI by passing an object with  headless: false  to  launch  method. Lets navigate to login Open  https://github.com/login  in your browser. Right click on input box below  Username or email address  and select  Inspect . From developers tool, right click on the highlighted code and select  Copy  then  Copy selector . Paste that value to following constant Repeat the process for Password input box and Sign in button. You would have following Puppeteer provides methods  click  to click a DOM element and  type  to type text in some input box. Let's fill in the credentials then click login and wait for redirect. Up on top, require  creds.js  file. And then add this code to the function to fill in credentials and login Now, we have logged in. We can programmatically click on search box, fill it and on the results page, click users tab. But there’s an easy way. Search requests are usually GET requests. So, every thing is sent via URL. So, manually type  john  inside search box and then click users tab and copy the URL. It would be Rearranging a bit Lets navigate to this page and wait to see if it actually searched? We are interested in extracting  username  and  email  of users. Lets copy their DOM element selectors like we did earlier. You can see that I also added  LENGTH_SELECTOR_CLASS  above. If you look at the page's code inside developers tool, you will observe that  div s with class  user-list-item  are actually housing information about a single user each. Currently one way to extract text from an element is by using  evaluate  method of  Page  or  ElementHandle . When we navigate to page with search results, we will use  page.evaluate  method to get the length of users list on the page. The  evaluate  method evaluates the code inside browser context. Let’s loop through all the listed users and extract emails. As we loop through the DOM, we have to change index inside the selectors to point to the next DOM element. So, I’ve put the  INDEX  string at the place where we want to place the index as we loop through. The loop and extraction Now if you run the script with  node index.js  you would see usernames and their corresponding emails printed. First we would extimate the last page number with search results. At search results page, on top, you can see 69,769 usersat the time of this writing. Fun Fact: If you compare with the previous screenshot of the page, you will notice that 6 more john s have joined GitHub in the matter of a few hours. Copy its selector from developer tools. We would write a new function below the  run  function to return the number of pages we can go through. At the bottom of the search results page, if you hover the mouse over buttons with page numbers, you can see they link to the next pages. The link to 2nd page with results is  https://github.com/search?p=2&q=john&type=Users&utf8=%E2%9C%93 . Notice the  p=2  query parameter in the URL. This will help us navigate to the next page. After adding an outer loop to go through all the pages around our previous loop, the code looks like The part with  puppeteer  is over now. We will use  mongoose  to store the information in to  MongoDB . Its an  ORM , actually just a library to facilitate information storage and retrieval from the database. MongoDB is a Schema-less NoSQL database. But we can make it follow some rules using Mongoose. First we would have to create a  Model  which is just representation of MongoDB  Collection  in code. Create a directory  models . Create a file  user.js  inside and put the following code in it, the structure of our collection. Next whenever we insert something into  users  collection with mongoose, it would have to follow this structure. Let’s now actually insert. We don’t want duplicate emails in our database. So, we only insert a user’s information if the email is not already present. Otherwise we would just update the information. For this we would use mongoose’s  Model.findOneAndUpdate  method. At the top of  index.js  add the imports Add the following function at bottom of  index.js  to upsert (update or insert) the User model Start MongoDB server. Put following code inside the for loops at the place of comment  // TODO save this user  in order to save the user To check if you are actually getting users saved, get inside mongo shell You would see multiple users added there. This marks the crux of this guide. Chrome Headless and Puppeteer is the start of a new era in Web Scraping and Automated Testing. Chrome Headless also supports WebGL. You can deploy your scraper in cloud and sit back and let it do the heavy load. Remember to remove the  headless: false  option when you deploy on server. Here is the accompanying  GitHub repository  containing complete code. Deserts symbolize vastness and are witness of the struggles and sacrifices of people who  traversed  through these giant mountains of sand.  Thal  is desert in Pakistan spanning across multiple districts including my home district Bhakkar. Somewhat similar is the case with  Internet  that we  traversed  today in quest of data. That's why I named the repository  Thal . If you like this effort do remember to  star the project on GitHub  :). If you have any suggestions/questions comment here or approach me directly  @e_mad_ehsan . I would love to hear from you."
Mastering Python Web Scraping: Get Your Data Back,astering Python Web Scraping: Get Your Data Bac,"Do you ever find yourself in a situation where you need to get information out of a website that  conveniently  doesn’t have an export option? This happened to a client of mine who desperately needed lists of email addresses from a platform that did not allow you to export your  own data  and hid the data behind a series of UI hurdles. This client was about to pay out the nose for a data-entry worker to copy each email out by hand. Luckily, she remembered that web scraping is the way of the future and happens to be one of my favorite ways to rebel against “big brother”. I hacked something out fast (15 minutes) and saved her a lot of money. I know others out there face similar issues. So I wanted to share how to write a program that uses the web browser like you would and takes (back) the data! We will practice this together with a simple example: scraping a Google search. Sorry, not very creative :) But it’s a good way to start. Python (I use 2.7) Chrome Chromedriver If you don’t have Pandas and are lazy, I recommend heading over to  Anaconda  to get their distribution of Python that includes this essential & super useful library. Otherwise, download it with pip from the terminal/command line & all of its dependencies.   pip install pandas If you don’t have Splinter (and  are not  using Anaconda’s Python), simply download it with pip from the terminal/command line.   pip install splinter If you don’t have Splinter (and  are  using Anaconda’s Python), download it with Anaconda’s package manager from the terminal/command line.   conda install splinter If you want to set this up in a virtual environment (which has many advantages) but don’t know where to start, try reading our other  blog post about virtual environments. Here we will import all the libraries we need and set up a browser object. If the page you are trying to scrape is responsive, use set_window_size to ensure all the elements you need are displayed. The code above will open a Google Chrome browser. Now that the browser is all set up, let’s visit Google. Great, so far we have made it to the front page. Now we need to focus on how to navigate the website. There are two main steps to achieving this: To find an HTML element you need to use the Chrome developer tools. Right click on the website and select “Inspect”. This will open a box on the right side of the Chrome browser. Then click on the inspect icon (highlighted in red). Next use the inspector cursor to click on a section of the website that you want to control. When you have clicked, the HTML that creates that section will be highlighted on the right. In the photo below, I have clicked on the search bar which is an input. Next right click on the HTML element, and select under “Copy” -> “Copy XPath” Congrats! You’ve now got the keys to the kingdom. Let’s move on to how to use Splinter to control that HTML element from Python. That XPath is the most important piece of information! First, keep this XPath safe by pasting into a variable in Python. Next we will pass this XPath to a great method from the Splinter Browser object: find_by_xpath().  This method will extract all the elements that match the XPath you pass it and return a list of Element objects.  If there is only one element, it will return a list of length 1. There are other methods such as find_by_tag(), find_by_name(), find_by_text(), etc. The code above now gives you navigation of this individual HTML element. There are two useful methods I use for crawling: fill() and click() The code above types CodingStartups.com into the search bar and clicks the search button. Once you execute the last line, you will be brought to the search results page! Tip: Use fill() and click() to navigate login pages ;) For the purpose of this exercise, we will scrape off the titles and links for each search result on the first page. Notice that each search result is stored within a h3-tag with a class “r”. Also take note that both the title and the link we want is stored within an a-tag. The XPath of that highlighted a tag is: //*[@id=”rso”]/div/div/div[1]/div/div/h3/a But this is just the first link. We want all of the links on the search page, not just the first one. So we are going to change this a bit to make sure our find_by_xpath method returns all of the search results in a list. Here is how to do it. See the code below: This XPath tells Python to look for all h3-tags with a class “r”. Then inside each of them, extract the a-tag & all its data. Now, lets iterate through the search result link elements that the find_by_xpath method returned. We will extract the title and link for each search result. It’s very simple: Cleaning the data in  search_result.text  can sometimes be the most frustrating part. Text on the web is very messy. Here are some helpful methods for cleaning data: .replace() .encode() .strip() All of the titles and links are now in the scraped_data list. Now to export our data to csv. Instead of the csv library chaos, I like to use a pandas dataframe. It’s 2 lines: The code above creates a csv file with the headers Title, Link and then all of the data that was in the scraped_data list. Congrats! Now go forth and take (back) the data! In case you want a big picture view, here is the  full code available on our GitHub account . Thanks for reading! If you have questions feel free to comment & I will try to get back to you. Connect with me on Instagram  @lauren__glass  &  LinkedIn Check out my  essentials list on Amazon Visit my website!"
Data Science Skills: Web scraping using python,ata Science Skills: Web scraping using pytho,"One of the first tasks that I was given in my job as a Data Scientist involved Web Scraping. This was a completely alien concept to me at the time, gathering data from websites using code, but is one of the most logical and easily accessible sources of data. After a few attempts, web scraping has become second nature to me and one of the many skills that I use almost daily. In this tutorial I will go through a simple example of how to scrape a website to gather data on the top 100 companies in 2018 from  Fast Track . Automating this process with a web scraper avoids manual data gathering, saves time and also allows you to have all the data on the companies in one structured file. The first question to ask before getting started with any python application is ‘Which libraries do I need?’ For web scraping there are a few different libraries to consider, including: In this example we will be using Beautiful Soup. Using  pip , the Python package manager, you can install Beautiful Soup with the following: With these libraries installed, let’s get started! To know which elements that you need to target in your python code, you need to first inspect the web page. To gather data from  Tech Track Top 100 companies  you can inspect the page by right clicking on the element of interest and select inspect. This brings up the HTML code where we can see the element that each field is contained within. Since the data is stored in a table, it will be straight forward to scrape with just a few lines of code. This is a good example and a good place to start if you want to familiarise yourself with scraping websites, but bear in mind that it will not always be so simple! All 100 results are contained within rows in  <tr>  elements and these are all visible on the one page. This will not always be the case and when results span over many pages you may need to either change the number of results displayed on a webpage, or loop over all pages to gather all the information. On the League Table webpage, a table containing 100 results is displayed. When inspecting the page it is easy to see a pattern in the html. The results are contained in rows within the table: The repeated rows  <tr>  will keep our code minimal by using a loop within python to find the data and write to a file! Side note: Another check that can be done is to check whether a HTTP GET request is being made on the website which may already return the results as structured response such as a JSON or XML format. You can check this from within the network tab in the inspect tools, often in the XHR tab. Once a page is refreshed it will display the requests as they are loaded and if the response contains a formatted structure, it is often easier to make a request using a REST Client such as  Insomnia  to return the output. Now that you have looked at the structure of the html and familiarised yourself with what you are scraping, it’s time to get started with python! The first step is to import the libraries that you will be using for your web scraper. We have already talked about BeautifulSoup above, which helps us to handle the html. The next library we are importing is  urllib  which makes the connection to the webpage. Finally, we will be writing the output to a csv so we also need to import the  csv  library. As an alternative, the  json  library could be used here instead. The next step is to define the url that you are scraping. As discussed in the previous section, this webpage presents all results on one page so the full url as in the address bar is given here. We then make the connection to the webpage and we can parse the html using BeautifulSoup, storing the object in the variable ‘soup’. We can print the soup variable at this stage which should return the full parsed html of the webpage we have requested. If there is an error or the variable is empty, then the request may not have been successful. You may wish to implement error handling at this point using the  urllib.error  module. As all of the results are contained within a table, we can search the soup object for the table using the  find  method. We can then find each row within the table using the  find_all  method. If we print the number of rows we should get a result of 101, the 100 rows plus the header. We can therefore loop over the results to gather the data. Printing the first 2 rows in the soup object, we can see that the structure of each row is: There are 8 columns in the table containing: Rank, Company, Location, Year End, Annual Sales Rise, Latest Sales, Staff and Comments, all of which are interesting data that we can save. This structure is consistent throughout all rows on the webpage (which may not always be the case for all websites!), and therefore we can again use the  find_all  method to assign each column to a variable that we can write to a csv or JSON by searching for the  <td>  element. In python, it is useful to append the results to a list to then write the data to a file. We should declare the list and set the headers of the csv before the loop with the following: This will print out the first row that we have added to the list containing the headers. You might notice that there are a few extra fields  Webpage  and  Description  which are not column names in the table, but if you take a closer look in the html from when we printed the soup variable above, the second row contains more than just the company name. We can use some further extraction to get this extra information. The next step is to loop over the results, process the data and append to  rows  which can be written to a csv. To find the results in the loop: Since the first row in the table contains only the headers, we can skip this result, as shown above. It also does not contain any  <td>  elements so when searching for the element, nothing is returned. We can then check that only results containing data are processed by requiring the length of the data to be non-zero. We can then start to process the data and save to variables. The above simply gets the text from each of the columns and saves to variables. Some of this data however needs further cleaning to remove unwanted characters or extract further information. If we print out the variable  company , the text not only contains the name of the company but also a description. If we then print out  sales , it contains unwanted characters such as footnote symbols that would be useful to remove. We would like to split  company  into the company name and the description which we can do in a few lines of code. Looking again at the html, for this column there is a  <span>  element that contains only the company name. There is also a link in this column to another page on the website that has more detailed information about the company. We will be using this a little later! To separate  company  into two fields, we can use the find  method to save the  <span>  element and then use either  strip  or  replace  to remove the company name from the  company  variable, so that it leaves only the description.  To remove the unwanted characters from  sales , we can again use strip  and  replace  methods! The last variable we would like to save is the company website. As discussed above, the second column contains a link to another page that has an overview of each company. Each company page has it’s own table, which most of the time contains the company website. To scrape the url from each table and save it as a variable, we need to use the same steps as above: Looking at a few of the company pages, as in the screenshot above, the urls are in last row in the table so we can search within the last row for the  <a>  element. There also may be cases where the company website is not displayed so we can use a  try   except  condition, in case a url is not found. Once we have saved all of the data to variables, still within the loop, we can add each result to the list  rows . It is then useful to print the variable outside of the loop, to check that it looks as you expect before writing it to a file! You may want to save this data for analysis and this can be done very simply within python from our list. When running the python script your output file will be generated containing 100 rows of results that you can look at in further detail! This brief tutorial on web scraping with python has outlined: This is my first tutorial so let me know if you have any questions or comments if things are not clear! Thank you for reading! If you enjoyed my article then  subscribe  to my monthly newsletter where you can get my latest articles and top resources delivered right to your inbox! You can follow me on  Medium  for more articles, follow me on  Twitter  or find out more about what I’m up to on my  website ."
"How To Make Money, Using Web Scraping","ow To Make Money, Using Web Scrapin","Did you know what your reading right now is data? It may just seem like a few words to you but on the back end, everything you read online is data that can be taken, picked apart, and manipulated with. Simplified this is what a  Web Scraper  is. They go through the code that was created to make a website (HTML code) or database and take the data they want. Virtually any website can be scraped. Some sites do involve measures that stop these scrapers from taking their data, but if your good enough you can essentially scrape 99% of websites online. If you didn’t know what a Web Scraper is, well now you have an idea and we can get to the point of why your reading this article… Money. Web Scraping can be a unique way to make money that isn’t as difficult as it sounds. In fact all the methods and examples I'm going to show you took less than 50 lines of code to make and can be learned in only a couple of hours. So with that said let me show you... A bot is just a technical term for a program that does a specific action. Depending what you make this action to be, you can sell it to those who don’t have the technical abilities to make it themselves. To show how you can create a bot and sell it, I created an Airbnb bot. This bot allows the user to input a location and it will return all the houses that  Airbnb  offers at that location including the price, rating, number of guests allowed, bedrooms, beds, and baths. All of this being done by web scraping the data of each posting on the Airbnb website. To demonstrate the bot in action I’m going to input a location. Lets say I want to search for Airbnb’s in Rome, Italy. I simply input Rome into the bot, and it returns 272 unique Airbnb’s within seconds in an organized excel spreadsheet. It is now much easier to see all the houses/features and their comparisons to other postings. It is also much easier to filter through. I live in a family of 4 and if we were to go to Rome we would look for an Airbnb with at least 2 beds at a decent price. Now with this clean organized spreadsheet, excel makes it extremely easy to filter to match my needs. And out of 272 results 7 returned with my matching needs. Within these 7 the one I would pick is the Vatican St.Peter Daniel, it has a very good rating and is cheapest out of the 7 with a cost of $61 per night. So after I pick the one I want, I would simply copy the link of the posting into a browser and book it then. Looking for places to stay can be an extremly daunting task when going on vacation, I’m sure most of us have felt that at one time or another. Because of this there are those that are willing to pay just to make this process easier. With this bot I made the process easier. You just saw me book a room with all my matching needs at a good price within 5 minutes. Trust me people are willing to pay to make their lives just a bit easier. One of the most common uses of web scraping, is getting prices off websites. There are those who create web scraping programs that run everyday and return the price of a specific product, and when the price drops to a certain amount the program will automatically buy the product before its sold out. Then since the demand for the product will be higher than the supply they resell the product at a higher price to make a profit. This is just one example of the many reselling tactics that web scrapers use. Another one which I will show you an example of can save you a lot of money and make a lot for you too. Every retail website has limited deals and sales, where they will display the original price and the sale price. But what they don’t do is show how much is actually discounted off the original price. For example if a watch originally costs $350 and the sale price is $300 you would think $50 off would be a lot of money but it’s actually only a 14.2% discount. Now if a T-shirt originally costs $50 and the sale price is $40, you might see $10 being not that much off the original price, but in fact the discount is larger than the watch at 20%. Therefore you can save/make money by buying the products with the highest discounted %. Using  Hudson's’ Bay , a department store that has numerous of sales on all kinds of brands, were going to use web scraping to get the original and sale price of all the products and find the product with the highest discount. After scraping the website it returned over 900 products and as you can see there is only 1 product out of the 900 with over a 50% discount. That would be the Perry Ellis Solid Non-Iron Dress Shirt. This sale price is only a limited time offer, so the price for this shirt will eventually go back up to around $90. So if I were to buy it now at $40 than sell it at $30 below its original at $60 when the limited sale ends, I would still make a profit of $20. This is a method where if you find the right niche to do this is in, there is a potential to make a large amount of money. There are millions of datasets online that are free and accessible to everyone. This data is often easily gathered and thereby offered to anyone who wants to use them. On the other hand some data is not as easy to get, and takes either time or a lot of work to put in a nice clean dataset. This has become the evolution of selling data. There are companies that focus on getting data that may be hard to obtain and structuring that data into a nice clean spreadsheet or dashboard that others can use at a certain cost. BigDataBall is a sports data website that sells player logs, play-by-play data, and other stats at a price of $30 for a single seasons worth of data. The reason they can ask for this price is not because there the only ones that have this data, but there one of the only websites out there that offer this data in a very structured and clean dashboard that is easy to read. Now what I’m going to do is get the same data as BigDataBall has for free and I’m going to put it into a structured dataset like the ones I did before. Like I said before they aren’t the only ones with this type of data.  Basketball-Reference.com  has all the same data but its not structured meaning its data is all over the place and hard to read, and you simply cannot just download the dataset you want. This is where web scraping comes in. I’m going to web scrape the website of all the players logs for each game and put it into a structured dataset like BigDataBall. After web scraping  Basketball-Reference.com  we got over 16000 player logs for the season so far. You can see why this data in a nice clean format can be monetized, because no one in their right mind would manually get 16000 logs of data and put it into their own dataset. But with the help of web scraping we were able to get this data in a couple of minutes and save ourselves $30. Needless to say though you can do what BigDataBall does. Find data that is hard to obtain manually, let your computer do the work, and than sell it to those interested in having that data in a structured dataset. If you find this article useful and are interested in learning Web Scraping, I have a course that teaches you how to go from Complete Beginner to Advanced Web Scraper, and the course teaches you step-by-step how to make money with it. Check it out by clicking the link below! christopher-zita-24ef.mykajabi.com"
Web scraping is now legal,eb scraping is now lega,"This story was sponsored by  https://www.corbettanalytics.com . In late 2019, the US Court of Appeals denied  LinkedIn’s request  to prevent HiQ, an analytics company, from scraping its data. The decision was a historic moment in the data privacy and data regulation era. It showed that any data that is…"
Web Scraping Tutorial with Python: Tips and Tricks,eb Scraping Tutorial with Python: Tips and Trick,"I was searching for flight tickets and noticed that ticket prices fluctuate during the day. I tried to find out when the best time to buy tickets is, but there was nothing on the Web that helped. I built a small program to automatically collect the data from the web — a so-called scraper. It extracted information for my specific flight destination on predetermined dates and notified me when the price got lower. Web scraping is a technique used to extract data from websites through an automated process. I learned a lot from this experience with Web scraping, and I want to share it. This post is intended for people who are interested to know about the common design patterns, pitfalls and rules related to the web scraping. The ariticle presents several  use cases  and a collection of typical  problems , such as how  not to be detected ,  dos  and  don’ts , and how to  speed up (parallelization)  your scraper. Everything will be accompanied by  python snippets , so that you can start straight away. This document will also go through several useful python packages. There are many reasons and use cases why you would want to scrape data. Let me list some of them: Structure of the tutorial: Before we start:  Be NICE to the servers; you DON’T want to crash a website. There is no universal solution for web scraping because the way data is stored on each website is usually specific to that site. In fact, if you want to scrape the data, you need to understand the website’s structure and either build your own solution or use a highly customizable one. However, you don’t need to reinvent the wheel: there are many packages that do the most work for you. Depending on your programming skills and your intended use case, you might find different packages more or less useful. 1.1 Inspect option Most of the time you will finding yourself inspecting the  HTML  the website. You can easily do it with an “inspect”  option  of your bowser. The section of the website that holds my name, my avatar and my description is called  hero hero--profile u-flexTOP (how interesting that Medium calls its writers ‘heroes’ :)). The <h1> class that holds my name is called ui-h2 hero-title  and the description is contained within the <p> class  ui-body hero-description . You can read more about  HTML tags , and differences between  classes  and  ids   here . 1.2 Scrapy There is a stand-alone ready-to-use data extracting framework called   Scrapy .  Apart from extracting HTML the package offers lots of functionalities like exporting data in formats, logging etc. It is also highly customisable: run different spiders on different processes, disable cookies¹ and set download delays². It can also be used to   extract data using API. However, the learning curve is not smooth for the new programmers: you need to read tutorials and examples to get started. ¹ Some sites use cookies to identify bots. ² The website can get overloaded due to a huge amount of crawling requests. For my use case it was too much ‘out of the box’: I just wanted to extract the links from all pages, access each link and extract information out of it. 1.3 BeautifulSoup with Requests BeautifulSoup  is a library that allows you to parse the HTML source code in a beautiful way. Along with it you need a  Request  library that will fetch the content of the url. However, you should take care of everything else like error handling, how to export data, how to parallelize the web scraper, etc. I chose BeautifulSoup as it would force me to figure out a lot of stuff that Scrapy handles on its own, and hopefully help me learn faster from my mistakes. It’s very straightforward to start scraping a website. Most of the time you will find yourself inspecting  HTML  of the website to access the classes and IDs you need. Lets say we have a following html structure and we want to extract the  main_price  elements. Note:  discounted_price  element is optional. The basic code would be to import the libraries, do the request, parse the html and then to find the  class main_price . It can happen that the  class main_price  is present in another section of the website. To avoid extracting unnecessary  class main_price  from any other part of the webpage we could have first addressed the  id listings_prices  and only then find all elements with  class main_price . 3.1 Check robots.txt The scraping rules of the websites can be found in the  robots.txt  file. You can find it by writing robots.txt after the main domain, e.g  www.website_to_scrape.com/robots.txt . These rules identify which parts of the websites are not allowed to be automatically extracted or how frequently a bot is allowed to request a page. Most people don’t care about it, but try to be respectful and at least look at the rules even if you don’t plan to follow them. 3.2 HTML can be evil HTML tags can contain id, class or both. HTML id specifies a  unique  id and HTML class is non-unique. Changes in the class name or element could either break your code or deliver wrong results. There are two ways to avoid it or at least to be alerted about it: However, because some fields can be optional (like  discounted_price  in our HTML example), corresponding elements would not appear on each listing. In this case you can count the percentage of how many times this specific element returned None to the number of listings. If it is 100%, you might want to check if the element name was changed. 3.3 User agent spoofing Everytime you visit a website, it gets your  browser information  via  user agent . Some websites won’t show you any content unless you provide a user agent. Also, some sites offer different content to different browsers. Websites do not want to block genuine users but you would look suspicious if you send 200 requests/second with the same user agent. A way out might be either to generate (almost) random user agent or to set one yourself. 3.4 Timeout request By default, Request  will keep waiting for a response indefinitely. Therefore, it is advised to set the timeout parameter. 3.5 Did I get blocked? Frequent appearance of the  status codes  like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked. You may want to check for those error codes and proceed accordingly. Also, be ready to handle exceptions from the request. 3.6 IP Rotation Even if you randomize your user agent, all your requests will be from the same IP address. That doesn’t sound abnormal because libraries, universities, and also companies have only a few IP addresses. However, if there are uncommonly many requests coming from a single IP address, a server can detect it.  Using shared  proxies, VPNs or TOR  can help you become a ghost ;). By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website. 3.7 Honeypots Honeypots are means to detect  crawlers  or scrapers. These can be ‘hidden’ links that are not visible to the users but can be extracted by scrapers/spiders. Such links will have a CSS style set to  display:none , they can be blended by having the color of the background, or even be moved off of the visible area of the page. Once your crawler visits such a link, your IP address can be flagged for further investigation, or even be instantly blocked. Another way to spot crawlers is to add links with infinitely deep directory trees. Then one would need to limit the number of retrieved pages or limit the traversal depth. Again, do not overload the website by sending hundreds of requests per second. If you decide to parallelize your program, be careful with your implementation so you don’t slam the server. And be sure you read the  Dos and Don’ts  section. Check out the the definitions of parallelization vs concurrency, processors and threads  here  and  here . If you extract a huge amount of information from the page and do some preprocessing of the data while scraping, the number of requests per second you send to the page can be relatively low. For my other project where I scraped apartment rental prices, I did heavy preprocessing of the data while scraping, which resulted in 1 request/second. In order to scrape 4K ads, my program would run for about one hour. In order to send requests in parallel you might want to use a  multiprocessing  package. Let’s say we have 100 pages and we want to assign every processor equal amount of pages to work with. If  n  is the number of CPUs, you can evenly chunk all pages into the  n  bins and assign each bin to a processor. Each process will have its own name, target function and the arguments to work with. The name of the process can be used afterwards to enable writing data to a specific file. I assigned 1K pages to each of my 4 CPUs which yielded 4 requests/second and reduced the scraping time to around 17 mins."
How to Make Money From Web Scraping Without Selling Data,ow to Make Money From Web Scraping Without Selling Dat,"So you want to make money with the knowledge of web scraping you have, you create a bot that successfully gets the valuable data you wished for, but you can’t find anyone to sell either the data you obtained or the bot you created. I’ve been there before and it feels like precious…"
"Better web scraping in Python with Selenium, Beautiful Soup, and pandas","etter web scraping in Python with Selenium, Beautiful Soup, and panda","Using the Python programming language, it is possible to “scrape” data from the web in a quick and efficient manner. Web scraping is defined as: a tool for turning the unstructured data on the web into machine readable, structured data which is ready for analysis. ( source ) Web scraping is a valuable  tool in the data scientist’s skill set . Now, what to scrape? The  KanView  website supports “Transparency in Government”. That is also the slogan of the site. The site provides payroll data for the State of Kansas. And that’s great! Yet, like many government websites, it buries the data in drill-down links and tables. This often requires “best guess navigation” to find the specific data you are looking for. I wanted to use the public data provided for the universities within Kansas in a research project. Scraping the data with Python and saving it as JSON was what I needed to do to get started. Web scraping with Python often requires no more than the use of the  Beautiful Soup  module to reach the goal.  Beautiful Soup  is a popular Python library that makes web scraping by traversing the DOM (document object model) easier to implement. However, the  KanView  website uses JavaScript links. Therefore, examples using Python and Beautiful Soup will not work without some extra additions. The  Selenium package  is used to automate web browser interaction from Python. With Selenium, programming a Python script to automate a web browser is possible. Afterwards, those pesky JavaScript links are no longer an issue. Selenium will now start a browser session. For Selenium to work, it must access the browser driver. By default, it will look in the same directory as the Python script. Links to Chrome, Firefox, Edge, and Safari drivers  available here . The example code below uses Firefox: The  python_button.click()  above is telling Selenium to click the JavaScript link on the page. After arriving at the Job Titles page, Selenium hands off the page source to Beautiful Soup. Beautiful Soup  remains the best way to traverse the DOM and scrape the data. After defining an empty list and a counter variable, it is time to ask Beautiful Soup to grab all the links on the page that match a regular expression: You can see from the example above that Beautiful Soup will retrieve a JavaScript link for each job title at the state agency. Now in the code block of the for / in loop, Selenium will click each JavaScript link. Beautiful Soup will then retrieve the table from each page. Beautiful Soup passes the findings to pandas. Pandas uses its  read_html  function to read the HTML table data into a dataframe. The dataframe is appended to the previously defined empty list. Before the code block of the loop is complete, Selenium needs to click the back button in the browser. This is so the next link in the loop will be available to click on the job listing page. When the for / in loop has completed, Selenium has visited every job title link. Beautiful Soup has retrieved the table from each page. Pandas has stored the data from each table in a dataframe. Each dataframe is an item in the datalist. The individual table dataframes must now merge into one large dataframe. The data will then be converted to JSON format with  pandas.Dataframe.to_json : Now Python creates the JSON data file. It is ready for use! The automated web scraping process described above completes quickly. Selenium opens a browser window you can see working. This allows me to show you a screen capture video of how fast the process is. You see how fast the script follows a link, grabs the data, goes back, and clicks the next link. It makes retrieving the data from hundreds of links a matter of single-digit minutes. Here is the full Python code. I have included an import for tabulate. It requires an extra line of code that will use tabulate to pretty print the data to your command line interface: Web scraping  with  Python  and  Beautiful Soup  is an excellent tool to have within your skillset. Use web scraping when the data you need to work with is available to the public, but not necessarily conveniently available. When JavaScript provides or “hides” content, browser automation with  Selenium  will insure your code “sees” what you (as a user) should see. And finally, when you are scraping tables full of data,  pandas  is the Python data analysis library that will handle it all. The following article was a helpful reference for this project: https://pythonprogramminglanguage.com/web-scraping-with-pandas-and-beautifulsoup/ Reach out to me any time on  LinkedIn  or  Twitter . And if you liked this article, give it a few claps. I will sincerely appreciate it. https://www.linkedin.com/in/davidagray/ twitter.com"
An Introduction to Web Scraping with Puppeteer,n Introduction to Web Scraping with Puppetee,"Learn Puppeteer with me in this article. I saw a video a few days ago on  DevTips  where they attempted to use Puppeteer, I’ve never used it myself and thought it looked really cool. So I gave it a try and I’m sharing what I’ve learned here. —  Prerequisites  —  What is Puppeteer?  — —  A Headless Browser  — —  An API  —  Why use any of this?  —  On with the code!  — —  Prerequisites  — —  Project Setup  — —  A Simple Example  — —  Grabbing Data — Preparations  — —  Notes for the data I want  — —  My Selectors  — —  Grabbing Data — In Code  — —  Saving Data to a File  —  More Advanced Scraping  —  References and Links This tutorial is beginner friendly, no advanced knowledge of code is required. If you’re following along with the project then more requirements will be listed below in the code section. All code will be available in a repository on GitHub linked here. github.com Before we just dive into the code it’s important to understand what a technology we’re using is and why it exists. Puppeteer comes with Chromium and runs “headless” by default. What is a headless browser? A headless browser is a  browser for machines . It has no UI and allows a program — often called a scraper or a crawler — to read and interact with it. Headless browsers are great and all, but they can be a pain to use sometimes. Puppeteer, however, provides a really nice API or set of functions for interacting with it. There’s so much you can do with Puppeteer and web scraping in general! let’s get started! If you’re following along you’ll need  NodeJS  installed, basic knowledge of the command line, knowledge of JavaScript and knowledge of the DOM. Note : Your scraper code doesn’t have to be perfect. When doing your own projects don’t overthink it. Now let’s try something simple ( but really cool! ) to verify that our setup is working. We’re going to take a screenshot of a web page and generate a PDF file. ( yes this is simple to do ) For most of my examples, I’ll be using  scrapethissite.com . You can use any site you want as long as they allow you to scrape them. Search for their policy and try looking at  site/robots.txt  for example  https://medium.com/robots.txt This is all the code that’s required to start the headless browser, navigate to a web page then take a screenshot and generate a pdf of it. Click  here  for more information on screenshots and  here  for more information on pdf generation. Screenshots and pdfs are fun but how does that help me grab data faster? Those features are good if you want pdfs and screenshots specifically. When you want to grab and possibly manipulate data there are other tools at your disposal. Using the same site from the example above we will grab some data and save it to a file. Let’s say in this scenario we only want the team name, year, wins and losses. The first step is to  create some selectors . A  selector is just a path to the data . ( think CSS selectors ) We’ll come up with the paths here by using our browser’s developer tools. Open them on the page by opening your browser menu and looking for “developer tools”. I’ll be using Chrome and you can just press  CTRL + Shift + I  to open them. On the site open the elements tab in your developer tools and find what data you want to grab. Take note of its structure, classes, etc. If you happen to have a specific unique piece that you want to grab then you can just right click on the node and choose “copy selector”. The selectors I came up with for this example are: Read more about CSS selectors here  if you’re new to them. Time to apply this to our code. The main part of this is  page.evaluate()  this lets us  run JS code in the browser and communicate back  any data we want. This is all it takes to fetch data. You may have noticed that we have access to the DOM here — this is the very nice and familiar API that Puppeteer provides! As a final touch, we’ll save this data to a file. In my case, I want the data in JSON format because that’s most easily used with JS. nodejs.org developer.mozilla.org Puppeteer supports things like single page applications ( SPA ), simulating input, tests and more. They’re beyond the scope of this tutorial, but you can find examples in the Puppeteer documentation ( listed below ) and also in  this other article . developers.google.com github.com If you found this article too difficult then I’d recommend  this one . It covers the same stuff, but in more detail. Thanks for reading! Leave any feedback or questions in the comments below."
