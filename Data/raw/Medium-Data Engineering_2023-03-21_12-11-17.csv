blogTitle,blogSubheading,blogContent
An A-Z of useful Python tricks,n A-Z of useful Python trick,"Python is one of the world‚Äôs most popular, in-demand programming languages. This is for many reasons: I use Python daily as an integral part of my job as a data scientist. Along the way, I‚Äôve picked up a few useful tricks and tips. Here, I‚Äôve made an attempt at sharing some of them in an A-Z format. Most of these ‚Äòtricks‚Äô are things I‚Äôve used or stumbled upon during my day-to-day work. Some I found while browsing the  Python Standard Library docs . A few others I found searching through  PyPi . However, credit where it is due ‚Äî I discovered four or five of them over at  awesome-python.com . This is a curated list of hundreds of interesting Python tools and modules. It is worth browsing for inspiration! One of the many reasons why Python is such a popular language is because it is readable and expressive. It is often joked that Python is ‚Äò executable pseudocode ‚Äô. But when you can write code like this, it‚Äôs difficult to argue otherwise: You want to plot graphs in the console? You can have graphs in the console. Python has some great default datatypes, but sometimes they just won‚Äôt behave exactly how you‚Äôd like them to. Luckily, the Python Standard Library offers  the collections module . This handy add-on provides you with further datatypes. Ever wondered how you can look inside a Python object and see what attributes it has? Of course you have. From the command line: This can be a really useful feature when running Python interactively, and for dynamically exploring objects and modules you are working with. Read more  here . Yes,  really . Don‚Äôt pretend you‚Äôre not gonna try it out‚Ä¶ üëç One consequence of Python‚Äôs popularity is that there are always new versions under development. New versions mean new features‚Ää‚Äî‚Ääunless your version is out-of-date. Fear not, however. The  __future__ module  lets you import functionality from future versions of Python. It‚Äôs literally like time travel, or magic, or something. Why not have a go  importing curly braces ? Geography can be a challenging terrain for programmers to navigate (ha, a pun!). But  the geopy module  makes it unnervingly easy. It works by abstracting the APIs of a range of different geocoding services. It enables you to obtain a place‚Äôs full street address, latitude, longitude, and even altitude. There‚Äôs also a useful distance class. It calculates the distance between two locations in your favorite unit of measurement. Stuck on a coding problem and can‚Äôt remember that solution you saw before? Need to check StackOverflow, but don‚Äôt want to leave the terminal? Then you need  this useful command line tool . Ask it whatever question you have, and it‚Äôll do its best to return an answer. Be aware though ‚Äî it scrapes code from top answers from StackOverflow. It might not always give the most helpful information‚Ä¶ Python‚Äôs  inspect module  is great for understanding what is happening behind the scenes. You can even call its methods on itself! The code sample below uses  inspect.getsource()  to print its own source code. It also uses  inspect.getmodule()  to print the module in which it was defined. The last line of code prints out its own line number. Of course, beyond these trivial uses, the inspect module can prove useful for understanding what your code is doing. You could also use it for writing self-documenting code. The Jedi library is an autocompletion and code analysis library. It makes writing code quicker and more productive. Unless you‚Äôre developing your own IDE, you‚Äôll probably be most interested in  using Jedi as an editor plugin . Luckily, there are already loads available! You may already be using Jedi, however. The IPython project makes use of Jedi for its code autocompletion functionality. When learning any language, there are many milestones along the way. With Python, understanding the mysterious  **kwargs  syntax probably counts as one. The double-asterisk in front of a dictionary object lets you pass the contents of that dictionary as  named arguments to a function . The dictionary‚Äôs keys are the argument names, and the values are the values passed to the function. You don‚Äôt even need to call it  kwargs ! This is useful when you want to write functions that can handle named arguments not defined in advance. One of my favourite things about programming in Python are its  list comprehensions . These expressions make it easy to write very clean code that reads almost like natural language. You can read more about how to use them  here . Python supports functional programming through a number of inbuilt features. One of the most useful is the  map()  function ‚Äî especially in combination with  lambda functions . In the example above,  map()  applies a simple lambda function to each element in  x . It returns a map object, which can be converted to some iterable object such as a list or tuple. If you haven‚Äôt seen it already, then be prepared to have your mind blown by  Python‚Äôs newspaper module . It lets you retrieve news articles and associated meta-data from a range of leading international publications. You can retrieve images, text and author names. It even has some  inbuilt NLP functionality . So if you were thinking of using BeautifulSoup or some other DIY webscraping library for your next project, save yourself the time and effort and  $ pip install newspaper3k  instead. Python provides support for  operator overloading , which is one of those terms that make you sound like a legit computer scientist. It‚Äôs actually a simple concept. Ever wondered why Python lets you use the  +  operator to add numbers and also to concatenate strings? That‚Äôs operator overloading in action. You can define objects which use Python‚Äôs standard operator symbols in their own specific way. This lets you use them in contexts relevant to the objects you‚Äôre working with. Python‚Äôs default  print  function does its job. But try printing out any large, nested object, and the result is rather ugly. Here‚Äôs where the  Standard Library‚Äôs pretty-print module  steps in. This prints out complex structured objects in an easy-to-read format. A must-have for any Python developer who works with non-trivial data structures. Python supports multithreading, and this is facilitated by the Standard Library‚Äôs Queue module. This module lets you implement queue data structures. These are data structures that let you add and retrieve entries according to a specific rule. ‚ÄòFirst in, first out‚Äô (or FIFO) queues let you retrieve objects in the order they were added. ‚ÄòLast in, first out‚Äô (LIFO) queues let you access the most recently added objects first. Finally, priority queues let you retrieve objects according to the order in which they are sorted. Here‚Äôs an example of how to use queues  for multithreaded programming in Python. When defining a class or an object in Python, it is useful to provide an ‚Äòofficial‚Äô way of representing that object as a string. For example: This makes debugging code a lot easier. Add it to your class definitions as below: Python makes a great scripting language. Sometimes using the standard os and subprocess libraries can be a bit of a headache. The  sh library  provides a neat alternative. It lets you call any program as if it were an ordinary function ‚Äî useful for automating workflows and tasks, all from within Python. Python is a dynamically-typed language. You don‚Äôt need to specify datatypes when you define variables, functions, classes etc. This allows for rapid development times. However, there are few things more annoying than a runtime error caused by a simple typing issue. Since Python 3.5 , you have the option to provide type hints when defining functions. You can also define type aliases: Although not compulsory, type annotations can make your code easier to understand. They also allow you to use type checking tools to catch those stray TypeErrors before runtime. Probably worthwhile if you are working on large, complex projects! A quick and easy way to generate Universally Unique IDs (or ‚ÄòUUIDs‚Äô) is through the  Python Standard Library‚Äôs uuid module . This creates a randomized 128-bit number that will almost certainly be unique. In fact, there are over 2¬π¬≤¬≤ possible UUIDs that can be generated. That‚Äôs over five undecillion (or 5,000,000,000,000,000,000,000,000,000,000,000,000). The probability of finding duplicates in a given set is extremely low. Even with a trillion UUIDs, the probability of a duplicate existing is much, much less than one-in-a-billion. Pretty good for two lines of code. This is probably my favorite Python thing of all. Chances are you are working on multiple Python projects at any one time. Unfortunately, sometimes two projects will rely on different versions of the same dependency. Which do you install on your system? Luckily, Python‚Äôs  support for virtual environments  lets you have the best of both worlds. From the command line: Now you can have standalone versions and installations of Python running on the same machine. Sorted! Wikipedia has a great API that allows users programmatic access to an unrivalled body of completely free knowledge and information. The  wikipedia module  makes accessing this API almost embarrassingly convenient. Like the real site, the module provides support for multiple languages, page disambiguation, random page retrieval, and even has a  donate()  method. Humour is a key feature of the Python language ‚Äî after all, it is named after the British comedy sketch show  Monty Python‚Äôs Flying Circus . Much of Python‚Äôs official documentation references the show‚Äôs most famous sketches. The sense of humour isn‚Äôt restricted to the docs, though. Have a go running the line below: Never change, Python. Never change. YAML stands for ‚Äò YAML Ain‚Äôt Markup Language ‚Äô. It is a data formatting language, and is a superset of JSON. Unlike JSON, it can store more complex objects and refer to its own elements. You can also write comments, making it particularly suited to writing configuration files. The  PyYAML module  lets you use YAML with Python. Install with: And then import into your projects: PyYAML lets you store Python objects of any datatype, and instances of any user-defined classes also. One last trick for ya, and it really is a cool one. Ever needed to form a dictionary out of two lists? The  zip()  inbuilt function takes a number of iterable objects and returns a list of tuples. Each tuple groups the elements of the input objects by their positional index. You can also ‚Äòunzip‚Äô objects by calling  *zip()  on them. So there you have it, an A-Z of Python tricks ‚Äî hopefully you‚Äôve found something useful for your next project. Python‚Äôs a very diverse and well-developed language, so there‚Äôs bound to be many features I haven‚Äôt got round to including. Please share any of your own favorite Python tricks by leaving a response below!"
Learning Python: From Zero to Hero,earning Python: From Zero to Her,"This post was originally published at  TK's Blog . First of all, what is Python? According to its creator, Guido van Rossum, Python is a: ‚Äúhigh-level programming language, and its core design philosophy is all about code readability and a syntax which allows programmers to express concepts in a few lines of code.‚Äù For me, the first reason to learn Python was that it is, in fact, a beautiful   programming language. It was really natural to code in it and express my thoughts. Another reason was that we can use coding in Python in multiple ways: data science, web development, and machine learning all shine here. Quora, Pinterest and Spotify all use Python for their backend web development. So let‚Äôs learn a bit about it. You can think about variables as words that store a value. Simple as that. In Python, it is really easy to define a variable and set a value to it. Imagine you want to store number 1 in a variable called ‚Äúone.‚Äù Let‚Äôs do it: How simple was that? You just assigned the value 1 to the variable ‚Äúone.‚Äù And you can assign any other  value  to whatever other  variables  you want. As you see in the table above, the variable ‚Äú two ‚Äù stores the integer  2 , and ‚Äú some_number ‚Äù stores  10,000 . Besides integers, we can also use booleans (True / False), strings, float, and so many other data types. ‚Äú If ‚Äù uses an expression to evaluate whether a statement is True or False. If it is True, it executes what is inside the ‚Äúif‚Äù statement. For example: 2  is greater than  1 , so the ‚Äú print ‚Äù code is executed. The ‚Äú else ‚Äù statement will be executed if the ‚Äú if ‚Äù expression is  false . 1  is not greater than  2 , so the code inside the ‚Äú else ‚Äù statement will be executed. You can also use an ‚Äú elif ‚Äù statement: In Python, we can iterate in different forms. I‚Äôll talk about two:  while   and  for . While  Looping: while the statement is True, the code inside the block will be executed. So, this code will print the number from  1  to  10 . The  while  loop needs a ‚Äú loop condition. ‚Äù If it stays True, it continues iterating. In this example, when  num  is  11  the  loop condition  equals  False . Another basic bit of code to better understand it: The  loop condition  is  True  so it keeps iterating ‚Äî until we set it to  False . For Looping : you apply the variable ‚Äú num ‚Äù to the block, and the ‚Äú for ‚Äù statement will iterate it for you. This code will print the same as  while  code: from  1  to  10 . See? It is so simple. The range starts with  1  and goes until the  11 th element ( 10  is the  10 th element). Imagine you want to store the integer 1 in a variable. But maybe now you want to store 2. And 3, 4, 5 ‚Ä¶ Do I have another way to store all the integers that I want, but not in  millions of variables ? You guessed it ‚Äî there is indeed another way to store them. List  is a collection that can be used to store a list of values (like these integers that you want). So let‚Äôs use it: It is really simple. We created an array and stored it on  my_integer . But maybe you are asking: ‚ÄúHow can I get a value from this array?‚Äù Great question.  List  has a concept called  index . The first element gets the index 0 (zero). The second gets 1, and so on. You get the idea. To make it clearer, we can represent the array and each element with its index. I can draw it: Using the Python syntax, it‚Äôs also simple to understand: Imagine that you don‚Äôt want to store integers. You just want to store strings, like a list of your relatives‚Äô names. Mine would look something like this: It works the same way as integers. Nice. We just learned how  Lists  indices work. But I still need to show you how we can add an element to the  List  data structure (an item to a list). The most common method to add a new value to a  List  is  append . Let‚Äôs see how it works: append  is super simple. You just need to apply the element (eg. ‚Äú The Effective Engineer ‚Äù) as the  append  parameter. Well, enough about  Lists .  Let‚Äôs talk about another data structure. Now we know that  Lists  are indexed with integer numbers. But what if we don‚Äôt want to use integer numbers as indices? Some data structures that we can use are numeric, string, or other types of indices. Let‚Äôs learn about the  Dictionary  data structure.  Dictionary  is a collection of key-value pairs. Here‚Äôs what it looks like: The  key  is the index pointing to the   value . How do we access the  Dictionary   value ? You guessed it ‚Äî using the  key . Let‚Äôs try it: I created a  Dictionary  about me. My name, nickname, and nationality. Those attributes are the  Dictionary   keys . As we learned how to access the  List  using index, we also use indices ( keys  in the  Dictionary  context) to access the  value  stored in the  Dictionary . In the example, I printed a phrase about me using all the values stored in the  Dictionary . Pretty simple, right? Another cool thing about  Dictionary  is that we can use anything as the value. In the  Dictionary   I created, I want to add the  key  ‚Äúage‚Äù and my real integer age in it: Here we have a  key  (age)  value  (24) pair using string as the  key  and integer as the  value . As we did with  Lists , let‚Äôs learn how to add elements to a  Dictionary . The  key   pointing to a   value  is a big part of what  Dictionary  is. This is also true when we are talking about adding elements to it: We just need to assign a  value  to a  Dictionary   key . Nothing complicated here, right? As we learned in the  Python Basics , the  List  iteration is very simple. We  Python   developers commonly use  For  looping. Let‚Äôs do it: So for each book in the bookshelf, we ( can do everything with it ) print it. Pretty simple and intuitive. That‚Äôs Python. For a hash data structure, we can also use the  for  loop, but we apply the  key  : This is an example how to use it. For each  key  in the  dictionary  , we  print  the  key  and its corresponding  value . Another way to do it is to use the  iteritems  method. We did name the two parameters as  key  and  value , but it is not necessary. We can name them anything. Let‚Äôs see it: We can see we used attribute as a parameter for the  Dictionary   key , and it works properly. Great! Objects  are a representation of real world objects like cars, dogs, or bikes. The objects share two main characteristics:  data  and  behavior . Cars have  data,  like number of wheels, number of doors, and seating capacity They also exhibit  behavior : they can accelerate, stop, show how much fuel is left, and so many other things. We identify  data  as  attributes  and  behavior  as  methods  in object-oriented programming. Again: Data ‚Üí Attributes and Behavior ‚Üí Methods And a  Class  is the blueprint from which individual objects are created. In the real world, we often find many objects with the same type. Like cars. All the same make and model (and all have an engine, wheels, doors, and so on). Each car was built from the same set of blueprints and has the same components. Python, as an Object-Oriented programming language, has these concepts:  class  and  object . A class is a blueprint, a model for its objects. So again, a class it is just a model, or a way to define  attributes  and  behavior  (as we talked about in the theory section). As an example, a vehicle  class  has its own  attributes  that define what  objects  are vehicles. The number of wheels, type of tank, seating capacity, and maximum velocity are all attributes of a vehicle. With this in mind, let‚Äôs look at Python syntax for  classes : We define classes with a  class statement ‚Äî  and that‚Äôs it. Easy, isn‚Äôt it? Objects  are instances of a  class . We create an instance by naming the class. Here  car  is an  object  (or instance) of the  class   Vehicle . Remember that our vehicle  class  has four  attributes : number of wheels, type of tank, seating capacity, and maximum velocity. We set all these  attributes  when creating a vehicle  object . So here, we define our  class  to receive data when it initiates it: We use the  init   method . We call it a constructor method. So when we create the vehicle  object , we can define these  attributes . Imagine that we love the  Tesla Model S,  and we want to create this kind of  object . It has four wheels, runs on electric energy, has space for five seats, and the maximum velocity is 250km/hour (155 mph). Let‚Äôs create this  object: Four wheels + electric ‚Äútank type‚Äù + five seats + 250km/hour maximum speed. All attributes are set. But how can we access these attributes‚Äô values? We  send a message to the object asking about them . We call it a  method . It‚Äôs the  object‚Äôs behavior . Let‚Äôs implement it: This is an implementation of two methods:  number_of_wheels  and  set_number_of_wheels . We call it  getter  &  setter . Because the first gets the attribute value, and the second sets a new value for the attribute. In Python, we can do that using  @property  ( decorators ) to define  getters  and  setters . Let‚Äôs see it with code: And we can use these methods as attributes: This is slightly different than defining methods. The methods work as attributes. For example, when we set the new number of wheels, we don‚Äôt apply two as a parameter, but set the value 2 to  number_of_wheels . This is one way to write  pythonic   getter  and  setter  code. But we can also use methods for other things, like the ‚Äú make_noise ‚Äù method. Let‚Äôs see it: When we call this method, it just returns a string  ‚Äú VRRRRUUUUM. ‚Äù Encapsulation is a mechanism that restricts direct access to objects‚Äô data and methods. But at the same time, it facilitates operation on that data (objects‚Äô methods). ‚ÄúEncapsulation can be used to hide data members and members function. Under this definition, encapsulation means that the internal representation of an  object  is generally hidden from view outside of the object‚Äôs definition.‚Äù ‚Äî Wikipedia All internal representation of an object is hidden from the outside. Only the object can interact with its internal data. First, we need to understand how  public  and  non-public  instance variables and methods work. For a Python class, we can initialize a  public instance variable  within our constructor method. Let‚Äôs see this: Within the constructor method: Here we apply the  first_name  value as an argument to the  public instance variable . Within the class: Here, we do not need to apply the  first_name  as an argument, and all instance objects will have a  class attribute  initialized with  TK . Cool. We have now learned that we can use  public instance variables  and  class attributes . Another interesting thing about the  public  part is that we can manage the variable value. What do I mean by that? Our  object  can manage its variable value:  Get  and  Set  variable values. Keeping the  Person  class in mind, we want to set another value to its  first_name  variable: There we go. We just set another value ( kaio ) to the  first_name  instance variable and it updated the value. Simple as that. Since it‚Äôs a  public  variable, we can do that. We don‚Äôt use the term ‚Äúprivate‚Äù here, since no attribute is really private in Python (without a generally unnecessary amount of work). ‚Äî  PEP 8 As the  public instance variable  , we can define the  non-public instance variable  both within the constructor method or within the class. The syntax difference is: for  non-public instance variables  , use an underscore ( _ ) before the  variable  name. ‚Äú‚ÄòPrivate‚Äô instance variables that cannot be accessed except from inside an object don‚Äôt exist in Python. However, there is a convention that is followed by most Python code: a name prefixed with an underscore (e.g.  _spam ) should be treated as a non-public part of the API (whether it is a function, a method or a data member)‚Äù ‚Äî  Python Software Foundation Here‚Äôs an example: Did you see the  email  variable? This is how we define a  non-public variable  : We can access and update it.  Non-public variables  are just a convention and should be treated as a non-public part of the API. So we use a method that allows us to do it inside our class definition. Let‚Äôs implement two methods ( email  and  update_email ) to understand it: Now we can update and access  non-public variables  using those methods. Let‚Äôs see: With  public methods , we can also use them out of our class: Let‚Äôs test it: Great ‚Äî we can use it without any problem. But with  non-public methods  we aren‚Äôt able to do it. Let‚Äôs implement the same  Person  class, but now with a  show_age   non-public method  using an underscore ( _ ). And now, we‚Äôll try to call this  non-public method  with our object: We can access and update it.  Non-public methods  are just a convention and should be treated as a non-public part of the API. Here‚Äôs an example for how we can use it: Here we have a  _get_age   non-public method  and a  show_age   public method . The  show_age  can be used by our object (out of our class) and the  _get_age  only used inside our class definition (inside  show_age  method). But again: as a matter of convention. With encapsulation we can ensure that the internal representation of the object is hidden from the outside. Certain objects have some things in common: their behavior and characteristics. For example, I inherited some characteristics and behaviors from my father. I inherited his eyes and hair as characteristics, and his impatience and introversion as behaviors. In object-oriented programming, classes can inherit common characteristics (data) and behavior (methods) from another class. Let‚Äôs see another example and implement it in Python. Imagine a car. Number of wheels, seating capacity and maximum velocity are all attributes of a car. We can say that an   ElectricCar  class inherits these same attributes from the regular  Car  class. Our  Car  class implemented: Once initiated, we can use all  instance variables  created. Nice. In Python, we apply a  parent class  to the  child class  as a parameter. An  ElectricCar  class can inherit from our  Car  class. Simple as that. We don‚Äôt need to implement any other method, because this class already has it (inherited from  Car  class). Let‚Äôs prove it: Beautiful. We learned a lot of things about Python basics: Congrats! You completed this dense piece of content about Python. If you want a complete Python course, learn more real-world coding skills and build projects, try  One Month Python Bootcamp . See you there ‚ò∫ For more stories and posts about my journey learning & mastering programming, follow my publication  The Renaissance Developer . Have fun, keep learning, and always keep coding. I hope you liked this content. Support my work on Ko-Fi My  Twitter  &  Github . ‚ò∫"
How to build your own Neural Network from scratch in Python,ow to build your own Neural Network from scratch in Pytho,"Update : When I wrote this article a year ago, I did not expect it to be  this  popular. Since then, this article has been viewed more than 450,000 times, with more than 30,000 claps. It has also made it to the front page of Google, and it is among the first few search results for ‚Äò Neural Network ‚Äô. Many of you have reached out to me, and I am deeply humbled by the impact of this article on your learning journey. This article also caught the eye of the editors at Packt Publishing. Shortly after this article was published, I was offered to be the sole author of the book  Neural Network Projects with Python .  Today, I am happy to share with you that my book has been published! The book is a continuation of this article, and it covers end-to-end implementation of neural network projects in areas such as face recognition, sentiment analysis, noise removal etc. Every chapter features a unique neural network architecture, including Convolutional Neural Networks, Long Short-Term Memory Nets and Siamese Neural Networks. If you‚Äôre looking to create a strong machine learning portfolio with deep learning projects, do consider getting the book! You can get the book from Amazon:  Neural Network Projects with Python Motivation:  As part of my personal journey to gain a better understanding of Deep Learning, I‚Äôve decided to build a Neural Network from scratch without a deep learning library like TensorFlow. I believe that understanding the inner workings of a Neural Network is important to any aspiring Data Scientist. This article contains what I‚Äôve learned, and hopefully it‚Äôll be useful for you as well! Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output. Neural Networks consist of the following components The diagram below shows the architecture of a 2-layer Neural Network ( note that the input layer is typically excluded when counting the number of layers in a Neural Network ) Creating a Neural Network class in Python is easy. Training the Neural Network The output  ≈∑  of a simple 2-layer Neural Network is: You might notice that in the equation above, the weights  W  and the biases  b  are the only variables that affects the output  ≈∑. Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as  training the Neural Network. Each iteration of the training process consists of the following steps: The sequential graph below illustrates the process. As we‚Äôve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is: Let‚Äôs add a feedforward function in our python code to do exactly that. Note that for simplicity, we have assumed the biases to be 0. However, we still need a way to evaluate the ‚Äúgoodness‚Äù of our predictions (i.e. how far off are our predictions)? The  Loss Function  allows us to do exactly that. There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we‚Äôll use a simple  sum-of-sqaures error  as our loss function. That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference. Our goal in training is to find the best set of weights and biases that minimizes the loss function. Now that we‚Äôve measured the error of our prediction (loss), we need to find a way to  propagate  the error back, and to update our weights and biases. In order to know the appropriate amount to adjust the weights and biases by, we need to know the  derivative of the loss function with respect to the weights and biases . Recall from calculus that the derivative of a function is simply the slope of the function. If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as  gradient descent . However, we can‚Äôt directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the  chain rule  to help us calculate it. Phew! That was ugly but it allows us to get what we needed ‚Äî the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly. Now that we have that, let‚Äôs add the backpropagation function into our python code. For a deeper understanding of the application of calculus and the chain rule in backpropagation, I strongly recommend this tutorial by 3Blue1Brown. Now that we have our complete python code for doing feedforward and backpropagation, let‚Äôs apply our Neural Network on an example and see how well it does. Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn‚Äôt exactly trivial for us to work out the weights just by inspection alone. Let‚Äôs train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss  monotonically decreasing towards a minimum.  This is consistent with the gradient descent algorithm that we‚Äôve discussed earlier. Let‚Äôs look at the final prediction (output) from the Neural Network after 1500 iterations. We did it! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values. Note that there‚Äôs a slight difference between the predictions and the actual values. This is desirable, as it prevents  overfitting  and allows the Neural Network to  generalize  better to unseen data. Fortunately for us, our journey isn‚Äôt over. There‚Äôs still  much  to learn about Neural Networks and Deep Learning. For example: I‚Äôll be writing more on these topics soon, so do follow me on Medium and keep and eye out for them! I‚Äôve certainly learnt a lot writing my own Neural Network from scratch. Although Deep Learning libraries such as TensorFlow and Keras makes it easy to build deep nets without fully understanding the inner workings of a Neural Network, I find that it‚Äôs beneficial for aspiring data scientist to gain a deeper understanding of Neural Networks. This exercise has been a great investment of my time, and I hope that it‚Äôll be useful for you as well!"
Why Python is not the programming language of the future,hy Python is not the programming language of the futur,"It  took the programming community a couple of decades to appreciate Python. But since the early 2010‚Äôs, it has been booming ‚Äî and eventually surpassing C, C#, Java and JavaScript in popularity. But until when will that trend continue? When will Python eventually be replaced by other languages, and why? Putting an exact expiry date on Python would be so much speculation, it might as well pass as Science-Fiction. Instead, I will assess the virtues that are boosting Python‚Äôs popularity right now, and the weak points that will break it in the future. Python‚Äôs success is reflected in the  Stack Overflow trends , which measure the count of tags in posts on the platform. Given the size of StackOverflow, this is quite a good indicator for language popularity. While R has been plateauing over the last few years, and many other languages are on a steady decline, Python‚Äôs growth seems unstoppable. Almost 14% of all StackOverflow questions are tagged ‚Äúpython‚Äù, and the trend is going up. And there are several reasons for that. Python has been around since the nineties. That doesn‚Äôt only mean that it has had plenty of time to grow. It has also acquired a large and supportive community. So if you have any issue while you‚Äôre coding in Python, the odds are high that you‚Äôll be able to solve it with a single Google search. Simply because somebody will have already encountered your problem and written something helpful about it. It‚Äôs not only the fact that it has been around for decades, giving programmers the time to make brilliant tutorials. More than that, the syntax of Python is very human-readable. For a start, there‚Äôs no need to specify the data type. You just declare a variable; Python will understand from the context whether it‚Äôs an integer, a float value, a boolean or something else. This is a huge edge for beginners. If you‚Äôve ever had to program in C++, you know how frustrating it is your program won‚Äôt compile because you swapped a float for an integer. And if you‚Äôve ever had to read Python and C++ code side-by-side, you‚Äôll know how understandable Python is. Even though C++ was designed with English in mind, it‚Äôs a rather bumpy read compared to Python code. medium.com Since Python has been around for so long, developers have made a package for every purpose. These days, you can find a package for almost everything. Want to crunch numbers, vectors and matrices?  NumPy  is your guy.  Want to do calculations for tech and engineering? Use  SciPy .  Want to go big in data manipulation and analysis? Give  Pandas  a go. Want to start out with Artificial Intelligence? Why not use  Scikit-Learn . Whichever computational task you‚Äôre trying to manage, chances are that there is a Python package for it out there. This makes Python stay on top of recent developments, can be seen from the surge in Machine Learning over the past few years. Based on the previous elaborations, you could imagine that Python will stay on top of sh*t for ages to come. But like every technology, Python has its weaknesses. I will go through the most important flaws, one by one, and assess whether these are fatal or not. Python is slow. Like, really slow. On average, you‚Äôll need about 2‚Äì10 times longer to complete a task with Python than with any other language. There are  various reasons  for that. One of them is that it‚Äôs dynamically typed ‚Äî remember that you don‚Äôt need to specify data types like in other languages. This means that a lot of memory needs to be used, because the program needs to reserve enough space for each variable that it works in any case. And lots of memory usage translates to lots of computing time. Another reason is that Python can only execute one task at a time. This is a consequence of flexible datatypes ‚Äî Python needs to make sure each variable has only one datatype, and parallel processes could mess that up. In comparison, your average web browser can run a dozen different threads at once. And there are some other theories around, too. But at the end of the day, none of the speed issues matter. Computers and servers have gotten so cheap that we‚Äôre talking about fractions of seconds. And the end user doesn‚Äôt really care whether their app loads in 0.001 or 0.01 seconds. medium.com Originally, Python was  dynamically scoped . This basically means that, to evaluate an expression, a compiler first searches the current block and then successively all the calling functions. The problem with dynamic scoping is that every expression needs to be tested in every possible context ‚Äî which is tedious. That‚Äôs why most modern programming languages use static scoping. Python tried to transition to static scoping, but  messed it up . Usually, inner scopes ‚Äî for example functions within functions ‚Äî would be able to see  and  change outer scopes. In Python, inner scopes can only see outer scopes, but not change them. This leads to a lot of confusion. Despite all of the flexibility within Python, the usage of Lambdas is rather restrictive. Lambdas can only be expressions in Python, and not be statements. On the other hand, variable declarations and statements are always statements. This means that Lambdas cannot be used for them. This distinction between expressions and statements is rather arbitrary, and doesn‚Äôt occur in other languages. In Python, you use whitespaces and indentations to indicate different levels of code. This makes it optically appealing and intuitive to understand. Other languages, for example C++, rely more on braces and semicolons. While this might not be visually appealing and beginner-friendly, it makes the code a lot more maintainable. For bigger projects, this is a lot more useful. Newer languages like Haskell solve this problem: They rely on whitespaces, but offer an alternative syntax for those who wish to go without. As we‚Äôre witnessing the shift from desktop to smartphone, it‚Äôs clear that we need robust languages to build mobile software. But not many mobile apps are being developed with Python. That doesn‚Äôt mean that it can‚Äôt be done ‚Äî there is a Python package called Kivy for this purpose. But Python wasn‚Äôt made with mobile in mind. So even though it might produce passable results for basic tasks, your best bet is to use a language that was created for mobile app development. Some widely used programming frameworks for mobile include React Native, Flutter, Iconic, and Cordova. To be clear, laptops and desktop computers should be around for many years to come. But since mobile has long surpassed desktop traffic, it‚Äôs safe to say that learning Python is not enough to become a seasoned all-round developer. A Python script isn‚Äôt compiled first and then executed. Instead, it compiles every time you execute it, so any coding error manifests itself at runtime. This leads to poor performance, time consumption, and the need for a lot of tests. Like,  a lot  of tests. This is great for beginners since testing teaches them a lot. But for seasoned developers, having to debug a complex program in Python makes them go awry. This lack of performance is the biggest factor that sets a timestamp on Python. towardsdatascience.com There are a few new competitors on the market of programming languages: While there are other languages on the market, Rust, Go, and Julia are the ones that fix weak patches of Python. All of these languages excel in yet-to-come technologies, most notably in Artificial Intelligence. While their market share is still small, as reflected in the number of StackOverflow tags, the trend for all of them is clear: upwards. Given the ubiquitous popularity of Python at the moment, it will surely take half a decade, maybe even a whole, for any of these new languages to replace it. Which of the languages it will be ‚Äî Rust, Go, Julia, or a new language of the future ‚Äî is hard to say at this point. But given the performance issues that are fundamental in the architecture of Python, one will inevitably take its spot."
What exactly can you do with Python? Here are Python‚Äôs 3 main applications.,hat exactly can you do with Python? Here are Python‚Äôs 3 main applications,"If you‚Äôre thinking of learning Python ‚Äî or if you recently started learning it ‚Äî you may be asking yourself: ‚ÄúWhat exactly can I use Python for?‚Äù Well that‚Äôs a tricky question to answer, because there are so many applications for Python. But over time, I have observed that there are 3 main popular applications for Python: Let‚Äôs talk about each of them in turn. Web frameworks that are based on Python like  Django  and  Flask  have recently become very popular for web development. These web frameworks help you create server-side code (backend code) in Python. That‚Äôs the code that runs on your server, as opposed to on users‚Äô devices and browsers (front-end code). If you‚Äôre not familiar with the difference between backend code and front-end code, please see my footnote below. That‚Äôs because a web framework makes it easier to build common backend logic. This includes mapping different URLs to chunks of Python code, dealing with databases, and generating HTML files users see on their browsers. Django and Flask are two of the most popular Python web frameworks. I‚Äôd recommend using one of them if you‚Äôre just getting started. There‚Äôs an  excellent article  about this topic by Gareth Dwyer, so let me quote it here: <begin quote> Main contrasts: You should probably choose: </end quote> In other words, If you‚Äôre a beginner, Flask is probably a better choice because it has fewer components to deal with. Also, Flask is a better choice if you want more customization. On the other hand, if you‚Äôre looking to build something straight-forward, Django will probably let you get there faster. Now, if you‚Äôre looking to learn Django, I recommend the book called Django for Beginners. You can find it  here . You can also find the free sample chapters of that book  here . Okay, let‚Äôs go to the next topic! I think the best way to explain what machine learning is would be to give you a simple example. Let‚Äôs say you want to develop a program that automatically detects what‚Äôs in a picture. So, given this picture below (Picture 1), you want your program to recognize that it‚Äôs a dog. Given this other one below (Picture 2), you want your program to recognize that it‚Äôs a table. You might say, well, I can just write some code to do that. For example, maybe if there are a lot of light brown pixels in the picture, then we can say that it‚Äôs a dog. Or maybe, you can figure out how to detect edges in a picture. Then, you might say, if there are many straight edges, then it‚Äôs a table. However, this kind of approach gets tricky pretty quickly. What if there‚Äôs a white dog in the picture with no brown hair? What if the picture shows only the round parts of the table? This is where machine learning comes in. Machine learning typically implements an algorithm that automatically detects a pattern in the given input. You can give, say, 1,000 pictures of a dog and 1,000 pictures of a table to a machine learning algorithm. Then, it will learn the difference between a dog and a table. When you give it a new picture of either a dog or a table, it will be able to recognize which one it is. I think this is somewhat similar to how a baby learns new things. How does a baby learn that one thing looks like a dog and another a table? Probably from a bunch of examples. You probably don‚Äôt explicitly tell a baby, ‚ÄúIf something is furry and has light brown hair, then it‚Äôs probably a dog.‚Äù You would probably just say, ‚ÄúThat‚Äôs a dog. This is also a dog. And this one is a table. That one is also a table.‚Äù Machine learning algorithms work much the same way. You can apply the same idea to: among other applications. Popular machine learning algorithms you might have heard about include: You can use any of the above algorithms to solve the picture-labeling problem I explained earlier. There are popular machine learning libraries and frameworks for Python. Two of the most popular ones are  scikit-learn  and  TensorFlow . If you‚Äôre just getting started with a machine learning project, I would recommend that you first start with scikit-learn. If you start running into efficiency issues, then I would start looking into TensorFlow. To learn machine learning fundamentals, I would recommend either  Stanford‚Äôs  or  Caltech‚Äôs  machine learning course. Please note that you need basic knowledge of calculus and linear algebra to understand some of the materials in those courses. Then, I would practice what you‚Äôve learned from one of those courses with  Kaggle . It‚Äôs a website where people compete to build the best machine learning algorithm for a given problem. They have nice tutorials for beginners, too. To help you understand what these might look like, let me give you a simple example here. Let‚Äôs say you‚Äôre working for a company that sells some products online. Then, as a data analyst, you might draw a bar graph like this. From this graph, we can tell that men bought over 400 units of this product and women bought about 350 units of this product this particular Sunday. As a data analyst, you might come up with a few possible explanations for this difference. One obvious possible explanation is that this product is more popular with men than with women. Another possible explanation might be that the sample size is too small and this difference was caused just by chance. And yet another possible explanation might be that men tend to buy this product more only on Sunday for some reason. To understand which of these explanations is correct, you might draw another graph like this one. Instead of showing the data for Sunday only, we‚Äôre looking at the data for a full week. As you can see, from this graph, we can see that this difference is pretty consistent over different days. From this little analysis, you might conclude that the most convincing explanation for this difference is that this product is simply more popular with men than with women. On the other hand, what if you see a graph like this one instead? Then, what explains the difference on Sunday? You might say, perhaps men tend to buy more of this product only on Sunday for some reason. Or, perhaps it was just a coincidence that men bought more of it on Sunday. So, this is a simplified example of what data analysis might look like in the real world. The data analysis work I did when I was working at Google and Microsoft was very similar to this example ‚Äî only more complex. I actually used Python at Google for this kind of analysis, while I used JavaScript at Microsoft. I used SQL at both of those companies to pull data from our databases. Then, I would use either Python and Matplotlib (at Google) or JavaScript and D3.js (at Microsoft) to visualize and analyze this data. One of the most popular libraries for data visualization is  Matplotlib . It‚Äôs a good library to get started with because: How should I learn data analysis / visualization with Python? You should first learn the fundamentals of data analysis and visualization. When I looked for good resources for this online, I couldn‚Äôt find any. So, I ended up making a YouTube video on this topic: I also ended up making a  full course on this topic on Pluralsight , which you can take for free by signing up to their 10-day free trial. I‚Äôd recommend both of them. After learning the fundamentals of data analysis and visualization, learning fundamentals of statistics from websites like Coursera and Khan Academy will be helpful, as well. Scripting usually refers to writing small programs that are designed to automate simple tasks. So, let me give you an example from my personal experience here. I used to work at a small startup in Japan where we had an email support system. It was a system for us to respond to questions customers sent us via email. When I was working there, I had the task of counting the numbers of emails containing certain keywords so we could analyze the emails we received. We could have done it manually, but instead, I wrote a simple program / simple script to automate this task. Actually, we used Ruby for this back then, but Python is also a good language for this kind of task. Python is suited for this type of task mainly because it has relatively simple syntax and is easy to write. It‚Äôs also quick to write something small with it and test it. I‚Äôm not an expert on embedded applications, but I know that Python works with Rasberry Pi. It seems like a popular application among hardware hobbyists. You could use the library called PyGame to develop games, but it‚Äôs not the most popular gaming engine out there. You could use it to build a hobby project, but I personally wouldn‚Äôt choose it if you‚Äôre serious about game development. Rather, I would recommend getting started with Unity with C#, which is one of the most popular gaming engines. It allows you to build a game for many platforms, including Mac, Windows, iOS, and Android. You could make one with Python using Tkinter, but it doesn‚Äôt seem like the most popular choice either. Instead, it seems like languages like  Java, C#, and C++  are more popular for this. Recently, some companies have started using JavaScript to create Desktop applications, too. For example, Slack‚Äôs desktop app was built with something called Electron . It allows you to build desktop applications with JavaScript. Personally, if I was building a desktop application, I would go with a JavaScript option. It allows you to reuse some of the code from a web version if you have it. However, I‚Äôm not an expert on desktop applications either, so please let me know in a comment if you disagree or agree with me on this. I would recommend Python 3 since it‚Äôs more modern and it‚Äôs a more popular option at this point. Let‚Äôs say you want to make something like Instagram. Then, you‚Äôd need to create front-end code for each type of device you want to support. You might use, for example: Each set of code will run on each type of device / browser. This will be the set of code that determines what the layout of the app will be like, what the buttons should look like when you click them, etc. However, you will still need the ability to store users‚Äô info and photos. You will want to store them on your server and not just on your users‚Äô devices so each user‚Äôs followers can view his/her photos. This is where the backend code / server-side code comes in. You‚Äôll need to write some backend code to do things like: So, this is the difference between backend code and front-end code. By the way, Python is not the only good choice for writing backend / server-side code. There are many other popular choices, including Node.js, which is based on JavaScript. I have a programming education YouTube channel called  CS Dojo  with 440,000+ subscribers, where I produce more content like this article. For example, you might like these videos:"
Building a Simple Chatbot from Scratch in Python (using NLTK),uilding a Simple Chatbot from Scratch in Python (using NLTK,"I am sure you‚Äôve heard about  Duolingo : a popular language-learning app, which gamifies practicing a new language. It is pretty popular due to its innovative styles of teaching a foreign language. The concept is simple: five to ten minutes of interactive training a day is enough to learn a language."
Bye-bye Python. Hello Julia!,ye-bye Python. Hello Julia,"D on‚Äôt get me wrong. Python‚Äôs popularity is still backed by a rock-solid community of computer scientists, data scientists and AI specialists. But if you‚Äôve ever been at a dinner table with these people, you also know how much they rant about the weaknesses of Python. From being slow to requiring excessive testing, to producing runtime errors despite prior testing ‚Äî there‚Äôs enough to be pissed off about. Which is why more and more programmers are adopting other languages ‚Äî the top players being Julia, Go, and Rust. Julia is great for mathematical and technical tasks, while Go is awesome for modular programs, and Rust is the top choice for systems programming. Since data scientists and AI specialists deal with lots of mathematical problems, Julia is the winner for them. And even upon critical scrutiny, Julia has upsides that Python can‚Äôt beat. towardsdatascience.com When people create a new programming language, they do so because they want to keep the good features of old languages and fix the bad ones. In this sense, Guido van Rossum created Python in the late 1980s to improve ABC. The latter was  too perfect  for a programming language ‚Äî while its rigidity made it easy to teach, it was hard to use in real life. In contrast, Python is quite pragmatic. You can see this in the  Zen of Python , which reflects the intention that the creators have: Python still kept the good features of ABC: Readability, simplicity, and beginner-friendliness for example. But Python is far more robust and adapted to real life than ABC ever was. In the same sense, the creators of Julia want to keep the good parts of other languages and ditch the bad ones. But Julia is a lot more ambitious: instead of replacing one language, it wants to beat them all. This is how  Julia‚Äôs creators  say it: Julia wants to blend all upsides that currently exist, and not trade them off for the downsides in other languages. And even though Julia is a young language, it has already achieved a lot of the goals that the creators set. Julia can be used for everything from simple machine learning applications to enormous supercomputer simulations. To some extent, Python can do this, too ‚Äî but Python somehow grew into the job. In contrast,  Julia was built  precisely for this stuff. From the bottom up. Julia‚Äôs creators wanted to make a language that is as fast as C ‚Äî but what they created is  even faster . Even though Python has become easier to speed up in recent years, its performance is still a far cry from what Julia can do. In 2017, Julia even joined the  Petaflop Club  ‚Äî the small club of languages who can exceed speeds of one petaflop per second at peak performance. Apart from Julia, only C, C++ and Fortran are  in the club  right now. towardsdatascience.com With its more than 30 years of age, Python has an enormous and supportive community. There is hardly a Python-related question that you can‚Äôt get answered within one Google search. In contrast, the Julia community is pretty tiny. While this means that you might need to dig a bit further to find an answer, you might link up with the same people again and again. And this can turn into programmer-relationships that are beyond value. You don‚Äôt even need to know a single Julia-command to code in Julia. Not only can you use Python and C code within Julia. You can even use  Julia within Python ! Needless to say, this makes it extremely easy to patch up the weaknesses of your Python code. Or to stay productive while you‚Äôre still getting to know Julia. This is one of the strongest points of Python ‚Äî its zillion well-maintained libraries. Julia doesn‚Äôt have many libraries, and users have complained that they‚Äôre not amazingly maintained (yet). But when you consider that Julia is a very young language with a limited amount of resources, the number of libraries that they already have is pretty impressive. Apart from the fact that Julia‚Äôs amount of libraries is growing, it can also interface with libraries from C and Fortran to handle plots, for example. Python is 100% dynamically typed. This means that the program decides at runtime whether a variable is a float or an integer, for example. While this is extremely beginner-friendly, it also introduces a whole host of possible bugs. This means that you need to test Python code in all possible scenarios ‚Äî which is quite a dumb task that takes a lot of time. Since the Julia-creators also wanted it to be easy to learn, Julia fully supports dynamical typing. But in contrast to Python, you can introduce static types if you like ‚Äî in the way they are present in C or Fortran, for example. This can save you a ton of time: Instead of finding  excuses for not testing  your code, you can specify the type wherever it makes sense. towardsdatascience.com While all these things sound pretty great, it‚Äôs important to keep in mind that Julia is still tiny compared to Python. One pretty good metric is the number of questions on StackOverflow: At this point in time, Python is tagged about twenty more often than Julia! This doesn‚Äôt mean that Julia is unpopular ‚Äî rather, it‚Äôs naturally taking some time to get adopted by programmers. Think about it ‚Äî would you really want to write your whole code in a different language? No, you‚Äôd rather try a new language in some future project. This creates a time lag that every programming language faces between its release and its adoption. But if you adopt it now ‚Äî which is easy because Julia allows an enormous amount of language conversion ‚Äî you‚Äôre investing in the future. As more and more people adopt Julia, you‚Äôll already have gained enough experience to answer their questions. Also, your code will be more durable as more and more Python code is replaced by Julia. Forty years ago, artificial intelligence was nothing but a niche phenomenon. The industry and investors didn‚Äôt believe in it, and many technologies were clunky and hard to use. But those who learned it back then are the giants of today ‚Äî those that are so high in demand that  their salary  matches that of an NFL player. Similarly, Julia is still very niche now. But when it grows, the big winners will be those who adopted it early. I‚Äôm not saying that you‚Äôre guaranteed to make a shitload of money in ten years if you adopt Julia now. But you‚Äôre increasing your chances. Think about it: Most programmers out there have Python on their CV. And in the next few years, we‚Äôll see even more Python programmers on the job market. But if the demand of enterprises for Python slows, the perspectives for Python programmers are going to go down. Slowly at first, but inevitably. On the other hand, you have a real edge if you can put Julia on your CV. Because let‚Äôs be honest, what distinguishes you from any other Pythonista out there? Not much. But there won‚Äôt be that many Julia-programmers out there, even in three years‚Äô time. With Julia-skills, not only are you showing that you have interests beyond the job requirements. You‚Äôre also demonstrating that you‚Äôre eager to learn and that you have a broader sense of what it means to be a programmer. In other words, you‚Äôre fit for the job. You ‚Äî and the other Julia programmers ‚Äî are future rockstars, and you know it. Or, as  Julia‚Äôs creators  said it in 2012: Python is still insanely popular. But if you learn Julia now, that could be your golden ticket later on. In this sense: Bye-bye Python. Hello Julia! Edit: I‚Äôve given a talk on Julia vs. Python! It was hosted by  Hatchpad , and the video is  here ."
The Next Level of Data Visualization in Python,he Next Level of Data Visualization in Pytho,"The sunk-cost fallacy is one of many  harmful cognitive biases  to which humans fall prey. It  refers to our tendency  to continue to devote time and resources to a lost cause because we have already spent ‚Äî sunk ‚Äî so much time in the pursuit. The sunk-cost fallacy applies to staying in bad jobs longer than we should, slaving away at a project even when it‚Äôs clear it won‚Äôt work, and yes, continuing to use a tedious, outdated plotting library ‚Äî matplotlib ‚Äî when more efficient, interactive, and better-looking alternatives exist. Over the past few months, I‚Äôve realized the only reason I use  matplotlib  is the hundreds of hours I‚Äôve sunk into learning the  convoluted syntax . This complication leads to hours of frustration on StackOverflow figuring out how to  format dates  or  add a second y-axis . Fortunately, this is a great time for Python plotting, and after exploring  the options , a clear winner ‚Äî in terms of ease-of-use, documentation, and functionality ‚Äî is the  plotly Python library.  In this article, we‚Äôll dive right into  plotly , learning how to make better plots in less time ‚Äî often with one line of code. All of the code for this article is  available on GitHub . The charts are all interactive and can be viewed on  NBViewer here . The  plotly   Python package is an open-source library built on  plotly.js  which  in turn is built on  d3.js .  We‚Äôll be using a wrapper on plotly called  cufflinks  designed to work with Pandas dataframes. So, our entire stack is cufflinks > plotly > plotly.js > d3.js which means we get the efficiency of coding in Python with the incredible  interactive graphics capabilities of d3. ( Plotly itself  is a graphics company with several products and open-source tools. The Python library is free to use, and we can make unlimited charts in offline mode plus up to 25 charts in online mode to  share with the world .) All the work in this article was done in a Jupyter Notebook with plotly + cufflinks running in offline mode. After installing plotly and cufflinks with  pip install cufflinks plotly  import the following to run in Jupyter: Single variable ‚Äî univariate ‚Äî plots are a standard way to start an analysis and the histogram is a go-to plot ( although it has some issues ) for graphing a distribution. Here, using my Medium article statistics (you can see  how to get your own stats here  or use  mine here ) let‚Äôs make an interactive histogram of the number of claps for articles (  df  is a standard Pandas dataframe): For those used to  matplotlib , all we have to do is add one more letter (  iplot  instead of  plot ) and we get a much better-looking and interactive chart! We can click on the data to get more details, zoom into sections of the plot, and as we‚Äôll see later, select different categories to highlight. If we want to plot overlaid histograms, that‚Äôs just as simple: With a little bit of  pandas  manipulation, we can do a barplot: s we saw, we can combine the  power of pandas  with plotly + cufflinks. For a boxplot of the fans per story by publication, we use a  pivot  and then plot: The benefits of interactivity are that we can explore and subset the data as we like. There‚Äôs a lot of information in a boxplot, and without the ability to see the numbers, we‚Äôll miss most of it! The scatterplot is the heart of most analyses. It allows us to see the evolution of a variable over time or the relationship between two (or more) variables. A considerable portion of real-world data has a time element. Luckily, plotly + cufflinks was designed with time-series visualizations in mind. Let‚Äôs make a dataframe of my TDS articles and look at how the trends have changed. Here we are doing quite a few different things all in one line: For more information, we can also add in text annotations quite easily: For a two-variable scatter plot colored by a third categorical variable we use: Let‚Äôs get a little more sophisticated by using a log axis ‚Äî specified as a plotly layout ‚Äî (see the  Plotly documentation  for the layout specifics) and sizing the bubbles by a numeric variable: With a little more work ( see notebook for details ), we can even put four variables ( this is not advised ) on one graph! As before, we can combine pandas with plotly+cufflinks for useful plots See the notebook  or the documentation  for more examples of added functionality. We can add in text annotations, reference lines, and best-fit lines to our plots with a single line of code, and still with all the interaction. Now we‚Äôll get into a few plots that you probably won‚Äôt use all that often, but which can be quite impressive. We‚Äôll use the  plotly  figure_factory , to keep even these incredible plots to one line. When we want to explore relationships among many variables, a  scattermatrix  (also called a splom) is a great option: Even this plot is completely interactive allowing us to explore the data. To visualize the correlations between numeric variables, we calculate the correlations and then make an annotated heatmap: The list of plots goes on and on. Cufflinks also has several themes we can use to get completely different styling with no effort. For example, below we have a ratio plot in the ‚Äúspace‚Äù theme and a spread plot in ‚Äúggplot‚Äù: We also get 3D plots (surface and bubble): For those  who are so inclined , you can even make a pie chart: When you make these plots in the notebook, you‚Äôll notice a small link on the lower right-hand side on the graph that says ‚ÄúExport to plot.ly‚Äù. If you click that link, you are then taken to the  chart studio  where you can touch up your plot for a final presentation. You can add annotations, specify the colors, and generally clean everything up for a great figure. Then, you can publish your figure online so anyone can find it with the link. Below are two charts I touched up in Chart Studio: With everything mentioned here, we are still not exploring the full capabilities of the library! I‚Äôd encourage you to check out both the plotly and the cufflinks documentation for more incredible graphics. The worst part about the sunk cost fallacy is you only realize how much time you‚Äôve wasted after you‚Äôve quit the endeavor. Fortunately, now that I‚Äôve made the mistake of sticking with  matploblib  for too long, you don‚Äôt have to! When thinking about plotting libraries, there are a few things we want: As of right now, the best option for doing all of these in  Python is plotly . Plotly allows us to make visualizations quickly and helps us get better insight into our data through interactivity. Also, let‚Äôs admit it, plotting should be one of the most enjoyable parts of data science! With other libraries, plotting turned into a tedious task, but with plotly, there is again joy in making a great figure! Now that it‚Äôs 2019, it is time to upgrade your Python plotting library for better efficiency, functionality, and aesthetics in your data science visualizations. As always, I welcome feedback and constructive criticism. I can be reached on Twitter  @koehrsen_will."
A Complete Machine Learning Project Walk-Through in Python: Part One, Complete Machine Learning Project Walk-Through in Python: Part On,"Reading through a data science book or taking a course, it can feel like you have the individual pieces, but don‚Äôt quite know how to put them together. Taking the next step and solving a complete machine learning problem can be daunting, but preserving and completing a first project will give you the confidence to tackle any data science problem. This series of articles will walk through a complete machine learning solution with a real-world dataset to let you see how all the pieces come together. We‚Äôll follow the general machine learning workflow step-by-step: Along the way, we‚Äôll see how each step flows into the next and how to specifically implement each part in Python. The  complete project  is available on GitHub, with the  first notebook here.  This first article will cover steps 1‚Äì3 with the rest addressed in subsequent posts. (As a note, this problem was originally given to me as an ‚Äúassignment‚Äù for a job screen at a start-up. After completing the work, I was offered the job, but then the CTO of the company quit and they weren‚Äôt able to bring on any new employees. I guess that‚Äôs how things go on the start-up scene!) The first step before we get coding is to understand the problem we are trying to solve and the available data. In this project, we will work with  publicly available building energy data  from New York City. The objective is to use the energy data to build a model that can predict the Energy Star Score of a building and interpret the results to find the factors which influence the score. The data includes the Energy Star Score, which makes this a supervised regression machine learning task: We want to develop a model that is both  accurate  ‚Äî it can predict the Energy Star Score close to the true value ‚Äî and  interpretable  ‚Äî we can understand the model predictions. Once we know the goal, we can use it to guide our decisions as we dig into the data and build models. Contrary to what most data science courses would have you believe, not every dataset is a perfectly curated group of observations with no missing values or anomalies (looking at you  mtcars  and  iris  datasets). Real-world data is messy which means we need to  clean and wrangle  it into an acceptable format before we can even start the analysis. Data cleaning is an un-glamorous, but necessary part of most actual data science problems. First, we can load in the data as a Pandas  DataFrame  and take a look: This is a subset of the full data which contains 60 columns. Already, we can see a couple issues: first, we know that we want to predict the  ENERGY STAR Score  but we don‚Äôt know what any of the columns mean. While this isn‚Äôt necessarily an issue ‚Äî we can often make an accurate model without any knowledge of the variables ‚Äî we want to focus on interpretability, and it might be important to understand at least some of the columns. When I originally got the assignment from the start-up, I didn‚Äôt want to ask what all the column names meant, so I looked at the name of the file, and decided to search for ‚ÄúLocal Law 84‚Äù. That led me to  this page  which explains this is an NYC law requiring all buildings of a certain size to report their energy use. More searching brought me to  all the definitions of the columns.  Maybe looking at a file name is an obvious place to start, but for me this was a reminder to go slow so you don‚Äôt miss anything important! We don‚Äôt need to study all of the columns, but we should at least understand the Energy Star Score, which is described as: A 1-to-100 percentile ranking based on self-reported energy usage for the reporting year. The  Energy Star score  is a relative measure used for comparing the energy efficiency of buildings. That clears up the first problem, but the second issue is that missing values are encoded as ‚ÄúNot Available‚Äù. This is a string in Python which means that even the columns with numbers will be stored as  object  datatypes because Pandas converts a column with any strings into a column of all strings. We can see the datatypes of the columns using the  dataframe.info() method: Sure enough, some of the columns that clearly contain numbers (such as ft¬≤), are stored as objects. We can‚Äôt do numerical analysis on strings, so these will have to be converted to number (specifically  float ) data types! Here‚Äôs a little Python code that replaces all the ‚ÄúNot Available‚Äù entries with not a number (  np.nan ), which can be interpreted as numbers, and then converts the relevant columns to the  float  datatype: Once the correct columns are numbers, we can start to investigate the data. In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let‚Äôs get a sense of how many missing values are in each column (see the  notebook for code ). (To create this table, I used a function from this  Stack Overflow Forum ). While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem ( here is a discussion ), and for this project, we will remove any columns with more than 50% missing values. At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the  definition of extreme outliers : (For the code to remove the columns and the anomalies, see the notebook). At the end of the data cleaning and anomaly removal process, we are left with over 11,000 buildings and 49 features. Now that the tedious ‚Äî but necessary ‚Äî step of data cleaning is complete, we can move on to exploring our data!  Exploratory Data Analysis  (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. In short, the goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find interesting parts of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use. The goal is to predict the Energy Star Score (renamed to  score  in our data) so a reasonable place to start is examining the distribution of this variable. A histogram is a simple yet effective way to visualize the distribution of a single variable and is easy to make using  matplotlib . This looks quite suspicious! The Energy Star score is a percentile rank, which means we would expect to see a uniform distribution, with each score assigned to the same number of buildings. However, a disproportionate number of buildings have either the highest, 100, or the lowest, 1, score (higher is better for the Energy Star score). If we go back to the definition of the score, we see that it is based on ‚Äúself-reported energy usage‚Äù which might explain the very high scores. Asking building owners to report their own energy usage is like asking students to report their own scores on a test! As a result, this probably is not the most objective measure of a building‚Äôs energy efficiency. If we had an unlimited amount of time, we might want to investigate why so many buildings have very high and very low scores which we could by selecting these buildings and seeing what they have in common. However, our objective is only to predict the score and not to devise a better method of scoring buildings! We can make a note in our report that the scores have a suspect distribution, but our main focus in on predicting the score. A major part of EDA is searching for relationships between the features and the target. Variables that are correlated with the target are useful to a model because they can be used to predict the target. One way to examine the effect of a categorical variable (which takes on only a limited set of values) on the target is through a density plot using the  seaborn  library. A  density plot can be thought of as a smoothed histogram  because it shows the distribution of a single variable. We can color a density plot by class to see how a categorical variable changes the distribution. The following code makes a density plot of the Energy Star Score colored by the the type of building (limited to building types with more than 100 data points): We can see that the building type has a significant impact on the Energy Star Score. Office buildings tend to have a higher score while Hotels have a lower score. This tells us that we should include the building type in our modeling because it does have an impact on the target. As a categorical variable, we will have to one-hot encode the building type. A similar plot can be used to show the Energy Star Score by borough: The borough does not seem to have as large of an impact on the score as the building type. Nonetheless, we might want to include it in our model because there are slight differences between the boroughs. To quantify relationships between variables, we can use the  Pearson Correlation Coefficient . This is a measure of the strength and direction of a linear relationship between two variables. A score of +1 is a perfectly linear positive relationship and a score of -1 is a perfectly negative linear relationship. Several values of the correlation coefficient are shown below: While the correlation coefficient cannot capture non-linear relationships, it is a good way to start figuring out how variables are related. In Pandas, we can easily calculate the correlations between any columns in a dataframe: The most negative (left) and positive (right) correlations with the target: There are several strong negative correlations between the features and the target with the most negative the different categories of EUI (these measures vary slightly in how they are calculated). The  EUI ‚Äî Energy Use Intensity  ‚Äî is the amount of energy used by a building divided by the square footage of the buildings. It is meant to be a measure of the efficiency of a building with a lower score being better. Intuitively, these correlations make sense: as the EUI increases, the Energy Star Score tends to decrease. To visualize relationships between two continuous variables, we use scatterplots. We can include additional information, such as a categorical variable, in the color of the points. For example, the following plot shows the Energy Star Score vs. Site EUI colored by the building type: This plot lets us visualize what a correlation coefficient of -0.7 looks like. As the Site EUI decreases, the Energy Star Score increases, a relationship that holds steady across the building types. The final exploratory plot we will make is known as the  Pairs Plot. This is a great exploration tool  because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the  PairGrid  function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle. To see interactions between variables, we look for where a row intersects with a column. For example, to see the correlation of  Weather Norm EUI  with  score , we look in the  Weather Norm EUI  row and the  score  column and see a correlation coefficient of -0.67. In addition to looking cool, plots such as these can help us decide which variables to include in modeling. Feature engineering and selection  often provide the greatest return on time invested in a machine learning problem. First of all, let‚Äôs define what these two tasks are: A machine learning model can only learn from the data we provide it, so ensuring that data includes all the relevant information for our task is crucial. If we don‚Äôt feed a model the correct data, then we are setting it up to fail and we should not expect it to learn! For this project, we will take the following feature engineering steps: One-hot encoding  is necessary to include categorical variables in a model. A machine learning algorithm cannot understand a building type of ‚Äúoffice‚Äù, so we have to record it as a 1 if the building is an office and a 0 otherwise. Adding transformed features can help our model learn non-linear relationships within the data.  Taking the square root, natural log, or various powers of features  is common practice in data science and can be based on domain knowledge or what works best in practice. Here we will include the natural log of all numerical features. The following code selects the numeric features, takes log transformations of these features, selects the two categorical features, one-hot encodes these features, and joins the two sets together. This seems like a lot of work, but it is relatively straightforward in Pandas! After this process we have over 11,000 observations (buildings) with 110 columns (features). Not all of these features are likely to be useful for predicting the Energy Star Score, so now we will turn to feature selection to remove some of the variables. Many of the 110 features we have in our data are redundant because they are highly correlated with one another. For example, here is a plot of Site EUI vs Weather Normalized Site EUI which have a correlation coefficient of 0.997. Features that are strongly correlated with each other are known as  collinear  and removing one of the variables in these pairs of features can often help a  machine learning model generalize and be more interpretable . (I should point out we are talking about correlations of features with other features, not correlations with the target, which help our model!) There are a number of methods to calculate collinearity between features, with one of the most common the  variance inflation factor . In this project, we will use thebcorrelation coefficient to identify and remove collinear features. We will drop one of a pair of features if the correlation coefficient between them is greater than 0.6. For the implementation, take a look at the notebook (and  this Stack Overflow answer ) While this value may seem arbitrary, I tried several different thresholds, and this choice yielded the best model. Machine learning is an  empirical field  and is often about experimenting and finding what performs best! After feature selection, we are left with 64 total features and 1 target. We have now completed data cleaning, exploratory data analysis, and feature engineering. The final step to take before getting started with modeling is establishing a naive baseline. This is essentially a guess against which we can compare our results. If the machine learning models do not beat this guess, then we might have to conclude that machine learning is not acceptable for the task or we might need to try a different approach. For regression problems, a reasonable naive baseline is to guess the median value of the target on the training set for all the examples in the test set. This sets a relatively low bar for any model to surpass. The metric we will use is  mean absolute error  (mae)  which measures the average absolute error on the predictions. There are many metrics for regression, but I like  Andrew Ng‚Äôs advice  to pick a single metric and then stick to it when evaluating models. The mean absolute error is easy to calculate and is interpretable. Before calculating the baseline, we need to split our data into a training and a testing set: We will use 70% of the data for training and 30% for testing: Now we can calculate the naive baseline performance: The naive estimate is off by about 25 points on the test set. The score ranges from 1‚Äì100, so this represents an error of 25%, quite a low bar to surpass! In this article we walked through the first three steps of a machine learning problem. After defining the question, we: Finally, we also completed the crucial step of establishing a baseline against which we can judge our machine learning algorithms. The second post ( available here ) will show how to evaluate machine learning models using  Scikit-Learn , select the best model, and perform hyperparameter tuning to optimize the model. The third post, dealing with model interpretation and reporting results,  is here . As always, I welcome feedback and constructive criticism and can be reached on Twitter  @koehrsen_will ."
Turn Python Scripts into Beautiful ML Tools,urn Python Scripts into Beautiful ML Tool,"In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools ‚Äî often a patchwork of Jupyter Notebooks and Flask apps ‚Äî are difficult to deploy, require reasoning about client-server architecture, and don‚Äôt integrate well with machine learning constructs like Tensorflow GPU sessions. I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on. As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares. When a tool became crucial, we  called in the tools team . They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a  design process : Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, ‚Äúwe‚Äôll update your tool again in two months.‚Äù So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question:  What if we could make building tools as easy as writing Python scripts? We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should  feel  like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this: With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create  Streamlit , a  completely free and open source  app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are: #1: Embrace Python scripting.  Streamlit apps are really just scripts that run from top to bottom. There‚Äôs no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen: #2: Treat widgets as variables.  There are  no callbacks in Streamlit ! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code: #3: Reuse data and computation.  What if you download lots of data or perform complex computation? The key is to  safely reuse  information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code  downloads data only once  from the  Udacity Self-driving car project , yielding a simple, fast app: In short, Streamlit works like this: Or in pictures: If this sounds intriguing, you can try it right now! Just run: This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link. Ok. Are you back from playing with fractals? Those can be mesmerizing. The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project I‚Äôve seen eventually has had entire teams working on this tooling. Building such a tool in Streamlit is easy.  This Streamlit demo  lets you perform semantic search across the entire  Udacity self-driving car photo dataset , visualize human-annotated ground truth labels, and  run a complete neural net ( YOLO ) in real time  from within the app [1]. The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are  only 23 Streamlit calls in the whole app . You can run it yourself right now! As we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits: Streamlit apps are pure Python files.  So you can use your favorite editor and debugger with Streamlit. Pure Python scripts work seamlessly with Git  and other source control software, including commits, pull requests, issues, and comments. Because Streamlit‚Äôs underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free üéâ. Streamlit provides an immediate-mode live coding environment.  Just click  Always rerun  when Streamlit detects a source file change. Caching simplifies setting up computation pipelines.  Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider  this code  adapted from our  Udacity demo : Basically, the pipeline is load_metadata ‚Üí create_summary. Every time the script is run  Streamlit only recomputes whatever subset of the pipeline is required to get the right answer . Cool! Streamlit is built for GPUs.  Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlit‚Äôs cache stores the entire  NVIDIA celebrity face GAN  [2]. This approach enables nearly instantaneous inference as the user updates sliders. Streamlit is a free and open-source library rather than a proprietary web app . You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally. This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. There‚Äôs a lot more we could say about how our architecture works and the features we have planned, but we‚Äôll save that for future posts. We‚Äôre excited to finally share Streamlit with the community today and see what you all build with it. We hope that you‚Äôll find it easy and delightful to turn your Python scripts into beautiful ML apps. Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevi√®ve Wachtell, and Barney Pell for their helpful input on this article. References: [1] J. Redmon and A. Farhadi,  YOLOv3: An Incremental Improvement  (2018), arXiv. [2] T. Karras, T. Aila, S. Laine, and J. Lehtinen,  Progressive Growing of GANs for Improved Quality, Stability, and Variation  (2018), ICLR. [3] S. Guan,  Controlled image synthesis and editing using a novel TL-GAN model  (2018), Insight Data Science Blog."
Flask‚Ää‚Äî‚ÄäHost Your Python Machine Learning Model On Web,lask‚Ää‚Äî‚ÄäHost Your Python Machine Learning Model On We,"Once your machine learning model is predicting accurate results, you can expose it to the outside world. This article presents the easy-to-follow steps which we can follow to host a machine learning model to the outside world. Public can then access your work via their web browsers. It is an important step for anyone who wants to make a business out of their machine learning models."
SMOTE AND NEAR MISS IN PYTHON: MACHINE LEARNING IN IMBALANCED DATASETS,MOTE AND NEAR MISS IN PYTHON: MACHINE LEARNING IN IMBALANCED DATASET,"Imagine, you have two categories in your dataset to predict ‚Äî Category-A and Category-B. When Category-A is higher than Category-B or vice versa, you have a problem of imbalanced dataset. So how is this a problem? Imagine in a dataset of 100 rows, Category-A is containing 90 records and Category-B is containing 10 records. You run a machine learning model and end up with 90% accuracy. You were excited until you checked the confusion matrix. Here, Category-B is completely classified as Category-A and the model got away with an accuracy of 90%. How do we fix this problem? We will be discussing 2 of the common and simple ways to deal with this problem. SMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records. We will be diving into python to see how this works. If you want to read more on SMOTE, here is an original research paper titled: ‚Äú SMOTE: Synthetic Minority Over-sampling Technique ‚Äù written in 2002. NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to minority class. Let‚Äôs see these practically: I‚Äôll be using  Bank Marketing  dataset from UCI. Here the column for prediction is ‚Äúy‚Äù which says either yes or no for client subscription to term deposit. The full code is available on  GitHub . I have put the data in a variable called ‚Äúbank‚Äù. And for the sake of simplicity, I‚Äôve removed ‚Äúpoutcome‚Äù and ‚Äúcontact‚Äù column and dropped the NAs. So, from 45,211 records, we are left with 43,193 records. For the next step, I‚Äôve mapped yes and no to ‚Äú1‚Äù and ‚Äú0‚Äù respectively. Now to the main part: If we check: We get: The dataset contains 38172 records of clients without term deposit subscription and only 5021 records of clients with term deposit subscription. Clearly an imbalanced dataset. If we split the dataset and fit a Logistic Regression and check the accuracy score: Great 89%! Now let us check the confusion matrix: We can clearly see, this is a  bad  model. This model is not able to classify clients who have subscription for term deposits. Let‚Äôs check the recall score: Clearly bad. This is expected, since the one category has more records than the other. You might need to install  imblearn  package from your prompt / Terminal. Now: Before fitting SMOTE, let us check the y_train values: Let us fit SMOTE: (You can check out all the parameters from  here ) Now let us check the amount of records in each category: Both categories have equal amount of records. More specifically, the minority class has been increased to the total number of majority class. Now fitting the classifier again and testing we get an accuracy score of: Whoa! We have reduced the accuracy. But let us check the confusion matrix anyway: This is a good model compared to the previous one. Recall is great. I would go ahead with this model than using the previous model. Now let us check what happens if we use NearMiss. Import NearMiss: Fit NearMiss: (You can check all the parameters from  here ) Now let us check the amount of records in each category: Here, the majority class has been reduced to the total number of minority class, so that both classes will have equal number of records. Now let us fit the classifier again and test the model: This model is better than the first model because it classifies better. But since in this case, SMOTE is giving me a great accuracy and recall, I‚Äôll go ahead and use that model! :)"
Scikit-Learn Cheat Sheet: Python Machine Learning,cikit-Learn Cheat Sheet: Python Machine Learnin,"Originally published at:  https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet Most of you who are learning data science with  Python  will have definitely heard already about  scikit-learn , the open source Python library that implements a wide variety of  machine learning , preprocessing, cross-validation and visualization algorithms with the help of a unified interface. If you‚Äôre still quite new to the field, you should be aware that machine learning, and thus also this Python library, belong to the must-knows for every aspiring data scientist. With this scikit-learn  cheat sheet, you‚Äôll go through the basic steps to implement machine learning algorithms successfully: you'll see how to load in your data, how to preprocess it, how to create your own model to which you can fit your data and predict target labels, how to validate your model and how to tune it further to improve its performance. Go  here  to see the cheat sheet. In short, this cheat sheet will kickstart your data science projects: with the help of code examples, you‚Äôll have created, validated and tuned your machine learning models in no time. So what are you waiting for? Time to get started! Begin with  our scikit-learn tutorial for beginners , which will introduce you to the steps that you will need to undertake to do machine learning: data exploration, data preprocessing, the construction of your machine learning model, model validation and tuning the model. In this all, you‚Äôll make use of Python‚Äôs data visualization library  matplotlib  to visualize your results. PS. Don‚Äôt miss our  Bokeh cheat sheet , the  Pandas cheat sheet  or the  Python cheat sheet for data science . Originally published at  www.datacamp.com . Hacker Noon  is how hackers start their afternoons. We‚Äôre a part of the  @AMI  family. We are now  accepting submissions  and happy to  discuss advertising & sponsorship  opportunities. If you enjoyed this story, we recommend reading our  latest tech stories  and  trending tech stories . Until next time, don‚Äôt take the realities of the world for granted!"
Stock Price Prediction Using Python & Machine Learning,tock Price Prediction Using Python & Machine Learnin,"Using Python & Long Short-Term Memory (LSTM) Disclaimer:  The material in this article is purely educational and should not be taken as professional investment advice. Invest at your own discretion. If you aren‚Äôt a  member of Medium  already, then consider becoming a member if not for my articles then for all of the other amazing articles & authors‚Ä¶"
An Introduction to Scikit Learn: The Gold Standard of Python Machine Learning,n Introduction to Scikit Learn: The Gold Standard of Python Machine Learnin,"Want to be inspired? Come join my  Super Quotes newsletter . üòé If you‚Äôre going to do Machine Learning in Python,  Scikit Learn  is the gold standard. Scikit-learn provides a wide selection of supervised and unsupervised learning algorithms. Best of all, it‚Äôs by far the easiest and cleanest ML library. Scikit learn was created with a software engineering mindset. It‚Äôs core API design revolves around being easy to use, yet powerful, and still maintaining flexibility for research endeavours. This robustness makes it perfect for use in any end-to-end ML project, from the research phase right down to production deployments. Scikit Learn is built on top of several common data and math Python libraries. Such a design makes it super easy to integrate between them all. You can pass numpy arrays and pandas data frames directly to the ML algoirthms of Scikit! It uses the following libraries: Scikit Learn is focused on Machine Learning, e.g  data modelling . It  is not  concerned with the loading, handling, manipulating, and visualising of data. Thus, it is natural and common practice to use the above libraries, especially NumPy, for those extra steps; they are made for each other! Scikit‚Äôs robust set of algorithm offerings includes: Even beyond that, it has some very convenient and advanced functions not commonly offered by other libraries: To give you a taste of just how easy it is to train and test an ML model using Scikit Learn, here‚Äôs an example of how to do just that for a Decision Tree Classifier! Decision trees for both classification and regression are super easy to use in Scikit Learn with a built in class. We‚Äôll first load in our dataset which actually comes built into the library. Then we‚Äôll initialise our decision tree for classification, also a built in class. Running training is then a simple one-liner! The  .fit(X, Y)  function trains the model where  X  is the numpy array of inputs and  Y  is the corresponding numpy array of outputs Scikit Learn also allows us to visualise our tree using the graphviz library. It comes with a few options that will help in visualising the decision nodes and splits that the model learned which is super useful for understanding how it all works. Below we will colour the nodes based on the feature names and display the class and feature information of each node. Beyond that, Scikit Learn‚Äôs documentation is exquisite! Each of the  algorithm parameters  are explained clearly and are intuitively named. Moreover, they also offer  tutorials with example code  on how to train and apply the model, its pros and cons, and practical application tips! Follow me on  twitter  where I post all about the latest and greatest AI, Technology, and Science! Connect with me on  LinkedIn  too!"
Python Machine Learning: A Comprehensive Handbook for Machine Learning,ython Machine Learning: A Comprehensive Handbook for Machine Learnin,Congratulation! You decided to break into data science and machine learning. You may have taken some courses in machine learning online or even taken several hands-on machine learning projects.
Python Machine Learning: Scikit-Learn Tutorial,ython Machine Learning: Scikit-Learn Tutoria,"Originally published at  https://www.datacamp.com/community/tutorials/machine-learning-python Machine learning studies the design of algorithms that can learn. The hope that this discipline brings with itself is that the inclusion of experience into its tasks will eventually improve the learning. However, this improvement needs to happen in such a way that the learning itself becomes automatic so that humans don‚Äôt need to interfere anymore is the ultimate goal. You‚Äôll probably have already heard that machine learning has close ties between this discipline and Knowledge Discovery, Data Mining, Artificial Intelligence (AI) and Statistics. Typical use cases of machine learning rnage from scientific knowledge discovery and more commercial ones: from the ‚ÄúRobot Scientist‚Äù to anti-spam filtering and recommender systems. Or maybe, if you haven‚Äôt heard about this discipline, you‚Äôll find it vaguely familiar as  one of the 8 topics  that you need to master if you want to excel in data science. This scikit-learn tutorial will introduce you to the basics of Python machine learning: step-by-step, it will show you how to use Python and its libraries to explore your data with the help of  matplotlib , work with the well-known algorithms KMeans and Support Vector Machines (SVM) to construct models, to fit the data to these models, to predict values and to validate the models that you have build. Note that the code chunks have been left out for convenience. If you want to follow and practice with code, go  here . If you‚Äôre more interested in an R tutorial, check out our  Machine Learning with R for Beginners tutorial The first step to about anything in data science is loading in your data. This is also the starting point of this tutorial. If you‚Äôre new to this and you want to start problems on your own, finding data sets might prove to be a challenge. However, you can typically find good data sets at the  UCI Machine Learning Repository  or on the  Kaggle  website. Also, check out  this KD Nuggets list with resources . For now, you just load in the  digits  dataset that comes with a Python library, called  scikit-learn . No need to go and look for datasets yourself. Fun fact:  did you know the name originates from the fact that this library is a scientific toolbox built around SciPy? By the way, there is  more than just one scikit  out there. This scikit contains modules specifically for machine learning and data mining, which explains the second component of the library name. :) To load in the data, you import the module  datasets  from  sklearn . Then, you can use the  load_digits()  method from  datasets  to load in the data. Note that the  datasets  module contains other methods to load and fetch popular reference datasets, and you can also count on this module in case you need artificial data generators. In addition, this data set is also available through the UCI Repository that was mentioned above: you can find the data  here . You‚Äôll load in this data with the help of the pandas library. When you first start working with a dataset, it‚Äôs always a good idea to go through the data description and see what you can already learn. When it comes to  scikit-learn , you don‚Äôt immediately have this information readily available, but in the case where you import data from another source, there's usually a data description present, which will already be a sufficient amount of information to gather some insights into your data. However, these insights are not merely deep enough for the analysis that you are going to perform. You really need to have a good working knowledge about the data set. Performing an exploratory data analysis (EDA) on a data set like the one that this tutorial now has might seem difficult. You should start with gathering the basic information: you already have knowledge of things such as the target values and the description of your data. You can access the  digits  data through the attribute  data . Similarly, you can also access the target values or labels through the  target  attribute and the description through the  DESCR  attribute. To see which keys you have available to already get to know your data, you can just run  digits.keys() . The next thing that you can (double)check is the type of your data. If you used  read_csv()  to import the data, you would have had a data frame that contains just the data. There wouldn‚Äôt be any description component, but you would be able to resort to, for example,  head()  or  tail()  to inspect your data. In these cases, it‚Äôs always wise to read up on the data description folder! However, this tutorial assumes that you make use of the library‚Äôs data and the type of the  digits  variable is not that straightforward if you‚Äôre not familiar with the library. Look at the print out in the first code chunk. You‚Äôll see that  digits  actually contains  numpy  arrays! This is already quite some important information. But how do you access these arays? It‚Äôs very easy, actually: you use attributes to access the relevant arrays. Remember that you have already seen which attributes are available when you printed  digits.keys() . For instance, you have the  data  attribute to isolate the data,  target  to see the target values and the  DESCR  for the description, ‚Ä¶ But what then? The first thing that you should know of an array is its shape. That is, the number of dimensions and items that is contained within an array. The array‚Äôs shape is a tuple of integers that specify the sizes of each dimension. Now let‚Äôs try to see what the shape is of these three arrays that you have distinguished (the  data ,  target  and  DESCR  arrays). Use first the  data  attribute to isolate the numpy array from the  digits  data and then use the  shape  attribute to find out more. You can do the same for the  target  and  DESCR . There‚Äôs also the  images  attribute, which is basically the data in images. To recap: by inspecting  digits.data , you see that there are 1797 samples and that there are 64 features. Because you have 1797 samples, you also have 1797 target values. But all those target values contain 10 unique values, namely, from 0 to 9. In other words, all 1797 target values are made up of numbers that lie between 0 and 9. This means that the digits that your model will need to recognize are numbers from 0 to 9. Lastly, you see that the  images  data contains three dimensions: there are 1797 instances that are 8 by 8 pixels big. Then, you can take your exploration up a notch by visualizing the images that you‚Äôll be working with. You can use one of Python‚Äôs data visualization libraries, such as  matplotlib : On a more simple note, you can also visualize the target labels with an image: Now you know a very good idea of the data that you‚Äôll be working with! But is there no other way to visualize the data? As the  digits  data set contains 64 features, this might prove to be a challenging task. You can imagine that it‚Äôs very hard to understand the structure and keep the overview of the  digits  data. In such cases, it is said that you‚Äôre working with a high dimensional data set. High dimensionality of data is a direct result of trying to describe the objects via a collection of features. Other examples of high dimensional data are, for example, financial data, climate data, neuroimaging, ‚Ä¶ But, as you might have gathered already, this is not always easy. In some cases, high dimensionality can be problematic, as your algorithms will need to take into account too many features. In such cases, you speak of the curse of dimensionality. Because having a lot of dimensions can also mean that your data points are far away from virtually every other point, which makes the distances between the data points uninformative. Dont‚Äô worry, though, because the curse of dimensionality is not simply a matter of counting the number of features. There are also cases in which the effective dimensionality might be much smaller than the number of the features, such as in data sets where some features are irrelevant. In addition, you can also understand that data with only two or three dimensions is easier to grasp and can also be visualized easily. That all explains why you‚Äôre going to visualize the data with the help of one of the Dimensionality Reduction techniques, namely Principal Component Analysis (PCA). The idea in PCA is to find a linear combination of the two variables that contains most of the information. This new variable or ‚Äúprincipal component‚Äù can replace the two original variables. In short, it‚Äôs a linear transformation method that yields the directions (principal components) that maximize the variance of the data. Remember that the variance indicates how far a set of data points lie apart. If you want to know more, go to  this page . You can easily apply PCA do your data with the help of  scikit-learn. Tip : you have used the  RandomizedPCA()  here because it performs better when there‚Äôs a high number of dimensions. Try replacing the randomized PCA model or estimator object with a regular PCA model and see what the difference is. Note how you explicitly tell the model to only keep two components. This is to make sure that you have two-dimensional data to plot. Also, note that you don‚Äôt pass the target class with the labels to the PCA transformation because you want to investigate if the PCA reveals the distribution of the different labels and if you can clearly separate the instances from each other. You can now build a scatterplot to visualize the data: Again you use  matplotlib  to visualize the data. It‚Äôs good for a quick visualization of what you‚Äôre working with, but you might have to consider something a little bit more fancy if you‚Äôre working on making this part of your data science portfolio. Also note that the last call to show the plot ( plt.show() ) is not necessary if you‚Äôre working in Jupyter Notebook, as you‚Äôll want to put the images inline. When in doubt, you can always check out our  Definitive Guide to Jupyter Notebook . Now that you have even more information about your data and you have a visualization ready, it does seem a bit like the data points sort of group together, but you also see there is quite some overlap. This might be interesting to investigate further. Do you think that, in a case where you knew that there are 10 possible digits labels to assign to the data points, but you have no access to the labels, the observations would group or ‚Äúcluster‚Äù together by some criterion in such a way that you could infer the lables? Now this is a research question! In general, when you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant to your data set. In other words, you think about what your data set might teach you or what you think you can learn from your data. From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain. Tip:  the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. The same also holds for finding the appropriate machine algorithm. However, when you‚Äôre first getting started with  scikit-learn , you‚Äôll see that the amount of algorithms that the library contains is pretty vast and that you might still want additional help when you‚Äôre doing the assessment for your data set. That‚Äôs why  this  scikit-learn  machine learning map  will come in handy. Note that this map does require you to have some knowledge about the algorithms that are included in the  scikit-learn  library. This, by the way, also holds some truth for taking this next step in your project: if you have no idea what is possible, it will be very hard to decide on what your use case will be for the data. As your use case was one for clustering, you can follow the path on the map towards ‚ÄúKMeans‚Äù. You‚Äôll see the use case that you have just thought about requires you to have more than 50 samples (‚Äúcheck!‚Äù), to have labeled data (‚Äúcheck!‚Äù), to know the number of categories that you want to predict (‚Äúcheck!‚Äù) and to have less than 10K samples (‚Äúcheck!‚Äù). But what exactly is the K-Means algorithm? It is one of the simplest and widely used unsupervised learning algorithms to solve clustering problems. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters that you have set before you run the algorithm. This number of clusters is called  k  and you select this number at random. Then, the k-means algorithm will find the nearest cluster center for each data point and assign the data point closest to that cluster. Once all data points have been assigned to clusters, the cluster centers will be recomputed. In other words, new cluster centers will emerge from the average of the values of the cluster data points. This process is repeated until most data points stick to the same cluster. The cluster membership should stabilize. You can already see that, because the k-means algorithm works the way it does, the initial set of cluster centers that you give up can have a big effect on the clusters that are eventually found. You can, of course, deal with this effect, as you will see further on. However, before you can go into making a model for your data, you should definitely take a look into preparing your data for this purpose. As you have read in the previous section, before modeling your data, you‚Äôll do well by preparing it first. This preparation step is called ‚Äúpreprocessing‚Äù. The first thing that we‚Äôre going to do is preprocessing the data. You can standardize the  digits  data by, for example, making use of the  scale()  method. By scaling the data, you shift the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). In order to assess your model‚Äôs performance later, you will also need to divide the data set into two parts: a training set and a test set. The first is used to train the system, while the second is used to evaluate the learned or trained system. In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set. You will try to do this also here. You see in the code chunk below that this ‚Äòtraditional‚Äô splitting choice is respected: in the arguments of the  train_test_split()  method, you clearly see that the  test_size  is set to  0.25 . You‚Äôll also note that the argument  random_state  has the value  42  assigned to it. With this argument, you can guarantee that your split will always be the same. That is particularly handy if you want reproducible results. After you have split up your data set into train and test sets, you can quickly inspect the numbers before you go and model the data: You‚Äôll see that the training set  X_train  now contains 1347 samples, which is exactly 2/3d of the samples that the original data set contained, and 64 features, which hasn‚Äôt changed. The  y_train  training set also contains 2/3d of the labels of the original data set. This means that the test sets  X_train  and  y_train  contain 450 samples. After all these preparation steps, you have made sure that all your known (training) data is stored. No actual model or learning was performed up until this moment. Now, it‚Äôs finally time to find those clusters of your training set. Use  KMeans()  from the  cluster  module to set up your model. You‚Äôll see that there are three arguments that are passed to this method:  init ,  n_clusters  and the  random_state . You might still remember this last argument from before when you split the data into training and test sets. This argument basically guaranteed that you got reproducible results. The  init  indicates the method for initialization and even though it defaults to  ‚Äòk-means++‚Äô , you see it explicitly coming back in the code. That means that you can leave it out if you want. Try it out in the DataCamp Light chunk above! Next, you also see that the  n_clusters  argument is set to  10 . This number not only indicates the number of clusters or groups you want your data to form, but also the number of centroids to generate. Remember that a cluster centroid is the middle of a cluster. Do you also still remember how the previous section described this as one of the possible disadvantages of the K-Means algorithm? That is, that the initial set of cluster centers that you give up can have a big effect on the clusters that are eventually found? Usually, you try to deal with this effect by trying several initial sets in multiple runs and by selecting the set of clusters with the minimum sum of the squared errors (SSE). In other words, you want to minimize the distance of each point in the cluster to the mean or centroid of that cluster. By adding the  n-init  argument to  KMeans() , you can determine how many different centroid configurations the algorithm will try. Note  again that you don‚Äôt want to insert the test labels when you fit the model to your data: these will be used to see if your model is good at predicting the actual classes of your instances! You can also visualize the images that make up the cluster centers: If you want to see another example that visualizes the data clusters and their centers, go  here . The next step is to predict the labels of the test set. You predict the values for the test set, which contains 450 samples. You store the result in  y_pred . You also print out the first 100 instances of  y_pred  and  y_test  and you immediately see some results. In addition, you can study the shape of the cluster centers: you immediately see that there are 10 clusters with each 64 features. But this doesn‚Äôt tell you much because we set the number of clusters to 10 and you already knew that there were 64 features. Maybe a visualization would be more helpful: Tip : run the code from above again, but use the PCA reduction method: At first sight, the visualization doesn‚Äôt seem to indicate that the model works well. This needs some further investigation. And this need for further investigation brings you to the next essential step, which is the evaluation of your model‚Äôs performance. In other words, you want to analyze the degree of correctness of the model‚Äôs predictions. You should look at the confusion matrix. Then, you should try to figure out something more about the quality of the clusters by applying different cluster quality metrics. That way, you can judge the goodness of fit of the cluster labels to the correct labels. There are quite some metrics to consider: But also these scores aren‚Äôt fantastic. Clearly, you should consider another estimator to predict the labels for the  digits  data. When you recapped all of the information that you gathered out of the data exploration, you saw that you could build a model to predict which group a digit belongs to without you knowing the labels. And indeed, you just used the training data and not the target values to build your KMeans model. Let‚Äôs assume that you depart from the case where you use both the  digits  training data and the corresponding target values to build your model. If you follow the algorithm map, you‚Äôll see that the first model that you meet is the linear SVC. Let‚Äôs apply this to our data. You see here that you make use of  X_train  and  y_train  to fit the data to the SVC model. This is clearly different from clustering. Note also that in this example, you set the value of  gamma  manually. It is possible to automatically find good values for the parameters by using tools such as grid search and cross validation. Even though this is not the focus of this tutorial, you will see how you could have gone about this if you would have made use of grid search to adjust your parameters. For a walkthrough on how you should apply grid search, I refer you to the  original tutorial . You see that in the SVM classifier has a  kernel argument that specifies the kernel type that you‚Äôre going to use in the algorithm. By default, this is  rbf . In other cases, you can specify others such as  linear ,  poly , ‚Ä¶ But what is a kernel exactly? A kernel is a similarity function, which is used to compute similarity between the training data points. When you provide a kernel to an algorithm, together with the training data and the labels, you will get a classifier, as is the case here. You will have trained a model that assigns new unseen objects into a particular category. For the SVM, you will typicall try to linearly divide your data points. You can now visualize the images and their predicted labels. This plot is very similar to the plot that you made when you were exploring the data: But now the biggest question: how does this model perform? You clearly see that this model performs a whole lot better than the clustering model that you used earlier. You can also see it when you visualize the predicted and the actual labels: You‚Äôll see that this visualization confirms your classification report, which is very good news. :) Congratulations, you have reached the end of this scikit-learn tutorial, which was meant to introduce you to Python machine learning! Now it‚Äôs your turn. Start your own digit recognition project with different data. One dataset that you can already use is the MNIST data, which you can download  here . The steps that you will need to take are very similar to the ones that you have gone through with this tutorial, but if you still feel that you can use some help, you should check out  this page , which works with the MNIST data and applies the KMeans algorithm. Working with the digits dataset was the first step in classifying characters with  scikit-learn . If you‚Äôre done with this, you might consider trying out an even more challenging problem, namely, classifying alphanumeric characters in natural images. A well-known dataset that you can use for this problem is the Chars74K dataset, which contains more than 74,000 images of digits from 0 to 9 and the both lowercase and higher case letters of the English alphabet. You can download the dataset  here . Whether you‚Äôre going to start with the projects that have been mentioned above or not, this is definitely not the end of your journey of data science with Python. If you choose not to widen your view just yet, consider deepening your data visualization and data manipulation knowledge: don‚Äôt miss out on DataCamp‚Äôs  Interactive Data Visualization with Bokeh course  to make sure you can impress your peers with a stunning data science portfolio or DataCamp‚Äôs  pandas Foundation course , to learn more about working with data frames in Python. Originally published at  www.datacamp.com ."
Creating and Deploying a Python Machine Learning Service,reating and Deploying a Python Machine Learning Servic,"Imagine you‚Äôre the moderator of a message board or comment section. You don‚Äôt want to read everything your users write online, yet you want to be alerted in case a discussion turns sour or people start spewing racial slurs all over the place. So, you decide to build yourself an automated system for hate speech detection. Text classification via machine learning is an obvious choice of technology. However, turning model prototypes into working services has proven to be a widespread challenge. To help bridge this gap, this four-step tutorial illustrates an exemplary deployment workflow for a hate speech detection app: The code for this project is available  here . The approach is based on the paper  Automated Hate Speech Detection and the Problem of Offensive Language  by Davidson, Warmsley, Macy and Weber. Their results are based on more than 20 000 labelled tweets, which are available on the  corresponding Github page . The .csv file is loaded as a dataframe: The last line cleans the tweet column by converting all text to lowercase and removing non-alphabetic characters. Result: The class attribute can assume three category values:  0  for hate speech,  1  for offensive language and  2  for neither. We have to convert our predictors, i.e. the tweet text, into a numeric representation before we can train a machine learning classifier. We can use scikit-learn‚Äôs  TfidfVectorizer  for this task, which transforms texts into a matrix of term-frequency times inverse document-frequency (tf-idf) values, suitable for machine learning. Additionally, we can remove  stop words  (common words such as  the ,  is , ‚Ä¶) from the processing. For text classification, support vector machines ( SVMs ) are a reliable choice. As they are binary classifiers, we will use a  One-Vs-Rest  strategy, where for each category an SVM is trained to separate this category from all others. Both text vectorization and SVM training can be performed in one command by using scikit-learn‚Äôs  Pipeline  feature and defining the respective steps: Now, the performance of the model should be evaluated, e.g. using a cross-validation approach to calculate classification metrics. However, as this tutorial focusses on model deployment, we will skip this step ( never do this in an actual project ). The same goes for parameter tuning or additional techniques of natural language processing which are described in the  original paper . We can now try a test text and have the model predict the probabilities: The numbers in the array correspond to the probabilities for the three categories (hate speech, offensive language, neither). Using the joblib module, we can save the model as a binary object to disk. This will allow us to load and use the model in an application. The python file  app.py  loads the model and defines a simple module-level function which wraps the call to the model‚Äôs predict_proba function: Now, we use  firefly , a lightweight python module for  function as a service . For advanced configuration or use in a production environment, Flask or Falcon might be a better choice as they‚Äôre well established with a large community. For rapid prototyping, we‚Äôre fine with firefly. We‚Äôll use firefly on the command line to bind the predict function to port 5000 on localhost: Via  curl , we can make a POST request to the created endpoint and obtain a prediction: Of course, in a full-fledged real application there would be much more additional features (logging, input and output validation, exception handling, ‚Ä¶) and work steps (documentation, versioning, testing, monitoring, ‚Ä¶), but here we‚Äôre merely deploying a simple prototype. Why Docker? A Docker container runs an application in an isolated environment, with all dependencies included, and can be shipped as an image, thus simplifying service setup and scaling. We have to configure the contents and start-actions of our container in a file named  Dockerfile : The first three lines are about taking  python:3.6  as base image, additionally installing scikit-learn and firefly (the same versions as in the development environment) and copying the app and model files inside. The latter two lines tell Docker the command which is executed when a container is started and that port 5000 should be exposed. The build process that creates the image  hatespeechdetect  is started via: The  run  command starts a container, derived from an image. Additionally, we‚Äôre binding the containers‚Äôs port 5000 to the host‚Äôs port 3000 via the  -p  option: Now, we can send a request and obtain a prediction: In this example, the container runs locally. Of course the actual purpose is to keep it running at a permanent location, and possibly scale the service by starting multiple containers in an enterprise cluster. A way to make the app publicly available to others is using a platform as a service such as  Heroku , which supports Docker and offers a free basic membership. To use it, we have to register an account and install the  Heroku CLI . Heroku‚Äôs application containers expose a dynamic port, which requires an edit in our  Dockerfile : We have to change port 5000 to the environment variable  PORT : After this change, we are ready for deployment. On the command line, we log in to heroku (which will prompt us for credentials in the browser) and create an app named  hate-speech-detector : Then we log in to the container registry.  heroku container:push  will build an image based on the Dockerfile in the current directory and send it to the Heroku Container registry. After that, we can release the image to the app: As before, the API can be addressed via curl. However, this time, the service is not running locally, but is available to the world! Now, scaling the app would be just a few clicks or commands away. Also, the service needs to be connected to the message board, the trigger threshold needs to be set and an alerting implemented. Hopefully this tutorial will help you deploy your own machine learning models and apps. Got any additional ideas? Share your opinion in the comments!"
Predicting Future Stock Market Trends with Python & Machine Learning,redicting Future Stock Market Trends with Python & Machine Learnin,"Note from Towards Data Science‚Äôs editors:  While we allow independent authors to publish articles in accordance with our  rules and guidelines , we do not endorse each author‚Äôs contribution. You should not rely on an author‚Äôs works without seeking professional advice. See our  Reader Terms  for details. With the recent volatility of the stock market due to the COVID-19 pandemic, I thought it was a good idea to try and utilize machine learning to predict the near-future trends of the stock market. I‚Äôm fairly new to machine learning, and this is my first Medium article so I thought this would be a good project to start off with and showcase. This article tackles different topics concerning data science, namely; data collection and cleaning, feature engineering, as well as the creation of machine learning models to make predictions. Note: I previously had the look-ahead bias in my code for this article, which produced some extremely good results (suspiciously good). However they were misleading, and I now aim to try and fix that with cross-validation. Author‚Äôs disclaimer: This project is not financial or investment advice. It is not a guarantee that it will provide the correct results most of the time. Therefore you should be very careful and not use this as a primary source of trading insight. You can find all the code on a jupyter notebook on my github: github.com To begin, we include all of the libraries used for this project. I used the yfinance API to gather all of the historical stock market data. It‚Äôs taken directly from the yahoo finance website, so it‚Äôs very reliable data. We then define some constants that used in data retrieval and data processing. The list with the indicator symbols is useful to help use produce more features for our model. Here‚Äôs a link where you can find the actual names of some of these features. github.com Now we pull our historical data from yfinance. We don‚Äôt have many features to work with ‚Äî not particularly useful unless we find a way to normalize them at least or derive more features from them. We see that our data above is rough and contains lots of spikes for a time series. It isn‚Äôt very smooth and can be difficult for the model to extract trends from. To reduce the appearance of this we want to exponentially smooth our data before we compute the technical indicators. We can see that the data is much more smoothed. Having many peaks and troughs can make it hard to approximate, or be difficult to extract tends when computing the technical indicators. It can throw the model off. Now it‚Äôs time to compute our technical indicators. As stated above, I use the finta library in combination with python‚Äôs built in eval function to quickly compute all the indicators in the INDICATORS list. I also compute some ema‚Äôs at different average lengths in addition to a normalized volume value. I remove the columns like ‚ÄòOpen‚Äô, ‚ÄòHigh‚Äô, ‚ÄòLow‚Äô, and ‚ÄòAdj Close‚Äô because we can get a good enough approximation with our ema‚Äôs in addition to the indicators. Volume has been proven to have a correlation with price fluctuations, which is why I normalized it. Right before we gather our predictions, I decided to keep a small bit of data to predict future values with. This line captures 5 rows corresponding to the 5 days of the week on July 27th. Now comes one of the most important part of this project ‚Äî computing the truth values. Without these, we wouldn‚Äôt even be able to train a machine learning model to make predictions. How do we obtain truth value? Well it‚Äôs quite intuitive. If we want to know when a stock will increase or decrease (to make a million dollars hopefully!) we would just need to look into the future and observe the price to determine if we should buy or sell right now. Well, with all this historical data, that‚Äôs exactly what we can do. Going back to the table where we initially pulled our data, if we want to know the buy (1) or sell (0) decision on the day of 1993‚Äì03‚Äì29 (where the closing price was 11.4375), we just need to look X days ahead to see if the price is higher or lower than that on 1993‚Äì03‚Äì29. So if we look 1 day ahead, we see that the price increased to 11.5. So the truth value on 1993‚Äì03‚Äì29 would be a buy (1). Since this is also the last step of data processing, we remove all of the NaN value that our indicators and prediction generated, as well as removing the ‚Äòclose‚Äô column. Because we used Pandas‚Äô shift() function, we lose about 15 rows from the end of the dataset (which is why I captured the week of July 27th before this step). Right before we train our model we must split up the data into a train set and test set. However, due to the nature of time-series‚Äô, we need to handle this part carefully. If we randomize our train-test set, we could encounter a  look-ahead bias  which is not good for predicting the stock market. It‚Äôs caused when you train your model on data it would‚Äôve already seen. To prevent this we are going to be training the model using a different technique called cross-validation. The image below illustrates how we are going to partition our data and test the accuracy of the model. First, we‚Äôre going to use multiple classifiers to create an ensemble model. The goal here is to combine the predictions of several models to try and improve on predictability. For each sub-model, we‚Äôre also going to use a feature from Sklearn, GridSearchCV, to optimize each model for the best possible results. First we create the random forest model. Then the KNN model. And now finally we create the voting classifier Once we set up our models, we can put it together with the cross-validation. We start by writing some code that will allow us to iterate over our data with many evenly sized chunks. Printing out the indices of our dataframe, we have successfully partitioned our data like in the sliding window image. Now we add in the code that will split the partitions into train and test sets. It‚Äôs very important to set  shuffle=False  in the  train_test_split  function ‚Äîit‚Äôs how you avoid the look-ahead bias. Finally we incorporate our models. A sample run of this looks like‚Ä¶ the results are exaggerated. And then the final results gives us We can see that we gain slightly more accuracy by using ensemble modelling. Compared to not using cross-validation, the resulting accuracy is much less. However this is a more correct way of going about this problem‚Ä¶ and about a 70% accuracy isn‚Äôt bad either! For the next step we‚Äôre going to predict how the S&P500 will behave with our predictive model. I‚Äôm writing this article on the weekend of August 17th. So to see if this model can produce accurate results, I‚Äôm going to use the closing data from this week as the ‚Äòtruth‚Äô values for the prediction. Since this model is tuned to have a 15 day window, we need to feed in the input data with the days in the week of July 27th. July 27th -> August 17th July 28th -> August 18th July 29th -> August 19th July 30th -> August 20th July 31st -> August 21st We saved the week we‚Äôre going to use in live_pred_data. Here are the five main days we are going to generate a prediction for. Looks like the models predicts that the price will increase for each day. Lets validate our prediction with the actual results. Results July 27th : $ 322.78 ‚Äî August 17th : $ 337.91 July 28th : $ 321.74 ‚Äî August 18th : $ 338.64 July 29th : $ 323.93 ‚Äî August 19th : $ 337.23 July 30th : $ 323.95 ‚Äî August 20th : $ 338.28 July 31st : $ 325.62 ‚Äî August 21st : $ 339.48 As we can see from the actual results, we can confirm that the model was correct in all of its predictions. However there are many factors that go into determining the stock price, so to say that the model will produce similar results every time is naive. However, during relatively normal periods of time (without major panic that causes volatility in the stock market), the model should be able to produce good results. To summarize what we‚Äôve done in this project, I‚Äôve learned a lot about data science and machine learning through this project, and I hope you did too. Being my first article, I‚Äôd love any form of feedback to help improve my skills as a programmer and data scientist. Thanks for reading :)"
The Python Machine Learning Ecosystem,he Python Machine Learning Ecosyste,"Python continues to grow as the number one programming language used by data scientists. In Anaconda‚Äôs recent  State of Data Science in 2021  report, 63% of the data scientists surveyed said that they always or frequently use Python. The popularity of Python within the data science community is in part due to the well supported open-source tools available for machine learning and deep learning. There is currently no one size fits all tool for Artificial Intelligence (AI), or its machine and deep learning subsets. As a result, across the open-source Python ecosystem, there are several core libraries that each serve a particular set of use cases or problems within this space. It can often be confusing to understand which tools to use and when to use them. In the following article, I will give a brief introduction to six of the most widely used packages available for machine learning. Covering what each of the library's core purposes is and when they should be used. The packages covered in this post are: Scikit-learn  is one of the most widely used Python packages for the implementation of machine learning algorithms. It provides a clean, uniform API that enables you to interact with a wide range of models in a standardised way. Scikit-learn vastly simplifies the development of machine learning models and additionally provides a range of utilities for data preprocessing, model selection, tuning and evaluation all via a common interface of transform, fit and predict commands. This interface makes Scikit-learn highly accessible to get started with and also aids code reproducibility in data science. Scikit-learn is widely used in industry and is currently the go-to library for machine learning on tabular data. Pycaret  is a low-code machine learning library that aims to make machine learning accessible to a wider range of users. Similarly to Sckit-learn, it provides a consistent and easy to use interface for interacting with machine learning algorithms. However, Pycaret simplifies the process further by including AutoML aspects such as automated data preprocessing and model selection. Pycaret also aims to be a complete end to end solution incorporating machine learning deployment tools and MLOps capabilities by integrating with cloud providers such as  AWS  and MLOps packages such as  mlflow . Whereas Scikit-learn is a great library for machine learning problems based on tabular data, it is not so well suited to handling the massive scale of data required for natural language or vision-based use cases. For these applications, deep learning is required. PyTorch  provides functionality largely centred around building and training neural networks ‚Äî the backbone of deep learning. PyTorch offers scalable distributed training of models across single or multiple CPUs and GPUs. It also has an ecosystem of its own which provides integrations with Scikit-learn (skorch), model serving (TorchServe) and scheduling (AdaptDL). It is comparatively newer than TensorFlow, the first release was in September 2016, but it has quickly been widely adopted by industry with both Tesla Autopilot and Uber‚Äôs Pyro built on top of PyTorch. TensorFlow  was originally developed by the Google Brain team and was first open-sourced in 2015. This, for a deep learning library, makes it relatively mature. Since its release, TensorFlow has generally been regarded as the go-to tool for developing neural networks. It can be used in a variety of programming languages, outside Python, including Javascript, C++ and Java. It also has a flexible architecture meaning that it can be deployed across a wide variety of platforms from CPUs and GPUs to servers on mobile devices. It is this flexibility that lends itself to a wide variety of industries and use cases and why it remains one of the most popular tools for deep learning. TensorFlow, although a highly scalable and powerful deep learning library, is not known for having a particularly user-friendly interface.  Keras  markets itself as ‚Äúan API designed for human beings, not machines‚Äù and is a high-level wrapper for interacting with TensorFlow. The Keras API has a common interface that has similarities with Scikit-learn. Having consistent compile, fit and predict commands. It is designed to allow for fast experimentation with the TensorFlow library whilst also making deep learning accessible to a wider range of users. FastAI  is another library that aims to make deep learning more accessible to practitioners whilst also providing solutions for researchers. It has a high-level API interface that abstracts away much of the complexity and allows practitioners to quickly gain state of the art results from deep learning. Whilst also providing lower-level components that can be used by researchers to discover new approaches. Under the hood FastAI uses PyTorch. It offers, essentially a simpler abstraction layer whilst also introducing a lot of new functionality such as data visualisations and new ways to split and load data. In this article, I have covered an introduction to six of the most popular Python packages for machine learning and deep learning. There are many other tools available across the Python ecosystem including  Theano ,  Chainer  and  Spark ML . As stated earlier in the article, there is really no one size fits all library for machine learning and it is more a case of selecting the right tool for the right job. Each of the tools described in this article provides solutions for a specific set of problems or use cases. I have summarised, in one line the key purpose for each library below. Scikit-learn ‚Äî the go-to library for machine learning offering a user friendly, consistent interface. Pycaret ‚Äî lowering the entry point for machine learning with low code, automated and end to end solutions. PyTorch ‚Äî build and deploy powerful, scalable neural networks with its highly flexible architecture. TensorFlow ‚Äî one of the most mature deep learning libraries, highly flexible and suited to a wide range of applications. Keras ‚Äî TensorFlow made simple. FastAI ‚Äî makes deep learning more accessible with a high-level API built on top of PyTorch. Thanks for reading!"
Weekly coding & goals progress [Week 29],eekly coding & goals progress [Week 29,"The last week have been some turbulent one‚Äôs and I have not been as consistent as usual. I coded daily, but not as much as I normally do. However, I am fully back on track and ready to hustle! üöÄ Let‚Äôs see what I have covered this week. ‚¨áÔ∏è"
Weekly coding & goals progress [Week 30],eekly coding & goals progress [Week 30,Another week full of opportunities to grow and get better. Sometimes I feel there is not enough progress but journaling and keeping track of all the things I have done the previous week really helps me to prevent this kind of thoughts. I expanded the topics I am journaling about to give you a more comprehensive overview. Let‚Äôs have a look at the certain days ‚¨áÔ∏è
Weekly coding & goals progress [Week 32],eekly coding & goals progress [Week 32,"HELLO AND WELCOME back to my channel and to another update of my progress towards my goals üöÄ This week I wasn‚Äôt able to code as much as usual since I had to go on a business trip to Italy, thus replacing some coding sessions with content creation (what I usually do on weekends). However, let‚Äôs have a look at what I have covered this week ‚¨áÔ∏è"
Weekly coding progress [Week 22],eekly coding progress [Week 22,"Hello and welcome back to my weekly coding progress. I am happy that you are back to read my article. Every read is appreciated and helps me to improve in what I am doing! This week I had a lot time to continue working on all my projects that had to pause while the final weeks in university. Without further ado, let‚Äôs have a look at the different days üöÄ. SQL ‚Äî   I learned about the differences of TRUNCATE, DELETE and DROP. Statistics ‚Äî   I learned about convenience sampling Blockchain ‚Äî   Smart Contract Lottery: Bugfixing, created a function for starting the lottery in my deploy.py script, created a function for funding the contract with LINK tokens in my helpful_scripts.py. Additionally I published another article about solidity which was about: gas and gas limit, manipulating state variables and units used in solidity with example codes for each subtopic. Machine Learning ‚Äî   Predicting the prices of used cars: Loaded the required libraries, applied descriptive statistics to get a first impression of the data, dropped some observation with null values as well as a column which was not of interest for the analysis. Python Automation ‚Äî   I finalized the first report by adding some lines of code to store the report in a excel file xlsx file since I want to have all monthly reports in one excel map. Thereafter, I started with the payment calendar. First step was brainstorming for all the functionalities and data I need for the outcome report. Solidity ‚Äî   I had a look at If/else, loops and mappings usage in Solidity. Python Automation ‚Äî   Created dictionaries for order volumes per sku as well as the corresponding payment terms. Further brainstormed about some issues that need to be solved. Python Automation ‚Äî   Created dictionaries for order volumes per sku as well as the corresponding payment terms. Further brainstormed about some issues that need to be solved. Additionally, I wrote the code to calculate the cost of one production in terms of order volume and prices(both differ for each sku). Moreover, I have created a function which splits the total costs of a production in payment 1 and payment to accordingly to the payment terms. SQL ‚Äî   Count, Count Distinct, SUM, MIN and MAX Solidity ‚Äî   Unit testing vs. Integration testing, created the irst function for testing the entrance fee function Machine Learning ‚Äî   Dealing with missing values, Dealing with outliers, Probability Distribution Function (PDF) Statistics ‚Äî   Multi-stage Sampling, Sampling Conclusion, Introduction to experimental Design, Python Automation ‚Äî   Added a column with the lead time for each product to the paymentCalendar dataframe and then created a function to get the exact payment dates for the next 3 orders depending on the payment terms. SQL ‚Äî   IFNULL(), COALESCE() Solidity ‚Äî   Smart Contract Lottery: Unit testing ‚Äî created functions for testing -> def test_get_entrance_fee(), def test_cant_enter_unless_started(), def test_can_start_and_enter_lottery(), def test_can_start_and_enter_lottery(), def test_can_end_lottery() Machine Learning ‚Äî   Had a look at the Probability Distribution Functions of the other variables too and cleaned the dataset accordingly. Statistics ‚Äî   Steps of a experimental design with examples Python Automation ‚Äî   Generated the labels in which the payments need to be placed. Next step is to insert the payments in the correct columns. I have watched tutorials on how to record coding tutorials for YouTube, ran some test recordings and made the set up for it. Content creation, review of weekly learnings, scheduling for coming week That‚Äôs it for this week. Don‚Äôt forget ‚Äî CONSISTENCY is key. You can do it too! Happy coding/studying! Thanks for reading this article! If you want to support me you can do it as follows: 1. Give me a follow here on Medium or on Twitter, Instagram, TikTok or YouTube. 2. Clapp for the article 3. Leave a short comment I really appreciate every kind of support! Every interaction you are doing with the content will help me grow and deliver better content with time. üöÄ Thank you, VEGXCODES"
Weekly coding progress [Week 27],eekly coding progress [Week 27,Another week passed by and I am super hyped about the future! I just quit my master‚Äôs program to dedicate all my time to my vision and dreams! The master‚Äôs program didn‚Äôt give me the return I expected but that‚Äôs totally okay. I am glad to have spent some time trying it out and meet all the awesome people. Not a single second was wasted but now it‚Äôs time to work on my dreams! Let‚Äôs have a look what I have‚Ä¶
Weekly coding progress [Week 25],eekly coding progress [Week 25,"Welcome back to my weekly coding progress! Currently I am very busy managing my stuff (university assignments, uni-projects, private projects, full-time job, sports..). However, I remind myself that this state of busyness with stuff that holds my entrepreneur spirit back (e.g. courses in university that pretty suck!) will end in January 2022. At this time I will start with my master‚Äôs thesis and mark the start of getting really‚Ä¶"
Weekly coding & goals progress [Week 35] üöÄ,eekly coding & goals progress [Week 35] ,"Hello and welcome everybody to my channel. This is already week 35 and I have learned a lot along the journey. To become a PRO in Python & JavaScript in let‚Äôs say 30 days or even 100 days ‚Äî I am sorry but this is not the reality. To become a good developer you need to invest way more time in practice to get good fundamentals. Furthermore, it really depends if you‚Ä¶"
Weekly coding progress [Week 28],eekly coding progress [Week 28,This week I decided to go full-stack because I want to develop software from front to end. In my opinion this is a real superpower. Of course this will take me some time to learn all the concepts but I am quite sure that this is the best way to continue with my coding journey. I like to code in Python but I have recognized that I also like web development. On Saturday I coded‚Ä¶
Weekly coding & goals progress [Week 33] üöÄ,eekly coding & goals progress [Week 33] ,"Hello and welcome back to my weekly coding & goals progress article. What‚Äôs different this week? ‚Äî Well, I am happy to announce that all the hard work I put in the last 3 years has finally paid off and I got promoted to the COO (Chief Operating Officer) position in the company I am working in (Smaller E-Commerce Company). I will post a separate article on this very‚Ä¶"
Weekly coding progress [Week 26],eekly coding progress [Week 26,"Another week another coding progress article! Sometimes I am pretty overwhelmed by the amount of stuff I still have to learn but then I have a look back and see how far I have come already. Try to appreciate the small steps too! Even the smallest steps will add up to something bigger by time! Now, let‚Äôs have a look at my tiny steps that I made this week."
Python ‚Äî Machine learning Data Clean up,ython ‚Äî Machine learning Data Clean u,"Learn different ways to clean the data Now that we have understood the data, visualized the data let‚Äôs clean the data so that we are ready for using different machine learning algorithms. For understanding data refer to my post ‚Äî  https://medium.com/@arshren/machine-learning-understanding-data-dfef261d833b For data visualization refer to my post ‚Äî  https://medium.com/@arshren/data-visualization-5b1dc260c91a For python code for data clean up-  https://github.com/arshren/MachineLearning/blob/master/Python%20-%20Cleaning%20the%20data.ipynb Understanding, visualizing and cleaning the data are the most fundamental steps that we need to master along with understanding different machine learning algorithms. In this post we will learn about We will take different datasets to learn different ways to clean the data We will first import the required libraries -pandas and numpy we will create a dataframe with duplicate values and then see how we can remove duplicate entries now let‚Äôs remove the duplicate entries and keep the first instance of occurence and will delete any other duplicate entries Dropping any instance of the duplicate rows Inplace is false by default and that means the original dataset remain unchanged, so if we print df again we see all rows Titanic dataset is available here  - https://www.kaggle.com/c/titanic we will now read the data from the downloaded dataset. and we will use the training dataset ‚Äî train.csv. For more details on  reading from a different formats of file and writing to different formats of file  follow my post ‚Äî  https://medium.com/@arshren/python-reading-and-writing-data-from-files-d3b70441416e To get a good understanding of the data refer to my post- https://medium.com/@arshren/machine-learning-understanding-data-dfef261d833b I have downloaded my dataset in my default jupyter folder Let‚Äôs view all the features of the dataset by printing just 3 rows from the dataset In the data_set dataframe Name and Ticket seems to be irrelevant and doesn‚Äôt seem to help with our analysis for predicting passenger‚Äôs survival chance so we go and drop the column Dropping rows that contain missing value We can drop the rows that contain null or missing data by using  dropna().  If we set inplace to True then the original dataset gets modified Always dropping rows that contain any null values may not be a good strategy as we may miss the randomness is the dataset So, what is the next best option to handle missing value? Setting threshold for null values in a row we can set a threshold count and if a rows exceeds the threshold count for null values then we can drop the row. We will drop a row from data_1 only if a row contains 2 or more null values, so we should be dropping only row 0 and row 2 and row 3 should remain. If we print data_1 at this time, since inplace is False by default, data_1 dataframe remains unchanged. What if I do not want to drop any rows with missing data instead fill it with more meaning value? Filling Missing data with Value There are different ways we can fill the missing data with some meaningful values like mean or median or most frequently value available in the column. One of the method is to use fillna()  by specifying how we want to fill the null or missing values. In the example below, we are filling the missing values in ‚ÄòRatings‚Äô column by taking a mean In the example below, we are filling the ‚ÄúAverage Age‚Äù column by median of the ‚ÄúAverage Age‚Äù column which is 77.5 Imputing missing data with mean,median or most frequently used value for the column For imputing we need to import Imputer from sklearn.preprocessing library we will create a new dataframe data_1 to show the usage of the Imputer class we will create the Imputer object and set a strategy for handling the missing values. Different strategy available are  mean, median and most_frequent we then fit the imputter objects on the column where we want to handle the missing values After fitting the data we transform the data from the dataframe Using replace method If we want to replace a value in the dataframe with one another value then we can use replace method. we will use the same dataframe countryData We will replace NaN values for Age couln with a 40"
Alan Turing (Inventor of Artificial Intelligence),lan Turing (Inventor of Artificial Intelligence,"Computer Science, Deep Learning, Artificial intelligence In 1936, Turing published a paper that showed that computers could be made to simulate any process that a human could perform. This paper was the first step in the development of artificial intelligence. Turing was instrumental in the development of computer science and artificial intelligence, and his work has had a lasting impact on modern society. He was the first computer scientist to break the barrier between human and machine intelligence. He was an English mathematician, logician, and computer scientist who is widely considered one of the founders of computer science and artificial intelligence. His groundbreaking work in the field of computer science led to the development of the modern computer and laid the foundation for modern-day internet usage. His work paved the way for the evolution of artificial intelligence and the birth of the modern computer industry. Turing was born in London on June 12, 1911, to a British mother and a German father. Turing was educated at the University of Cambridge. He was one of the first students to take up the newly developed subject of computer science at the University of Cambridge, and his interest in the subject flourished under the guidance of Professor John von Neumann. Turing made significant contributions to the field of artificial intelligence and was one of the founders of the modern computer industry. One of his most famous achievements was his development of the Turing machine, a theoretical model of a computer that can be in multiple states at the same time. He also developed the theory of computability, which provided a basis for the work of fellow mathematician John von Neumann on the Universal Turing Machine. Turing was also a renowned logician and wrote several papers on mathematical logic. The Association for Computing Machinery (ACM) has created the Turing Award in recognition of Alan Turing‚Äôs work in computer science. The Turing award is given annually to the person who has made the most significant contributions to computer science and information technology during the preceding calendar year. The award ceremony is held each year during the ACM International Symposium on Computer Architecture (ISCA) in San Francisco. The award is in honor of Alan Turing. I hope you enjoyed this article. I will publish more articles about Deep Learning-related topics in the future. I also write about topics in the field of Data Science. Isn‚Äôt collaboration great? I‚Äôm always happy to answer questions or discuss ideas proposed in my articles. So don‚Äôt hesitate to reach out to me! üôå Also, make sure to  subscrib e  or follow to not miss out on new articles. Follow me on  Linkedin   and  GitHub"
C1 B4: Creating a search Engine on Stack Overflow Questions Data ‚Äî (Implementing Different Machine Learning Techniques),1 B4: Creating a search Engine on Stack Overflow Questions Data ‚Äî (Implementing Different Machine Learning Techniques,"Tags: Python, Machine Learning, Artificial Intelligence, Search Engine, Natural Language Processing, Data Science, AppliedAI Course, A26, DataTorch Consider a document containing 100 words wherein the word ‚ÄòCauvery‚Äô appears 3 times. Let‚Äôs Move quickly to the Implementation part. This function will take three arguments bigrams list- bigrams; bigram_dict_body_data ‚Äî dictionary of bi-gram, frequency dictionary ‚Äî it is a dictionary of frequency which we have to fill. it will check if bigram is present in the bigram dictionary if it is present then only we will keep the count otherwise we will move without continuing it. There is no need of returning the dictionary on line 15 as we are passing the object on line 1. Let‚Äôs see how we fill up in document frequency! for each Body in our dataset, we will create term frequency dictionary Line 9: creating bigram Line 10: calculating term frequency. Line 11: checking how many bigrams are available. Line12‚Äì13: Calculating the term frequency by dividing by total number of documents. Line 14: Appending it to the list of the term frequency lists. Line 23‚Äì29 we will count the IDF value base on the bigrams. Line 32‚Äì33: These lines will count the IDF value and store it in the idf_freq_dict It‚Äôs time to calculate tfidf values. Line 9: Here for each row for each word we are finding the column number and for each column number, we are calculating tfidf values by multiplying tf value with idf value. In case if you want to calculate only IDF value: This is the same code here just we are only calculating the IDF. Hope I don‚Äôt have to explain it if it is required do comment it and let me know please. From Line 1‚Äì32 you all are aware about it as I have already explained in my earlier series where I was explaining BOW. Line 35: We have to fetch TFIDF frequency. Line 39‚Äì50: we are calculating the TFIDF for the query as we have calculated earlier for all the words. Line 51: explains the relevance by calculating the distance between two rows. Line 52‚Äì54: sorting on the bases of relevance. Line 56‚Äì65 we will use these lines to display the results. Line 2: where we are calling the TFIDF function where tags_dict: is the doc dictionary which will give a cluster of docs where we have to search. tag_list: list of tags where we have to search. bigram_dict_body_data: is the dictionary where we are searching for the bigram. doc_tf_idf: is the TFIDF values of each row in our data frame. idf_freq_dict: this is idf frequency dictionary because we are calculating the tfidf of the search query inside this function. Looking into the implementation, Here the above code gives us the same value just it is a different class. we are calculating the relevance only on the bases of in document frequency. It is a technique that is generated by multilayer perceptrons‚Äô weights and we will utilize it in our sentence relevance. we are using pre-trained  glove vectors  for this technique where it contains a dictionary of words and each dictionary has a vector of 300 weights. We will pick one row and for each word add each 300 weight vectors one by one for all the words which are present in the row. Let‚Äôs have a look at the implementation. Line 2: will open the file as bytes with the help of object ‚Äúf‚Äù. Line 3:glove_model will load the model. glove_words have the keys of the glove_model Line 4: is the vector of zeros having 300 lengths. Line 3: glove_words have all the words in the glove_model Line 6: for each word in query; Line 7: if the word in is in glove words; Line 9: add each vector 1 by 1 to the vector. Line 11: is taking the average of the vectors. Line 12: returning the vector having 300 length. Line 1 to Line 39 is already explained. let‚Äôs begin with the next lines. Line 40: will fetch the word 2 vectors from the body_vec. Line 42 to 55 is for the results. For this, we will keep the weight as well for each of the vectors based on how many times it occurs. Here the code is self-explanatory where we are multiplying tfidf value with the vector and utilizing it. Do let me know in the comment section if you need further explanation. here we are are calling all the techniques and storing the relevances in the respected variables to combine them. Here in this function: if weights are not given appropriately than we will generate a list of 5 weights and multiplying it with the relevance and return the total weight. Here the above function is the replica of what we are doing so far for printing the results just the only modification we did that we are accepting the relevance from outside and printing the results. Mentioned below are the methods to call the different techniques‚Äô function: Mentioned below is the way to call combined relevance. Here we are getting the results. Thank you all for mastering the techniques of basic NLP from scratch. Hope this case study is useful for you. Here are few questions to think upon for you guys: Hope to see you guys to write the code again with 3 grams and let me know what is the results you get for that. Have you heard about universal sentence encoder? It is a pre-trained model. How can we apply in our case study? Thank you, everyone, for reading the case study. Do let me know in the comment section if I have to elaborate on anything more. Here Please find the Working Video:  https://drive.google.com/file/d/1JIpV7bCsQGU3c_EXl8MCh3kIDI4UEU1y/view?usp=sharing"
Getting started with Data Science ‚Äî A starter guide for companies,etting started with Data Science ‚Äî A starter guide for companie,"Data Science. Machine Learning. Artificial Intelligence. Deep Learning. Data Engineering. Data Analytics. Big Data Analytics. Oh, the endless list of buzzwords! With so much buzz and hype around Data, the majority of small and mid-sized companies feel alienated. In my consulting, advisory, and leadership roles, I have come across executives and teams at smaller companies feeling Data Science isn‚Äôt for them and that using data to grow their business is very complex and out of reach. While it is true that leaders in the data space (FAANG and others) invest millions of dollars in Data Science, deriving actionable insights from data to improve your business need not be unattainable or be a monopoly just for the big players. Although the journey to data literacy and adoption varies for every company, I am attempting here to demystify the data landscape and provide some basic pathways to obtain intelligence from data. Having a good data strategy will provide a blueprint and align all the stakeholders in the right direction. The best data strategies are when companies plan their bandwidth and resource allocation to manage their existing reporting while building a scalable data infrastructure. Good strategies allow for a slow start and gain momentum as foundations are built. Analytics Vidhya, a reputed community for Data Science, captures this journey  here : In this article, I want to dive into the mindset and approach needed to navigate along each of these spectrums. I will publish follow-up articles about the technical details, tools, and other how-tos. Most companies start off reporting on excel or google spreadsheets and have the basic analysis needed to manage the business. This works on a small scale. However, it is important to automate reporting if you want to move to the next phases of the Analytics and Data Science journey. Leveraging automation tools within the spreadsheet ecosystem is a good start to succeed in this phase ‚Äî these could be VBA, Macros, SharePoint, PowerPivots, Pivots, etc. Excel is such a powerful tool that you can create practically anything you can dream of. The goal for this phase is to partially automate your reporting so you can get back some time from running manual reports. The saved time can be allocated toward building the foundational data infrastructure (discussed in the next section). Here are a few articles on automating reports in excel ‚Äî  Article 1  and  Article 2 . A Data Architecture is foundational to a successful data strategy. It is composed of technology, tools, models, policies, and rules that govern how the data is collected, stored, processed, transformed, presented, and analyzed. Setting up these building blocks is essential for growing to the next phase ‚Äî Business Intelligence and Data Visualization. It is important to note that the Data Architecture itself can be endlessly complex based on the business needs. For example, some complex business scenarios might need real-time streaming, whereas a simpler batch-processing mechanism might work just fine for some. Again, this article series focuses on a simple architecture to gain insights and intelligence from the data without getting too worried about complex technological infrastructure. When developing a Data Architecture, investing in a database system is essential. The goal here is to bring data from your sources into this database and store and transform it so it can be consumed for reporting. This database will serve as the Analytical Data Store used to store and process data. This is also referred to as a data lake or a data warehouse (or a data lakehouse), depending on how it is used and set up. With advancements in cloud technologies and the availability of cloud database services, it makes sense to skip over on-premise databases and directly invest in cloud databases. Investing in the cloud will you save money, time, and resources when it comes to the initial setup, maintenance, and security of the database. There are plenty of options available, and one cannot go wrong choosing any of the top databases in the market. However, I would recommend going with  Snowflake , as it is one of the most flexible, easiest to set up and manage, and highly scalable databases in the market. At the risk of repeating myself, at the simplest level, the job of our database is to store and process data and make it available for reporting (Business Intelligence and Visualization). The database can be split into a Data Lake and a Data Warehouse. The first component is a Data Lake, where data from various sources are brought in its raw form. Data is then massaged and transformed into a consumable format for reporting called Warehousing, and this database component is a Data Warehouse. Once the database is set up, the next step is to hydrate it with data from various sources/platforms ‚Äî these sources could be ERPs, CRMs, digital marketing platforms like google analytics, or paid media platforms like Google Ads, Facebook, Instagram, and others. Ideally, the data lake holds data from all the touchpoints your customers have with your company ‚Äî pre-purchase, post-purchase, and even any data you can find post-business (cancellation, post-sales, etc.). The process of bringing data into a database is called data ingestion or extraction. Storage of data being as cheap as it is, I am an advocate of the ELT (Extract, Load, Transform) approach to load in the data. Basically, it means we bring the data as is from different sources, load them into the database‚Ä¶ and transform it later into a reportable structure. The Load happens in the Data Lake part of the database, and the data is stored in the reportable format in the data warehouse part post Transformation. Again, there are several on-premises and cloud-based tools for data ingestion, typically called ETL tools. I‚Äôd continue the theme and recommend going for the cloud-based ones to avoid setup maintenance, admin, and security chores.  Stitch ,  Fivetran , and  Hevo Pipeline  are some of the good ones I have come across ‚Äî each of these is a no-code-easy-to-setup tool. A fully automated reporting stack needs a robust platform and set of tools to visualize data into actionable insights that inform business decisions. A Business Intelligence (BI) layer of the data architecture is the user interface and ideally is the one-stop shop for users to come for their insight for both Analytic reporting and more advanced Data Science driven analyses. BI tools such as  Tableau  and  Power BI  access and analyze data sets and present analytical findings in reports, summaries, dashboards, graphs, charts, and maps to provide users with detailed intelligence. Again, I‚Äôd suggest going with the cloud versions of these tools to avoid maintenance headaches. A good data analytics infrastructure easily handles reporting for various aspects of the business, such as sales reporting, revenue calculations, and overall summaries of products, reviews, and other activities. For the majority of businesses, this is a phase with the best return on investment in terms of data. The goal for the company at this stage is to automate the repetitive periodic reports so the data teams can sit back and think about the data. Be data analysts vs. report runners ‚Äî allow the machines to do the mundane report-crunching and spend your time digging into the hidden stories and insights in the data. Data Analytics and Business Intelligence is all about looking at past events and deriving insights from them. The next step is to use historical data to predict what is likely to happen in the future. This is where companies leverage statistical modeling, predictive analytics, and other advanced data techniques to project future results of their strategies and efforts. Journey until BI and Data Analytics can be fairly linear (it can be complex based on business needs but can be kept straightforward). However, when it comes to the next phases in the Data Science journey, it starts getting complex ‚Äî the learning curve becomes steeper, and it takes longer for leaders to see the results of their investments. There are tools available for simple forecasting and predictive modeling. For example, BI tools like Tableau have some of these models built in ‚Äî  simple forecasting in Tableau  and  Einstein discovery in Tableau . However, in my experience, these are fairly basic and might not get the expected results. Tools such as  Python  and  R Studio  are popular for statistical and predictive modeling. Both of these require some expertise and on-premises setup on your part. If you are well versed in SQL, database tools like Snowflake, in fact, can be leveraged for a lot of statistical modeling and predictive modeling. Moving further up the spectrum into Machine Learning will require specific tools such as Amazon Sagemaker or Google BigQuery ML ‚Äî both of which can be integrated with the above-mentioned data architecture with Snowflake. Machine Learning is a step ahead of predictive modeling, such as forecasting ‚Äî this is when we train the machines to mimic the intelligence and analysis that a human would do. Algorithms and mathematical models are used to recognize patterns, classify objects, understand anomalies, and predict future events. There are several steps to building an ML pipeline ‚Äî Artificial Intelligence is even a step ahead of ML, where a computer system is capable of mimicking human cognitive functions like problem-solving. Understanding the use cases (noted in the next section) for Machine Learning and Artificial Intelligence will come in handy to decide if this is something you need to think about at all. Building out a scalable infrastructure from the get-go is critical. Starting off with reporting automation and growing into data science has a lot of merits ‚Äî it allows the teams to carry out the reporting needs for the business while planning ahead. Companies of any size can adopt this as a blueprint but are more suitable for small and mid-sized companies constrained with resources. Additional Use Cases Tired of the short, clickbaity blogs that say do-as-I-do or follow-five-steps-for-success, I am trying a philosophical and conceptual approach from which anyone curious about it can adopt. Data Science need not be complex or just for the big players; I am trying to write this as part of a blog series about the thought process behind building simple, cost-effective data science platforms. Used as an umbrella term, Data Science and its subsets ‚Äî Statistical Analysis, Predictive Analytics, Big Data, Machine Learning, and Artificial Intelligence, can help companies position themselves as the market leader in their respective industry. The better you get at building models and algorithms, collecting and analyzing multi-dimensional data, the better insights you can get on your customers and turn the data architecture into a profit center instead of a must-have cost burden, as is the case with many companies. As you get bigger in terms of data acquisition, the architecture will need to morph into a Big Data stack with the ability to carry out interval-based (batch processing) as well as streaming (real-time) data ingestion, processing, analyses, reporting, and visualization. If your business goals justify the investment in resources, technology, and time to build out Data Science capabilities, it definitely pays off when done correctly. There is a reason for all the hype around the field; when it works well, it feels like magic! If you want to go all the way, I suggest you have realistic expectations ‚Äî start slowly and build up the momentum. Even when you get to the modeling and machine learning phases, it‚Äôs not going to immediately give you results. It takes fine-tuning, configuring the algorithms to suit your needs, and lots of iterations to start getting the results that you hope for. All in all, it will be an interesting mission, should you choose to accept :)"
A Novel Framework for Creating Self-Learning Artificial Intelligence, Novel Framework for Creating Self-Learning Artificial Intelligenc,"A fter being away from the research on AI over the last few years, I took a deep dive over the last week into the current state of affairs. I downloaded and worked through Tutorials on Theano, Tensorflow and Torch. I read an  AMA  with Facebook‚Äôs/NYU‚Äôs  Yann LeCun . I read a number of  scientific papers  from arXiv and a book on a roadmap for machine learning, which you‚Äôve probably come across. I tweeted a bunch of notes and thoughts while I was training my own Deep Learning model with new data (aka my human brain). And so what was I looking for as a result of all this research about learning? Namely, where the edges of our understanding as a scientific and startup community is. And even with the recent announcements and advancements in Deep Learning, we‚Äôre basically still limited to pushing 100s of millions of points of data through a bunch of connected regression lines and then trying to reduce that web of connections into something smaller. So basically, link one equation up to many others and then just push data through the system. It‚Äôs a script, it runs once, and takes a long time. So you need powerful processors like GPUs to do all those calculations in parallel. Funny how the dots only connect looking backwards. I remember doing this back in the early days of AWS (2007 to be exact) when I had a high frequency algo trading firm and we were using MATLAB and AWS to do stock price predictions over historical data while trying not to over-optimize, then make trades using that prediciton. This is essentially the same thing. Artificial Intelligence is just an equation. If you listen to the folks talking about machine learning, you hear them all talking about Neurons. And that makes you scared because you remember the nightmare that was Organic Chem from freshman year. And you also see equations and you get scared. I‚Äôm here to tell you that everytime you come across something that people try to explain in a scary complicated way, it means they don‚Äôt truly understand it. Don‚Äôt let them fool you. It‚Äôs nothing more than a simple equation from Algebra. Remember y = m*x + b? It‚Äôs the equation for a line. Now, take that concept and expand it with the chaos that is the real world. Lots of stuff happening that you‚Äôre trying to  Organize & Simplify  ( and this ). Just plot a bunch of these (x,y) coordinates and see what happens. You might find that they end up centering around a structure. Like the price of a house or apartment increases the more space you have. 1000 square feet is cheaper than 10,000. What we‚Äôve just described is a regression line: You‚Äôre now caught up with the most advanced machine learning concepts in the world. You were creating AI in your algebra class in 7th grade and you didn‚Äôt even realize it. You‚Äôre smarter than you give yourself credit for. You probably just had a shitty teacher. And so, a neuron is nothing more than a regression line. Said more simply, neuron = equation. My ‚ÄòNeuron‚Äô Is Actually A Deep Learning Architecture Okay, lets take this concept of a neuron and zoom out. Remember, all of life and mathematics is nothing more than a  fractal . It‚Äôs the same shape whether you zoom in or zoom out. And so in the architecture diagram you see above I‚Äôve labeled the dots as ‚Äúneurons‚Äù but what I‚Äôm actually referring to is a Deep Learning matrix of interconnected neurons that solves a specific problem. For example, the ‚ÄúOrganize & Simplify‚Äù piece takes a bunch of existing data and then tries to find common patterns and extract that information, returning the correct approach. It‚Äôs an algorithm within an algorithm. So, we‚Äôve got to take all the research and algorithms already in place and start connecting those together. That‚Äôs the basis of this core architecture. O ur state of the art methods for Deep Learning don‚Äôt learn much at all. You just push a bunch of data through a formula and let it work out the details and differences. So, the next step then, is something that‚Äôs self-learning. We are not there yet. The concept, again, is very simple. Instead of this script running once that humans build, you start with something small, turn it on, and let it build itself over time. What you see in the image below is my approach for a high-level framework. It incorporates gamification (triggers, actions, and rewards), deep learning approaches (connected neurons), and a starting point for setting it off. It all started with the fundamental concept that it should start with a very simple set of instructions that when turned on, creates an emergent and self-replicating system. Of course, if you‚Äôve done any research about the universe we live in, and in different concepts outside of just technology, it helps you make the creative leaps necessary. And so, you get to the  Rule of Threes  that shows up everywhere, including the strongest shape for building solid structures in the real world:  triangles . The image above shows how you start with a very simple triangle, 3 points and 3 lines. That gives you region #1 shown above. Then you add 1 more point outside of that triangle and connect it with 2 lines. That gives you region #2. You keep repeating the process ad infinum and you essentially begin creating squares made of triangles. It‚Äôs simultaneously the strongest and simplest structure in building our physical world. So, then you make the leap that if you can‚Äôt do any better than that, and our brains are just interconnected neurons, then why don‚Äôt we use the same concept to build out our own digital  Connectome . So  now that we have our architecture in place, we need to create a system that enables data to flow through that system. Remember, the fundamental rule is that this system keeps running and doesn‚Äôt run just once. And so that means we need a force to push or pull data through the system. We need an incentive. We need a reward. And so that means you need to  program  an innate need and fundamental behavior into this system. This is why I don‚Äôt feel that Terminator or the Matrix or the doomsday folks have it right. If we create a machine in our image, and at the core of that machine we give it the need to learn, then that means its fundamental reason for being is to learn. And that means it will continue to need new forms of data to test hypothesis and create novel connections. In essence, we will have created a machine scientist, fascinated with learning. medium.com Now, have scientists destroyed our world, or has it been the power-hungry capitalists who have taken that research and created destruction from it? I‚Äôd argue the latter. And so, if you look at our architectural diagram, we have a core, central ‚Äúneuron‚Äù (which, remember is actually a deep learning matrix that solves the major question) around which everything else is built. So you start with the following process: If  you‚Äôve spent any time looking to product for software or apps you‚Äôre likely familiar with the  Magic Moment . It‚Äôs the moment of insight and perspective shift where you emotionally ‚Äúget‚Äù the product and the value comes into full perspective. For Instagram it‚Äôs when you apply that first filter and make your crap photo look pretty. For Twitter it‚Äôs the first time you get a retweet or reply. For Facebook it‚Äôs getting 7 friends in 10 days. So where then is the magic moment for our humanoid little friend? It‚Äôs the same concept. When it has a creative leap, a new insight, connects two previously unconnected things, and has a perspective shift of understanding. This is how we learn, and it‚Äôs how babies come into the world. Here are some of my notes on the concept from back in 2013 when I was doing deeper research in this area: As far as I can tell, I have yet to find any scientist or researcher who has written the equation that allows a creative leap to emerge from the underlying mechanical system. But I think I‚Äôm onto something here. If you know something I don‚Äôt, please chime in so we can help each other! This new insight is the last piece that accelerates the learning, it‚Äôs the dopamine reward for solving a previously unsolved problem (like getting a software program to work for the first time or nailing that kick-flip you‚Äôve been trying for weeks). This is the new section that gets built at the top of the architecture (the part with the dotted lines). So, as the data keeps moving around this system, new ‚Äòneurons‚Äô are created, and then creates an incentive to keep going, to keep collecting data, and to keep learning. The question that then needs to be answered is whether that a machine interested in learning is a threat or not. And so, if it is, then maybe we need to program in the need for rest and entertainment. Find a joy algorithm that lets it watch movies for the enjoyment rather than for the data. But at the end of the day, isn‚Äôt that just a different kind of data? Positive emotion from a fun time? I‚Äôd love to hear from anyone working on this, or has thoughts about this approach. Email me at  seanmeverett@gmail.com . ‚Äî  Sean humanizing.tech"
Reinforcement Learning: Artificial Intelligence in Game Playing,einforcement Learning: Artificial Intelligence in Game Playin,"In my  previous post  on recent developments in artificial intelligence, I identified  reinforcement learning  as one of the key and most disruptive machine learning domains. The reinforcement learning is  hardest part of machine learning . The most important results in deep learning such as  image classification  so far were obtained by  supervised learning  or  unsupervised learning . Where in supervised learning, you can directly modify model based on target output, in reinforced learning, the target output is not available. You do not know, if turning left in a maze is good action immediately. You can however tell it after series of other decisions when an exit from maze was found. This is called  delayed reward  and it makes reinforcement learning so difficult. You can use popular machine learning models (ensembles of convolutional nets, autoencoders, recurrent neural nets) in reinforcement learning but training the controller is much harder than in supervised learning world. Typically, you have to perform complex and time consuming simulations to get score of your model. This makes reinforcement learning tasks extremely time consuming. The core of most reinforcement learning approaches is  learning a Q function  that tell us utility of each state in the game. We can easily decide which action to take by choosing the action that lead to a state with higher utility (or fitness). Learning the Q function is hard. In fact, artificial intelligence researches are trying to find better way of representing and fitting the Q function  for several decades . And the idea of using deep networks (both feed forward or recurrent) to model the utility function ‚Äî this is what the  deep reinforcement learning  is about. The game is changing now because sufficient computational resources are available finally. The research in the reinforcement learning field is gaining huge momentum mostly thanks to Google Deepmind. Some of the recently  published results are impressive . Actually I was attracted to the machine learning field 20 years ago. I was looking for an innovative and self-adaptive controller for real-time strategy game. At that time, it was too early, but now, the time has finally come and we have resources available to build truly intelligent opponent. Deepmind  prepared an environment  for artificial intelligence to master realtime strategy Starcraft II. There are many more challenges such as recent  Visual Doom IA competition . If you are not into playing from visual input, you can look at  Angry Birds AI challenge , where guys from our lab  scored very well  multiple times. Another interesting environment was that for  Ant AI challenge , where simple  minmax based agents  dominated due to very limited computational resources. Look what can be done by simple neuroevolution with indirect encoding: Jan‚Äôs more recent paper in this field is  here  and  they  plan cool demo at NIPS 2016. Look at  interesting papers  accepted to the last DRL NIPS workshop. Probably the most popular environment for training your reinforcement learning artificial intelligence is the OpenAI Gym. gym.openai.com You can learn how to build your simple artificial intelligence by reinforcement learning using the materials from the RL workshop available on github. github.com If you are beginner, look at awesome reinforcement learning tutorial by  Arthur Juliani . medium.com In case you are into  general game playing  look at  this . The  artificial general intelligence  is even more interesting and ambitious so you might consider participating in  this challenge . We also open new course in February:"
"10 Machine Learning, Artificial Intelligence Advances in 2017","0 Machine Learning, Artificial Intelligence Advances in 201","2017 was a big year for Artificial Intelligence (AI) and Machine Learning (ML), and although some of it was pure hype, a lot of what was achieved will be significant as the field continues to grow and have an impact in every industry. Below I summarize what I consider the main achievements or events of the year, and briefly explain why they‚Äôre important. The list includes technical achievements as well as commercial ones because in the end, the real impact of technological transformation happens when technology reaches the masses. While some of them are preliminary in nature and most were not realized from scratch in 2017, they do plant important seeds for the future. I hope this list is helpful in identifying what was most important in 2017- there were many other advances, of course. But, in thinking about the future, how might each of these affect you, your business, and society in the future?"
Learning Artificial Intelligence ‚Äî Formal Education or Online Self-learning ‚Äî Ph.D. or MOOC,earning Artificial Intelligence ‚Äî Formal Education or Online Self-learning ‚Äî Ph.D. or MOO,"This isn‚Äôt a time to relax and think what should you learn next? Build skills around what‚Äôs one of the most significant technologies of the coming decade ‚ÄìArtificial Intelligence. Despite of recent growth in interest, AI is a skill possessed by relatively few people. Many of the roles, needed skills and business titles of the future are unknown to us. Talent is no longer same as it used be five years before. To prepare yourself for a future-ready career you will require new skills. This isn‚Äôt about some distant future of work we are talking here ‚Äî change is already happening, and is accelerating. Competition for the right talent is fierce. Lifelong learning will become increasingly more important for the future workforce. Google CEO  says  that AI is ‚Äòmore profound than electricity or fire‚Äô. A recent  PwC report   claims  that Artificial intelligence (AI) can revolutionize the productivity and GDP potential of the global economy and total economic impact of AI in the period to 2030 will be up to $15.7 trillion Artificial Intelligence is an enormous field. It consists of multiple disciplines and a variety of tools and platforms. There is no limit to what these AI techniques can be applied across numerous domains and industries. AI is changing the world and you need to be a part of this global transformation. AI can be applied to a lot of problems and different markets. In fact, it should be viewed of as a fundamentally new approach to every problem. Now let‚Äôs examine what are your career objectives and interests. Do you crave to join research or academia and contribute or you prefer to build some great product/feature and launch it within next 18 months? If you can set a routine to learn every day ‚Äî for months, and enjoy making your hands dirty playing with various AI tools, then you can explore many online learning platform. If you choose to immerse yourself into complex and interesting problems for a long long time, meet highly technical creative individuals and help shape the development of artificial intelligence, then you should consider Ph.D. in Artificial Intelligence To accomplish any form of learning, recognize your areas of strength and weakness, identify what environment suits your learning and what distracts you. Once done, you will be prepared to meet the answer to your most important career question ‚Äî Formal AI education or online-learning? A Ph.D in Artificial Intelligence or MOOC The rapid growth of online education means that employees can re-skill themselves faster than ever before, and mostly for free. MOOC (massive open online course) is an economical alternative to formal education, can be accessed by anybody anywhere as long as they have an internet connection and a computer. To start, you could go for short introductory videos on YouTube and Udemy and later jump straight to Coursera. Start with beginners, then intermediate, afterward moving on to the expert level courses. Make sure you complete all projects and assessments. After completing learning you need to build a few small projects, and gradually you will figure it out what you like most, what you can directly relate to your area of expertise. Economic flexibility:  You start with the free trial, pay monthly or based on course specialization or based your learning goal. Some MOOC platforms offer financial aid too. Time flexibility:  You can pick up your own timelines. If you have burst of time you can spend days and weeks learning AI, later take a break or just learn only on weekend/holidays. Remote flexibility : Since you will be learning these course from remotely on your laptop, so it doesn‚Äôt matter where you are located. You could be at home, work and on holiday, Whenever you got 30 minutes spare attend a lecture or watch next video Your way : Research and knowledge creation is something you all do all the time daily based on your routine. Discipline and persistence are the key issues. Self-regulating the course progress and setting up learning targets isn‚Äôt something we are habitual of. It is troublesome to adjust the course trajectory with interactive participations from various different skill sets and cultures. Available learning objectives might not appeal to all participants fairly. With no individual student attention, some students can feel disconnected and unengaged in the course A Ph.D. from a top institution can never be ‚Äúinsignificant‚Äù. An atmosphere full of intellectually stimulating work with competent colleagues, working on positively help shaping the development of artificial intelligence. Other than just AI functional subjects, a Ph.D. also prepare you for various other skills like communicating complex concepts, scientific publication, teaching and mentoring If you are already enjoying your career in a specific industry and that aren‚Äôt pure research based, going on to get a PhD isn‚Äôt an advantage and apparently isn‚Äôt worth the time and cost. With so much hype around the field of artificial intelligence it is surprisingly easy to get it wrong. Real-world global problems, life-long learning, keeping yourself up-to-date with new research, widening your expertise to other realms ‚Äî if this excites you then choose wisely on your Phd. decision and explore various institutions, their syllabus, research type availability ‚Äî based on your budget, region and long-term career objectives. Today most of research is published in the open, creating opportunities for the outside students to establish the same level of expertise. With increasing number of quality MOOC specialized courses and certifications it is quicker to enter into industry and produce some real great work in the field of Artificial Intelligence ‚Äî all without any formal education in Artificial Intelligence Every Ph.D. is laborious, no matter what is the field. Just by doing Ph.D. you might not get a chance to create a significant direct real-world impact. You won‚Äôt either get super-rich doing AI research. You will be spending 4 to 6 years, great money and could be away from your home. It could be rough on mental health and could develop into a difficult psychological struggle. Most institutions have industry partners and affiliates that you could explore for career advancements. A lot depends on  a)  Quality and nature of your PhD thesis  b)  Your letters of recommendation,  c)  Your publications and  d)  Your capability to ‚Äúsell‚Äù your work A key step towards preparing individuals for the economy of the future is providing quality education and work opportunities. But unlike the past, things have changed ‚Äî and if you plan a fixed future career goals and work towards it then you continue to get nothing. You need to flip through your mental rolodex, understand the future impact of disruptive technologies, analyze ever-evolving scenarios and build confidence in the willingness to innovate. Organizations won‚Äôt protect jobs that are made redundant by technology automation, nobody can. Some companies recognize the responsibility towards their employee and nurture them for flexibility and re-skilling. Do not decline any such opportunity to learn future-ready skills. It won‚Äôt be pleasant because there is a lot of uncertainty about the future with people losing job due to automation. This article barely touches the technological view of Artificial Intelligence, but the most best known attempts to understand AI are not about technology as AI is more than coding, tools, and algorithms. Machine Learning, Deep Learning, Cognitive Computing, Neural Networks, Computer Vision, Natural Language Processing (NLP), and Language Translation ‚Äî there is no limit to what these AI techniques can be used across multiple domains and industries."
How To Reinvent Your Career By Learning Artificial Intelligence?,ow To Reinvent Your Career By Learning Artificial Intelligence,"Employees at all stages of their careers are challenged by the technological and socio-economical changes that are limiting the suitability of these employee‚Äôs current skills and learning. Widening gap between the skills available and skills in demand is certainly alarming and you should not overlook a timely career advice. To brace yourself for a future-ready career you will require advanced technical training or specialized education. Dynamic re-skilling and learning on-the-go are keys to be successful in the competitive job market. Everybody is talking about Artificial Intelligence. AI is drifting the status quo and this world is not yet ready to comprehend all the implications that it will manifest. AI ecosystem is remarkably close to reaching an inflection point as it is exponentially growing and amplifying the impact on all other relevant technologies. Your ability to anticipate and prepare for the future skills will be most critical for your success Life-long learning  ability and dynamically updating your skills will prepare you to take advantage of unseen opportunities and pave the path for the personal vision for success. You need to go beyond your academic education, skills, and experience. While designing your career strategy ‚Äî start with self-awareness, have a craftsman mindset, and seek out for short-term and long-term skills that need to be adopted or enhanced. Before you chose a career transition or a new career path you should be well aware of what type of work you would like to do. Your fitment for chosen career option is extremely critical as you will be responsible for success and progression. There are many job profiles and specializations for an Artificial Intelligence engineer. Following are few examples of these profiles:- In a traditional career path education, skills, and experience and job roles: should be aligned as much as possible, but a career in Artificial Intelligence can be realized in many constructs and in virtually all industries. Medical, military, art, research, manufacturing, marketing, finance, and transportation ‚Äî all these industry sectors need Artificial Intelligence skills. Another area of massive opportunities includes ethics, philosophy, policy making, and civic planning. Real-world problems,  life-long learning , keeping yourself up-to-date with new research, expanding your expertise to other domains ‚Äî if this excites you then you are already on the path to preparing yourself for the economy of the future. Technologies like Artificial Intelligence, Machine Learning, and Cognitive Computing are easy to learn but if you can‚Äôt set a routine to learn every day for months and don‚Äôt want to get your hands‚Äô dirty playing with various AI tools ‚Äî then you should choose formal education path. If you can plan and persistently invest a good chunk of time then take various courses from Coursera, Edx, Udacity; they can provide you enough understanding of Artificial Intelligence. Make sure you do all projects and assessments. After completing learning you need to build a few small projects, and slowly you will figure it out what you like most, what you can directly relate to your area of expertise. Artificial Intelligence is a vast field, it includes multiple disciplines and a variety of tools and platforms. Any of my attempt to structure how once should learn AI would just attract more debate. My alert readers and AI professionals would recognize that AI is more than programming languages, tools, and algorithms. Machine Learning, Deep Learning, Cognitive Computing, Neural Networks, Computer Vision, Natural Language Processing (NLP), and Language Translation ‚Äî there is no limit to what these AI techniques can be used across multiple domains and industries. The rapid growth of online education means that employees can re-skill themselves faster than ever before, and mostly for free. Learning Artificial Intelligence using various self-learning platforms allows you the flexibility of economy, learning on your own timelines and opportunities to deal with real-world issues, and early industry experience. If you are already in a specific industry and want to build more relevant experience in the same industry then online learning is a great help. You could revamp your career by slowly and steadily gaining AI skills from various MooC (Massive Open Online Course) platforms like Coursera, Edx, Udacity. If you are at the beginning of your career and crave to immerse yourself in deep and interesting problems ‚Äî formal education from the top institution will provide right launching platform. You can spend time with highly technical and creative people. Your formal AI study (Ph.D.) and can contribute to the development of Artificial Intelligence. Like every Ph.D. it would be harsh for mental health and a psychological struggle for a long time. A lot will depend on the quality of your publications and thesis, letters of recommendations, and how effectively you can sell yourself. Keep in mind that Ph.D. won‚Äôt make you super rich and if you can‚Äôt handle intellectually challenging research work without much feedback ‚Äî Ph.D. in Artificial Intelligence is not for you. Most of the industry jobs are not pure research-based and spending 4 to 6 years to get a Ph.D. isn‚Äôt valuable enough Artificial Intelligence should be used to identify domains more likely to benefit from the technology innovations. In the years to come, there will be more technological building blocks for artificial intelligence. With more data and more people interested in solving more problems, you should be able to imagine how artificial intelligence can supercharge your learning and career. We can use various languages like any of the Python, R, but the bigger story is how we apply Artificial Intelligence to solve a given problem determines your success. you need to have a deep appreciation for solving the problem that you have. You can‚Äôt just learn code and solve those problems. You need to be a thinker and craftsman with a lot of imagination. The only way to understand Artificial Intelligence is to solve some problem quickly, learn it better and improve. If you are into managed service business you should be able to think of various ways to automate, extend and improve existing processes and methods. You could come up with ideas that cross-pollinate multiple domains. From end-users, service agents to applications, service delivery process at every level can be automated using Artificial Intelligence and Cognitive Computing. Robotic process automation (RPA) can be used to execute repetitive tasks that were earlier performed by humans. These digital labor algorithms can become very complex and sometimes capable of learning and adjusting on its own. I believe that a successful machine learning algorithm doesn‚Äôt have to be perfect it just needs to be better than human. I started into Machine Learning with Andrew Ng‚Äôs Machine Learning course on  Coursera with absolutely no experience with AI, algorithms or programming. This course was created by Stanford University taught by  Andrew Ng  who was formerly head of Baidu AI Group/Google Brain. I was overwhelmed and highly discouraged. Later I re-started with small online courses on Udemy, YouTube, and Lynda. They were helpful as their curriculum was mostly code-less. Later I dived into algorithms using Python and R without a proper understanding of coding, and then back to Andrew Ng‚Äôs Machine Learning course and  completed that . It took almost another year to learn specific machine learning stacks like Azure ML, TensorFlow, AWS Lex, AWS Polly, AWS Rekognition and AWS SageMaker The first machine learning algorithm I wrote was kind of funny, it was able to predict whether our  housemaid is coming to work or not  (at least it was predicting better than me and my wife). It involved a lot of learning and hacking. I also had to deal with  data ethics issue  that was quite unexpected. From  sorting cucumbers to curing cancers , machine learning algorithms are highly prominent in our daily lives. Current efficiencies of AI is so great that the greatest minds on this planet are  predicting  that in near future AI will triumph over humanity and will present a direct threat to human survival. If your career derailment risk factor is increasing it is not because you lack talent, experience or education. These technological advances are posing unprecedented and fundamental challenges ‚Äìcausing disruption in the deep relationship between employment growth and productivity growth, called  The Great Decoupling . futuremonger.com medium.com One of the major themes of this article is to inspire you to take charge of your future career. If you look around and talk to people it seems as if a day cannot go by without some new warning saying that Artificial Intelligence is  taking away our jobs . These changes are coming fast and we need to face the truth. You have this opportunity to re-invent your career by learning Artificial Intelligence. It will be challenging but at the same time highly rewarding for your future career"
"Learning Data Analytics is not tough, Starting is Tough!","earning Data Analytics is not tough, Starting is Tough","People often ask me ‚Äú How can I learn Data Analytics? ‚Äù and I often stumble upon this question ‚Äò How to become a Data Analyst ‚Äù on Quora too. The answer is pretty much clearly available all over the internet. The actual issue is not how to become a data analyst but it is if we are ready to become one? This post aims to take a newbie into the world of Data Analytics with a simple freely available public data and R (the open-source champ of Data Science). Data collection: The Data that‚Äôs used here is  the public leaderboard  of a recent  Analytics Vidya Hackathon . Just copy the table data and paste it in MS Excel and save as ‚Äò av-hackathon.csv ‚Äô, your data for analysis is ready. Reading Input Data: Let‚Äôs read the csv into R Studio using  read.csv . mind_lb <- read.csv(‚Äòav-hackathon.csv‚Äô,header = T,stringsAsFactors = F) Basic Summary: Understanding the dataset is the primary operation any analyst should perform. We can use  str()  or  summary()  to explore the basic summary of our dataset and to see sample values. We could clearly see that there are 1037  observations  ( rows / entries ) and 3 variables /c olumns  and their  data types  ‚Äî two of  numeric  (of which one is just a serial number and the other is Score) and name which is of  character  type. Data Analysis: When we scroll through our dataset (the read input file), we could see some users explicitly having their email id as user name. Can we try to see how many such users have ‚Äò@‚Äô in their user name? Let‚Äôs use  grepl  (regular expression)  to match the names containing @ symbol. Using  grepl(‚Äò@‚Äô,mind_lb$Name)  would return us just TRUE/FALSE against each observation but what we actually need is count. so let‚Äôs use table() function in R to find it out. table(grepl(‚Äò@‚Äô,mind_lb$Name))  returns us the actual count (absolute figures) of usernames with @ and without @. But wouldn‚Äôt it‚Äôd be better to represent in terms of percentage? prop.table() along with table() which takes values from  grepl(‚Äò@‚Äô,mind_lb$Name) gives us the values in decimals which in turn multiplied with 100 gives us the actual percentage of usernames with @ and without @. Now we know that there‚Äôs almost 6.5% usernames with @, obviously  gmail  must be contributing the most part of it, but can we try to find if there‚Äôs something else apart from  gmail ? Let‚Äôs perform some  logical operation . We want all usernames   with @   but  not gmail  rewriting them to understand the logic:  @   AND   NOT gmail  Voila! Here‚Äôs the code for it: mind_lb$Name[grepl(‚Äò@‚Äô,mind_lb$Name) & !grepl(‚Äògmail‚Äô,mind_lb$Name)] which results in identifying us usernames with Some good insight isn‚Äôt it? Someone from HP Enterprise, IIT-BHU and IIM-A. Let‚Äôs try some more string operations. Usernames would be unique so we can‚Äôt really find out repetition in those names. But can we find which alphabet forms the most starting letter? Let‚Äôs create a new variable ‚Äòf_letter‚Äô for this purpose. It‚Äôs just a  substring  operation. mind_lb$f_letter <- tolower(substr(mind_lb$Name,0,1)) Data Visualization: Let‚Äôs make a simple graph out of it to see which letter wins. library(ggplot2) qplot (data = mind_lb, f_letter) It seems ‚Äòs‚Äô is the winner and ‚Äòa‚Äô is the runner. Well that‚Äôs for fun.  But can we make some better looking visualization? Let‚Äôs draw the hackathon score distribution. The complete code can be found here on my  github . Final Notes: Doesn‚Äôt it seem easier to find out some valuable insights in a dataset? Data Analytics in fact easier. All you need is an open mind to see through the data and the tool and syntax you select would come handy once you start. This is not a tutorial post but just to show a glimpse of the easiness of R and Data Analytics. Are you ready to dive into the world of Data Analytics? If so,  download R  and  R Studio  and start today. Also create a  github account  and share your code and visualization and comment the link here. Hacker Noon  is how hackers start their afternoons. We‚Äôre a part of the  @AMI  family. We are now  accepting submissions  and happy to  discuss advertising & sponsorship  opportunities. If you enjoyed this story, we recommend reading our  latest tech stories  and  trending tech stories . Until next time, don‚Äôt take the realities of the world for granted!"
"Completely change your data analytics strategy with Deep Learning ‚Äî Artificial Intelligence, Data Science, Machine learning","ompletely change your data analytics strategy with Deep Learning ‚Äî Artificial Intelligence, Data Science, Machine learnin","A lot has happened since the last episode. I left corporate industry and joined  Abe  as Chief Data Officer. Abe is a startup from Florida building artificial intelligence solutions for personal finance, supporting both banks and people with their money, via a conversational and natural experience through the many channels they are used to, such as Facebook messenger, mobile app, SMS, but also smart speakers like Google home, and Amazon Alexa. I am recording this episode from South Africa, exactly in Cape Town, as we are currently participating to the Barclays accelerator powered by Techstars, putting our AI expertise to the test in the complex financial environments of corporations like Barclays and their affiliates. I am personally having a lot of fun, tackling really challenging problems in financial analytics and advanced machine learning on one side. But also more AI related problems typically in the field of natural language processing or NLP. Which is exactly what I planned for this episode. Over the past few years, neural networks have re-emerged as powerful machine-learning models, reaching state-of-the-art results in several fields like image recognition and speech processing. More recently, neural network models started to be applied also to textual data in order to deal with natural language, also with promising results. For a long time, core NLP techniques have been dominated by machine-learning approaches that are rather based on statistics and function optimisation. I am referring to linear models such as support vector machines (SVM) or regression, trained over high dimensional data and sparse feature vectors. In contrast, neural networks are nonlinear objects and since there are not so many linearities in complex languages like those spoken or written by humans, this explains why neural networks are showing such promising results. With this said, a lot needs to be done to ‚Äî let‚Äôs say ‚Äî crack the problem of natural language modeling, but these are definitely exciting times for AI. Every time I mention neural network modeling, the most common question I get is about the number of architectures currently in the literature and the fragmentation that so many architectures are creating. I‚Äôd like to clarify this point. Despite the number of different neural network architectures, there are basically two major families of models:  Feed-forward networks  and  Recurrent/Recursive networks Both networks can contain convolutional and pooling layers. These are matrix operations that make the network robust specifically against scaling, transforming, resizing, and therefore being more appropriate for images and pixels. It turns out that convolutional and pooling architectures are showing promising results also on characters, words and tasks such as document classification. As a matter of fact all of the networks are basically classifiers, of course each one with with different strengths and properties that make them more suitable for text, sound, or mixed data. Without any doubt, the most important finding with deep learning for NLP has been the concept of  word embedding . So far words have been represented in very high dimensional spaces, like  one-hot encoding  for instance or  bag-of-words , generating very sparse feature vectors and definitely increasing the dimensionality of the problem, leading to computational complexity and shortage of memory. The key idea behind the neural approach is that each word in a vocabulary can be represented with a dense vector of real numbers, so that  similar  words are forced to have similar vectors. This is usually done by training the network on large textual datasets in a skip-gram fashion in which the network is trained to predict each word given a number of previous words (usually referred to as  the context ). This can be done just by scanning or reading digital books millions of times, and after some hours of computation, words that are similar will have similar numeric vectors. Of course word similarity is hard to define and is usually task-dependent. What does similar mean? Are two words similar because they have similar characters, or similar length, or similar meaning? The most common hypothesis in this case is the  distributional hypothesis  : words are similar if they appear in similar contexts. No matter what the meaning of similarity is, word embedding reduces the dimensionality of the problem and the memory footprint of any neural model, especially those with many layers, also called  deep networks . With this said, it is important to mention the fact that neural networks can also fail in many different ways and diverse scenarios Here are probably the most common causes of failure of deep learning While deep learning is a great technique to learn structures from data, understanding its limitations may lead to better algorithms and/or better theoretical guarantees. A paper that better explores the ways deep learning can fail is ‚ÄúFailures of Deep Learning‚Äù on  arxiv As a matter of fact, data is expensive. Crawling data and pre-training deep learning models with less related but public datasets can be effective in some cases. Academic datasets are usually perfectly balanced. In the real world, however, datasets are messy, unbalanced and incomplete. As an example, the MNIST image dataset contains an equal number of samples per digit which is hardly the case in real world image classification problems; in finance, very few individuals are at risk of overdraft with respect to the entire population, and fortunately very few of them perform fraudolent activities of money laundering and illegal credit card payments. Still artificial intelligence models should be capable of learning from those very few examples and profiles. Scenarios like the one described do not change in domains like healthcare, where fortunately very few individuals are affected by disorders or diseases with respect to the rest of the population. This in turn might give a hard time to a deep learning medical image classifier. A consequence of the challenging problems of the type just described, is represented by models that make mistakes in the form of false positives (or false alarms) and false negatives (missed signals). In finance this might mean missing fraudulent activities or stopping payments for no reason. Both the cases are expensive for a bank and definitely annoying for the end user. These are the real problems that researchers have to deal with complex models like deep learning. My take home message however is about being aware that complexity of fancy models can be detrimental, and not the silver bullet that many claim. Even in the realm of deep learning,  simplicity is the ultimate sophistication . Originally published at  https://amethix.com  on August 22, 2018."
Does the Banking Industry Need Digital Voice Assistants?,oes the Banking Industry Need Digital Voice Assistants,"Based on the combination of  Machine Learning, Artificial Intelligence , Data analytics and cloud computing, the  digital voice-enabled assistant  may not sound like a new phrase to you. You might have heard of a personal assistant helping you organize and manage your daily life routines. Here the concept of voice-based services extends far beyond just personalized assistance to touch the banking sector at large. People have been using finance apps for personal banking for the long time since it allows them to perform banking tasks on their mobile devices without personal interactions. But many of us never knew that digital voice banking is getting prepared to grow invisible and creep into consumer‚Äôs daily life. So the new ripple in a tech-focused market is the compelling question: Does banking needs digital voice assistant? Here is the answer: Remember the time when we used to rely on IVR customer services and support to resolve issues related to the computer problem or mobile services. It was surprising how we used to interact with our mobile devices and type on the glass screen to communicate our concerns. However, everything wasn‚Äôt really that smooth and functional every time the approach to customer service is made. The massive issue was that most IVR systems didn‚Äôt offer quick problem learning or problem-solving abilities. They didn‚Äôt have learning capabilities that today‚Äôs virtual voice assistant develops over time. Moreover, typing was not practically the most effective way to interact with a device or a computer for most common problems. It wasn‚Äôt that long ago that we had to invest a lot of efforts to fulfil the minor routines. The world without the voice-driven AI services would seem slow, inefficient and strenuous now that we can think of adopting enlightened virtual assistants. They enable us to take informed decisions and solve an issue in a matter of seconds. Imagine being informed of next big step you need to take to make your finance management even better, can you? Also, have you ever thought that you will be presented with an intelligent conversation where all your questions are answered before you utter anything? Now you can. With the help of  voice-powered digital banking chatbots , all your interactions are read, processed and understood properly so that you will receive most comprehensive solutions from the bank or business your approach. Based on your needs, preferences, transaction patterns and behaviours, the AI-enabled voice assistant for android can guide and lead you to where exactly you should go saving you time-consuming repeated steps down the road. E-commerce business apps have already started showing most relevant and refined options to their regular visitors based on their past journey. The UI of such apps are nowadays designed to revolve around user experience, giving it more personalized shape. The same principle applies to voice banking with slightly different logic. As technology improves, basics of commerce communication are replaced with more effective, intelligent and powerful tools that involve your own voice. Voice-first finance system enables people to recognize tons of opportunities to deal with other most essential aspects of life where their efforts are expected. A touch of  artificial intelligence  not only paves the way for personalized and intuitive user experience, but it also champions enlightened digital banking where voice-based services offer intelligent assistance. Voice recognition system enhanced with text-to-speech voices creates a great amount of convenience for bank customers, leading them to whole new voice-first banking. The process is further given a boost by intrusion of  artificial intelligence  that helps decode human emotion and intents through its self-learning abilities. Nuance was the first pioneer to introduce such system though with limited audio and text capabilities. Apple‚Äôs Siri became the first technology to support this interactive algorithm with more mature potential. However, the success of  voice assistant  software has now led to fundamentally changing the way customers interact with financial organizations and bank commerce. The reason these technologies are largely integrated into the banking system is that it offers unbelievable convenience and luxury while executing functions like digital transactions, payments, loan process and deposits without having to engage in personal interactions. A new trend of using freshly invented technology has emerged to become more than a trend now. Looking at how speedily people connect sharply with riveting innovations, we can say all consumer sectors, including finance market, are about to be touched by a positive impact technology brings. This is what Bob O‚ÄôDonnell, the founder and chief analyst of Technalysis has to say. He states that when people accept new innovations and become accustomed to speaking to their personal devices, the use of intelligent voice assistant will transform the way we interact with technology. He is even confident that digital aid in finance will affect the choices people make regarding purchasing devices, applications and services. This surely goes to say that things are about to shift and change. The rise of demand for more advanced digital assistance has inspired great revolutions and  developments in the world of voice-driven technology . Lets us briefly get a glimpse of them. In early months of 2017,  American Express  made an announcement saying that they will integrate Amazon‚Äôs Echo in order to enable customers to check their balance, view offers, make transactions and much more. According to  eMarketer , nearly 35.6 million Americans will rely on a voice-activated digital assistant device like Google Home or Amazon Echo once a month. Nearly 60 million smartphone users will adopt virtual voice assistant software like Alexa, Siri, Cortana and Google voice in their devices for quick voice-enabled services. When it comes to  voice banking ,  Garanti Bank  and  Satander  are two financial service providers who have started offering voice-activated banking services now. Top Fintech companies bury themselves developing smart APIs that integrate cleverly with voice assistants with a bank system. At the time of the launch of Amazon‚Äôs Alexa, many tech analysts formed the commonplace opinion that Alexa was just the new talk of the town. However, things came out differently. When Bank of America‚Äôs introduced  Erica  in 2016, the technology hit another milestone.  Forbes  depicted how Erica of Bank of America can change the usual interactions that happen between a customer and their bank. With the help of the Artificial Intelligence used by Bank of America, they could see the millions of customers using their mobile banking app to interact with their bank account and perform transactions. Using its smart  machine learning  inputs, Bank of America had a clear idea about how to program the algorithm of Erica for offering voice banking aid. Erica has much more to offer than what a person at first look would gather. Closely observing the customers transactions and analysing their financial pattern, Erica not only makes statements about account balance like a typical ATM machine but it also keeps the customers updated about their credit score, debt management, savings, future investment plans etc. When a customer interacts with a bank, he leaves behind a footprint of financial data that Erica over time reads and digests to offer useful advice to the customer. It will scan user‚Äôs financial state and favour them by offering real-time opportunities to control and optimize their spending. From this, it is clear that Erica has potential to take banking  chatbots  to next level and cater sophisticated financial services to customers who are highly privileged. Erica gets integrated into the mobile apps used by bank customers. Being a proactive voice assistant, Erica can monitor customers‚Äô vital banking functions and recommend taking fair steps to improve their market reputation and low credit score. She can even surprise you by telling you to reduce interest on your existing credit card by making somewhat higher payment on an outstanding balance. It was not that long ago that people used to foster the misconception of how machines may not give as much of a competitive advantage, profitability or interactive finesse as human contacts. A few years back the possibility of having an AI-powered voice assistant was a wild dream. However, Erica of Bank of America showed has debunked the myth. What Erica is programmed to do can erase the gap that existed between a human and a machine. Erica‚Äôs  machine learning  system aligns perfectly with digital advancements made to improve banking experience. It is meant to revolutionize the way banking is approached as it peeks a little deeper into the financial aspect (which is perhaps the most crucial of all) of our lives. The voice  chatbot  of Erica synchronizes all banking decisions and information of a customer to enable a different level of banking commerce and redefine personal banking using digital technology. Brian Roemmele, the founder at Payfinders.com states, ‚ÄúThe more a personal assistant knows about a consumer and daily life patterns, the better it can interact with millions of financial (and non-financial) options at any given moment.‚Äù This statement can be extended to mean a lot in terms of the impact of having personal banking assistant in real life. It is true that having an enlightened voice assistant who is listening to your activities 24/7 is a boon to your finance-related decisions. In today‚Äôs world, when people are busy organizing every aspect of their lives, they have little time left to manage and focus on their daily finance. Hence, it is really nice to have someone who recognizes your voice, follows you closely and understands your life like no other. At any given moment, all you need to do is to ask your digital voice-enabled finance guide to initiate intelligent communication that involves questions and solutions. Moreover, the vision of sophisticated banking necessarily involves the idea of proactive banking where a person is reminded of their accurate financial status and practicable recommendations to diminish the risk factor. This is the most critical question thousands of tech enthusiasts and finance experts are asking. Definitely, it is undeniable that the rise of  Fintech startup companies  and their development promises have fortified the vision of entrenching the invisible voice banking infrastructure in the world. Despite achieving considerable success, the question is still there: Will traditional banking completely disappear? We can say there is a possibility of everything in today‚Äôs world. KMPG  report says that certain major components of conventional banking may disappear and will be replaced by virtual voice assistant called  Eva . Most banks may think of digitalizing their customer call centres, branches, sales teams, financial advisers, marketers, etc. Data will be the hero in the whole digital  AI  setting and so will be their generous technology partnerships. The discussion on whether or not banking needs digital voice assistant can conclude with many possibilities, one of which is ‚Äòprobably yes‚Äô. This is because it is impossible to avert from the shift that leading tech giants have promised in the banking sector through voice-enabled devices, AI-driven technology and evolved data processing science. The invisible banking bot system is still at its thoughtful stage with some of its components becoming true as  Artificial Intelligence  is combined with voice. Despite the digital progress, the banking sector is still not perfectly prepared to dive into absolute  voice-enabled AI-driven services . The complete transformation is perhaps still a few years away."
5 Reasons why you should consider learning Data Analytics, Reasons why you should consider learning Data Analytic,"In today‚Äôs data driven world where 90% of the world‚Äôs data has been created in the last two years alone, Analytics isn‚Äôt just the way of the future, it‚Äôs the way of now. Academy of Real and Meta Analytics ( ARMA ) has launched a training platform to produce competent Data Analysts that provide data driven solutions to companies. The consistent demand for data driven decisions in companies and competent professionals that can interpret and make decisions based on valuable data has grown tremendously over the past few years. Data Analytics has become a crucial skillset globally and in Nigeria, as it helps in improving business, decision making driven by data and providing the biggest competitive advantage over competitors. This applies for organizations as well as professionals in every sector. For professionals, who are skilled in Data Analytics, there is a sea of opportunities out there. Here are 5 reasons why you should consider learning Data Analytics. 1. Increasing Demand for Data Analysts One of the top job recruitment websites in the world, Indeed.com has provided a research graph that shows Data Analytics demand growing steadily in direct proportion to the number of job opportunities available globally. The demand cuts across various industries that touch on Data analytics, but to specify, there are two major talent deficits in the market, which are Data Scientists that perform analytics and Analytics Consultants that understand and use data to make decisions. The demand for these positions is constantly growing as competent professionals within this space are becoming increasingly rare. 2. Salary Aspects Increasing demand for Data Analytics skills and consultants is resulting in an upward rise in wages for competent professionals and making Data pay big bucks for the right skill. This phenomenon is being seen globally where countries like Canada, Australia and the U.K are witnessing this upward swing in demand for Data Analysts. According to the Skills and Salary Survey Report published by the Institute of Analytics Professionals of Australia (IAPA), the annual average salary for data analysts is  $130,000 i.e. N46, 800, 000 , up 4% from last year. 3. Data Analytics: A Key Factor in Decision Making Business analytics is giving managers the opportunity to understand the different dynamics of their business, anticipate market shifts and mitigate risks. Rather than relying on ‚Äúgut feelings‚Äù when making top decisions such as; pricing, inventory managements, marketing, talent acquisition and countless other decisions, companies are embracing data analytics and statistical reasoning to make decisions that improve efficiency, risk management and profits. Data driven organizations not only make better strategic decisions, but also enjoy high operational efficiency, improved customer satisfaction, and robust profit and revenue levels. 4. Data Analytics to Boost Customer Acquisition and Retention A business success is significantly dependent on its ability to acquire customers and retain them. There is no business that can establish a claim to success without first having a very strong customer base. Although, even with a strong customer base, a company can‚Äôt afford to disregard the competition. Once a business is slow in picking up what customers are interested in and looking for, then that leads to poor quality service offerings, and poor products in the end that have no value to the customer. Data analytics allows companies to analyze and observe different customer search and buying patterns and trends. Paying attention to this customer behavior provides opportunities to trigger loyalty. The more data a business collates, the more patterns and behaviors that business will identify with its customers. Today, because of all the data available through technology, it‚Äôs very easy to understand the modern customers, and strategies can be born from the massive data available that will enable business deliver products and services that fulfil needs of the customer and deliver products and services that your customers really want. 5. Data Analytics is Used Everywhere It is a given that there is a huge demand for Data Analytics owing to its awesome features. The tremendous growth is also due to the varied domain across which Analytics is being utilized. Some of the industries where Data analysts work include: Healthcare, retail, banking, manufacturing, public transportation, cyber security and so much more. Whether your goal is to get a full-time job in a new industry, advance in your existing career, or work for yourself in the data analytics field, ARMA can prepare you for the opportunity. You are guided for top-paid jobs in your dream company locally and internationally, grab opportunities to get associated with corporate clients ARMA are in partnership with, and have a prolific resume prepared to take your career to the next level. Be sure about upgrading your technical talents as the training courses are well equipped with latest tools and up-to-date curriculum. With this digital learning dais, you will experience quick, simple, and incredibly excellent ways to employment. Visit this  link  to know more about ARMA courses and find  Data Science Courses ."
"Big Data, Data Warehouses, Business Intelligence, Data Analytics Intern","ig Data, Data Warehouses, Business Intelligence, Data Analytics Inter","Want to learn more about data warehousing, data analytics, and cloud computing (AWS, Google, Azure)? Sonra Intelligence is looking for a computer science intern to join our growing team based in Dublin 7 Grangegorman DIT campus Starting asap The position is full time. Working remotely is an option. Duration: 3‚Äì6 months Allowance: 450 Euros Research, and understand big data cloud technologies (AWS, Google Cloud etc.), e.g. Snowflake, Spark, Azure SQL Warehouse, Teradata, AWS Redshift, Google BigQuery Implement and develop small pilot and proof of concept use cases Develop small features of Flexter, our ETL tool for semi-structured data Document your findings Good understanding of data and data warehouse architecture Good understanding of Cloud Computing SQL Relational databases One of the following programming languages: Python, Java, Scala Student of computer science or related field Good English language Communication Skills Strong analytical and problem solving skills Explore and evaluate new ideas and technologies. Demonstrated ability to work independently on complex exploratory project Strong affinity for teamwork and empowering fellow developers. Learn from the #1 data experts Work with latest technologies in high demand by employers Improve your programming, problem-solving and communication skills Flexible work arrangement: in our central office in Dublin or remotely Paid internship Brand new offices at Grangegorman DIT campus Gym, canteen, basketball courts etc. all on campus Experience startup culture first hand Submit CV 20 minute general interview Test (requires research and can be done remotely) 30 minute technical interview If you are interested  contact us. Originally published at  sonra.io  on February 18, 2019."
Mutrack is hiring a machine learning / data analytics engineer,utrack is hiring a machine learning / data analytics enginee,"‡∏ú‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡∏ä‡∏ä‡∏µ‡∏ô ‡πÄ‡∏•‡∏¥‡∏£‡πå‡∏ô‡∏ô‡∏¥‡πà‡∏á, ‡πÄ‡∏≠‡πÑ‡∏≠, ‡∏ö‡∏¥‡πä‡∏Å ‡πÄ‡∏î‡∏ï‡πâ‡∏≤, ‡πÄ‡∏î‡∏ï‡πâ‡∏≤ ‡∏≠‡∏∞‡∏ô‡∏≤‡πÑ‡∏•‡∏ï‡∏¥‡∏Å‡∏™‡πå, ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÄ‡∏ä‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏û‡∏π‡∏î‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢ ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏û‡∏π‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏î‡∏π‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏ï‡πà‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ó‡πå‡∏à‡∏£‡∏¥‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏¢‡∏±‡∏á‡∏´‡πà‡∏≤‡∏á‡πÑ‡∏Å‡∏•‡∏à‡∏≤‡∏Å‡∏à‡∏¥‡∏ô‡∏ï‡∏ô‡∏≤‡∏Å‡∏≤‡∏£ ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ü‡∏∏‡πâ‡∏á‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚Ä¶ ‡∏ú‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ù‡∏±‡∏ô‡∏≠‡∏¢‡∏≤‡∏Å‡∏´‡∏¢‡∏¥‡∏ö‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏´‡πá‡∏ô‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ó‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÅ‡∏ï‡πà‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏û‡∏π‡∏î ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏≠‡∏∏‡∏î‡∏°‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ù‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏£‡∏≤‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÅ‡∏ô‡∏ß‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ô‡∏±‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏à‡∏ö, ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏ô‡∏¥‡∏û‡∏ô‡∏ò‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏ß‡∏î‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏£‡∏≤‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡πâ‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡πÉ‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡πÉ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ó‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥) ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏•‡πâ‡∏≤‡∏´‡∏≤‡∏ç‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏ß‡∏° ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡∏ó‡∏µ‡πà‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏ï‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå ‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ , ‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô, ‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏∑‡πâ‡∏≠‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏µ‡πÇ‡∏°‡∏ï‡∏ó‡∏µ‡∏°‡πÉ‡∏ô‡∏î‡∏µ‡πÄ‡∏≠‡πá‡∏ô‡πÄ‡∏≠‡∏ô‡∏±‡πà‡∏ô‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏£‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏ã‡∏∂‡πà‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏á‡∏≤‡∏ô ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏ï‡πà‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢ ‡∏¢‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡πÅ‡∏õ‡∏•‡∏Å‡πÉ‡∏´‡∏°‡πà ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏ß‡∏õ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡∏ö‡∏≤‡∏¢‡πÅ‡∏≠‡∏û, ‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå, ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏£‡∏Å‡∏Å‡∏¥‡πâ‡∏á‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏≤‡∏£‡πå‡πÄ‡∏≠‡∏ü‡πÑ‡∏≠‡∏î‡∏µ (RFID), ‡∏ö‡∏•‡∏π‡∏ó‡∏π‡∏ò (Bluetooth), ‡∏≠‡∏≤‡∏£‡πå‡∏ó‡∏µ‡πÅ‡∏≠‡∏•‡πÄ‡∏≠‡∏™ (RTLS), ‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå (Sensor), ‡πÅ‡∏•‡∏∞‡πÑ‡∏≠‡πÇ‡∏≠‡∏ó‡∏µ (IoT) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏±‡∏ô‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏•‡∏Å‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÜ‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏™‡πÅ‡∏™‡∏£‡πâ‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ ‡∏ô‡∏±‡πà‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏≠‡∏µ‡∏Å ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚Äú‡πÅ‡∏°‡∏ä‡∏ä‡∏µ‡∏ô ‡πÄ‡∏•‡∏¥‡∏£‡πå‡∏ô‡∏ô‡∏¥‡πà‡∏á / ‡πÄ‡∏î‡∏ï‡πâ‡∏≤ ‡∏≠‡∏∞‡∏ô‡∏≤‡πÑ‡∏•‡∏ï‡∏¥‡∏Å‡∏™‡πå‚Äù ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏≤ ‚Ä¶ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏•‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏°‡∏ä‡∏ä‡∏µ‡∏ô ‡πÄ‡∏•‡∏¥‡∏£‡πå‡∏ô‡∏ô‡∏¥‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏∞‡∏™‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡πÅ‡∏°‡∏ä‡∏ä‡∏µ‡∏ô ‡πÄ‡∏•‡∏¥‡∏£‡πå‡∏ô‡∏ô‡∏¥‡πà‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏° ‡∏õ.‡∏•. ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÑ‡∏°‡πà‡∏´‡∏£‡∏≠‡∏Å ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ‡πÄ‡∏•‡∏¥‡∏®‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏≤‡∏à‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏´‡∏ô ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÉ‡∏´‡∏°‡πà‡πÜ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡πÜ ‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏°‡πÅ‡∏•‡∏∞‡∏ã‡∏µ‡∏ó‡∏µ‡πÇ‡∏≠ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏°‡πÅ‡∏•‡∏∞‡∏ã‡∏µ‡∏ó‡∏µ‡πÇ‡∏≠‡∏Ç‡∏≠‡∏á‡∏ú‡∏°‡∏£‡∏π‡πâ (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏±‡∏ö‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ô‡∏µ‡πâ) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏ô‡∏±‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤  ‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∏‡∏ì ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏£‡∏¥‡πà‡∏ô‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏µ‡πÇ‡∏°‡∏ï‡∏ó‡∏µ‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏î‡∏π‡πÅ‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏£‡∏≤‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏π‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏ß‡∏° ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏à‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤ ‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏°‡∏≤‡∏ó‡∏î‡πÅ‡∏ó‡∏ô‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏°‡∏ä‡∏ä‡∏µ‡∏ô ‡πÄ‡∏•‡∏¥‡∏£‡πå‡∏ô‡∏ô‡∏¥‡πà‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏ï‡πâ‡∏≤ ‡∏≠‡∏∞‡∏ô‡∏≤‡πÑ‡∏•‡∏ï‡∏¥‡∏Å‡∏™‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ö‡∏à‡∏≤‡∏ß‡πà‡∏≤ (Java), ‡∏™‡πÄ‡∏Å‡∏•‡πà‡∏≤ (Scala), ‡πÑ‡∏û‡∏ò‡πà‡∏≠‡∏ô, (Python), ‡∏≠‡∏≤‡∏£‡πå (R), ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏î‡∏ï‡πâ‡∏≤‡πÄ‡∏ö‡∏™‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πÇ‡∏≠‡πÇ‡∏ü‡∏£‡πå‡πÄ‡∏à (Neo4j), ‡πÄ‡∏≠‡πá‡∏°‡∏Ñ‡∏¥‡∏ß‡∏ó‡∏µ‡∏ó‡∏µ (MQTT), ‡∏Ñ‡∏≤‡∏ü‡∏Ñ‡πà‡∏≤ (Apache Kafka) ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏°‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏™‡∏ô‡∏∏‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ô‡πÑ‡∏õ ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ú‡∏ä‡∏¥‡∏ç‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏û‡∏•‡πá‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ú‡∏π‡πâ‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ï‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡πÉ‡∏à‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ‡∏°‡∏µ‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà 7 ‡∏Ñ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢ 6 ‡∏Ñ‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© 1 ‡∏Ñ‡∏ô‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏¢‡∏¥‡∏ô‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏≠‡∏∞‡πÑ‡∏£‡∏°‡∏≤‡∏Å‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≠‡∏á‡∏°‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡πÑ‡∏õ‡∏ï‡∏•‡∏≠‡∏î‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï ‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®‡∏Ç‡∏≠‡∏á‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ ‡∏≠‡∏¢‡∏π‡πà‡∏¢‡πà‡∏≤‡∏ô‡∏≠‡πÇ‡∏®‡∏Å (‡∏ã‡∏≠‡∏¢‡∏™‡∏∏‡∏Ç‡∏∏‡∏°‡∏ß‡∏¥‡∏ó 23) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏°‡∏≤‡∏™‡∏∞‡∏î‡∏ß‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ã‡∏µ‡∏ó‡∏µ‡πÇ‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡∏™‡∏ß‡∏¥‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏ô‡∏î‡πå ‡πÄ‡∏ß‡∏µ‡∏¢‡∏î‡∏ô‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏±‡∏°‡∏û‡∏π‡∏ä‡∏≤ ‡∏Å‡πá‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏à‡∏≠‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ö‡πâ‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü‡πÇ‡∏î‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡πÅ‡∏•‡∏Ñ ‡πÄ‡∏ó‡∏£‡∏•‡πÇ‡∏•‡πà ‡πÄ‡∏ö‡∏™‡πÅ‡∏Ñ‡∏°‡∏õ‡πå ( ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÑ‡∏•‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ü‡∏™‡∏ö‡∏∏‡πä‡∏Ñ ) ‡πÅ‡∏•‡∏∞‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô ‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡∏≠‡∏±‡∏û‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ï‡∏±‡∏ß ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏ô‡∏•‡∏∞ 8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô) ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡∏£‡∏≤‡∏ö‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡πÑ‡∏ß‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏£‡∏π‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ñ‡∏¢‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏î‡πâ‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå‡πÅ‡∏ï‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏¥‡∏ß‡πÅ‡∏ó‡∏£‡∏Ñ ‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡∏≠‡∏±‡∏û‡πÑ‡∏ó‡∏¢ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÄ‡∏£‡∏≤‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ó‡πå‡πÉ‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‚Äú‡πÅ‡∏ó‡∏£‡∏Å‡∏Å‡∏¥‡πâ‡∏á‡πÅ‡∏≠‡∏ô‡∏î‡πå‡∏≠‡∏∞‡∏ô‡∏≤‡πÑ‡∏•‡∏ï‡∏¥‡∏Å‡∏™‡πå‡πÅ‡∏û‡∏•‡πá‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‚Äù ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå, ‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå, ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏£‡∏Å‡∏Å‡∏¥‡πâ‡∏á‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏≤‡∏£‡πå‡πÄ‡∏≠‡∏ü‡πÑ‡∏≠‡∏î‡∏µ (RFID), ‡∏ö‡∏•‡∏π‡∏ó‡∏π‡∏ò (Bluetooth), ‡∏≠‡∏≤‡∏£‡πå‡∏ó‡∏µ‡πÅ‡∏≠‡∏•‡πÄ‡∏≠‡∏™ (RTLS), ‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå (Sensor), ‡πÅ‡∏•‡∏∞‡πÑ‡∏≠‡πÇ‡∏≠‡∏ó‡∏µ (IoT) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏´‡∏•‡πà‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ï‡∏±‡∏ß ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ê‡∏≤‡∏ô‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏Æ‡∏•‡∏ó‡πå‡πÅ‡∏Ñ‡∏£‡πå‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô ‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÅ‡∏≠‡∏û‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡πà‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£ ‡∏à‡∏∞‡∏ú‡∏¥‡∏î‡∏°‡∏±‡πâ‡∏¢‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≠‡∏ô‡πÄ‡∏£‡∏≤‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ö‡∏≠‡∏Å‡∏≠‡∏∞‡πÑ‡∏£‡∏•‡∏∂‡∏Å‡πÜ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡∏î‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏°‡∏≤‡∏™‡∏±‡πâ‡∏ô‡πÜ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡πå‡∏°‡∏≤‡∏ó‡∏µ‡πà  hiring@mutrack.co ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡πÑ‡∏õ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏±‡∏ö‡∏°‡∏±‡∏ô‡∏™‡∏±‡∏Å‡∏ô‡∏¥‡∏î ‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ü‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡πÅ‡∏ô‡∏ö‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ‡∏°‡∏±‡∏ô‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÇ‡∏•‡∏Å ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏£‡πà‡∏≥‡∏£‡∏ß‡∏¢‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≠‡∏á‡∏´‡∏≤‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß ‡πÄ‡∏£‡∏≤‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 8 ‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢‡πÉ‡∏à‡∏à‡∏î‡∏à‡πà‡∏≠ ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏ó‡∏µ‡πà‡∏™‡∏î‡πÉ‡∏™‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö"
"The 2023 MAD (Machine Learning, Artificial Intelligence & Data) Landscape","he 2023 MAD (Machine Learning, Artificial Intelligence & Data) Landscap","It has been less than 18 months since we published our last MAD landscape, and it has been full of drama. When we left, the data world was booming in the wake of the gigantic Snowflake IPO, with a whole ecosystem of startups organizing around it. Since then, of course, public markets crashed, a recessionary economy appeared and VC funding dried up. A whole generation of data/AI startups has had to adapt to a new reality. Meanwhile, the last few months saw the unmistakable, exponential acceleration of Generative AI, with arguably the formation of a new mini-bubble. Beyond technological progress, it feels that AI has gone mainstream, with a broad group of non-technical people around the world now getting to experience its power firsthand. The rise of data, ML and AI is one of the most fundamental trends in our generation. Its importance goes well beyond the purely technical, with a deep impact on society, politics, geopolitics and ethics. Yet it is a complicated, technical, and rapidly evolving world that is often confusing even for practitioners in the space. There‚Äôs a jungle of acronyms, technologies, products and companies out there that are hard to keep track of, let alone master: The annual MAD (Machine Learning, Artificial Intelligence and Data) landscape is our attempt at making sense of this vibrant space. Its general philosophy, much like our event series  Data Driven NYC , has been to open source work that we would do anyway, and start a conversation with the community. So, here we are again, in 2023. This is our ninth annual landscape and ‚Äústate of the union‚Äù of the data and AI ecosystem. Here are the prior versions:  2012 ,  2014 ,  2016 ,  2017 ,  2018 , 2019 ( Part I  and  Part II ),  2020  and  2021 . This annual state of the union post is organized in four parts: After much research and effort, we are proud to present the 2023 version of the MAD landscape. When I say ‚Äúwe‚Äù, I mean a little group, whose nights will be haunted for months to come by memories of moving tiny logos in and out of crowded little boxes on a PDF:  Katie Mills ,  Kevin Zhang  and  Paolo Campos . Immense thanks to them. And yes, I meant it when I told them at the onset ‚Äúoh, it‚Äôs a light project, maybe a day or two, it‚Äôll be fun, please sign here‚Äù. So, here it is (cue in drum roll, smoke machine). The MAD landscape comes in two modes of consumption this year: PDF (static) version : <<<<<<<<  CLICK HERE FOR PDF VERSION  >>>>>>>> (yes, it‚Äôs all very high resolution, and you can easily zoom on both desktop and mobile) <New!> Interactive version : In addition, this year for the first time, we are jumping head first into what the youngsters call the ‚ÄúWorld Wide Web‚Äù, with a fully interactive version of the MAD Landscape that should make it fun to explore the various categories. <<<<<<<<  CLICK HERE FOR THE INTERACTIVE VERSION  >>>>>>>> Notes on the interactive version: For all questions and comments, please email  MAD2023@firstmarkcap.com General approach First, we‚Äôve made the decision this year again to  keep both data infrastructure and ML/AI on the same landscape . One could argue that those two worlds are increasingly distinct. However, we continue to believe that there is an essential symbiotic relationship between those areas. Data feeds ML/AI models. The distinction between a data engineer and a machine learning engineer is often pretty fluid. Enterprises need to have a solid data infrastructure in place in order before properly leveraging ML/AI. The landscape is built more or less on the same structure as every annual landscape since  our first version in 2012 . The loose  logic is to follow the flow of data, from left to right  ‚Äî from storing and processing to analyzing to feeding ML/AI models and building user-facing, AI-driven or data-driven applications. This year again, we‚Äôve kept a separate ‚Äúopen source‚Äù section. It‚Äôs always been a bit of an awkward organization as we effectively separate commercial companies from the open source project they‚Äôre often the main sponsor of. But equally, we want to capture the reality that for one open source project (for example, Kafka), you have many commercial companies and/or distributions (for Kafka ‚Äî Confluent, Amazon, Aiven, etc.). Also, some open source projects appearing in the box are not fully commercial companies yet. The vast majority of the organizations appearing on the MAD landscape are unique companies, with a very large number of VC-backed startups. A number of others are products (such as products offered by cloud vendors) or open source projects. Company selection This year, we have a total of  1,416 logos  appearing on the landscape. For comparison, there were  139 in our first version in 2012 . Each year we say we can‚Äôt possibly fit more companies on the landscape and each year, somehow, we need to. This comes with the territory of covering one of the most explosive areas of technology. However, this year in particular, we‚Äôve had to take a  more editorial, opinionated approach  to deciding which companies make it to the landscape. Despite the surging number of companies in the category, we‚Äôre long past the stage where we can fit nearly everyone, so we have had to make choices. In prior years, we tended to give disproportionate representation to growth-stage companies, based on funding stage (typically Series B-C or later) and ARR (when available), in addition to all the large incumbents. However this year, particularly given the explosion of brand new areas like Generative AI where most companies are 1 or 2 years old, we‚Äôve made the editorial decision to feature  many more very young startups  on the landscape. A couple of disclaimers: Categorization One of the harder parts of the process is categorization ‚Äî in particular, what to do when a company‚Äôs product offering straddles two or more areas. It‚Äôs becoming a more salient issue every year, as many startups progressively expand their offering, a trend we discuss in ‚ÄúPart III ‚Äî Data Infrastructure‚Äù. Equally, it would be just untenable to put every startup in multiple boxes in this already overcrowded landscape. Therefore, our general approach has been to categorize a company based on its core offering, or what it‚Äôs mostly known for. As a result, startups generally appear in only one box, even if they do more than just one thing. We make exceptions for the cloud hyperscalers (many AWS, Azure and GCP products across the various boxes), as well as some public companies (e.g. Datadog) or very large private companies (e.g., Databricks). What‚Äôs new this year Main changes in ‚ÄúInfrastructure‚Äù: Main changes in ‚ÄúAnalytics‚Äù: Main changes in ‚ÄúMachine Learning / Artificial Intelligence‚Äù: Main changes in ‚ÄúApplications‚Äù: Other noteworthy changes: We considerably expanded our ‚ÄúData Services‚Äù category and rebranded it  ‚ÄúData & AI Consulting‚Äù , to reflect the growing importance of consulting services to help customers facing a complex ecosystem, as well as the fact that some pure-play consulting shops are starting to reach early scale. READ NEXT :  MAD 2023, PART II: FINANCINGS, M&A AND IPOs"
Machine Learning ‚Äî Data Visualization,achine Learning ‚Äî Data Visualizatio,"Machine learning  is mainly about Data and Algorithms. Good data is the key ingredient in the success of ML algorithms. One of the key steps in data processing is the  data visualization . The good news is that, for learning, there are tons of datasets available online. Some of the online datasets are available on  this  GitHub. And, of-course,  Kaggle  has some top quality datasets available as well. For the data visualization part, I, want to use a dataset which I can relate to and help me to connect with the observations. I look no further than my mobile phone which has all the data. You can download your personal data from applications like Facebook, Twitter, Google etc. One such application ‚ÄúSamsung Health‚Äù provides me an easy way to download and explore the data. Open Samsung Health Application -> Settings-> Download Personal Data The extracted data can be seen below: After downloading the data, I would like to see how I have been doing with my fitness. I start with the exercise data. We will use well known python libraries like  pandas   for data analysis ,  matplotlib  for data visualization . Let us start with the python programming. Importing the libraries. To open the csv file, we use pandas. If we open and check the exercise file, it has an extra row at the top which we want to skip. Now we have the  exercise_df  as the pandas D ataFrame . Let us see the different columns it contains: We see many columns in the table, capturing every detail of the exercise. For more details and information about each column, please refer  here . There are many csv files extracted from the application having health data related to heart-rate, food intake, weight management etc. All the details about data types can be found from the Samsung Health developer  API . Let us continue with the exercise data analysis and see what the data looks like. Since there are many columns , for ease of visualization, a few are chosen for analysis like distance, duration, calorie, exercise_type, rest are ignored. Let us see how the data looks like  df.head(5) : Here, we see  exercise_type  column has some numbers, we would like to know the exact name of the corresponding exercise. Also,  duration  is in milliseconds,  distance  in meters. We refer to the developer site to get the details of the  exercise_type   here . Change the duration into minutes.  df['duration] = df['duration]/60000  The top 5 rows after the change  df.head(5) The table looks better now. I would like to know, what all exercises I do?  df['exercise_type'].value_counts() I am mostly walking and a few runs in between. Surprised to see Circuit Training. We will like to plot bar chart of the data, with the % of each  exercise_type . Pie charts are another good visualization plots. Using matplotlib  pie  API, let us show % of each  exercise_type on the plot. Since, we have seen from the above data ‚ÄòArm extension‚Äô and ‚ÄòCricket‚Äô is only single instance, thus very small % to represent. To ignore these from the plot, we can use lambda function , [lambda x: x >5] . We only show if the exercise count is > 5. Looks cool. Neatly labelled with % shown in the plot. For more variations in the pie plots, please refer  here . So far so good.  How about my daily calories spent in walking? How am I doing my daily walks?  To see my daily walking pattern, convert the  start_time  to weekdays. Hmm. So Friday is the best day for my walking. Looks like on weekends , I , am a bit lazy. Interesting!!! How about calories spent in different exercises?  Pandas  groupby  comes to the rescue. So grouping by  exercise_type , allows us to plot mean calories consumed by each exercise. What about finding my mean calories spent across all  exercise_type  for each day?  We need to group by multiple columns  groupby(['day_of the week, 'exercise_type]) Wow!!! Sunday is the best day for my running. It was very interesting for me to visualize my own data and getting insights from the various plots. One thing is clear: I need to be better with my exercises. Maybe go for cycling, gym etc. My goals are set now. Thanks for reading this article, recommend and share if you like it."
Business Intelligence-Data Visualization: Tableau,usiness Intelligence-Data Visualization: Tablea,"Spark, Bigdata, NoSQL, Hadoop are some of the most using and top in charts technologies that we frequently use in Knoldus, when these terms used than one thing comes into picture is ‚ÄòHuge Data, millions/billions of records‚Äô Knoldus developers use these terms frequently, managing (and managing means here- storing data, rectifying data, normalizing it, cleaning it and much more) such amount of data is really not at all an easy task. But user do no understand what they are talking about they just need to know the real essence of whole matter/data/story/facts. From here the term ‚Äòvisualization‚Äô comes into picture, so Data Visualization/Intelligence is as important & vast as handling it. Data visualization brings Business Intelligence Tools for accomplishing visualization goals and the market of BI tools is really huge, there are number of tools with different features, pricing, capabilities etc., if we start comparing them than there is no limit but there are some major Tools that do everything for you and your selection in search of best BI tool is totally based on your business requirements. Somehow as our title appended Tableau so we will talk about it here. Tableau  is groundbreaking data visualization software created by Tableau Software. Tableau connects easily to nearly any data source, be it corporate Data Warehouse, Microsoft Excel or web-based data. Tableau allows for instantaneous insight by transforming data into visually appealing, interactive visualizations called dashboards. This process takes only seconds or minutes rather than months or years, and is achieved through the use of an easy to use drag-and-drop interface. Flavors  of Tableau: Tableau is really huge, so they divided it into four flavors as per users needs. Tableau Desktop:  All your development is done here. From creating reports, charts, formatting them, putting them together as a dashboard all the work is done on Tableau Desktop. Tableau Desktop comes in three editions: Public, Personal, and Professional. All of them allow you to save your workbook to the Tableau Public platform. Tableau Desktop (Public Edition) is limited to  15,000,000  rows of data per workbook and ten gigabytes (10 GB) of storage space for your workbooks (aka. vizzes). Tableau Server:  The dashboards you create are shared with other users using Tableau Server. When you upload(publish) a Dashboard to Tableau Server from Tableau Desktop, other users can access those Dashboards by logging on Tableau Server. Share everyone with a web browser. Tableau Server is highly secured for visualizing data. The Price of Tableau Server depend upon core licensing. Log in credentials are required. Users can be restricted. Tableau Reader:  is a free desktop application that you can use to open, view and interact with visualizations that have already been created in  Tableau  Desktop. ‚Ä¶ When they are done, they save it as a  Tableau  Packaged Workbook, or ‚Äú.twbx‚Äù file. They share that file with you. Want to know when to use Tableau Reader:  more Tableau Public :  It runs on cloud, Tableau Public is a free service that lets anyone publish interactive data visualizations to the web. Visualizations that have been published to Tableau Public (‚Äúvizzes‚Äù) can be embedded into web-pages and blogs, they can be shared via social media or email, and they can be made available for download to other users. Visualizations are created in the accompanying app Tableau Desktop Public Edition (or another Edition of Tableau Desktop) ‚Äî no programming skills are required. Be sure to look at the gallery to see some of the visualizations people in the community have been creating. For more FAQs read  here . Tableau desktop work is shared through tableau server & online. Although there are limited desktop capabilities available in server/online version, Tableau is planning to provide full desktop authoring capabilities in coming years for server/online users! But a good news to students as Tableau started  free version  for them. Cool reading, isn‚Äôt it? so we got to know about Tableau story though that is not so easy but it is not done here, our practical example(s) are still to go. Stay tuned."
Learning data visualization differently,earning data visualization differentl,"I find that the key strength of  Medium  as a writing platform is its simplicity. Medium simplifies writing to such a point that it forces writers to focus on the content rather than font size and style. I was going to write about data visualization but wanted to make the point that there is a value in simplicity, it reduces barriers to entry and improves quality of contents. A typical data visualization represents three things: Among all these, the most value from visualization actually comes from information ‚Äî e.g. what the data tell us about the relationship between two or more variables ‚Äî whereas, aesthetics and style add relatively little. If we could quantify, a hypothetical equation might look something like this: Total value (100) = Information (80) + Aesthetics (10) + Style (10) Despite this, a disproportionate amount of emphasis is given to aesthetics and styling in data visualization, and to the point that people often forget the real purpose of it. I am not saying aesthetics is not important, what I am saying is, beginners and intermediate learners often get confused about what they are actually doing and why. If we want to lower barriers to entry in data science, this is an area that needs some attention. Okay, enough philosophy, let‚Äôs get back to work! A visual representation of data serves two objectives: a) it shows how a single variable is distributed with respect to its central tendency and dispersion; b) how one variable changes with respect to one or more other variables in the feature space. Let‚Äôs do some examples with the Python seaborn  library. Of course, we are keeping aesthetics & styling at bay and focus on the key purpose ‚Äî information. We‚Äôll import some familiar libraries: The  seaborn  package comes with built-in datasets, I‚Äôm going to use one of them‚Äî the ‚Äútips‚Äù dataset ‚Äî for our demonstration. One reason for choosing this dataset is that it has all three datatypes I looking for‚Äî continuous (e.g. total bill, tips), categorical (e.g. sex, day of the week), & count/integer (e.g. visitor size) variables. Let‚Äôs load that in and call a couple of basic functions to check out what the dataset is about. The key variable of interest here is the amount of tip people give during their restaurant visits. Other associated variables of interest are total bills paid, sex of the visitor and day of the week the visit took place etc. To a data scientist, the overarching question is whether the amount of tip is distributed at random or has any association with sex, day of the week and total bills paid. So here is the plan for the remainder of this article: As I just said, visualization of a SINGLE variable tells us how it is distributed with respect to the central tendency and dispersion (e.g. mean, median, quartiles etc.). We‚Äôll pick the variable  total_bill  and see its distribution using different visualization techniques. In a way, all these techniques present similar kinds of information, with just some variation in presentation. I‚Äôm not going to explain too much, as these figures are self-explanatory, it should be quite easy for readers to examine codes and compare similarities and differences between the plots. In a two-dimensional space, it‚Äôs easy to visualize two variables that people can easily comprehend. These two variables can be any combination of continuous, categorical and integer data types. Scatterplot and linear models are best represented with continuous variables and that‚Äôs what the first two figures below are about. Boxplot, violin plot, swarm plot and bar plots are great for presenting categorical variables with continuous and integer data types. A scatterplot shows how one variable is distributed with respect to another variable and is the simplest form of bivariate plots that immediately reveal valuable information that is also visible to ‚Äúnacked eyes‚Äù. Linear model formalizes the statistical relationship between two variables presented in a scatterplot with associated confidence intervals. Boxplots can compare categorical variables and their distribution on a continuous variable. Here I‚Äôve disaggregated tip amount by the time of restaurant visits, and as you can tell, people are more generous during dinner time compared to lunch visits. The variation is also higher during dinner visits with much more outliers values. Boxplot above is great, but swarm plots can show each data point and their distribution. You lose some statistical information as in boxplots but you gain something else! So they can be complementary during exploratory data analysis (EDA). Violin plot smooths out ‚Äúrough edges‚Äù of swarm plot but in this case, you cannot see the individual data points. However, you may notice that there is a boxplot inside each of the violin plots ‚Äî which certainly adds value. Barplot is an outdated visualization technique with little information, but I‚Äôm including it here anyway so you can compare the differences with other plots at a glance. In two-dimensional plots, you really don‚Äôt have that many options to visualize multi-dimensional values. Multivariate plots are nothing but a representation of one or more additional variables in bivariate plots. So basically, multivariate plots take 2D plots one step further by adding variables via parameters such as hue, style, bubble size etc. Although, in theory, you can represent more variables, but it can quickly become crowded and difficult to read as we will see shortly. Let‚Äôs examine some plots below. In the scatterplot, we can separate different categories (e.g. male and female) with different shades of color (called the ‚Äúhue‚Äù parameter). So here we have 3 variables: tip, total_bill and sex represented. On top of the above plot, we could add another variable separated by the ‚Äústyle‚Äù parameter. Now we have 4 variables: tip, total_bill, sex, visit time. Great, but how are you able to track & interpret all that information in a single 2D plot? We can get even crazier and add a ‚Äúsize‚Äù parameter for yet another variable. We can go on, but at this point you must be lost if you are trying to interpret what this plot is all about! The point I am trying to make is this ‚Äî just because you can, you shouldn't try to accomplish too many things in a single visualization. Beyond a certain point it becomes junk! As I said in the beginning ‚Äî simplicity matters. But there are legitimate ways to show multiple variables and for that, we can go back to our boxplot, violin plot and swarm plot. They work perfectly in a 2D plot to represent multiple variables. I could go on but you get the idea. I touched upon a few different things, so I felt I need to summarise them. First, simplicity matters; it puts less pressure on the audience and forces the creators to explain complex information in a digestible way. Second, when you are learning visualization, you should first focus on how to communicate information using visualization as a medium. Styles and aesthetics will come naturally later on but that‚Äôs the cherry on top. It‚Äôs also easy to get lost when your one-liner code becomes a complex one. And finally, adding more variables in the plot is possible, but may not be desirable, as the figure quickly becomes crowded with too much information that is difficult to interpret. The purpose of data visualization is communication, so the simpler the better. I hope this was a useful post. If you have comments feel free to write them down below. You can follow me on  Medium ,  Twitter  or  LinkedIn . Happy plotting!"
Machine Learning Data Visualization,achine Learning Data Visualizatio,"Have you ever struggled while finding insights from various graphs? Then this article is for you. If you are a Data Scientist or Data Analyst you may have plotted lots of graphs for studying data but now you can do the same in just a single plot. Machine learning data visualization is important to understand how data is used in a particular machine learning model it helps in analyzing it. Facets is an open-source python library that can be used to visualize, analyze the data easily without much effort. Facets consist of two different visualizations for understanding the data we have Facets Overview and for analyzing the data we have Facets Dive. In this article, we will explore both of these visualizations. Let‚Äôs get started‚Ä¶ We will start by installing a Facets library by using pip. The command given below will do that. In this step, we will import the required libraries for loading the dataset and visualizing it. Fo this article, we will use the famous Diabetes dataset which can be downloaded from online sources. For creating the visualizations Facets use a predefined HTML template. In this step, we will load the frontend and create the visualizations. Here, you can clearly visualize data on a different axis. Facets create a graphical user interface as you can see here, we can select different columns that we want to visualize, and axis respectively. We can use this visualization to understand the data and also analyze the association between different features of the data. Go ahead try this with different datasets and create different visualizations using Facets. In case you find any difficulty please let me know in the response section. Check out different Machine Learning Projects that you can work on in the link below. www.dezyre.com This article is in collaboration with  Piyush Ingale . Thanks  for reading! If you want to get in touch with me, feel free to reach me at hmix13@gmail.com or my  LinkedIn Profile . You can view my  Github   profile for different data science projects and packages tutorials. Also, feel free to explore  my profile  and read different articles I have written related to Data Science."
Open Machine Learning Course. Topic 2. ,pen Machine Learning Course. Topic 2.,"In the field of Machine Learning,  data visualization  is not just making fancy graphics for reports; it is used extensively in day-to-day work for all phases of a project. To start with, visual exploration of data is the first thing one tends to do when dealing with a new task. We do preliminary checks and analysis using graphics and tables to summarize the data and leave out the less important details. It is much more convenient for us, humans, to grasp the main points this way than by reading many lines of raw data. It is amazing how much insight can be gained from seemingly simple charts created with available visualization tools. Next, when we analyze the performance of a model or report results, we also often use charts and images. Sometimes, for interpreting a complex model, we need to project high-dimensional spaces onto more visually intelligible 2D or 3D figures. All in all, visualization is a relatively fast way to learn something new about your data. Thus, it is vital to learn its most useful techniques and make them part of your everyday ML toolbox. In this article, we are going to get hands-on experience with visual exploration of data using popular libraries such as  pandas ,  matplotlib  and  seaborn . The following material is better viewed as a  Jupyter notebook  and can be reproduced locally with Jupyter if you clone the  course repository . Before we get to the data, let‚Äôs initialize our environment: In the first article, we looked at the data on customer churn for a telecom operator. We will load again that dataset into a  DataFrame : To get acquainted with our data, let‚Äôs look at the first 5 entries using  head() : Here is the description of our features: The last data column,  Churn , is our target variable. It is binary:  True  indicates that that the company eventually lost this customer, and  False  indicates that the customer was retained. Later, we will build models that predict this variable based on the remaining features. This is why we call it a  target . Univariate  analysis looks at one variable at a time. When we analyze a feature independently, we are usually mostly interested in the  distribution of its values  and ignore the other variables in the dataset. Below, we will consider different statistical types of variables and the corresponding tools for their individual visual analysis. Quantitative features  take on ordered numerical values. Those values can be  discrete , like integers, or  continuous , like real numbers, and usually express a count or a measurement. Histograms and density plots The easiest way to take a look at the distribution of a numerical variable is to plot its  histogram  using the  DataFrame 's method  hist() . A histogram groups values into  bins  of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type of it, most often Gaussian. In the above plot, we see that the variable  Total day minutes  is normally distributed, while  Total intl calls  is prominently skewed right (its tail is longer on the right). There is also another, often clearer, way to grasp the distribution:  density plots  or, more formally,  Kernel Density Plots . They can be considered a  smoothed  version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let‚Äôs create density plots for the same two variables: It is also possible to plot a distribution of observations with  seaborn 's  distplot() . For example, let's look at the distribution of  Total day minutes . By default, the plot displays both the histogram with the  kernel density estimate  (KDE) on top. The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin. Box plot Another useful type of visualization is a  box plot .  seaborn  does a great job here: Let‚Äôs see how to interpret a box plot. Its components are a  box  (obviously, this is why it is called a  box plot ), the so-called  whiskers , and a number of individual points ( outliers ). The box by itself illustrates the interquartile spread of the distribution; its length is determined by the 25th (Q1) and 75th (Q3) percentiles. The vertical line inside the box marks the median (50%) of the distribution. The whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval (Q1‚àí1.5‚ãÖIQR, Q3+1.5‚ãÖIQR), where IQR=Q3‚àíQ1 is the  interquartile range . Outliers that fall out of the range bounded by the whiskers are plotted individually as black points along the central axis. We can see that a large number of international calls is quite rare in our data. Violin plot The last type of distribution plots that we will consider is a  violin plot . Look at the figures below. On the left, we see the already familiar box plot. To the right, there is a  violin plot  with the kernel density estimate on both sides. The difference between the box and violin plots is that the former illustrates certain statistics concerning  individual examples  in a dataset while the violin plot concentrates more on the smoothed  distribution as a whole . In our case, the violin plot does not contribute any additional information about the data as everything is clear from the box plot alone. describe() In addition to graphical tools, in order to get the exact numerical statistics of the distribution, we can use the method  describe()  of a  DataFrame : Its output is mostly self-explanatory.  25% ,  50%  and  75%  are the corresponding  percentiles . Categorical features  take on a fixed number of values. Each of these values assigns an observation to a corresponding group, known as a  category , which reflects some qualitative property of this example.  Binary  variables are an important special case of categorical variables when the number of possible values is exactly 2. If the values of a categorical variable are ordered, it is called  ordinal . Frequency table Let‚Äôs check the class balance in our dataset by looking at the distribution of the target variable: the  churn rate . First, we will get a frequency table, which shows how frequent each value of the categorical variable is. For this, we will use the  value_counts()  method: By default, the entries in the output are sorted from the most to the least frequently-occurring values. In our case, the data is not  balanced ; that is, our two target classes, loyal and disloyal customers, are not represented equally in the dataset. Only a small part of the clients canceled their subscription to the telecom service. As we will see in the following articles, this fact may imply some restrictions on measuring the classification performance, and, in the future, we may want to additionaly penalize our model errors in predicting the minority ‚ÄúChurn‚Äù class. Bar plot The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the  seaborn 's function  countplot() . There is another function in  seaborn  that is somewhat confusingly called  barplot()  and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature. Let‚Äôs plot the distributions for two categorical variables: While the histograms, discussed above, and bar plots may look similar, there are several differences between them: The left chart above vividly illustrates the imbalance in our target variable. The bar plot for  Customer service calls  on the right gives a hint that the majority of customers resolve their problems in maximum 2‚Äì3 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let‚Äôs move on to some more interesting and powerful visual techniques. Multivariate  plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed. We are going to start with the interaction between quantitative variables. Correlation matrix Let‚Äôs look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well. First, we will use the method  corr()  on a  DataFrame  that calculates the correlation between each pair of features. Then, we pass the resulting  correlation matrix  to  heatmap()  from  seaborn , which renders a color-coded matrix for the provided values: From the colored correlation matrix generated above, we can see that there are 4 variables such as  Total day charge  that have been calculated directly from the number of minutes spent on phone calls ( Total day minutes ). These are called  dependent  variables and can therefore be left out since they do not contribute any additional information. Let‚Äôs get rid of them: Scatter plot The  scatter plot  displays values of two numerical variables as  Cartesian coordinates  in 2D space. Scatter plots in 3D are also possible. Let‚Äôs try out the function  scatter()  from the  matplotlib  library: We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellpise-like shape is aligned with the axes. There is a slightly fancier option to create a scatter plot with the  seaborn  library: The function  jointplot()  plots two histograms that may be useful in some cases. Using the same function, we can also get a smoothed version of our bivariate distribution: This is basically a bivariate version of the  Kernel Density Plot  discussed earlier. Scatterplot matrix In some cases, we may want to plot a  scatterplot matrix  such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix. Sometimes, such visualization may help draw conclusions about data; but, in this case, everything is pretty clear with no surprises. In this section, we will make our simple quantitative plots a little more exciting. We will try to gain new insights for churn prediction from the interactions between the numerical and categorical features. More specifically, let‚Äôs see how the input variables are related to the target variable Churn. Previously, you learned about scatter plots. Additionally, their points can be color or size coded so that the values of a third categorical variable are also presented in the same figure. We can achieve this with the  scatter()  function seen above, but, let's try a new function called  lmplot()  and use the parameter  hue  to indicate our categorical feature of interest: It seems that our small proportion of disloyal customers lean towards the top-right corner; that is, such customers tend to spend more time on the phone during both day and night. But this is not absolutely clear, and we won‚Äôt make any definitive conclusions from this chart. Now, let‚Äôs create box plots to visualize the distribution statistics of the numerical variables in two disjoint groups: the loyal customers ( Churn=False ) and those who left ( Churn=True ). From this chart, we can see that the greatest discrepancy in distribution between the two groups is for three variables:  Total day minutes ,  Customer service calls , and  Number vmail messages . Later in this course, we will learn how to determine feature importance in classification using  Random Forest  or  Gradient Boosting ; there, we will see that the first two features are indeed very important for churn prediction. Let‚Äôs look at the distribution of day minutes spoken for the loyal and disloyal customers separately. We will create box and violin plots for  Total day minutes  grouped by the target variable. In this case, the violin plot does not contribute any additional information about our data as everything is clear from the box plot alone: disloyal customers tend to talk on the phone more. An interesting observation : on average, customers that discontinue their contracts are more active users of communication services. Perhaps they are unhappy with the tariffs, so a possible measure to prevent churn could be a reduction in call rates. The company will need to undertake additional economic analysis to find out whether such measures would be beneficial. When we want to analyze a quantitative variable in two categorical dimensions at once, there is a suitable function for this in the  seaborn  library called  factorplot() . For example, let's visualize the interaction between  Total day minutes  and two categorical variables in the same plot: From this, we could conclude that, starting with 4 calls,  Total day minutes  may no longer be the main factor for customer churn. Perhaps, in addition to our previous guess about the tariffs, there are customers that are dissatisfied with the service due to other problems, which might lead to fewer number of day minutes spent on calls. As we saw earlier in this article, the variable  Customer service calls  has few unique values and, thus, can be considered either numerical or ordinal. We have already seen its distribution with a  count plot . Now, we are interested in the relationship between this ordinal feature and the target variable  Churn . Let‚Äôs look at the distribution of the number of calls to the customer service, again using a  count plot . This time, let‚Äôs also pass the parameter  hue=Churn  that adds a categorical dimension to the plot: An observation : the churn rate increases significantly after 4 or more calls to the customer service. Now, let‚Äôs look at the relationship between  Churn  and the binary features,  International plan  and  Voice mail plan . An observation : when  International Plan  is enabled, the churn rate is much higher; the usage of the international plan by the customer is a strong feature. We do not observe the same effect with  Voice mail plan . Contingency table In addition to using graphical means for categorical analysis, there is a traditional tool from statistics: a  contingency table , also called a  cross tabulation . It represents multivariate frequency distribution of categorical variables in tabular form. In particular, it allows us to see the distribution of one variable conditional on the other by looking along a column or row. Let‚Äôs try to see how  Churn  is related to the categorical variable  State  by creating a cross tabulation: In the case of  State , the number of distinct values is rather high: 51. We see that there are only a few data points available for each individual state ‚Äî only 3 to 17 customers in each state abandoned the operator. Let‚Äôs ignore that for a second and calculate the churn rate for each state, sorting it from high to low: At first glance, it seems that the churn rate in  New Jersey  and  California  are above 25% and less than 6% for Hawaii and Alaska. However, these conclusions are based on too few examples, and our observation could be a mere property of our particular dataset. We can confirm this with the  Matthews  and  Cramer  correlation hypotheses, but this would be beyond the scope of this article. We have been looking at different  facets  of our dataset by guessing interesting features and selecting a small number of them at a time for visualization. We have only delt with two to three variables at once and were easily able to observe the structure and relationships in data. But, what if we want to display all the features and still be able to interpret the resulting visualization? We could use  hist()  or create a scatterplot matrix with  pairplot()  for the whole dataset to look at all of our features simultaneously. But, when the number of features is high enough, this kind of visual analysis quickly becomes slow and inefficient. Besides, we would still be analyzing our variables in a pairwise fashion, not all at once. Most real-world datasets have many features, sometimes, many thousands of them. Each of them can be considered as a dimension in the space of data points. Consequently, more often than not, we deal with high-dimensional datasets, where entire visualization is quite hard. To look at a dataset as a whole, we need to decrease the number of dimensions used in visualization without losing much information about data. This task is called  dimensionality reduction  and is an example of an  unsupervised learning  problem because we need to derive new, low-dimensional features from the data itself, without any supervised input. One of the well-known dimensionality reduction methods is  Principal Component Analysis  (PCA), which we will study later in this course. Its limitation is that it is a  linear  algorithm that implies certain restrictions on the data. There are also many non-linear methods, collectively called  Manifold Learning . One of the best-known of them is  t-SNE . Let‚Äôs create a  t-SNE  representation of the same churn data we have been using. The name of the method looks complex and a bit intimidating:  t-distributed Stohastic Neighbor Embedding . Its math is also impressive (we will not delve into it here, but, if you feel brave, here is the  original article  by Laurens van der Maaten and Geoffrey Hinton from  JMLR ). Its basic idea is simple: to find a projection for a high-dimensional feature space onto a plane (or a 3D hyperplane, but it is almost always 2D) such that those points that were far apart in the initial n-dimensional space will end up far apart on the plane. Those that were originally close would remain close to each other. Essentially,  neighbor embedding  is a search for a new and less-dimensional data representation that preserves neighborship of examples. Now, let‚Äôs do some practice. First, we need to import some additional classes: We will leave out the  State  and  Churn  features and convert the values ‚ÄúYes‚Äù/‚ÄùNo‚Äù of the binary features into numerical values using  pandas.Series.map() : We also need to normalize the data. For this, we will subtract the mean from each variable and divide it by its standard deviation. All of this can be done with  StandardScaler . Now, let‚Äôs build a t-SNE representation: and plot it: Let‚Äôs color this t-SNE representation according to the churn (green for loyal customers, and red for those who left). We can see that customers who churned are concentrated in a few areas of the lower dimensional feature space. To better understand the picture, we can also color it with the remaining binary features:  International Plan  and  Voicemail . The green dots here indicate the objects that are positive for the corresponding binary feature. Now it is clear that, for example, many dissatisfied customers who canceled their subscription are crowded together in the most south-west cluster that represents the people with the international plan but no voice mail. Finally, let‚Äôs note some disadvantages of t-SNE: Occasionally, using t-SNE, you can get a really good intuition for the data. The following is a good paper that shows an example of this for handwritten digits:  Visualizing MNIST . Sometimes t-SNE really helps to understand something, and sometimes you can just draw a Christmas tree toy :-) Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version:  Kaggle Kernel ,  nbviewer . Authors:  Egor Polusmak , and  Yury Kashnitskiy . Translated and edited by  Yuanyuan Pao ,  Christina Butsko ,  Anastasia Manokhina ,  Inga Kaydanova , and  Artem Trunov ."
How to Create a BI Dashboard Using a Pivot Table and a Charting Library,ow to Create a BI Dashboard Using a Pivot Table and a Charting Librar,"When thinking of  data analysis , probably the spreadsheet data is the first thing that comes to mind. But data analysis is more than just numbers and dry text. If approached correctly and combined with  visual components , analytics is able to completely change the way you‚Äôre doing business and bring it to a new level of development. No matter what industry you are working in, I‚Äôm deeply sure that a data visualization belongs to the top skills required in the labor market. That‚Äôs why today I want to describe how to maximize the potential of your presentation with the help of  pivoting  and  charting . Following these guidelines, in the end, you‚Äôll learn how to create an interactive  dashboard  with a pivot table and charts. In our case, interactiveness simply means that all the changes applied to the report will be reflected in the charts instantly. This data visualization technique helps to achieve an effective reporting and make informed decisions. Sounds intriguing? Let‚Äôs move on. Let me give you some tips on visualization: The whole process is quite simple and you  only need to know the basics of JavaScript to get started . I‚Äôll provide you with the code in the demo. If you want to get results right away, please check the CodePen demo at the end of the article. We‚Äôll use the following tools that provide data summarization and visualization capabilities: Let‚Äôs start! In this tutorial, we write a function that returns an array of JSON objects. Here‚Äôs an example of my marketing dataset: At this stage, I encourage you to choose your own data. I‚Äôve followed the instructions in a  Flexmonster Quick Start guide  that seemed to be beginner-friendly. The result ‚Äî the component is built into my project. Now it‚Äôs time to answer  what do you want to see on the grid and in the chart . In this report, I wanted to categorize sessions on my website by traffic and see how many sessions were over a certain period of time. Additionally, I wanted to check the percentage of time spent by users on the site. Let‚Äôs prepare the  slice for the report . Put the fields into columns, rows, measures and define an aggregation function for the measures. Firstly, add the  loader of Google Charts  by including its scripts into the webpage. Secondly, add the connector of Flexmonster: Add the following code snippet to load the packages and set a callback to know when charts are loaded: Now display the data from the grid in the chart by using  flexmonster.googlecharts.getData()  method. It performs the pre-processing of the data for the certain type of chart. Pass callback functions to this method that are fired when the data is loaded into the table or updated. Define functions which are responsible for creating and drawing of the chart. Don‚Äôt forget to choose its type: Now that you‚Äôve set all the basic functionality, let‚Äôs move on to an advanced customization. If you want to highlight the data that speaks the most and impress your coworkers or friends with an originality of the report, try using ‚ÄòConditional Formatting‚Äô feature. Let me show you what I‚Äôve got: I‚Äôve set ‚Äúconditions‚Äù to the pivot table instance. So as not to place much code, you can see the details in the demo at the end of the tutorial. Now the colors from the chart correspond to the colors of the cells and the analytical dashboard is ready to use. Conclusions Let‚Äôs sum up what we‚Äôve achieved with this simple tutorial: I hope you‚Äôve enjoyed the results you achieved with this tutorial. I‚Äôm looking forward to your feedback. Useful links gitconnected.com"
"Data Science ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£? ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ö Machine Learning, Data Mining, Data Analysis ‡∏¢‡∏±‡∏á‡πÑ‡∏á","ata Science ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£? ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ö Machine Learning, Data Mining, Data Analysis ‡∏¢‡∏±‡∏á‡πÑ",‡∏Ñ‡∏≥ ‡πÜ ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Buzzword ‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏µ ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚ÄúData Science‚Äù ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ Data Science ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ? ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢ ‡πÜ ‡∏ß‡πà‡∏≤ ‚ÄúMachine Learning‚Äù ‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á ‡∏ú‡∏°‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏á‡∏™‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Data Science ‡∏°‡∏±‡∏ô‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡∏±‡∏ô‡πÅ‡∏ô‡πà ‡∏à‡∏ô‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤ Introduction to Data Science ‡∏Å‡πá‡πÄ‡∏•‡∏¢‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏û‡∏≠‡πÉ‡∏à‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö Data Science  ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ (‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏î‡∏π‡∏î‡∏µ‡∏ß‡πà‡∏≤ Insights) ‡∏ô‡∏±‡πà‡∏ô‡πÄ‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏ü‡∏±‡∏á‡∏î‡∏π‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏´‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Data Science ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÄ‡∏ä‡πà‡∏ô: Data Mining  ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏ú‡∏ô (Pattern) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (Predict) ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÄ‡∏•‡∏¢‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏°‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏≠‡∏∞‡πÑ‡∏£‡∏à‡∏∞‡∏Ç‡∏≤‡∏¢‡∏î‡∏µ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÑ‡∏´‡∏ô ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Summer Project ‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏ó‡∏≥‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏¥‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ñ‡πâ‡∏≤‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏à‡∏ö‡∏•‡πà‡∏∞‡∏Ñ‡∏á‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏õ‡∏ö‡πâ‡∏≤‡∏á ‡πÑ‡∏î‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å‡∏Æ‡∏∞ Data Analysis  ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ ‡∏Å‡πá‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏û‡∏•‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü (Data Visualization) ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡∏£‡∏±‡∏ô Clustering ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏≥ Data Analysis ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ñ‡∏£‡∏±‡∏ö ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Data Science ‡∏´‡∏£‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÄ‡∏ô‡∏µ‡πà‡∏¢ ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏≤‡∏Ñ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ö Big Data ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏ô‡∏Å‡∏£‡∏≠‡∏Å‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏•‡∏á Excel ‡∏Å‡πá‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÅ‡∏•‡πâ‡∏ß (‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ü‡∏±‡∏á ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏°‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏ß‡πà‡∏≤ Big Data ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡∏∞‡∏Æ‡∏∞) ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏¢‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢ ‡πÇ‡∏î‡∏¢  Machine Learning  ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ ! ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Machine Learning ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡∏Å‡πá‡πÄ‡∏ä‡πà‡∏ô AlphaGo ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏•‡πà‡∏ô‡πÇ‡∏Å‡∏∞‡∏™‡∏∏‡∏î‡πÄ‡∏°‡∏û‡∏ó‡∏µ‡πà‡∏ä‡∏ô‡∏∞‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡πÇ‡∏Å‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢ ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Machine Learning ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Deep Learning ‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡πÑ‡∏õ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏£‡∏û‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏°‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏ô‡∏∞‡∏Æ‡∏∞ ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Machine Learning ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πã ‡πÜ ‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≠‡∏°‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡πà‡∏ô Mario ‡∏à‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡πà‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏≠‡∏¢‡∏´‡∏•‡∏±‡∏á ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ Machine Learning ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢ ‡∏≠‡∏≤‡∏ó‡∏¥: ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≤‡∏Å  AnalyticsVidhya Originally published at  Woratana Perth .
"DATA SCIENCE, WHERE DO I START FROM?","ATA SCIENCE, WHERE DO I START FROM","Data analytics, Data mining, Big Data, Machine Learning, Data Wrangling, and all these trending terms might be confusing at first but let‚Äôs focus on understanding what Data Science means in general‚Ä¶ Your eyes on the prize guys! It is very important for ‚Äòsanity-sake‚Äô to focus on one thing at a time or at least multi-task intelligently. The good thing about Data Science is that it covers and encompasses many of these popularly used terms already mentioned. Not to bore you with definitions but Data Science employs traditional, statistical, mathematical, and algorithmic techniques for analysis and predictions. I used to pretty much think Data Science was competitive; I still think same mostly because of the number of people I have personally seen try to take up a career in that field. But the good thing is that the sky is large enough to accommodate all the birds of the air. By the books, there has been a 344% increase in demand for Data Scientists between 2013 to 2018. LinkedIn also reported in August 2018 a shortage of 515,717 people with Data Science Skills in the USA. The general idea is as long as there is an increase in the generation and storage of data (which there usually is), then Data Scientists will be on high demand for a long time. I believe you‚Äôre convinced that you can be relevant in this field, so what next?! References https://towardsdatascience.com/my-top-5-visualization-tools-for-data-science-45a4968ae695 https://www.kdnuggets.com/2016/03/data-science-process.html https://financesonline.com/20-best-sql-editor-tools/"
"Perbedaan Machine Learning, Artificial Intelligence, Data Mining, dan Pattern Recognition dan Hubungannya.","erbedaan Machine Learning, Artificial Intelligence, Data Mining, dan Pattern Recognition dan Hubungannya","H ai semua, kalian pasti sudah tidak asing lagi dengan keempat istilah ini. Ini adalah salah satu tugas kuliahku pada semester 1 yang lalu dimana aku harus mencari tahu perbedaan dari  Machine Learning, Artificial Intelligence, Data Mining,  dan  Pattern Recognition  dari beberapa referensi. Serta aku harus mencari hubungan dari keempat bidang tersebut. Nah, kali ini aku mau mencoba menyampaikan pendapatku mengenai hal tersebut. Nah sebelum itu, kita kenalan dulu yuk sama mereka. Pada zaman sekarang, para ilmuwan dari ranah  computer science  berusaha untuk melatih komputer agar mempunyai kemampuan seperti manusia. Nah kemampuan tersebut disebut dengan  Artificial Intelligence  atau biasa disingkat dengan AI. Menurut buku  Artificial Intelligence : A Modern Approach  dari Stuart Russel, pendekatan AI dibagi menjadi 4, yaitu Oke, sudah terbayang ya definisinya. Nah, untuk membangun sebuah teknologi AI dapat digunakan empat teknik utama. Menurut buku  Artificial Intelligence  dari Suyanto, terdapat empat teknik AI, yaitu Contoh aplikasi AI di dunia nyata sudah banyak, salah satunya adalah AlphaGo, salah satu teknologi AI yang dapat mengalahkan Lee Se-dol, seorang master permainan Go. Hmmm kira-kira teknik apa saja yang dipakai dalam membangun teknologi AI ini ya? Silakan cari tahu. Menurut Tom M. Mitchell dari buku  Machine Learning , definisi  machine learning  adalah sebagai berikut Sebuah komputer dikatakan belajar dari eksperimen E, yang mengacu pada beberapa task T, dan ukuran performansi P, jika performansi dari task T, yang diukur dengan P, ditingkatkan dengan pengalaman E. Dengan menggunakan  Machine Learning , komputer dapat menjalankan tugasnya dengan baik dengan memanfaatkan pengalaman masa lalu. Pengalaman tersebut berbentuk data atau langkah-langkah yang pernah dilakukan sebelumnya. Contoh pengaplikasian dari  Machine Learning  adalah pengenalan wajah berbasis gambar. Dengan teknologi tersebut, komputer akan dapat mengenali wajah seseorang, baik itu namanya, maupun jenis kelamin ataupun usianya. Komputer akan dilatih dengan cara mempelajari berbagai macam gambar wajah orang tersebut dengan menggunakan algoritma  Machine Learning . Menurut Aggarwal dan Charu C. dalam buku  Data Mining The Textbook , definisi  Data Mining  adalah sebagai berikut Data Mining merupakan ilmu yang yang mempelajari tentang pengumpulan, pembersihan, pemrosesan, penganalisaan, dan mendapatkan insight dari data. Singkatnya, dalam  Data Mining  kita akan menggali  knowledge  atau  insight  yang penting dari sekumpulan data yang bervolume besar dan beragam. Tahapan  Data Mining  menurut buku  Data Mining The Textbook  adalah sebagai berikut Task  Data Mining  sendiri dibagi menjadi dua, yaitu Contoh pengaplikasian  Data Mining  adalah  market basket analysis . Dalam  market basket analysis , kita menganalisis kombinasi barang apa saja yang dibeli oleh pelanggan di toko. Dengan adanya informasi tersebut, para pegawai dapat mengatur kembali peletakan barang di rak, contohnya jika banyak pelanggan yang membeli susu dan sereal, maka susu dan sereal akan diletakan di tempat yang berdekatan. Okay, last one is pattern recognition . Pengertian  Pattern Recognition  menurut Christoper M. Bishop dalam buku  Pattern Recognition and Machine Learning  adalah sebagai berikut Bidang pengenalan pola berkaitan dengan penemuan secara otomatis keteraturan data dengan menggunakan algoritma komputer dan menggunakan keteraturan tersebut untuk melakukan beberapa aksi seperti mengklasifikasi data menjadi kategori yang berbeda. Jadi,  Pattern Recognition  ini adalah sebuah teknik untuk memanfaatkan keteraturan atau pola pada suatu data dengan menggunakan algoritma tertentu. Pola yang dimaksudkan adalah fitur yang merepresentasikan suatu data. Lalu apa bedanya dengan  Machine Learning ? Nah singkatnya di  Pattern Recognition  ini ada sebuah teknik dimana kita tidak memerlukan teknik  Machine Learning , yaitu  template matching . Contohnya adalah sebagai berikut Dalam kasus ini, kita ingin mendeteksi huruf K dengan menggunakan template huruf K. Template ini akan dicocokan dengan gambar target yang berisi kumpulan huruf, sehingga pada akhirnya huruf K akan ditemukan setelah dilakukan pencarian  per patch  gambar. Nah kita sudah berkenalan dengan keempat bidang tersebut. Jadi bagaimana hubungannya? Berdasarkan penjelasan sebelumnya, hubungan keempat bidang tersebut dapat digambarkan sebagai berikut Dari gambar tersebut, kita dapat fokus ke tiga poin penting ini Nah sekian penjelasan dariku. Jika pendapat kalian bertentangan bisa mencoba berdiskusi di kolom komentar ya. Terimakasih sudah berkunjung. Semoga bermanfaat!"
How artificial intelligence fits into e-commerce,ow artificial intelligence fits into e-commerc,"According to the recent Forrester report,  Predictions 2017: Artificial Intelligence Will Drive The Insights Revolution , AI will grow 300% in 2017, and ‚Äúwill steal $1.2 trillion per annum from their less informed peers by 2020.‚Äù Numbers like these are behind the surge in retailers betting big on AI. Particularly for retailers looking to gain a leg up in e-commerce, AI is hard to ignore. Still, while retailers have generally recognized the importance of AI, there is also plenty of confusion when it comes to relevant terminology and real-world applications because of the marketing noise around the technology. Market confusion on what AI is and what it‚Äôs capable of has in large part been driven by the simple misuse of the term AI. AI is not a singular technology. It‚Äôs comprised of multiple components, such as machine learning. These component technologies that make up AI each have their own inherent value ‚Äî but as with many things when it comes to marketing cutting-edge technology, the nuance is lost for the buzz-worthy. This is one reason AI has become a catch-all word for multiple technologies. To move AI forward, we need to embrace the nuance of what it actually is. Here are three important aspects of  AI that e-commerce businesses  need to understand: Beyond these three components, AI also encompasses Neural Networks (computer systems modeled on the human brain and nervous system) and Robotics. However, neither of these fields are relevant for e-commerce at this time. Depending on your goals, find the gaps in your data and fill them By better understanding the technology underpinnings of AI, you can better view the world of possible applications and the specific impact it can have on retailers. A few examples include: As an example, marketers already measure open rates on email, so they know if they‚Äôre getting customers‚Äô attention. They also measure website clicks as a gauge of customer interest. If that click is on the ‚Äúthank you‚Äù page on an ecommerce site, they know the customer has already purchased something, and likely even what that item is. Using this data, enriched with other data about the individual, purchase history and other factors through Data Mining and Machine Learning, allows marketers to communicate with consumers in real time with the right offer at the right place and time. They can understand when each customer will be most receptive to and offer, and what that offer should entail in order to capture their attention. As an example, a customer may be shopping for a particular brand of mascara on a website. Using Data Mining on past information about the customer and others like her, and Machine Learning to react to this new data about the customer, a bot agent could pop up and give her exclusive personalized offers that meet her needs and are relative to her color preferences, demographic, time of year, price triggers and sense of style. Given the amount of data to create a personalized interaction, a mere decision-tree logic would not work. Deep-learning customer models must be created in order to ‚Äúreact‚Äù to the consumer in real time, and NLP is needed to interact with her in a conversational way. The list of applications for AI could go on, but the first steps for implementing the technology are similar regardless of how you choose to employ it. First, get a comprehensive view of your existing data points. This could include CRM data, transactional data from online or mobile, demographic information, third-party sources ‚Äî any and all context you can gather on your customers and their preferences can be relevant. Second, determine your goals. Do you want to increase sales among your existing customers? Bring in new customers? Determine when you should have sales? Figure out which products you should be stocking? Having a specific goal will help you determine the best route to get there, and gauge the effectiveness of your efforts. Next, depending on your goals, find the gaps in your data and fill them. The power of AI is that it can uncover correlations that aren‚Äôt readily apparent, and no company has access to 100% of the data they need on their own. For example, Starbucks may not know how heavy traffic is today or what the weather is in your current location, but there could be important correlations between those pieces of data and whether you are buying a coffee because you‚Äôre tired or because it‚Äôs cold outside. Finally, it is important to understand that the Data Mining and Machine Learning process takes time. It is still a form of A/B testing, just done much more rapidly and on a massive scale. You can‚Äôt expect immediate results as the system needs to learn from successes and failures. For those that get a head start now, however, there is an incredible opportunity to grab a larger slice of that $1.2 trillion pie in the years to come. Grey Jean Technologies provides personalization technology to retailers."
All about Logistic regression in one article,ll about Logistic regression in one articl,"Behind every great leader there was an even greater logistician. Unlike other algorithms, Logistic Regression is easily misguided by young developers. Maybe because people still think that it is a regression machine learning algorithm. Logistic regression is a statistical machine learning algorithm that classifies the data by considering outcome variables on extreme ends and tries makes a logarithmic line that distinguishes between them. Logistic regression is a brother of linear regression but unlike its name, logistic regression is a classification algorithm. Let‚Äôs brush up with first linear regression: formula: where, With this values, we can predict y values such as. Now one thing to note from linear regression that it works with continuous data, But if we need linear regression for our classification algorithms, we need to further tweak our algorithm a bit. First we need to define a threshold such that if our predicted value is lower than threshold then it is of class1 otherwise class2. Now if you are thinking ‚Äúoh that‚Äôs easy we have to define linear regression with threshold and vola it becomes classification algorithm,there is a trick in it. We have to define threshold value by ourselves, and for large datasets it will be impossible for us to calculate threshold. Moreover the threshold value once defined will be same even if our predicted values change. For more reference go  here . On the other hand, a logistic regression produces a logistic curve, which is limited to values between 0 and 1. Logistic regression is similar to a linear regression, but the curve is constructed using the natural logarithm of the ‚Äúodds‚Äù of the target variable, rather than the probability. Moreover, the predictors do not have to be normally distributed or have equal variance in each group. If you still didn‚Äôt understand it, then i recommend you to see the following video which explains logistic regression in a simple way. To explain Logistic regression, i need some physical medium to express my knowledge in this digital medium. So i have written Logistic Regression formulas in my notebook and then took pictures and posted it here. If you want pdf version,  click here . github.com Further i will add other machine learning algorithms. The main motto of this article was to give an in depth knowledge of Logistic Regression without using any hard word and explain it from scratch. Further if you want to implement Logistic Regression, start from these datasets and you can comment your predicted score with code in the comments section. And if you want to learn more Machine learning algorithms then follow me as i am going to add all machine learning algorithms that i know of. Previously I have added Naive Bayes explanation in a very basic an informative way. towardsdatascience.com Till then, And Don‚Äôt forget to clap clap clap‚Ä¶"
On the said fall of the craftsman and the rise of the scientific programmer,n the said fall of the craftsman and the rise of the scientific programme,"There‚Äôs a  hot debate  brewing up for some time. Scientific developer jobs ‚Äî those involving machine learning, data mining, etc. ‚Äî are on the rise on job opening statistics. These jobs quite heavily require a CS background and thorough knowledge on algorithms and specifically, mathematics. The proponents claim that programming was always a scientific thing and denounce the movement of software craftsmanship. Software, scientific or not, is  complex . Writing software is about  managing complexity . It turns out that there isn‚Äôt a single, true way of managing complexity, and experience and vision are key virtues at it. Generally speaking, you improve your software skills as you get more experienced. That‚Äôs why design patterns emerged ‚Äî they are proven guidelines to a very nasty problem. They are there so you don‚Äôt have to reinvent the wheel. Over time, we began to realize that software is actually an art form. Each creator has their own style and approach. Furthermore, it‚Äôs a utilitarian art form; that is, it has a function ‚Äî your product is meant to be useful to the audience. That‚Äôs actually very similar in concept to a master carpenter who creates hand-crafted furniture. Its main function is to serve the audience as a tool ‚Äî a desk or a chair, for example. But its design is unique and you can almost feel the artistic touch of the carpenter. You can tell, by a mile, if the work is from the hands of a master carpenter or a novice. Software is no different. I‚Äôm talking about the big, enterprise software which has to serve a large audience and live for many years. Web applications, mobile applications, social media or even scientific tools such as MATLAB are all software that require utmost attention to detail and a good structure. That‚Äôs why we have architects, lead developers, senior developers and junior developers. We design software to scale. We write software in such a form that it defines the rest of your life maintaining that software. That‚Äôs where the analogy of software craftsmanship comes from. It‚Äôs a craft and how you build it drastically changes the outcome. The works speak for their creators; greatest works come from the greatest masters, and cheap ones (that are broken even before launch) come from the inept. Experience is not the sole factor defining your craftsmanship, you may be coding for half a century and you may still be a lousy craftsman. It requires taste, vision and an analytical, critical mind ‚Äî you have to be a critic of your own work in order to improve. When you look at it this way, it doesn‚Äôt really matter if you‚Äôre doing machine learning or games. They are the topics, but the fundamentals remain the same. A developer who knows how to create a maintainable, scalable architecture is an artisan, whether the topic is scientific or not. Software craftsmanship and the scientific programmer are not mutually exclusive. The term scientific programmer refers to the category of the work and the term software craftsmanship refers to the actual building process. I‚Äôve created enough software in my career that range from simple MATLAB simulations to very complex ones, from simple logic circuits in HDL to CPUs, from static web pages to huge portals, from simple desktop apps to complex mobile apps. I can tell that regardless of the language, the platform or the field of application, the maturity of your style shows itself on the end result. Scientific programmers may be good craftsmen or they may be bad craftsmen. The field of application doesn‚Äôt affect this. Being a scientific programmer is just like being a mobile programmer or a game programmer; you need expertise on the topic but in the end you‚Äôre writing software. On the other hand, being a software craftsman doesn‚Äôt mean that you are not following scientific breakthroughs or scientific methodology. In fact, those who rely on science and the scientific methodology do far more better than those who only rely on gut feeling. But applying science doesn‚Äôt make you a scientist and if you‚Äôre still writing software, it surely doesn‚Äôt  promote  you from a craftsman to a scientist. Real software craftsmen are also good engineers and you feel the difference between a gut-feeling-driven craftsman who doesn‚Äôt employ computer science fundamentals and a true engineering(-at-heart?) craftsman with analytical approach. It‚Äôs true that the former has a lot more population, but almost always the latter produces better outcome and proves to be better in the end. The existence of the former doesn‚Äôt imply that software craftsmanship is inferior. Software craftsmanship doesn‚Äôt deny science, it teaches the best practices to approach and make use of science. Finally, just as the scientists stand on the shoulders of giants, the developers also make use of the collective experience of former masters. I follow the software craftsmanship methodology by heart and I‚Äôm teaching tens of apprentices how to be a good artisan. The intricacies of great software, the delicate patterns, the hidden pitfalls and the hidden gems. Just as the master artisans, master software developers have their styles and they will expand those styles into schools. They teach the  path of development  to their apprentices. That‚Äôs actually the best way to transfer experience to a novice. Where experiencing and learning everything from scratch would take tens of years, finding a good master and being a hard-core apprentice takes only a few years. This model is so suitable for software, I haven‚Äôt seen any other approach perform better. It‚Äôs the best approach to  living up to our heritage of scientists . For more information and ideas on this topic, I would suggest reading  Apprenticeship Patterns: Guidance for the Aspiring Software Craftsman . It‚Äôs a short read, and the ideas are articulated concisely and perfectly. We will continue to make the world a better place by teaching the young the best practices of development. Our knowledge is the best thing we can leave to the coming generations. If you have a long history with software and you feel like there‚Äôs something missing in your life, find a young apprentice and train them. Give back to the community. Originally published at  arm.ag ."
Up for a (30 Day) Challenge?,p for a (30 Day) Challenge,"Think learning Data Engineering can help your career? You‚Äôre not wrong. Qwiklabs‚Äô Data Engineering quest provides excellent practice for your  Google Cloud Certified Professional Data Engineer Certification  exam, and certified engineers  can earn up to 200K a year. If that sounds appealing, August‚Äôs 30-day  Data Engineering  challenge is for you. Here‚Äôs how to join: Enter code  1q-thirty-14: Earn the badge by August 30, and get a second month free, plus an exclusive invitation to play an exclusive Data Engineering game, open only to those who complete the challenge. You‚Äôll compete for prizes like gift cards, swag, and most importantly, bragging rights. Some of our favorite labs in the challenge include: Creating a Data Transformation Pipeline with Cloud Dataprep   ‚Äî Use intelligent data service Cloud Dataprep to construct, export, and analyze datasets, and create a transformation pipeline to get scheduled inputs and outputs for the information. Building an IoT Analytics Pipeline on Google Cloud Platform  ‚Äî All devices connected across the planet through the internet are part of the Internet of Things (IoT). Use your newly acquired pipeline skills to connect and manage data from these devices (or their simulated counterparts) and analyze it. Predict Taxi Fare with a BigQuert ML Forecasting Model  ‚Äî New York City taxi cabs take millions of trips a year. Build a machine learning model that can analyze all these rides and predict how much your next cab fare will cost. Beats taking the subway! Enroll today , and you can earn this beautiful badge ‚Äî and more."
6 Charts That Explain Why the Machine Learning & AI Job Market is on Fire, Charts That Explain Why the Machine Learning & AI Job Market is on Fir,"As we enter 2022, the AI job market is on fire. Data science,  machine learning , data engineering, and similar  AI roles  continue to occupy the top spot by many measures, such as salary, desirability, and job prospects. This is all down to demand lead growth. Not only is the AI job market on fire but we believe it will continue to burn brightly. Here‚Äôs a look at 6 charts that explain why the AI job market is doing so well. Chart 1 from Pitchbook shows AI & ML investing was at an all-time high in 2021. Globally venture capitalists closed 4,021 AI deals with a record amount of funding of $83.7 billion. For startups, the hard work begins once the funds are wired and they need to put the money to work. AI startups spend much of the first and subsequent round funds on engineering and AI/ML talent. Thus, they need to hire the best in the field at an unprecedented pace to ensure success and to get to their next funding round. Additionally, in 2021 although the number of investments dipped slightly, the amount raised per startup increased significantly. Flush with cash, many AI startups are offering unprecedented amounts to already well-compensated and accomplished AI & ML experts, which is helping raise salaries across the board. For 2022, and the next 5 years, expect more of the same. Record investment and intense competition for top talent. Author‚Äôs note: Interested in AI startups? Check out  ODSC East AI Start-up Showcase . Chart Credit: Pitchbook 2021, Q3, Emerging Tech Report Not to be outdone by startups, corporations are weighing in heavily on AI & ML hiring. After years of handwavy hype around AI and related technologies, companies are finally buckling down and doing the hard work of reshaping their businesses to take advantage of AI. Chart 2 from PWC‚Äôs 2021 AI report nicely illustrates that: 58% of companies from their representative survey are fully committed to AI & ML. Fully 93% are committed to the AI path, and only a very small percentage of 7% have no plans to do so. That‚Äôs an astonishing uptick for a technology that first came on their radar less than 5 years ago. This chart also illustrates that there is a lot of AI implementation capacity to fill. Removing the 7% with no interest (thus far) and the 25% fully implementing AI still leaves 68% that need to fully engage AI. Much of this will come from new hiring. Thus we expect significant growth in the AI & ML job market over the next decade. Chart Credit: PWC 2021 AI Survey Report Thanks to Moore‚Äôs Law, which predicted computing power will double every two years, software, as Marc Andreessen famously stated, is eating the world. But that was back in 2011 before he knew there was a bigger fish lurking in the pond. According to this interesting chart from chip designer Synopsys, AI-driven software is doubling every 3 to 4 months. The left side of the chart shows the steady progression of Moore‚Äôs law. But it pales in comparison to the right side, which shows exponential growth beginning approximately the same time as the Alexnet competition breakthrough in 2012 up to to AplhaGoZero in 2020 (since surpassed). It‚Äôs evident that AI is the bigger fish in the software industry, so expect every aspect of software, from design, development, and deployment, to be consumed by AI over the next decade and beyond. This will lead not only to the reskilling of software and hardware engineering roles, but also the creation of hybrid roles, new roles (see chart 5), and generally increased demand for AI & ML experts. Above: Chip design through the ages: Now it‚Äôs AI‚Äôs turn. Chart Credit: Synopsys AI is Eating Software AI is not the only new(ish) kid on the block, but pundits like to put it in the  emerging tech  sector. Emerging tech is set to be the driving force for the global economy over the next decade. It‚Äôs a long list that includes everything from clean energy and autonomous machines to crypto and quantum computing. Many of these jobs will incorporate aspects of other technologies including AI. For example, there is significant excitement around quantum machine learning and AI-enabled cybersecurity. Chart 4 shows data from the CompTIA recent jobs market report. The emerging tech sector is huge, but AI & ML is taking a very respectable 12.6% (and growing) slice of that. The net result of this is increased demand for AI & ML in the jobs market. As emerging tech continues to take a bigger slice of the manufacturing and service economy, it will draw in AI & ML experts from around the world. Chart Data Source CompTIA Tech Jobs Report, December 2021 ODSC wasn‚Äôt the first data scientist conference when we started back in 2013 (as the Boston Data Festival). Many excellent academic conferences in AI have been around for decades. Although ODSC‚Äôs content is more focused on applied and open source technologies, over the last 8 years we‚Äôve kept a very close eye on new AI developments that make the move from academia to the real world. Much of the advancement in AI & ML emerged from either academia or open-source, oftentimes both. A good proxy of what‚Äôs to come in the industry can be gauged from research papers published in the field. Chart 5 illustrates data from arXiv.org for papers published in AI, ML, and related topics (computer vision, etc). Between 2018 and 2021 papers published in this field have more than doubled. Overall we expect academic/institutional research to continue to accelerate, driving new job growth over the next few decades. Note the new fields of AI emerging from research, such as differential privacy, machine learning safety, meta machine learning, just to name a few. These new techniques will drive the industry to new growth as AI becomes more ubiquitous and powerful. (authors note: if you‚Äôre interested in AI research, check out ODSC East‚Äôs focus areas ‚Äî Research Frontiers, Machine Learning Safety, and Responsible AI) We‚Äôve come a long way since 2012 when data science was hailed as the sexiest job of the 21st century. Many new roles (or relabeling of existent roles) have since emerged. Data scientist still holds one of the top spots, but machine learning engineer, data engineers, MLOps engineer, and AI engineer are starting to catch up. Chart 6 from Dice‚Äôs 2020 jobs report nicely illustrates these trends.The role of data engineering has been around for quite a while (in other guises), but thanks to the voracious data appetite of AI & ML the role has really taken off. The same can be said for the role of machine learning engineer. Few expect job roles to be stagnant. New roles are quickly emerging that are just as desirable, such as  MLOps Engineer,  responsible for deploying and maintaining production models,  Machine Learning QA/Test Engineer,  responsible for embedded models in autonomous systems, and  AI Ethicist,  responsible for the ethical dimension of AI and machine learning. AI Technical Writer, AI Project Manager, and AI Data Developer are a few more examples of expanding new job roles that will create new desirable positions and demand in the field. Chart Credit: Dice 2020 Tech Jobs Report ODSC East  is the leading applied conference for machine learning and artificial intelligence. Our program is geared towards acquiring hands-on experience and insights into the trends, tools, and topics driving the fields of ML & AI to new highs. Join us in April and learn firsthand what‚Äôs coming in 2022 and beyond.  Register now  for 70% off all ticket types. Original post here. Read more data science articles on  OpenDataScience.com , including tutorials and guides from beginner to advanced levels!  Subscribe to our weekly newsletter here  and receive the latest news every Thursday. You can also get data science training on-demand wherever you are with our  Ai+ Training  platform."
Why you should use User Stories in Data Analytics,hy you should use User Stories in Data Analytic,"Whether you are a project manager or a product owner, a project management tool and some basic agile techniques will significantly help you manage your project or product. At the very least, these tools will give you, the management, and your team a better overview. In addition, the results can be a faster implementation, fewer queries, better time estimation and greater motivation. In the following, I want to provide you some points that can help you improve the project management and the underlying user stories. Here, I would recommend Jira ‚Äîthe standard software for management purposes. You can easily start with the  free SaaS solution [1]. With Jira you can run your project either in a Scrum or Kanban Mode. In the free version you also have a lot of great features like Backlogs, Reports, Workflows, etc. Other possibilities are for example Trello (also for  free  [2]), which provides you a Kanban Board to manage your tasks and/or stories or the open source version Wekan (Free ‚Äî but you have to set up a server[3]). There are many more tools you can use but to get you started, the tools above will do the job. The next step after choosing your tool will probably be the decision of which project management method you want to use. When using Scrum I would recommend Jira as the preferred tool ‚Äî because it provides you with everything you need. Using Scrum you will need to set up roles, ceremonies and artifacts. Furthermore, Scrum focuses on complex software development. Scrum can show its full strengths when used in an environment, where a new complex software product or service is developed. Kanban, on the other hand, is suitable as a method in a controllable software process due to the core principle of continuous improvement. Kanban is often used in support or maintenance teams, where the tasks to be solved are complicated but usually not complex (such as software rollouts or maintenance work). Kanban focuses on process efficiencies and productivity gains. Discussing these principles or even traditional project management methods and deciding which one works best for you can be a rather difficult process. Here, you have to do a little research on your own. This also makes sense so you understand the theoretical background. The decision is also based on your organization and how people want to work together ‚Äî so in short: You have to find it out yourself. For myself, I really can recommend the Jira documentation[4]. My opinion is if you are working in the area of data integration, analytics, reporting, etc. it really makes sense to work agile ‚Äî due to often complex task and changing requirements. After setting up the basic infrastructure and deciding on how to work, let‚Äôs start with the user story. A user story is a software requirement formulated in everyday language and is deliberately kept short. For a deeper dive I would recommend the following article:  How to Write Good User Stories in Agile Software Development . In Jira I put the user story in the description field. Often it makes sense to put other important references in there, too. If necessary, you could use tasks that are related to the story. Talking about stories and tasks it is important to describe them very well ‚Äî especially in the field of Big Data. Mostly technical details have to be documented well so errors and questions from developers can be kept low. Here, I recommend a backlog refinement anti-cyclically to the sprint planning, where the product owner and the team can discuss stories and how to implement them technically. Ready and prioritized stories could then be marked and be pulled into the next sprint. When not working with Scrum team meetings and coordination in a similar way can also be useful. Acceptance criteria  (AC) are the conditions that a software product must meet to be  accepted  by a user, a customer, or other system. They are unique for each user story and define the feature behavior from the end-user‚Äôs perspective [5]. In Jira you can also put it in the description field or create a new text field for it. Here, it‚Äôs really important for the product owner to invest some time so that the business requirements are met ‚Äî it‚Äôs up to him to mediate between the business department and the development team. I would really recommend one or both of the above instruments. Story points are a unit used to describe the size of a user story. They represent the development effort. Traditional software teams create estimates in a time-based format, i.e., days, weeks, and months. However, many agile teams have moved to story points. Story Points are units of measure for estimating the total effort required to fully implement a product backlog item or other task item. Teams assign story points in relation to task complexity, work effort, and risks or uncertainties. Values are assigned to manage uncertainties better in order to more effectively break tasks into smaller pieces[1]. From my experience I like to use Man-days for tasks like ETL/ELT pipelines, setting up some databases and other related work which can be complex, too but with which the team is already familiar. For more complex tasks like developing a deep learning algorithm or building up a new cloud based Data Lake ‚Äî often things you did never before ‚Äî it makes sense to use Story Points. Last but not least, my advice is always to talk about what you and your team achieved and how your stories improves business and processes or makes it easier. My experience showed me that newer teams like BI, Data Science or Engineering in companies are often not so much in the focus (Colleagues might ask what are they actually doing). I hope this article gives you some inspiration and something to start with. The first step will always be to set up a toolset and of course to get in touch with your team as product or project manager. The tools and methods provided are in my opinion one of the most important ‚Äî especially when working in the field of Data. I really recommend using an agile process for a faster deployment of the developed system in order to minimize risks and undesirable developments in the development process. You can find many more inspirations and tools in the Sources and further Readings below. [1] Jira,  Our cloud products work even better together  (2020) [2] Trello,  https://trello.com  (2020) [3] Wekan,  Open-Source kanban  (2020) [4] Jira,  Is the Agile Manifesto still a thing?  (2020) [5] altexsoft,  Acceptance Criteria: Purposes, Formats, and Best Practices  (2020) [6] Qucikscrum,  Product Backlog Refinement  (2020) stackoverflow.com blog.easyagile.com"
