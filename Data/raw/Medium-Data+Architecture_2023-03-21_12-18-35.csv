blogTitle,blogSubheading,blogContent
An A-Z of useful Python tricks,n A-Z of useful Python trick,"Python is one of the world‚Äôs most popular, in-demand programming languages. This is for many reasons: I use Python daily as an integral part of my job as a data scientist. Along the way, I‚Äôve picked up a few useful tricks and tips. Here, I‚Äôve made an attempt at sharing some of them in an A-Z format. Most of these ‚Äòtricks‚Äô are things I‚Äôve used or stumbled upon during my day-to-day work. Some I found while browsing the  Python Standard Library docs . A few others I found searching through  PyPi . However, credit where it is due ‚Äî I discovered four or five of them over at  awesome-python.com . This is a curated list of hundreds of interesting Python tools and modules. It is worth browsing for inspiration! One of the many reasons why Python is such a popular language is because it is readable and expressive. It is often joked that Python is ‚Äò executable pseudocode ‚Äô. But when you can write code like this, it‚Äôs difficult to argue otherwise: You want to plot graphs in the console? You can have graphs in the console. Python has some great default datatypes, but sometimes they just won‚Äôt behave exactly how you‚Äôd like them to. Luckily, the Python Standard Library offers  the collections module . This handy add-on provides you with further datatypes. Ever wondered how you can look inside a Python object and see what attributes it has? Of course you have. From the command line: This can be a really useful feature when running Python interactively, and for dynamically exploring objects and modules you are working with. Read more  here . Yes,  really . Don‚Äôt pretend you‚Äôre not gonna try it out‚Ä¶ üëç One consequence of Python‚Äôs popularity is that there are always new versions under development. New versions mean new features‚Ää‚Äî‚Ääunless your version is out-of-date. Fear not, however. The  __future__ module  lets you import functionality from future versions of Python. It‚Äôs literally like time travel, or magic, or something. Why not have a go  importing curly braces ? Geography can be a challenging terrain for programmers to navigate (ha, a pun!). But  the geopy module  makes it unnervingly easy. It works by abstracting the APIs of a range of different geocoding services. It enables you to obtain a place‚Äôs full street address, latitude, longitude, and even altitude. There‚Äôs also a useful distance class. It calculates the distance between two locations in your favorite unit of measurement. Stuck on a coding problem and can‚Äôt remember that solution you saw before? Need to check StackOverflow, but don‚Äôt want to leave the terminal? Then you need  this useful command line tool . Ask it whatever question you have, and it‚Äôll do its best to return an answer. Be aware though ‚Äî it scrapes code from top answers from StackOverflow. It might not always give the most helpful information‚Ä¶ Python‚Äôs  inspect module  is great for understanding what is happening behind the scenes. You can even call its methods on itself! The code sample below uses  inspect.getsource()  to print its own source code. It also uses  inspect.getmodule()  to print the module in which it was defined. The last line of code prints out its own line number. Of course, beyond these trivial uses, the inspect module can prove useful for understanding what your code is doing. You could also use it for writing self-documenting code. The Jedi library is an autocompletion and code analysis library. It makes writing code quicker and more productive. Unless you‚Äôre developing your own IDE, you‚Äôll probably be most interested in  using Jedi as an editor plugin . Luckily, there are already loads available! You may already be using Jedi, however. The IPython project makes use of Jedi for its code autocompletion functionality. When learning any language, there are many milestones along the way. With Python, understanding the mysterious  **kwargs  syntax probably counts as one. The double-asterisk in front of a dictionary object lets you pass the contents of that dictionary as  named arguments to a function . The dictionary‚Äôs keys are the argument names, and the values are the values passed to the function. You don‚Äôt even need to call it  kwargs ! This is useful when you want to write functions that can handle named arguments not defined in advance. One of my favourite things about programming in Python are its  list comprehensions . These expressions make it easy to write very clean code that reads almost like natural language. You can read more about how to use them  here . Python supports functional programming through a number of inbuilt features. One of the most useful is the  map()  function ‚Äî especially in combination with  lambda functions . In the example above,  map()  applies a simple lambda function to each element in  x . It returns a map object, which can be converted to some iterable object such as a list or tuple. If you haven‚Äôt seen it already, then be prepared to have your mind blown by  Python‚Äôs newspaper module . It lets you retrieve news articles and associated meta-data from a range of leading international publications. You can retrieve images, text and author names. It even has some  inbuilt NLP functionality . So if you were thinking of using BeautifulSoup or some other DIY webscraping library for your next project, save yourself the time and effort and  $ pip install newspaper3k  instead. Python provides support for  operator overloading , which is one of those terms that make you sound like a legit computer scientist. It‚Äôs actually a simple concept. Ever wondered why Python lets you use the  +  operator to add numbers and also to concatenate strings? That‚Äôs operator overloading in action. You can define objects which use Python‚Äôs standard operator symbols in their own specific way. This lets you use them in contexts relevant to the objects you‚Äôre working with. Python‚Äôs default  print  function does its job. But try printing out any large, nested object, and the result is rather ugly. Here‚Äôs where the  Standard Library‚Äôs pretty-print module  steps in. This prints out complex structured objects in an easy-to-read format. A must-have for any Python developer who works with non-trivial data structures. Python supports multithreading, and this is facilitated by the Standard Library‚Äôs Queue module. This module lets you implement queue data structures. These are data structures that let you add and retrieve entries according to a specific rule. ‚ÄòFirst in, first out‚Äô (or FIFO) queues let you retrieve objects in the order they were added. ‚ÄòLast in, first out‚Äô (LIFO) queues let you access the most recently added objects first. Finally, priority queues let you retrieve objects according to the order in which they are sorted. Here‚Äôs an example of how to use queues  for multithreaded programming in Python. When defining a class or an object in Python, it is useful to provide an ‚Äòofficial‚Äô way of representing that object as a string. For example: This makes debugging code a lot easier. Add it to your class definitions as below: Python makes a great scripting language. Sometimes using the standard os and subprocess libraries can be a bit of a headache. The  sh library  provides a neat alternative. It lets you call any program as if it were an ordinary function ‚Äî useful for automating workflows and tasks, all from within Python. Python is a dynamically-typed language. You don‚Äôt need to specify datatypes when you define variables, functions, classes etc. This allows for rapid development times. However, there are few things more annoying than a runtime error caused by a simple typing issue. Since Python 3.5 , you have the option to provide type hints when defining functions. You can also define type aliases: Although not compulsory, type annotations can make your code easier to understand. They also allow you to use type checking tools to catch those stray TypeErrors before runtime. Probably worthwhile if you are working on large, complex projects! A quick and easy way to generate Universally Unique IDs (or ‚ÄòUUIDs‚Äô) is through the  Python Standard Library‚Äôs uuid module . This creates a randomized 128-bit number that will almost certainly be unique. In fact, there are over 2¬π¬≤¬≤ possible UUIDs that can be generated. That‚Äôs over five undecillion (or 5,000,000,000,000,000,000,000,000,000,000,000,000). The probability of finding duplicates in a given set is extremely low. Even with a trillion UUIDs, the probability of a duplicate existing is much, much less than one-in-a-billion. Pretty good for two lines of code. This is probably my favorite Python thing of all. Chances are you are working on multiple Python projects at any one time. Unfortunately, sometimes two projects will rely on different versions of the same dependency. Which do you install on your system? Luckily, Python‚Äôs  support for virtual environments  lets you have the best of both worlds. From the command line: Now you can have standalone versions and installations of Python running on the same machine. Sorted! Wikipedia has a great API that allows users programmatic access to an unrivalled body of completely free knowledge and information. The  wikipedia module  makes accessing this API almost embarrassingly convenient. Like the real site, the module provides support for multiple languages, page disambiguation, random page retrieval, and even has a  donate()  method. Humour is a key feature of the Python language ‚Äî after all, it is named after the British comedy sketch show  Monty Python‚Äôs Flying Circus . Much of Python‚Äôs official documentation references the show‚Äôs most famous sketches. The sense of humour isn‚Äôt restricted to the docs, though. Have a go running the line below: Never change, Python. Never change. YAML stands for ‚Äò YAML Ain‚Äôt Markup Language ‚Äô. It is a data formatting language, and is a superset of JSON. Unlike JSON, it can store more complex objects and refer to its own elements. You can also write comments, making it particularly suited to writing configuration files. The  PyYAML module  lets you use YAML with Python. Install with: And then import into your projects: PyYAML lets you store Python objects of any datatype, and instances of any user-defined classes also. One last trick for ya, and it really is a cool one. Ever needed to form a dictionary out of two lists? The  zip()  inbuilt function takes a number of iterable objects and returns a list of tuples. Each tuple groups the elements of the input objects by their positional index. You can also ‚Äòunzip‚Äô objects by calling  *zip()  on them. So there you have it, an A-Z of Python tricks ‚Äî hopefully you‚Äôve found something useful for your next project. Python‚Äôs a very diverse and well-developed language, so there‚Äôs bound to be many features I haven‚Äôt got round to including. Please share any of your own favorite Python tricks by leaving a response below!"
Learning Python: From Zero to Hero,earning Python: From Zero to Her,"This post was originally published at  TK's Blog . First of all, what is Python? According to its creator, Guido van Rossum, Python is a: ‚Äúhigh-level programming language, and its core design philosophy is all about code readability and a syntax which allows programmers to express concepts in a few lines of code.‚Äù For me, the first reason to learn Python was that it is, in fact, a beautiful   programming language. It was really natural to code in it and express my thoughts. Another reason was that we can use coding in Python in multiple ways: data science, web development, and machine learning all shine here. Quora, Pinterest and Spotify all use Python for their backend web development. So let‚Äôs learn a bit about it. You can think about variables as words that store a value. Simple as that. In Python, it is really easy to define a variable and set a value to it. Imagine you want to store number 1 in a variable called ‚Äúone.‚Äù Let‚Äôs do it: How simple was that? You just assigned the value 1 to the variable ‚Äúone.‚Äù And you can assign any other  value  to whatever other  variables  you want. As you see in the table above, the variable ‚Äú two ‚Äù stores the integer  2 , and ‚Äú some_number ‚Äù stores  10,000 . Besides integers, we can also use booleans (True / False), strings, float, and so many other data types. ‚Äú If ‚Äù uses an expression to evaluate whether a statement is True or False. If it is True, it executes what is inside the ‚Äúif‚Äù statement. For example: 2  is greater than  1 , so the ‚Äú print ‚Äù code is executed. The ‚Äú else ‚Äù statement will be executed if the ‚Äú if ‚Äù expression is  false . 1  is not greater than  2 , so the code inside the ‚Äú else ‚Äù statement will be executed. You can also use an ‚Äú elif ‚Äù statement: In Python, we can iterate in different forms. I‚Äôll talk about two:  while   and  for . While  Looping: while the statement is True, the code inside the block will be executed. So, this code will print the number from  1  to  10 . The  while  loop needs a ‚Äú loop condition. ‚Äù If it stays True, it continues iterating. In this example, when  num  is  11  the  loop condition  equals  False . Another basic bit of code to better understand it: The  loop condition  is  True  so it keeps iterating ‚Äî until we set it to  False . For Looping : you apply the variable ‚Äú num ‚Äù to the block, and the ‚Äú for ‚Äù statement will iterate it for you. This code will print the same as  while  code: from  1  to  10 . See? It is so simple. The range starts with  1  and goes until the  11 th element ( 10  is the  10 th element). Imagine you want to store the integer 1 in a variable. But maybe now you want to store 2. And 3, 4, 5 ‚Ä¶ Do I have another way to store all the integers that I want, but not in  millions of variables ? You guessed it ‚Äî there is indeed another way to store them. List  is a collection that can be used to store a list of values (like these integers that you want). So let‚Äôs use it: It is really simple. We created an array and stored it on  my_integer . But maybe you are asking: ‚ÄúHow can I get a value from this array?‚Äù Great question.  List  has a concept called  index . The first element gets the index 0 (zero). The second gets 1, and so on. You get the idea. To make it clearer, we can represent the array and each element with its index. I can draw it: Using the Python syntax, it‚Äôs also simple to understand: Imagine that you don‚Äôt want to store integers. You just want to store strings, like a list of your relatives‚Äô names. Mine would look something like this: It works the same way as integers. Nice. We just learned how  Lists  indices work. But I still need to show you how we can add an element to the  List  data structure (an item to a list). The most common method to add a new value to a  List  is  append . Let‚Äôs see how it works: append  is super simple. You just need to apply the element (eg. ‚Äú The Effective Engineer ‚Äù) as the  append  parameter. Well, enough about  Lists .  Let‚Äôs talk about another data structure. Now we know that  Lists  are indexed with integer numbers. But what if we don‚Äôt want to use integer numbers as indices? Some data structures that we can use are numeric, string, or other types of indices. Let‚Äôs learn about the  Dictionary  data structure.  Dictionary  is a collection of key-value pairs. Here‚Äôs what it looks like: The  key  is the index pointing to the   value . How do we access the  Dictionary   value ? You guessed it ‚Äî using the  key . Let‚Äôs try it: I created a  Dictionary  about me. My name, nickname, and nationality. Those attributes are the  Dictionary   keys . As we learned how to access the  List  using index, we also use indices ( keys  in the  Dictionary  context) to access the  value  stored in the  Dictionary . In the example, I printed a phrase about me using all the values stored in the  Dictionary . Pretty simple, right? Another cool thing about  Dictionary  is that we can use anything as the value. In the  Dictionary   I created, I want to add the  key  ‚Äúage‚Äù and my real integer age in it: Here we have a  key  (age)  value  (24) pair using string as the  key  and integer as the  value . As we did with  Lists , let‚Äôs learn how to add elements to a  Dictionary . The  key   pointing to a   value  is a big part of what  Dictionary  is. This is also true when we are talking about adding elements to it: We just need to assign a  value  to a  Dictionary   key . Nothing complicated here, right? As we learned in the  Python Basics , the  List  iteration is very simple. We  Python   developers commonly use  For  looping. Let‚Äôs do it: So for each book in the bookshelf, we ( can do everything with it ) print it. Pretty simple and intuitive. That‚Äôs Python. For a hash data structure, we can also use the  for  loop, but we apply the  key  : This is an example how to use it. For each  key  in the  dictionary  , we  print  the  key  and its corresponding  value . Another way to do it is to use the  iteritems  method. We did name the two parameters as  key  and  value , but it is not necessary. We can name them anything. Let‚Äôs see it: We can see we used attribute as a parameter for the  Dictionary   key , and it works properly. Great! Objects  are a representation of real world objects like cars, dogs, or bikes. The objects share two main characteristics:  data  and  behavior . Cars have  data,  like number of wheels, number of doors, and seating capacity They also exhibit  behavior : they can accelerate, stop, show how much fuel is left, and so many other things. We identify  data  as  attributes  and  behavior  as  methods  in object-oriented programming. Again: Data ‚Üí Attributes and Behavior ‚Üí Methods And a  Class  is the blueprint from which individual objects are created. In the real world, we often find many objects with the same type. Like cars. All the same make and model (and all have an engine, wheels, doors, and so on). Each car was built from the same set of blueprints and has the same components. Python, as an Object-Oriented programming language, has these concepts:  class  and  object . A class is a blueprint, a model for its objects. So again, a class it is just a model, or a way to define  attributes  and  behavior  (as we talked about in the theory section). As an example, a vehicle  class  has its own  attributes  that define what  objects  are vehicles. The number of wheels, type of tank, seating capacity, and maximum velocity are all attributes of a vehicle. With this in mind, let‚Äôs look at Python syntax for  classes : We define classes with a  class statement ‚Äî  and that‚Äôs it. Easy, isn‚Äôt it? Objects  are instances of a  class . We create an instance by naming the class. Here  car  is an  object  (or instance) of the  class   Vehicle . Remember that our vehicle  class  has four  attributes : number of wheels, type of tank, seating capacity, and maximum velocity. We set all these  attributes  when creating a vehicle  object . So here, we define our  class  to receive data when it initiates it: We use the  init   method . We call it a constructor method. So when we create the vehicle  object , we can define these  attributes . Imagine that we love the  Tesla Model S,  and we want to create this kind of  object . It has four wheels, runs on electric energy, has space for five seats, and the maximum velocity is 250km/hour (155 mph). Let‚Äôs create this  object: Four wheels + electric ‚Äútank type‚Äù + five seats + 250km/hour maximum speed. All attributes are set. But how can we access these attributes‚Äô values? We  send a message to the object asking about them . We call it a  method . It‚Äôs the  object‚Äôs behavior . Let‚Äôs implement it: This is an implementation of two methods:  number_of_wheels  and  set_number_of_wheels . We call it  getter  &  setter . Because the first gets the attribute value, and the second sets a new value for the attribute. In Python, we can do that using  @property  ( decorators ) to define  getters  and  setters . Let‚Äôs see it with code: And we can use these methods as attributes: This is slightly different than defining methods. The methods work as attributes. For example, when we set the new number of wheels, we don‚Äôt apply two as a parameter, but set the value 2 to  number_of_wheels . This is one way to write  pythonic   getter  and  setter  code. But we can also use methods for other things, like the ‚Äú make_noise ‚Äù method. Let‚Äôs see it: When we call this method, it just returns a string  ‚Äú VRRRRUUUUM. ‚Äù Encapsulation is a mechanism that restricts direct access to objects‚Äô data and methods. But at the same time, it facilitates operation on that data (objects‚Äô methods). ‚ÄúEncapsulation can be used to hide data members and members function. Under this definition, encapsulation means that the internal representation of an  object  is generally hidden from view outside of the object‚Äôs definition.‚Äù ‚Äî Wikipedia All internal representation of an object is hidden from the outside. Only the object can interact with its internal data. First, we need to understand how  public  and  non-public  instance variables and methods work. For a Python class, we can initialize a  public instance variable  within our constructor method. Let‚Äôs see this: Within the constructor method: Here we apply the  first_name  value as an argument to the  public instance variable . Within the class: Here, we do not need to apply the  first_name  as an argument, and all instance objects will have a  class attribute  initialized with  TK . Cool. We have now learned that we can use  public instance variables  and  class attributes . Another interesting thing about the  public  part is that we can manage the variable value. What do I mean by that? Our  object  can manage its variable value:  Get  and  Set  variable values. Keeping the  Person  class in mind, we want to set another value to its  first_name  variable: There we go. We just set another value ( kaio ) to the  first_name  instance variable and it updated the value. Simple as that. Since it‚Äôs a  public  variable, we can do that. We don‚Äôt use the term ‚Äúprivate‚Äù here, since no attribute is really private in Python (without a generally unnecessary amount of work). ‚Äî  PEP 8 As the  public instance variable  , we can define the  non-public instance variable  both within the constructor method or within the class. The syntax difference is: for  non-public instance variables  , use an underscore ( _ ) before the  variable  name. ‚Äú‚ÄòPrivate‚Äô instance variables that cannot be accessed except from inside an object don‚Äôt exist in Python. However, there is a convention that is followed by most Python code: a name prefixed with an underscore (e.g.  _spam ) should be treated as a non-public part of the API (whether it is a function, a method or a data member)‚Äù ‚Äî  Python Software Foundation Here‚Äôs an example: Did you see the  email  variable? This is how we define a  non-public variable  : We can access and update it.  Non-public variables  are just a convention and should be treated as a non-public part of the API. So we use a method that allows us to do it inside our class definition. Let‚Äôs implement two methods ( email  and  update_email ) to understand it: Now we can update and access  non-public variables  using those methods. Let‚Äôs see: With  public methods , we can also use them out of our class: Let‚Äôs test it: Great ‚Äî we can use it without any problem. But with  non-public methods  we aren‚Äôt able to do it. Let‚Äôs implement the same  Person  class, but now with a  show_age   non-public method  using an underscore ( _ ). And now, we‚Äôll try to call this  non-public method  with our object: We can access and update it.  Non-public methods  are just a convention and should be treated as a non-public part of the API. Here‚Äôs an example for how we can use it: Here we have a  _get_age   non-public method  and a  show_age   public method . The  show_age  can be used by our object (out of our class) and the  _get_age  only used inside our class definition (inside  show_age  method). But again: as a matter of convention. With encapsulation we can ensure that the internal representation of the object is hidden from the outside. Certain objects have some things in common: their behavior and characteristics. For example, I inherited some characteristics and behaviors from my father. I inherited his eyes and hair as characteristics, and his impatience and introversion as behaviors. In object-oriented programming, classes can inherit common characteristics (data) and behavior (methods) from another class. Let‚Äôs see another example and implement it in Python. Imagine a car. Number of wheels, seating capacity and maximum velocity are all attributes of a car. We can say that an   ElectricCar  class inherits these same attributes from the regular  Car  class. Our  Car  class implemented: Once initiated, we can use all  instance variables  created. Nice. In Python, we apply a  parent class  to the  child class  as a parameter. An  ElectricCar  class can inherit from our  Car  class. Simple as that. We don‚Äôt need to implement any other method, because this class already has it (inherited from  Car  class). Let‚Äôs prove it: Beautiful. We learned a lot of things about Python basics: Congrats! You completed this dense piece of content about Python. If you want a complete Python course, learn more real-world coding skills and build projects, try  One Month Python Bootcamp . See you there ‚ò∫ For more stories and posts about my journey learning & mastering programming, follow my publication  The Renaissance Developer . Have fun, keep learning, and always keep coding. I hope you liked this content. Support my work on Ko-Fi My  Twitter  &  Github . ‚ò∫"
Why Python is not the programming language of the future,hy Python is not the programming language of the futur,"It  took the programming community a couple of decades to appreciate Python. But since the early 2010‚Äôs, it has been booming ‚Äî and eventually surpassing C, C#, Java and JavaScript in popularity. But until when will that trend continue? When will Python eventually be replaced by other languages, and why? Putting an exact expiry date on Python would be so much speculation, it might as well pass as Science-Fiction. Instead, I will assess the virtues that are boosting Python‚Äôs popularity right now, and the weak points that will break it in the future. Python‚Äôs success is reflected in the  Stack Overflow trends , which measure the count of tags in posts on the platform. Given the size of StackOverflow, this is quite a good indicator for language popularity. While R has been plateauing over the last few years, and many other languages are on a steady decline, Python‚Äôs growth seems unstoppable. Almost 14% of all StackOverflow questions are tagged ‚Äúpython‚Äù, and the trend is going up. And there are several reasons for that. Python has been around since the nineties. That doesn‚Äôt only mean that it has had plenty of time to grow. It has also acquired a large and supportive community. So if you have any issue while you‚Äôre coding in Python, the odds are high that you‚Äôll be able to solve it with a single Google search. Simply because somebody will have already encountered your problem and written something helpful about it. It‚Äôs not only the fact that it has been around for decades, giving programmers the time to make brilliant tutorials. More than that, the syntax of Python is very human-readable. For a start, there‚Äôs no need to specify the data type. You just declare a variable; Python will understand from the context whether it‚Äôs an integer, a float value, a boolean or something else. This is a huge edge for beginners. If you‚Äôve ever had to program in C++, you know how frustrating it is your program won‚Äôt compile because you swapped a float for an integer. And if you‚Äôve ever had to read Python and C++ code side-by-side, you‚Äôll know how understandable Python is. Even though C++ was designed with English in mind, it‚Äôs a rather bumpy read compared to Python code. medium.com Since Python has been around for so long, developers have made a package for every purpose. These days, you can find a package for almost everything. Want to crunch numbers, vectors and matrices?  NumPy  is your guy.  Want to do calculations for tech and engineering? Use  SciPy .  Want to go big in data manipulation and analysis? Give  Pandas  a go. Want to start out with Artificial Intelligence? Why not use  Scikit-Learn . Whichever computational task you‚Äôre trying to manage, chances are that there is a Python package for it out there. This makes Python stay on top of recent developments, can be seen from the surge in Machine Learning over the past few years. Based on the previous elaborations, you could imagine that Python will stay on top of sh*t for ages to come. But like every technology, Python has its weaknesses. I will go through the most important flaws, one by one, and assess whether these are fatal or not. Python is slow. Like, really slow. On average, you‚Äôll need about 2‚Äì10 times longer to complete a task with Python than with any other language. There are  various reasons  for that. One of them is that it‚Äôs dynamically typed ‚Äî remember that you don‚Äôt need to specify data types like in other languages. This means that a lot of memory needs to be used, because the program needs to reserve enough space for each variable that it works in any case. And lots of memory usage translates to lots of computing time. Another reason is that Python can only execute one task at a time. This is a consequence of flexible datatypes ‚Äî Python needs to make sure each variable has only one datatype, and parallel processes could mess that up. In comparison, your average web browser can run a dozen different threads at once. And there are some other theories around, too. But at the end of the day, none of the speed issues matter. Computers and servers have gotten so cheap that we‚Äôre talking about fractions of seconds. And the end user doesn‚Äôt really care whether their app loads in 0.001 or 0.01 seconds. medium.com Originally, Python was  dynamically scoped . This basically means that, to evaluate an expression, a compiler first searches the current block and then successively all the calling functions. The problem with dynamic scoping is that every expression needs to be tested in every possible context ‚Äî which is tedious. That‚Äôs why most modern programming languages use static scoping. Python tried to transition to static scoping, but  messed it up . Usually, inner scopes ‚Äî for example functions within functions ‚Äî would be able to see  and  change outer scopes. In Python, inner scopes can only see outer scopes, but not change them. This leads to a lot of confusion. Despite all of the flexibility within Python, the usage of Lambdas is rather restrictive. Lambdas can only be expressions in Python, and not be statements. On the other hand, variable declarations and statements are always statements. This means that Lambdas cannot be used for them. This distinction between expressions and statements is rather arbitrary, and doesn‚Äôt occur in other languages. In Python, you use whitespaces and indentations to indicate different levels of code. This makes it optically appealing and intuitive to understand. Other languages, for example C++, rely more on braces and semicolons. While this might not be visually appealing and beginner-friendly, it makes the code a lot more maintainable. For bigger projects, this is a lot more useful. Newer languages like Haskell solve this problem: They rely on whitespaces, but offer an alternative syntax for those who wish to go without. As we‚Äôre witnessing the shift from desktop to smartphone, it‚Äôs clear that we need robust languages to build mobile software. But not many mobile apps are being developed with Python. That doesn‚Äôt mean that it can‚Äôt be done ‚Äî there is a Python package called Kivy for this purpose. But Python wasn‚Äôt made with mobile in mind. So even though it might produce passable results for basic tasks, your best bet is to use a language that was created for mobile app development. Some widely used programming frameworks for mobile include React Native, Flutter, Iconic, and Cordova. To be clear, laptops and desktop computers should be around for many years to come. But since mobile has long surpassed desktop traffic, it‚Äôs safe to say that learning Python is not enough to become a seasoned all-round developer. A Python script isn‚Äôt compiled first and then executed. Instead, it compiles every time you execute it, so any coding error manifests itself at runtime. This leads to poor performance, time consumption, and the need for a lot of tests. Like,  a lot  of tests. This is great for beginners since testing teaches them a lot. But for seasoned developers, having to debug a complex program in Python makes them go awry. This lack of performance is the biggest factor that sets a timestamp on Python. towardsdatascience.com There are a few new competitors on the market of programming languages: While there are other languages on the market, Rust, Go, and Julia are the ones that fix weak patches of Python. All of these languages excel in yet-to-come technologies, most notably in Artificial Intelligence. While their market share is still small, as reflected in the number of StackOverflow tags, the trend for all of them is clear: upwards. Given the ubiquitous popularity of Python at the moment, it will surely take half a decade, maybe even a whole, for any of these new languages to replace it. Which of the languages it will be ‚Äî Rust, Go, Julia, or a new language of the future ‚Äî is hard to say at this point. But given the performance issues that are fundamental in the architecture of Python, one will inevitably take its spot."
What exactly can you do with Python? Here are Python‚Äôs 3 main applications.,hat exactly can you do with Python? Here are Python‚Äôs 3 main applications,"If you‚Äôre thinking of learning Python ‚Äî or if you recently started learning it ‚Äî you may be asking yourself: ‚ÄúWhat exactly can I use Python for?‚Äù Well that‚Äôs a tricky question to answer, because there are so many applications for Python. But over time, I have observed that there are 3 main popular applications for Python: Let‚Äôs talk about each of them in turn. Web frameworks that are based on Python like  Django  and  Flask  have recently become very popular for web development. These web frameworks help you create server-side code (backend code) in Python. That‚Äôs the code that runs on your server, as opposed to on users‚Äô devices and browsers (front-end code). If you‚Äôre not familiar with the difference between backend code and front-end code, please see my footnote below. That‚Äôs because a web framework makes it easier to build common backend logic. This includes mapping different URLs to chunks of Python code, dealing with databases, and generating HTML files users see on their browsers. Django and Flask are two of the most popular Python web frameworks. I‚Äôd recommend using one of them if you‚Äôre just getting started. There‚Äôs an  excellent article  about this topic by Gareth Dwyer, so let me quote it here: <begin quote> Main contrasts: You should probably choose: </end quote> In other words, If you‚Äôre a beginner, Flask is probably a better choice because it has fewer components to deal with. Also, Flask is a better choice if you want more customization. On the other hand, if you‚Äôre looking to build something straight-forward, Django will probably let you get there faster. Now, if you‚Äôre looking to learn Django, I recommend the book called Django for Beginners. You can find it  here . You can also find the free sample chapters of that book  here . Okay, let‚Äôs go to the next topic! I think the best way to explain what machine learning is would be to give you a simple example. Let‚Äôs say you want to develop a program that automatically detects what‚Äôs in a picture. So, given this picture below (Picture 1), you want your program to recognize that it‚Äôs a dog. Given this other one below (Picture 2), you want your program to recognize that it‚Äôs a table. You might say, well, I can just write some code to do that. For example, maybe if there are a lot of light brown pixels in the picture, then we can say that it‚Äôs a dog. Or maybe, you can figure out how to detect edges in a picture. Then, you might say, if there are many straight edges, then it‚Äôs a table. However, this kind of approach gets tricky pretty quickly. What if there‚Äôs a white dog in the picture with no brown hair? What if the picture shows only the round parts of the table? This is where machine learning comes in. Machine learning typically implements an algorithm that automatically detects a pattern in the given input. You can give, say, 1,000 pictures of a dog and 1,000 pictures of a table to a machine learning algorithm. Then, it will learn the difference between a dog and a table. When you give it a new picture of either a dog or a table, it will be able to recognize which one it is. I think this is somewhat similar to how a baby learns new things. How does a baby learn that one thing looks like a dog and another a table? Probably from a bunch of examples. You probably don‚Äôt explicitly tell a baby, ‚ÄúIf something is furry and has light brown hair, then it‚Äôs probably a dog.‚Äù You would probably just say, ‚ÄúThat‚Äôs a dog. This is also a dog. And this one is a table. That one is also a table.‚Äù Machine learning algorithms work much the same way. You can apply the same idea to: among other applications. Popular machine learning algorithms you might have heard about include: You can use any of the above algorithms to solve the picture-labeling problem I explained earlier. There are popular machine learning libraries and frameworks for Python. Two of the most popular ones are  scikit-learn  and  TensorFlow . If you‚Äôre just getting started with a machine learning project, I would recommend that you first start with scikit-learn. If you start running into efficiency issues, then I would start looking into TensorFlow. To learn machine learning fundamentals, I would recommend either  Stanford‚Äôs  or  Caltech‚Äôs  machine learning course. Please note that you need basic knowledge of calculus and linear algebra to understand some of the materials in those courses. Then, I would practice what you‚Äôve learned from one of those courses with  Kaggle . It‚Äôs a website where people compete to build the best machine learning algorithm for a given problem. They have nice tutorials for beginners, too. To help you understand what these might look like, let me give you a simple example here. Let‚Äôs say you‚Äôre working for a company that sells some products online. Then, as a data analyst, you might draw a bar graph like this. From this graph, we can tell that men bought over 400 units of this product and women bought about 350 units of this product this particular Sunday. As a data analyst, you might come up with a few possible explanations for this difference. One obvious possible explanation is that this product is more popular with men than with women. Another possible explanation might be that the sample size is too small and this difference was caused just by chance. And yet another possible explanation might be that men tend to buy this product more only on Sunday for some reason. To understand which of these explanations is correct, you might draw another graph like this one. Instead of showing the data for Sunday only, we‚Äôre looking at the data for a full week. As you can see, from this graph, we can see that this difference is pretty consistent over different days. From this little analysis, you might conclude that the most convincing explanation for this difference is that this product is simply more popular with men than with women. On the other hand, what if you see a graph like this one instead? Then, what explains the difference on Sunday? You might say, perhaps men tend to buy more of this product only on Sunday for some reason. Or, perhaps it was just a coincidence that men bought more of it on Sunday. So, this is a simplified example of what data analysis might look like in the real world. The data analysis work I did when I was working at Google and Microsoft was very similar to this example ‚Äî only more complex. I actually used Python at Google for this kind of analysis, while I used JavaScript at Microsoft. I used SQL at both of those companies to pull data from our databases. Then, I would use either Python and Matplotlib (at Google) or JavaScript and D3.js (at Microsoft) to visualize and analyze this data. One of the most popular libraries for data visualization is  Matplotlib . It‚Äôs a good library to get started with because: How should I learn data analysis / visualization with Python? You should first learn the fundamentals of data analysis and visualization. When I looked for good resources for this online, I couldn‚Äôt find any. So, I ended up making a YouTube video on this topic: I also ended up making a  full course on this topic on Pluralsight , which you can take for free by signing up to their 10-day free trial. I‚Äôd recommend both of them. After learning the fundamentals of data analysis and visualization, learning fundamentals of statistics from websites like Coursera and Khan Academy will be helpful, as well. Scripting usually refers to writing small programs that are designed to automate simple tasks. So, let me give you an example from my personal experience here. I used to work at a small startup in Japan where we had an email support system. It was a system for us to respond to questions customers sent us via email. When I was working there, I had the task of counting the numbers of emails containing certain keywords so we could analyze the emails we received. We could have done it manually, but instead, I wrote a simple program / simple script to automate this task. Actually, we used Ruby for this back then, but Python is also a good language for this kind of task. Python is suited for this type of task mainly because it has relatively simple syntax and is easy to write. It‚Äôs also quick to write something small with it and test it. I‚Äôm not an expert on embedded applications, but I know that Python works with Rasberry Pi. It seems like a popular application among hardware hobbyists. You could use the library called PyGame to develop games, but it‚Äôs not the most popular gaming engine out there. You could use it to build a hobby project, but I personally wouldn‚Äôt choose it if you‚Äôre serious about game development. Rather, I would recommend getting started with Unity with C#, which is one of the most popular gaming engines. It allows you to build a game for many platforms, including Mac, Windows, iOS, and Android. You could make one with Python using Tkinter, but it doesn‚Äôt seem like the most popular choice either. Instead, it seems like languages like  Java, C#, and C++  are more popular for this. Recently, some companies have started using JavaScript to create Desktop applications, too. For example, Slack‚Äôs desktop app was built with something called Electron . It allows you to build desktop applications with JavaScript. Personally, if I was building a desktop application, I would go with a JavaScript option. It allows you to reuse some of the code from a web version if you have it. However, I‚Äôm not an expert on desktop applications either, so please let me know in a comment if you disagree or agree with me on this. I would recommend Python 3 since it‚Äôs more modern and it‚Äôs a more popular option at this point. Let‚Äôs say you want to make something like Instagram. Then, you‚Äôd need to create front-end code for each type of device you want to support. You might use, for example: Each set of code will run on each type of device / browser. This will be the set of code that determines what the layout of the app will be like, what the buttons should look like when you click them, etc. However, you will still need the ability to store users‚Äô info and photos. You will want to store them on your server and not just on your users‚Äô devices so each user‚Äôs followers can view his/her photos. This is where the backend code / server-side code comes in. You‚Äôll need to write some backend code to do things like: So, this is the difference between backend code and front-end code. By the way, Python is not the only good choice for writing backend / server-side code. There are many other popular choices, including Node.js, which is based on JavaScript. I have a programming education YouTube channel called  CS Dojo  with 440,000+ subscribers, where I produce more content like this article. For example, you might like these videos:"
How to build your own Neural Network from scratch in Python,ow to build your own Neural Network from scratch in Pytho,"Update : When I wrote this article a year ago, I did not expect it to be  this  popular. Since then, this article has been viewed more than 450,000 times, with more than 30,000 claps. It has also made it to the front page of Google, and it is among the first few search results for ‚Äò Neural Network ‚Äô. Many of you have reached out to me, and I am deeply humbled by the impact of this article on your learning journey. This article also caught the eye of the editors at Packt Publishing. Shortly after this article was published, I was offered to be the sole author of the book  Neural Network Projects with Python .  Today, I am happy to share with you that my book has been published! The book is a continuation of this article, and it covers end-to-end implementation of neural network projects in areas such as face recognition, sentiment analysis, noise removal etc. Every chapter features a unique neural network architecture, including Convolutional Neural Networks, Long Short-Term Memory Nets and Siamese Neural Networks. If you‚Äôre looking to create a strong machine learning portfolio with deep learning projects, do consider getting the book! You can get the book from Amazon:  Neural Network Projects with Python Motivation:  As part of my personal journey to gain a better understanding of Deep Learning, I‚Äôve decided to build a Neural Network from scratch without a deep learning library like TensorFlow. I believe that understanding the inner workings of a Neural Network is important to any aspiring Data Scientist. This article contains what I‚Äôve learned, and hopefully it‚Äôll be useful for you as well! Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output. Neural Networks consist of the following components The diagram below shows the architecture of a 2-layer Neural Network ( note that the input layer is typically excluded when counting the number of layers in a Neural Network ) Creating a Neural Network class in Python is easy. Training the Neural Network The output  ≈∑  of a simple 2-layer Neural Network is: You might notice that in the equation above, the weights  W  and the biases  b  are the only variables that affects the output  ≈∑. Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as  training the Neural Network. Each iteration of the training process consists of the following steps: The sequential graph below illustrates the process. As we‚Äôve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is: Let‚Äôs add a feedforward function in our python code to do exactly that. Note that for simplicity, we have assumed the biases to be 0. However, we still need a way to evaluate the ‚Äúgoodness‚Äù of our predictions (i.e. how far off are our predictions)? The  Loss Function  allows us to do exactly that. There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we‚Äôll use a simple  sum-of-sqaures error  as our loss function. That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference. Our goal in training is to find the best set of weights and biases that minimizes the loss function. Now that we‚Äôve measured the error of our prediction (loss), we need to find a way to  propagate  the error back, and to update our weights and biases. In order to know the appropriate amount to adjust the weights and biases by, we need to know the  derivative of the loss function with respect to the weights and biases . Recall from calculus that the derivative of a function is simply the slope of the function. If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as  gradient descent . However, we can‚Äôt directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the  chain rule  to help us calculate it. Phew! That was ugly but it allows us to get what we needed ‚Äî the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly. Now that we have that, let‚Äôs add the backpropagation function into our python code. For a deeper understanding of the application of calculus and the chain rule in backpropagation, I strongly recommend this tutorial by 3Blue1Brown. Now that we have our complete python code for doing feedforward and backpropagation, let‚Äôs apply our Neural Network on an example and see how well it does. Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn‚Äôt exactly trivial for us to work out the weights just by inspection alone. Let‚Äôs train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss  monotonically decreasing towards a minimum.  This is consistent with the gradient descent algorithm that we‚Äôve discussed earlier. Let‚Äôs look at the final prediction (output) from the Neural Network after 1500 iterations. We did it! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values. Note that there‚Äôs a slight difference between the predictions and the actual values. This is desirable, as it prevents  overfitting  and allows the Neural Network to  generalize  better to unseen data. Fortunately for us, our journey isn‚Äôt over. There‚Äôs still  much  to learn about Neural Networks and Deep Learning. For example: I‚Äôll be writing more on these topics soon, so do follow me on Medium and keep and eye out for them! I‚Äôve certainly learnt a lot writing my own Neural Network from scratch. Although Deep Learning libraries such as TensorFlow and Keras makes it easy to build deep nets without fully understanding the inner workings of a Neural Network, I find that it‚Äôs beneficial for aspiring data scientist to gain a deeper understanding of Neural Networks. This exercise has been a great investment of my time, and I hope that it‚Äôll be useful for you as well!"
Building a Simple Chatbot from Scratch in Python (using NLTK),uilding a Simple Chatbot from Scratch in Python (using NLTK,"I am sure you‚Äôve heard about  Duolingo : a popular language-learning app, which gamifies practicing a new language. It is pretty popular due to its innovative styles of teaching a foreign language. The concept is simple: five to ten minutes of interactive training a day is enough to learn a language."
The Next Level of Data Visualization in Python,he Next Level of Data Visualization in Pytho,"The sunk-cost fallacy is one of many  harmful cognitive biases  to which humans fall prey. It  refers to our tendency  to continue to devote time and resources to a lost cause because we have already spent ‚Äî sunk ‚Äî so much time in the pursuit. The sunk-cost fallacy applies to staying in bad jobs longer than we should, slaving away at a project even when it‚Äôs clear it won‚Äôt work, and yes, continuing to use a tedious, outdated plotting library ‚Äî matplotlib ‚Äî when more efficient, interactive, and better-looking alternatives exist. Over the past few months, I‚Äôve realized the only reason I use  matplotlib  is the hundreds of hours I‚Äôve sunk into learning the  convoluted syntax . This complication leads to hours of frustration on StackOverflow figuring out how to  format dates  or  add a second y-axis . Fortunately, this is a great time for Python plotting, and after exploring  the options , a clear winner ‚Äî in terms of ease-of-use, documentation, and functionality ‚Äî is the  plotly Python library.  In this article, we‚Äôll dive right into  plotly , learning how to make better plots in less time ‚Äî often with one line of code. All of the code for this article is  available on GitHub . The charts are all interactive and can be viewed on  NBViewer here . The  plotly   Python package is an open-source library built on  plotly.js  which  in turn is built on  d3.js .  We‚Äôll be using a wrapper on plotly called  cufflinks  designed to work with Pandas dataframes. So, our entire stack is cufflinks > plotly > plotly.js > d3.js which means we get the efficiency of coding in Python with the incredible  interactive graphics capabilities of d3. ( Plotly itself  is a graphics company with several products and open-source tools. The Python library is free to use, and we can make unlimited charts in offline mode plus up to 25 charts in online mode to  share with the world .) All the work in this article was done in a Jupyter Notebook with plotly + cufflinks running in offline mode. After installing plotly and cufflinks with  pip install cufflinks plotly  import the following to run in Jupyter: Single variable ‚Äî univariate ‚Äî plots are a standard way to start an analysis and the histogram is a go-to plot ( although it has some issues ) for graphing a distribution. Here, using my Medium article statistics (you can see  how to get your own stats here  or use  mine here ) let‚Äôs make an interactive histogram of the number of claps for articles (  df  is a standard Pandas dataframe): For those used to  matplotlib , all we have to do is add one more letter (  iplot  instead of  plot ) and we get a much better-looking and interactive chart! We can click on the data to get more details, zoom into sections of the plot, and as we‚Äôll see later, select different categories to highlight. If we want to plot overlaid histograms, that‚Äôs just as simple: With a little bit of  pandas  manipulation, we can do a barplot: s we saw, we can combine the  power of pandas  with plotly + cufflinks. For a boxplot of the fans per story by publication, we use a  pivot  and then plot: The benefits of interactivity are that we can explore and subset the data as we like. There‚Äôs a lot of information in a boxplot, and without the ability to see the numbers, we‚Äôll miss most of it! The scatterplot is the heart of most analyses. It allows us to see the evolution of a variable over time or the relationship between two (or more) variables. A considerable portion of real-world data has a time element. Luckily, plotly + cufflinks was designed with time-series visualizations in mind. Let‚Äôs make a dataframe of my TDS articles and look at how the trends have changed. Here we are doing quite a few different things all in one line: For more information, we can also add in text annotations quite easily: For a two-variable scatter plot colored by a third categorical variable we use: Let‚Äôs get a little more sophisticated by using a log axis ‚Äî specified as a plotly layout ‚Äî (see the  Plotly documentation  for the layout specifics) and sizing the bubbles by a numeric variable: With a little more work ( see notebook for details ), we can even put four variables ( this is not advised ) on one graph! As before, we can combine pandas with plotly+cufflinks for useful plots See the notebook  or the documentation  for more examples of added functionality. We can add in text annotations, reference lines, and best-fit lines to our plots with a single line of code, and still with all the interaction. Now we‚Äôll get into a few plots that you probably won‚Äôt use all that often, but which can be quite impressive. We‚Äôll use the  plotly  figure_factory , to keep even these incredible plots to one line. When we want to explore relationships among many variables, a  scattermatrix  (also called a splom) is a great option: Even this plot is completely interactive allowing us to explore the data. To visualize the correlations between numeric variables, we calculate the correlations and then make an annotated heatmap: The list of plots goes on and on. Cufflinks also has several themes we can use to get completely different styling with no effort. For example, below we have a ratio plot in the ‚Äúspace‚Äù theme and a spread plot in ‚Äúggplot‚Äù: We also get 3D plots (surface and bubble): For those  who are so inclined , you can even make a pie chart: When you make these plots in the notebook, you‚Äôll notice a small link on the lower right-hand side on the graph that says ‚ÄúExport to plot.ly‚Äù. If you click that link, you are then taken to the  chart studio  where you can touch up your plot for a final presentation. You can add annotations, specify the colors, and generally clean everything up for a great figure. Then, you can publish your figure online so anyone can find it with the link. Below are two charts I touched up in Chart Studio: With everything mentioned here, we are still not exploring the full capabilities of the library! I‚Äôd encourage you to check out both the plotly and the cufflinks documentation for more incredible graphics. The worst part about the sunk cost fallacy is you only realize how much time you‚Äôve wasted after you‚Äôve quit the endeavor. Fortunately, now that I‚Äôve made the mistake of sticking with  matploblib  for too long, you don‚Äôt have to! When thinking about plotting libraries, there are a few things we want: As of right now, the best option for doing all of these in  Python is plotly . Plotly allows us to make visualizations quickly and helps us get better insight into our data through interactivity. Also, let‚Äôs admit it, plotting should be one of the most enjoyable parts of data science! With other libraries, plotting turned into a tedious task, but with plotly, there is again joy in making a great figure! Now that it‚Äôs 2019, it is time to upgrade your Python plotting library for better efficiency, functionality, and aesthetics in your data science visualizations. As always, I welcome feedback and constructive criticism. I can be reached on Twitter  @koehrsen_will."
Bye-bye Python. Hello Julia!,ye-bye Python. Hello Julia,"D on‚Äôt get me wrong. Python‚Äôs popularity is still backed by a rock-solid community of computer scientists, data scientists and AI specialists. But if you‚Äôve ever been at a dinner table with these people, you also know how much they rant about the weaknesses of Python. From being slow to requiring excessive testing, to producing runtime errors despite prior testing ‚Äî there‚Äôs enough to be pissed off about. Which is why more and more programmers are adopting other languages ‚Äî the top players being Julia, Go, and Rust. Julia is great for mathematical and technical tasks, while Go is awesome for modular programs, and Rust is the top choice for systems programming. Since data scientists and AI specialists deal with lots of mathematical problems, Julia is the winner for them. And even upon critical scrutiny, Julia has upsides that Python can‚Äôt beat. towardsdatascience.com When people create a new programming language, they do so because they want to keep the good features of old languages and fix the bad ones. In this sense, Guido van Rossum created Python in the late 1980s to improve ABC. The latter was  too perfect  for a programming language ‚Äî while its rigidity made it easy to teach, it was hard to use in real life. In contrast, Python is quite pragmatic. You can see this in the  Zen of Python , which reflects the intention that the creators have: Python still kept the good features of ABC: Readability, simplicity, and beginner-friendliness for example. But Python is far more robust and adapted to real life than ABC ever was. In the same sense, the creators of Julia want to keep the good parts of other languages and ditch the bad ones. But Julia is a lot more ambitious: instead of replacing one language, it wants to beat them all. This is how  Julia‚Äôs creators  say it: Julia wants to blend all upsides that currently exist, and not trade them off for the downsides in other languages. And even though Julia is a young language, it has already achieved a lot of the goals that the creators set. Julia can be used for everything from simple machine learning applications to enormous supercomputer simulations. To some extent, Python can do this, too ‚Äî but Python somehow grew into the job. In contrast,  Julia was built  precisely for this stuff. From the bottom up. Julia‚Äôs creators wanted to make a language that is as fast as C ‚Äî but what they created is  even faster . Even though Python has become easier to speed up in recent years, its performance is still a far cry from what Julia can do. In 2017, Julia even joined the  Petaflop Club  ‚Äî the small club of languages who can exceed speeds of one petaflop per second at peak performance. Apart from Julia, only C, C++ and Fortran are  in the club  right now. towardsdatascience.com With its more than 30 years of age, Python has an enormous and supportive community. There is hardly a Python-related question that you can‚Äôt get answered within one Google search. In contrast, the Julia community is pretty tiny. While this means that you might need to dig a bit further to find an answer, you might link up with the same people again and again. And this can turn into programmer-relationships that are beyond value. You don‚Äôt even need to know a single Julia-command to code in Julia. Not only can you use Python and C code within Julia. You can even use  Julia within Python ! Needless to say, this makes it extremely easy to patch up the weaknesses of your Python code. Or to stay productive while you‚Äôre still getting to know Julia. This is one of the strongest points of Python ‚Äî its zillion well-maintained libraries. Julia doesn‚Äôt have many libraries, and users have complained that they‚Äôre not amazingly maintained (yet). But when you consider that Julia is a very young language with a limited amount of resources, the number of libraries that they already have is pretty impressive. Apart from the fact that Julia‚Äôs amount of libraries is growing, it can also interface with libraries from C and Fortran to handle plots, for example. Python is 100% dynamically typed. This means that the program decides at runtime whether a variable is a float or an integer, for example. While this is extremely beginner-friendly, it also introduces a whole host of possible bugs. This means that you need to test Python code in all possible scenarios ‚Äî which is quite a dumb task that takes a lot of time. Since the Julia-creators also wanted it to be easy to learn, Julia fully supports dynamical typing. But in contrast to Python, you can introduce static types if you like ‚Äî in the way they are present in C or Fortran, for example. This can save you a ton of time: Instead of finding  excuses for not testing  your code, you can specify the type wherever it makes sense. towardsdatascience.com While all these things sound pretty great, it‚Äôs important to keep in mind that Julia is still tiny compared to Python. One pretty good metric is the number of questions on StackOverflow: At this point in time, Python is tagged about twenty more often than Julia! This doesn‚Äôt mean that Julia is unpopular ‚Äî rather, it‚Äôs naturally taking some time to get adopted by programmers. Think about it ‚Äî would you really want to write your whole code in a different language? No, you‚Äôd rather try a new language in some future project. This creates a time lag that every programming language faces between its release and its adoption. But if you adopt it now ‚Äî which is easy because Julia allows an enormous amount of language conversion ‚Äî you‚Äôre investing in the future. As more and more people adopt Julia, you‚Äôll already have gained enough experience to answer their questions. Also, your code will be more durable as more and more Python code is replaced by Julia. Forty years ago, artificial intelligence was nothing but a niche phenomenon. The industry and investors didn‚Äôt believe in it, and many technologies were clunky and hard to use. But those who learned it back then are the giants of today ‚Äî those that are so high in demand that  their salary  matches that of an NFL player. Similarly, Julia is still very niche now. But when it grows, the big winners will be those who adopted it early. I‚Äôm not saying that you‚Äôre guaranteed to make a shitload of money in ten years if you adopt Julia now. But you‚Äôre increasing your chances. Think about it: Most programmers out there have Python on their CV. And in the next few years, we‚Äôll see even more Python programmers on the job market. But if the demand of enterprises for Python slows, the perspectives for Python programmers are going to go down. Slowly at first, but inevitably. On the other hand, you have a real edge if you can put Julia on your CV. Because let‚Äôs be honest, what distinguishes you from any other Pythonista out there? Not much. But there won‚Äôt be that many Julia-programmers out there, even in three years‚Äô time. With Julia-skills, not only are you showing that you have interests beyond the job requirements. You‚Äôre also demonstrating that you‚Äôre eager to learn and that you have a broader sense of what it means to be a programmer. In other words, you‚Äôre fit for the job. You ‚Äî and the other Julia programmers ‚Äî are future rockstars, and you know it. Or, as  Julia‚Äôs creators  said it in 2012: Python is still insanely popular. But if you learn Julia now, that could be your golden ticket later on. In this sense: Bye-bye Python. Hello Julia! Edit: I‚Äôve given a talk on Julia vs. Python! It was hosted by  Hatchpad , and the video is  here ."
Turn Python Scripts into Beautiful ML Tools,urn Python Scripts into Beautiful ML Tool,"In my experience, every nontrivial machine learning project is eventually stitched together with bug-ridden and unmaintainable internal tools. These tools ‚Äî often a patchwork of Jupyter Notebooks and Flask apps ‚Äî are difficult to deploy, require reasoning about client-server architecture, and don‚Äôt integrate well with machine learning constructs like Tensorflow GPU sessions. I saw this first at Carnegie Mellon, then at Berkeley, Google X, and finally while building autonomous robots at Zoox. These tools were often born as little Jupyter notebooks: the sensor calibration tool, the simulation comparison app, the LIDAR alignment app, the scenario replay tool, and so on. As a tool grew in importance, project managers stepped in. Processes sprouted. Requirements flowered. These solo projects gestated into scripts, and matured into gangly maintenance nightmares. When a tool became crucial, we  called in the tools team . They wrote fluent Vue and React. They blinged their laptops with stickers about declarative frameworks. They had a  design process : Which was awesome. But these tools all needed new features, like weekly. And the tools team was supporting ten other projects. They would say, ‚Äúwe‚Äôll update your tool again in two months.‚Äù So we were back to building our own tools, deploying Flask apps, writing HTML, CSS, and JavaScript, and trying to version control everything from notebooks to stylesheets. So my old Google X friend, Thiago Teixeira, and I began thinking about the following question:  What if we could make building tools as easy as writing Python scripts? We wanted machine learning engineers to be able to create beautiful apps without needing a tools team. These internal tools should arise as a natural byproduct of the ML workflow. Writing such tools should  feel  like training a neural net or performing an ad-hoc analysis in Jupyter! At the same time, we wanted to preserve all of the flexibility of a powerful app framework. We wanted to create beautiful, performant tools that engineers could show off. Basically, we wanted this: With an amazing beta community including engineers from Uber, Twitter, Stitch Fix, and Dropbox, we worked for a year to create  Streamlit , a  completely free and open source  app framework for ML engineers. With each prototype, the core principles of Streamlit became simpler and purer. They are: #1: Embrace Python scripting.  Streamlit apps are really just scripts that run from top to bottom. There‚Äôs no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. For example, this is how you write to the screen: #2: Treat widgets as variables.  There are  no callbacks in Streamlit ! Every interaction simply reruns the script from top to bottom. This approach leads to really clean code: #3: Reuse data and computation.  What if you download lots of data or perform complex computation? The key is to  safely reuse  information across runs. Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default, data store that lets Streamlit apps safely and effortlessly reuse information. For example, this code  downloads data only once  from the  Udacity Self-driving car project , yielding a simple, fast app: In short, Streamlit works like this: Or in pictures: If this sounds intriguing, you can try it right now! Just run: This will automatically pop open a web browser pointing to your local Streamlit app. If not, just click the link. Ok. Are you back from playing with fractals? Those can be mesmerizing. The simplicity of these ideas does not prevent you from creating incredibly rich and useful apps with Streamlit. During my time at Zoox and Google X, I watched as self-driving car projects ballooned into gigabytes of visual data, which needed to be searched and understood, including running models on images to compare performance. Every self-driving car project I‚Äôve seen eventually has had entire teams working on this tooling. Building such a tool in Streamlit is easy.  This Streamlit demo  lets you perform semantic search across the entire  Udacity self-driving car photo dataset , visualize human-annotated ground truth labels, and  run a complete neural net ( YOLO ) in real time  from within the app [1]. The whole app is a completely self-contained, 300-line Python script, most of which is machine learning code. In fact, there are  only 23 Streamlit calls in the whole app . You can run it yourself right now! As we worked with machine learning teams on their own projects, we came to realize that these simple ideas yield a number of important benefits: Streamlit apps are pure Python files.  So you can use your favorite editor and debugger with Streamlit. Pure Python scripts work seamlessly with Git  and other source control software, including commits, pull requests, issues, and comments. Because Streamlit‚Äôs underlying language is pure Python, you get all the benefits of these amazing collaboration tools for free üéâ. Streamlit provides an immediate-mode live coding environment.  Just click  Always rerun  when Streamlit detects a source file change. Caching simplifies setting up computation pipelines.  Amazingly, chaining cached functions automatically creates efficient computation pipelines! Consider  this code  adapted from our  Udacity demo : Basically, the pipeline is load_metadata ‚Üí create_summary. Every time the script is run  Streamlit only recomputes whatever subset of the pipeline is required to get the right answer . Cool! Streamlit is built for GPUs.  Streamlit allows direct access to machine-level primitives like TensorFlow and PyTorch and complements these libraries. For example in this demo, Streamlit‚Äôs cache stores the entire  NVIDIA celebrity face GAN  [2]. This approach enables nearly instantaneous inference as the user updates sliders. Streamlit is a free and open-source library rather than a proprietary web app . You can serve Streamlit apps on-prem without contacting us. You can even run Streamlit locally on a laptop without an Internet connection! Furthermore, existing projects can adopt Streamlit incrementally. This just scratches the surface of what you can do with Streamlit. One of the most exciting aspects of Streamlit is how these primitives can be easily composed into complex apps that look like scripts. There‚Äôs a lot more we could say about how our architecture works and the features we have planned, but we‚Äôll save that for future posts. We‚Äôre excited to finally share Streamlit with the community today and see what you all build with it. We hope that you‚Äôll find it easy and delightful to turn your Python scripts into beautiful ML apps. Thanks to Amanda Kelly, Thiago Teixeira, TC Ricks, Seth Weidman, Regan Carey, Beverly Treuille, Genevi√®ve Wachtell, and Barney Pell for their helpful input on this article. References: [1] J. Redmon and A. Farhadi,  YOLOv3: An Incremental Improvement  (2018), arXiv. [2] T. Karras, T. Aila, S. Laine, and J. Lehtinen,  Progressive Growing of GANs for Improved Quality, Stability, and Variation  (2018), ICLR. [3] S. Guan,  Controlled image synthesis and editing using a novel TL-GAN model  (2018), Insight Data Science Blog."
A Complete Machine Learning Project Walk-Through in Python: Part One, Complete Machine Learning Project Walk-Through in Python: Part On,"Reading through a data science book or taking a course, it can feel like you have the individual pieces, but don‚Äôt quite know how to put them together. Taking the next step and solving a complete machine learning problem can be daunting, but preserving and completing a first project will give you the confidence to tackle any data science problem. This series of articles will walk through a complete machine learning solution with a real-world dataset to let you see how all the pieces come together. We‚Äôll follow the general machine learning workflow step-by-step: Along the way, we‚Äôll see how each step flows into the next and how to specifically implement each part in Python. The  complete project  is available on GitHub, with the  first notebook here.  This first article will cover steps 1‚Äì3 with the rest addressed in subsequent posts. (As a note, this problem was originally given to me as an ‚Äúassignment‚Äù for a job screen at a start-up. After completing the work, I was offered the job, but then the CTO of the company quit and they weren‚Äôt able to bring on any new employees. I guess that‚Äôs how things go on the start-up scene!) The first step before we get coding is to understand the problem we are trying to solve and the available data. In this project, we will work with  publicly available building energy data  from New York City. The objective is to use the energy data to build a model that can predict the Energy Star Score of a building and interpret the results to find the factors which influence the score. The data includes the Energy Star Score, which makes this a supervised regression machine learning task: We want to develop a model that is both  accurate  ‚Äî it can predict the Energy Star Score close to the true value ‚Äî and  interpretable  ‚Äî we can understand the model predictions. Once we know the goal, we can use it to guide our decisions as we dig into the data and build models. Contrary to what most data science courses would have you believe, not every dataset is a perfectly curated group of observations with no missing values or anomalies (looking at you  mtcars  and  iris  datasets). Real-world data is messy which means we need to  clean and wrangle  it into an acceptable format before we can even start the analysis. Data cleaning is an un-glamorous, but necessary part of most actual data science problems. First, we can load in the data as a Pandas  DataFrame  and take a look: This is a subset of the full data which contains 60 columns. Already, we can see a couple issues: first, we know that we want to predict the  ENERGY STAR Score  but we don‚Äôt know what any of the columns mean. While this isn‚Äôt necessarily an issue ‚Äî we can often make an accurate model without any knowledge of the variables ‚Äî we want to focus on interpretability, and it might be important to understand at least some of the columns. When I originally got the assignment from the start-up, I didn‚Äôt want to ask what all the column names meant, so I looked at the name of the file, and decided to search for ‚ÄúLocal Law 84‚Äù. That led me to  this page  which explains this is an NYC law requiring all buildings of a certain size to report their energy use. More searching brought me to  all the definitions of the columns.  Maybe looking at a file name is an obvious place to start, but for me this was a reminder to go slow so you don‚Äôt miss anything important! We don‚Äôt need to study all of the columns, but we should at least understand the Energy Star Score, which is described as: A 1-to-100 percentile ranking based on self-reported energy usage for the reporting year. The  Energy Star score  is a relative measure used for comparing the energy efficiency of buildings. That clears up the first problem, but the second issue is that missing values are encoded as ‚ÄúNot Available‚Äù. This is a string in Python which means that even the columns with numbers will be stored as  object  datatypes because Pandas converts a column with any strings into a column of all strings. We can see the datatypes of the columns using the  dataframe.info() method: Sure enough, some of the columns that clearly contain numbers (such as ft¬≤), are stored as objects. We can‚Äôt do numerical analysis on strings, so these will have to be converted to number (specifically  float ) data types! Here‚Äôs a little Python code that replaces all the ‚ÄúNot Available‚Äù entries with not a number (  np.nan ), which can be interpreted as numbers, and then converts the relevant columns to the  float  datatype: Once the correct columns are numbers, we can start to investigate the data. In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let‚Äôs get a sense of how many missing values are in each column (see the  notebook for code ). (To create this table, I used a function from this  Stack Overflow Forum ). While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem ( here is a discussion ), and for this project, we will remove any columns with more than 50% missing values. At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the  definition of extreme outliers : (For the code to remove the columns and the anomalies, see the notebook). At the end of the data cleaning and anomaly removal process, we are left with over 11,000 buildings and 49 features. Now that the tedious ‚Äî but necessary ‚Äî step of data cleaning is complete, we can move on to exploring our data!  Exploratory Data Analysis  (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. In short, the goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find interesting parts of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use. The goal is to predict the Energy Star Score (renamed to  score  in our data) so a reasonable place to start is examining the distribution of this variable. A histogram is a simple yet effective way to visualize the distribution of a single variable and is easy to make using  matplotlib . This looks quite suspicious! The Energy Star score is a percentile rank, which means we would expect to see a uniform distribution, with each score assigned to the same number of buildings. However, a disproportionate number of buildings have either the highest, 100, or the lowest, 1, score (higher is better for the Energy Star score). If we go back to the definition of the score, we see that it is based on ‚Äúself-reported energy usage‚Äù which might explain the very high scores. Asking building owners to report their own energy usage is like asking students to report their own scores on a test! As a result, this probably is not the most objective measure of a building‚Äôs energy efficiency. If we had an unlimited amount of time, we might want to investigate why so many buildings have very high and very low scores which we could by selecting these buildings and seeing what they have in common. However, our objective is only to predict the score and not to devise a better method of scoring buildings! We can make a note in our report that the scores have a suspect distribution, but our main focus in on predicting the score. A major part of EDA is searching for relationships between the features and the target. Variables that are correlated with the target are useful to a model because they can be used to predict the target. One way to examine the effect of a categorical variable (which takes on only a limited set of values) on the target is through a density plot using the  seaborn  library. A  density plot can be thought of as a smoothed histogram  because it shows the distribution of a single variable. We can color a density plot by class to see how a categorical variable changes the distribution. The following code makes a density plot of the Energy Star Score colored by the the type of building (limited to building types with more than 100 data points): We can see that the building type has a significant impact on the Energy Star Score. Office buildings tend to have a higher score while Hotels have a lower score. This tells us that we should include the building type in our modeling because it does have an impact on the target. As a categorical variable, we will have to one-hot encode the building type. A similar plot can be used to show the Energy Star Score by borough: The borough does not seem to have as large of an impact on the score as the building type. Nonetheless, we might want to include it in our model because there are slight differences between the boroughs. To quantify relationships between variables, we can use the  Pearson Correlation Coefficient . This is a measure of the strength and direction of a linear relationship between two variables. A score of +1 is a perfectly linear positive relationship and a score of -1 is a perfectly negative linear relationship. Several values of the correlation coefficient are shown below: While the correlation coefficient cannot capture non-linear relationships, it is a good way to start figuring out how variables are related. In Pandas, we can easily calculate the correlations between any columns in a dataframe: The most negative (left) and positive (right) correlations with the target: There are several strong negative correlations between the features and the target with the most negative the different categories of EUI (these measures vary slightly in how they are calculated). The  EUI ‚Äî Energy Use Intensity  ‚Äî is the amount of energy used by a building divided by the square footage of the buildings. It is meant to be a measure of the efficiency of a building with a lower score being better. Intuitively, these correlations make sense: as the EUI increases, the Energy Star Score tends to decrease. To visualize relationships between two continuous variables, we use scatterplots. We can include additional information, such as a categorical variable, in the color of the points. For example, the following plot shows the Energy Star Score vs. Site EUI colored by the building type: This plot lets us visualize what a correlation coefficient of -0.7 looks like. As the Site EUI decreases, the Energy Star Score increases, a relationship that holds steady across the building types. The final exploratory plot we will make is known as the  Pairs Plot. This is a great exploration tool  because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the  PairGrid  function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle. To see interactions between variables, we look for where a row intersects with a column. For example, to see the correlation of  Weather Norm EUI  with  score , we look in the  Weather Norm EUI  row and the  score  column and see a correlation coefficient of -0.67. In addition to looking cool, plots such as these can help us decide which variables to include in modeling. Feature engineering and selection  often provide the greatest return on time invested in a machine learning problem. First of all, let‚Äôs define what these two tasks are: A machine learning model can only learn from the data we provide it, so ensuring that data includes all the relevant information for our task is crucial. If we don‚Äôt feed a model the correct data, then we are setting it up to fail and we should not expect it to learn! For this project, we will take the following feature engineering steps: One-hot encoding  is necessary to include categorical variables in a model. A machine learning algorithm cannot understand a building type of ‚Äúoffice‚Äù, so we have to record it as a 1 if the building is an office and a 0 otherwise. Adding transformed features can help our model learn non-linear relationships within the data.  Taking the square root, natural log, or various powers of features  is common practice in data science and can be based on domain knowledge or what works best in practice. Here we will include the natural log of all numerical features. The following code selects the numeric features, takes log transformations of these features, selects the two categorical features, one-hot encodes these features, and joins the two sets together. This seems like a lot of work, but it is relatively straightforward in Pandas! After this process we have over 11,000 observations (buildings) with 110 columns (features). Not all of these features are likely to be useful for predicting the Energy Star Score, so now we will turn to feature selection to remove some of the variables. Many of the 110 features we have in our data are redundant because they are highly correlated with one another. For example, here is a plot of Site EUI vs Weather Normalized Site EUI which have a correlation coefficient of 0.997. Features that are strongly correlated with each other are known as  collinear  and removing one of the variables in these pairs of features can often help a  machine learning model generalize and be more interpretable . (I should point out we are talking about correlations of features with other features, not correlations with the target, which help our model!) There are a number of methods to calculate collinearity between features, with one of the most common the  variance inflation factor . In this project, we will use thebcorrelation coefficient to identify and remove collinear features. We will drop one of a pair of features if the correlation coefficient between them is greater than 0.6. For the implementation, take a look at the notebook (and  this Stack Overflow answer ) While this value may seem arbitrary, I tried several different thresholds, and this choice yielded the best model. Machine learning is an  empirical field  and is often about experimenting and finding what performs best! After feature selection, we are left with 64 total features and 1 target. We have now completed data cleaning, exploratory data analysis, and feature engineering. The final step to take before getting started with modeling is establishing a naive baseline. This is essentially a guess against which we can compare our results. If the machine learning models do not beat this guess, then we might have to conclude that machine learning is not acceptable for the task or we might need to try a different approach. For regression problems, a reasonable naive baseline is to guess the median value of the target on the training set for all the examples in the test set. This sets a relatively low bar for any model to surpass. The metric we will use is  mean absolute error  (mae)  which measures the average absolute error on the predictions. There are many metrics for regression, but I like  Andrew Ng‚Äôs advice  to pick a single metric and then stick to it when evaluating models. The mean absolute error is easy to calculate and is interpretable. Before calculating the baseline, we need to split our data into a training and a testing set: We will use 70% of the data for training and 30% for testing: Now we can calculate the naive baseline performance: The naive estimate is off by about 25 points on the test set. The score ranges from 1‚Äì100, so this represents an error of 25%, quite a low bar to surpass! In this article we walked through the first three steps of a machine learning problem. After defining the question, we: Finally, we also completed the crucial step of establishing a baseline against which we can judge our machine learning algorithms. The second post ( available here ) will show how to evaluate machine learning models using  Scikit-Learn , select the best model, and perform hyperparameter tuning to optimize the model. The third post, dealing with model interpretation and reporting results,  is here . As always, I welcome feedback and constructive criticism and can be reached on Twitter  @koehrsen_will ."
Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,achine Learning is Fun! Part 4: Modern Face Recognition with Deep Learnin,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  ÊôÆÈÄöËØù ,  –†—É—Å—Å–∫–∏–π ,  ÌïúÍµ≠Ïñ¥ ,  Portugu√™s ,  Ti·∫øng Vi·ªát ,  ŸÅÿßÿ±ÿ≥€å  or  Italiano . Giant update:   I‚Äôve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you  like magic : This technology is called face recognition. Facebook‚Äôs algorithms are able to recognize your friends‚Äô faces after they have been tagged only a few times. It‚Äôs pretty amazing technology ‚Äî Facebook can recognize faces with  98% accuracy  which is pretty much as good as humans can do! Let‚Äôs learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem ‚Äî telling  Will Ferrell  (famous actor) apart from  Chad Smith  (famous rock musician)! So far in  Part 1 ,  2  and  3 , we‚Äôve used machine learning to solve isolated problems that have only one step ‚Äî  estimating the price of a house ,  generating new data based on existing data  and  telling if an image contains a certain object . All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result. But face recognition is really a series of several related problems: As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are  too good  at recognizing faces and end up seeing faces in everyday objects: Computers are not capable of this kind of high-level generalization ( at least not yet‚Ä¶ ), so we have to teach them how to do each step in this process separately. We need to build a  pipeline  where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms: Let‚Äôs tackle this problem one step at a time. For each step, we‚Äôll learn about a different machine learning algorithm. I‚Äôm not going to explain every single algorithm completely to keep this from turning into a book, but you‚Äôll learn the main ideas behind each one and you‚Äôll learn how you can build your own facial recognition system in Python using  OpenFace  and  dlib . The first step in our pipeline is  face detection . Obviously we need to locate the faces in a photograph before we can try to tell them apart! If you‚Äôve used any camera in the last 10 years, you‚Äôve probably seen face detection in action: Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we‚Äôll use it for a different purpose ‚Äî finding the areas of the image we want to pass on to the next step in our pipeline. Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a  way to detect faces  that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We‚Äôre going to use  a method invented in 2005  called Histogram of Oriented Gradients ‚Äî or just  HOG  for short. To find faces in an image, we‚Äôll start by making our image black and white because we don‚Äôt need color data to find faces: Then we‚Äôll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it: Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker: If you repeat that process for  every single pixel  in the image, you end up with every pixel being replaced by an arrow. These arrows are called  gradients  and they show the flow from light to dark across the entire image: This might seem like a random thing to do, but there‚Äôs a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the  direction  that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve! But saving the gradient for every single pixel gives us way too much detail. We end up  missing the forest for the trees . It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image. To do this, we‚Äôll break up the image into small squares of 16x16 pixels each. In each square, we‚Äôll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc‚Ä¶). Then we‚Äôll replace that square in the image with the arrow directions that were the strongest. The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way: To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces: Using this technique, we can now easily find faces in any image: If you want to try this step out yourself using Python and dlib,  here‚Äôs code  showing how to generate and view HOG representations of images. Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer: To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps. To do this, we are going to use an algorithm called  face landmark estimation . There are lots of ways to do this, but we are going to use the approach  invented in 2014 by Vahid Kazemi and Josephine Sullivan. The basic idea is we will come up with 68 specific points (called  landmarks ) that exist on every face ‚Äî the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face: Here‚Äôs the result of locating the 68 face landmarks on our test image: Now that we know were the eyes and mouth are, we‚Äôll simply rotate, scale and  shear  the image so that the eyes and mouth are centered as best as possible. We won‚Äôt do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called  affine transformations ): Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate. If you want to try this step out yourself using Python and dlib, here‚Äôs the  code for finding face landmarks  and here‚Äôs the  code for transforming the image  using those landmarks. Now we are to the meat of the problem ‚Äî actually telling faces apart. This is where things get really interesting! The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right? There‚Äôs actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can‚Äôt possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours. What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you‚Äôve ever watched a bad crime show like  CSI , you know what I am talking about: Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else? It turns out that the measurements that seem obvious to us humans (like eye color) don‚Äôt really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure. The solution is to train a Deep Convolutional Neural Network ( just like we did in Part 3 ). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart: After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements. Machine learning people call the 128 measurements of each face an  embedding . The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using  was invented in 2015 by researchers at Google  but many similar approaches exist. This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive  NVidia Telsa video card , it takes  about 24 hours  of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at  OpenFace  already did this and they  published several trained networks  which we can directly use. Thanks  Brandon Amos  and team! So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here‚Äôs the measurements for our test image: So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn‚Äôt really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person. If you want to try this step yourself, OpenFace  provides a lua script  that will generate embeddings all images in a folder and write them to a csv file. You  run it like this . This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image. You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We‚Äôll use a simple linear  SVM classifier , but lots of classification algorithms could work. All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person! So let‚Äôs try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon: Then I ran the classifier on every frame of the famous youtube video of  Will Ferrell and Chad Smith pretending to be each other  on the Jimmy Fallon show: It works! And look how well it works for faces in different poses ‚Äî even sideways faces! Let‚Äôs review the steps we followed: Now that you know how this all works, here‚Äôs instructions from start-to-finish of how run this entire face recognition pipeline on your own computer: UPDATE 4/9/2017:   You can still follow the steps below to use OpenFace. However, I‚Äôve released a new Python-based face recognition library called  face_recognition  that is much easier to install and use. So I‚Äôd recommend trying out  face_recognition  first instead of continuing below! I even put together  a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed . You can download and run it on your computer very easily. Give the virtual machine a shot if you don‚Äôt want to install all these libraries yourself! Original OpenFace instructions: If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter: You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I‚Äôd love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 5 !"
Machine Learning is Fun!,achine Learning is Fun,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  Êó•Êú¨Ë™û ,  Portugu√™s ,  Portugu√™s (alternate) ,  T√ºrk√ße ,  Fran√ßais ,  ÌïúÍµ≠Ïñ¥  ,  ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé‚Äé ,  Espa√±ol (M√©xico) ,  Espa√±ol (Espa√±a) ,  Polski ,  Italiano ,  ÊôÆÈÄöËØù ,  –†—É—Å—Å–∫–∏–π ,  ÌïúÍµ≠Ïñ¥  ,  Ti·∫øng Vi·ªát  or  ŸÅÿßÿ±ÿ≥€å . Giant update:   I‚Äôve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let‚Äôs change that! This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading  the wikipedia article , got frustrated and gave up wishing someone would just give them a high-level explanation. That‚Äôs what this is. The goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished. Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data. For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It‚Äôs the same algorithm but it‚Äôs fed different training data so it comes up with different classification logic. ‚ÄúMachine learning‚Äù is an umbrella term covering lots of these kinds of generic algorithms. You can think of machine learning algorithms as falling into one of two main categories ‚Äî  supervised learning  and  unsupervised learning . The difference is simple, but really important. Let‚Äôs say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there‚Äôs a problem ‚Äî you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don‚Äôt have your experience so they don‚Äôt know how to price their houses. To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it‚Äôs size, neighborhood, etc, and what similar houses have sold for. So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details ‚Äî number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price: Using that training data, we want to create a program that can estimate how much any other house in your area is worth: This is called  supervised learning . You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic. To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out. This kind of like having the answer key to a math test with all the arithmetic symbols erased: From this, can you figure out what kind of math problems were on the test? You know you are supposed to ‚Äúdo something‚Äù with the numbers on the left to get each answer on the right. In  supervised learning , you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type! Let‚Äôs go back to our original example with the real estate agent. What if you didn‚Äôt know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called  unsupervised  learning. This is kind of like someone giving you a list of numbers on a sheet of paper and saying ‚ÄúI don‚Äôt really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something ‚Äî good luck!‚Äù So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you‚Äôd find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts. Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions. Supervised learning is what we‚Äôll focus on for the rest of this post, but that‚Äôs not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer. Side note: There are lots of  other types  of machine learning algorithms. But this is a pretty good place to start. As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a ‚Äúfeel‚Äù for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of  Strong AI  research is to be able to replicate this ability with computers. But current machine learning algorithms aren‚Äôt that good yet ‚Äî they only work when focused a very specific, limited problem. Maybe a better definition for ‚Äúlearning‚Äù in this case is ‚Äúfiguring out an equation to solve a specific problem based on some example data‚Äù. Unfortunately  ‚ÄúMachine Figuring out an equation to solve a specific problem based on some example data‚Äù  isn‚Äôt really a great name. So we ended up with ‚ÄúMachine Learning‚Äù instead. Of course if you are reading this 50 years in the future and we‚Äôve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human. So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further. If you didn‚Äôt know anything about machine learning, you‚Äôd probably try to write out some basic rules for estimating the price of a house like this: If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change. Wouldn‚Äôt it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number: One way to think about this problem is that the  price  is a delicious stew and the ingredients are the  number of bedrooms , the  square footage  and the  neighborhood . If you could just figure out how much each ingredient impacts the final price, maybe there‚Äôs an exact ratio of ingredients to stir in to make the final price. That would reduce your original function (with all those crazy  if ‚Äôs and  else ‚Äôs) down to something really simple like this: Notice the magic numbers in bold ‚Äî  .841231951398213 ,  1231.1231231 ,  2.3242341421 ,   and  201.23432095 . These are our  weights . If we could just figure out the perfect weights to use that work for every house, our function could predict house prices! A dumb way to figure out the best weights would be something like this: Start with each weight set to  1.0: Run every house you know about through your function and see how far off the function is at guessing the correct price for each house: For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house. Now add up the squared amount you are off for each house you have in your data set. Let‚Äôs say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That‚Äôs how ‚Äúwrong‚Äù your function currently is. Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the  cost  of your function. If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that‚Äôs our goal ‚Äî get this cost to be as low as possible by trying different weights. Repeat Step 2 over and over   with  every single possible combination of weights . Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you‚Äôve solved the problem! That‚Äôs pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow! But here‚Äôs a few more facts that will blow your mind: Pretty crazy, right? Ok, of course you can‚Äôt just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you‚Äôd never run out of numbers to try. To avoid that, mathematicians have figured out lots of  clever ways  to quickly find good values for those weights without having to try very many. Here‚Äôs one way: First, write a simple equation that represents Step #2 above: Now let‚Äôs re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now): This equation represents how wrong our price estimating function is for the weights we currently have set. If we graph this cost equation for all possible values of our weights for  number_of_bedrooms  and  sqft , we‚Äôd get a graph that might look something like this: In this graph, the lowest point in blue is where our cost is the lowest ‚Äî thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we‚Äôll have our answer! So we just need to adjust our weights so we are ‚Äúwalking down hill‚Äù on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we‚Äôll eventually get there without having to try too many different weights. If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function‚Äôs tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill. So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we‚Äôll reach the bottom of the hill and have the best possible values for our weights. (If that didn‚Äôt make sense, don‚Äôt worry and keep reading). That‚Äôs a high level summary of one way to find the best weights for your function called  batch gradient descent . Don‚Äôt be afraid to  dig deeper  if you are interested on learning the details. When you use a machine learning library to solve a real problem, all of this will be done for you. But it‚Äôs still useful to have a good idea of what is happening. The three-step algorithm I described is called  multivariate linear regression . You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you‚Äôve never seen before based where that house would appear on your line. It‚Äôs a really powerful idea and you can solve ‚Äúreal‚Äù problems with it. But while the approach I showed you might work in simple cases, it won‚Äôt work in all cases. One reason is because house prices aren‚Äôt always simple enough to follow a continuous line. But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like  neural networks  or  SVMs  with  kernels ). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies. Also, I ignored the idea of  overfitting . It‚Äôs easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren‚Äôt in your original data set. But there are ways to deal with this (like  regularization  and using a  cross-validation  data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully. In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it‚Äôs a skill that any developer can learn! Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data! But it‚Äôs important to remember that machine learning only works if the problem is actually solvable with the data that you have. For example, if you build a model that predicts home prices based on the type of potted plants in each house, it‚Äôs never going to work. There just isn‚Äôt any kind of relationship between the potted plants in each house and the home‚Äôs sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two. So remember, if a human expert couldn‚Äôt use the data to solve the problem manually, a computer probably won‚Äôt be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly. In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn‚Äôt a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it‚Äôs getting a little better every day. If you want to try out what you‚Äôve learned in this article, I made  a course that walks you through every step of this article, including writing all the code . Give it a try! If you want to go deeper, Andrew Ng‚Äôs free  Machine Learning class on Coursera  is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math. Also, you can play around with tons of machine learning algorithms by downloading and installing  SciKit-Learn . It‚Äôs a python framework that has ‚Äúblack box‚Äù versions of all the standard algorithms. If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter: Also, please check out the  full-length course version of this article .  It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course  if you sign up with this link . You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I‚Äôd love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 2 !"
"Every single Machine Learning course on the internet, ranked by your reviews","very single Machine Learning course on the internet, ranked by your review","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own  data science master‚Äôs program  using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I‚Äôm almost finished now. I‚Äôve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role.   So I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few  coding classes  for the beginner data scientist. Then it was  statistics and probability classes . Then  introductions to data science . Also,  data visualization . For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings.  My end goal was to identify the three best courses available and present them to you, below. For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it‚Äôs David. I wrote this guide back in 2017. Since then, I‚Äôve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros. datamaverickhq.com Okay, back to the guide. Each course must fit three criteria: We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on  Udemy , we chose to consider the most-reviewed and highest-rated ones only. There‚Äôs always a chance that we missed something, though. So please let us know in the comments section if we left a good course out. We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on three factors: A popular definition originates from  Arthur Samuel  in 1959: machine learning is a subfield of computer science that gives  ‚Äúcomputers the ability to learn without being explicitly programmed.‚Äù  In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience. A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you‚Äôll find helpful visualization of these core steps: The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves. First off, let‚Äôs define deep learning. Here is a succinct description: ‚ÄúDeep learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.‚Äù ‚Äî Jason Brownlee from  Machine Learning Mastery As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we‚Äôve got you covered with the following  article : medium.freecodecamp.com My top three recommendations from that list would be: Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline. Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles ( programming ,  statistics ) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar. Stanford University‚Äôs  Machine Learning  on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at  Baidu , this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews. Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available. Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning. Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice: In the past, I‚Äôve tried to teach machine learning using a large variety of different programming languages including C++, Java, Python, NumPy, and also Octave ‚Ä¶ And what I‚Äôve seen after having taught machine learning for almost a decade is that you learn much faster if you use Octave as your programming environment. Though Python and R are likely more compelling choices in 2017 with the  increased popularity of those languages , reviewers note that that shouldn‚Äôt stop you from taking the course. A few prominent reviewers noted the following: Of longstanding renown in the MOOC world, Stanford‚Äôs machine learning course really is the definitive introduction to this topic. The course broadly covers all of the major areas of machine learning ‚Ä¶ Prof. Ng precedes each segment with a motivating discussion and examples. Andrew Ng is a gifted teacher and able to explain complicated subjects in a very intuitive and clear way, including the math behind all concepts. Highly recommended. The only problem I see with this course if that it sets the expectation bar very high for other courses. Columbia University‚Äôs  Machine Learning  is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn‚Äôt have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews. The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia‚Äôs is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding). Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course‚Äôs total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase. Below are a few of the aforementioned sparkling  reviews : Over all my years of [being a] student I‚Äôve come across professors who aren‚Äôt brilliant, professors who are brilliant but they don‚Äôt know how to explain the stuff clearly, and professors who are brilliant and know how explain the stuff clearly. Dr. Paisley belongs to the third group. This is a great course ‚Ä¶ The instructor‚Äôs language is precise and that is, to my mind, one of the strongest points of the course. The lectures are of high quality and the slides are great too. Dr. Paisley and his supervisor are ‚Ä¶ students of Michael Jordan, the father of machine learning. [Dr. Paisley] is the best ML professor at Columbia because of his ability to explain stuff clearly. Up to 240 students have selected his course this semester, the largest number among all professors [teaching] machine learning at Columbia. Machine Learning A-Z‚Ñ¢  on Udemy is an impressively detailed offering that provides instruction in  both  Python and R, which is rare and can‚Äôt be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered. It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an ‚Äúintuition‚Äù video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R. As a ‚Äúbonus,‚Äù the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren‚Äôt the strong points of the course. Eremenko and the SuperDataScience team are revered for their ability to ‚Äúmake the complex simple.‚Äù Also, the prerequisites listed are ‚Äújust some high school mathematics,‚Äù so this course might be a better option for those daunted by the Stanford and Columbia offerings. A few prominent reviewers  noted  the following: The course is professionally produced, the sound quality is excellent, and the explanations are clear and concise ‚Ä¶ It‚Äôs an incredible value for your financial and time investment. It was spectacular to be able to follow the course in two different programming languages simultaneously. Kirill is one of the absolute best instructors on Udemy (if not the Internet) and I recommend taking any class he teaches. ‚Ä¶ This course has a ton of content, like a ton! Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let‚Äôs look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide ‚Äî you can find those  here . The Analytics Edge  (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews. Python for Data Science and Machine Learning Bootcamp  (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews. Data Science and Machine Learning Bootcamp with R  (Jose Portilla/Udemy): The comments for Portilla‚Äôs above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews. Machine Learning Series  (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course‚Äôs description. Uses Python. Cost varies depending on Udemy discounts, which are frequent. Machine Learning  (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity‚Äôs Machine Learning Engineer Nanodegree and Georgia Tech‚Äôs Online Master‚Äôs Degree (OMS). Bite-sized videos, as is Udacity‚Äôs style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews. Implementing Predictive Analytics with Spark in Azure HDInsight  (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews. Data Science and Machine Learning with Python ‚Äî Hands On!  (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews. Scala and Spark for Big Data and Machine Learning  (Jose Portilla/Udemy): ‚ÄúBig data‚Äù focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews. Machine Learning Engineer Nanodegree  (Udacity): Udacity‚Äôs flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews. Learning From Data (Introductory Machine Learning)  (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech‚Äôs independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews. Learning From Data (Introductory Machine Learning)  (Yaser Abu-Mostafa/California Institute of Technology): ‚ÄúA real Caltech course, not a watered-down version.‚Äù Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn‚Äôt as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews. Mining Massive Datasets  (Stanford University): Machine learning with a focus on ‚Äúbig data.‚Äù Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews. AWS Machine Learning: A Complete Guide With Python  (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews. Introduction to Machine Learning & Face Detection in Python  (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews. StatLearning: Statistical Learning  (Stanford University): Based on the excellent textbook, ‚Äú An Introduction to Statistical Learning, with Applications in R ‚Äù and taught by the professors who wrote it. Reviewers note that the MOOC isn‚Äôt as good as the book, citing ‚Äúthin‚Äù exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews. Machine Learning Specialization  (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford‚Äôs or Caltech‚Äôs). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews. From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase  (Loony Corn/Udemy): ‚ÄúA down-to-earth, shy but confident take on machine learning techniques.‚Äù Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews. Principles of Machine Learning  (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews. Big Data: Statistical Inference and Machine Learning  (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews. Genomic Data Science and Clustering  (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD‚Äôs Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews. Intro to Machine Learning  (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity‚Äôs Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews. Machine Learning for Data Analysis  (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan‚Äôs Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews. Programming with Python for Data Science  (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews. Machine Learning for Trading  (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity‚Äôs Machine Learning Engineer Nanodegree and Georgia Tech‚Äôs Online Master‚Äôs Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews. Practical Machine Learning  (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU‚Äôs Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews. Machine Learning for Data Science and Analytics  (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews. Recommender Systems Specialization  (University of Minnesota/Coursera): Strong focus one specific type of machine learning ‚Äî recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews. Machine Learning With Big Data  (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD‚Äôs Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews. Practical Predictive Analytics: Models and Methods  (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW‚Äôs Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews. The following courses had one or no reviews as of May 2017. Machine Learning for Musicians and Artists  (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review. Applied Machine Learning in Python  (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available. Applied Machine Learning  (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. Machine Learning with Python  (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free. Machine Learning with Apache SystemML  (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free. Machine Learning for Data Science  (University of California, San Diego/edX): Doesn‚Äôt launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase. Introduction to Analytics Modeling  (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase. Predictive Analytics: Gaining Insights from Big Data  (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise‚Äôs Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase. Introducci√≥n al Machine Learning  (Universitas Telef√≥nica/Mir√≠ada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks. Machine Learning Path Step  (Dataquest): Taught in Python using Dataquest‚Äôs interactive in-browser platform. Multiple guided projects and a ‚Äúplus‚Äù project where you build your own machine learning system using your own data. Subscription required. The following six courses are offered by  DataCamp . DataCamp‚Äôs hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course. Introduction to Machine Learning  (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours. Supervised Learning with scikit-learn  (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours. Unsupervised Learning in R  (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours. Machine Learning Toolbox  (DataCamp): Teaches the ‚Äúbig ideas‚Äù in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours. Machine Learning with the Experts: School Budgets  (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school‚Äôs budget. DataCamp‚Äôs ‚ÄúSupervised Learning with scikit-learn‚Äù is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours. Unsupervised Learning in Python  (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours. Machine Learning  (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon‚Äôs graduate introductory machine learning course. A prerequisite to their second graduate level course, ‚ÄúStatistical Machine Learning.‚Äù Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A  2011 version  of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free. Statistical Machine Learning  (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon‚Äôs Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free. Undergraduate Machine Learning  (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below). Machine Learning  (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas‚Äô undergraduate course (above) apply here as well. This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the  first article , statistics and probability in the  second article , intros to data science in the  third article , and data visualization in the  fourth . medium.freecodecamp.com The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering. If you‚Äôre looking for a complete list of Data Science online courses, you can find them on Class Central‚Äôs  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ‚Äôs other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the üíö so more people will see it here on Medium. This is a condensed version of my  original article  published on Class Central, where I‚Äôve included detailed course syllabi."
Essential Cheat Sheets for Machine Learning and Deep Learning Engineers,ssential Cheat Sheets for Machine Learning and Deep Learning Engineer,"Machine learning is complex. For newbies, starting to learn machine learning can be painful if they don‚Äôt have right resources to learn from. Most of the machine learning libraries are difficult to understand and learning curve can be a bit frustrating. I am creating a repository on Github( cheatsheets-ai ) containing cheatsheets for different machine learning frameworks, gathered from different sources. Do visit the Github repository, also, contribute cheat sheets if you have any. Thanks. List of Cheatsheets:  1. Keras 2. Numpy 3. Pandas 4. Scipy 5. Matplotlib 6. Scikit-learn 7. Neural Networks Zoo 8. ggplot2 9. PySpark 10. R Studio 11. Jupyter Notebook 12. Dask 2. Numpy 3. Pandas 4. Scipy 5. Matplotlib 6. Scikit-learn 7. Neural Networks Zoo 8. ggplot2 9. PySpark 10. R Studio (dplyr and tidyr) 11. Jupyter Notebook 12. Dask Thank you for reading. If you want to get into contact, you can reach out to me at  ahikailash1@gmail.com About Me: I am a Co-Founder of  MateLabs , where we have built  Mateverse , an ML Platform which enables everyone to easily build and train Machine Learning Models, without writing a single line of code. Note : Recently, I published a book on GANs titled ‚ÄúGenerative Adversarial Networks Projects‚Äù, in which I covered most of the widely popular GAN architectures and their implementations. DCGAN, StackGAN, CycleGAN, Pix2pix, Age-cGAN, and 3D-GAN have been covered in details at the implementation level. Each architecture has a chapter dedicated to it. I have explained these networks in a very simple and descriptive language using Keras framework with Tensorflow backend. If you are working on GANs or planning to use GANs, give it a read and share your valuable feedback with me at  ahikailash1@gmail.com www.amazon.com You can grab a copy of the book from  http://www.amazon.com/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679 https://www.amazon.in/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679?fbclid=IwAR0X2pDk4CTxn5GqWmBbKIgiB38WmFX-sqCpBNI8k9Z8I-KCQ7VWRpJXm7I   https://www.packtpub.com/big-data-and-business-intelligence/generative-adversarial-networks-projects?fbclid=IwAR2OtU21faMFPM4suH_HJmy_DRQxOVwJZB0kz3ZiSbFb_MW7INYCqqV7U0c Triplebyte helps programmers find great companies to work at. They‚Äôll go through a technical interview with you, match you with companies that are looking for people with your specific skill sets, and then fast track you through their hiring processes. Looking for a new job? Take Triplebyte‚Äôs quiz and get a job at top companies!"
"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data","heat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Dat","Over the past few months, I have been collecting AI cheat sheets. From time to time I share them with friends and colleagues and recently I have been getting asked a lot, so I decided to organize and share the entire collection. To make things more interesting and give context, I added descriptions and/or excerpts for each major topic. This is the most complete list and the Big-O is at the very end, enjoy‚Ä¶ >>> Update: We have recently redesigned these cheat sheets into a Super High Definition PDF. Check them out below: becominghuman.ai chatbotslife.com aijobsboard.com This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. The flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it. >>>  See Latest Jobs in AI, ML & BIG DATA  <<< Scikit-learn  (formerly  scikits.learn ) is a  free software   machine learning   library  for the  Python  programming language. It features various  classification ,  regression  and  clustering  algorithms including  support vector machines ,  random forests ,  gradient boosting ,  k -means  and  DBSCAN , and is designed to interoperate with the Python numerical and scientific libraries  NumPy  and  SciPy . This machine learning cheat sheet from Microsoft Azure will help you choose the appropriate machine learning algorithms for your predictive analytics solution. First, the cheat sheet will asks you about the data nature and then suggests the best algorithm for the job. becominghuman.ai aijobsboard.com In May 2017 Google announced the second-generation of the TPU, as well as the availability of the TPUs in  Google Compute Engine . [12]  The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs provide up to 11.5 petaflops. becominghuman.ai In 2017, Google‚Äôs TensorFlow team decided to support Keras in TensorFlow‚Äôs core library. Chollet explained that Keras was conceived to be an interface rather than an end-to-end machine-learning framework. It presents a higher-level, more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library. NumPy targets the  CPython  reference  implementation  of Python, which is a non-optimizing  bytecode  interpreter. Mathematical algorithms written for this version of Python often run much slower than  compiled  equivalents. NumPy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy. The name ‚ÄòPandas‚Äô is derived from the term ‚Äú panel data ‚Äù, an  econometrics  term for multidimensional structured data sets. The term ‚Äúdata wrangler‚Äù is starting to infiltrate pop culture. In the 2017 movie  Kong: Skull Island , one of the characters, played by actor  Marc Evan Jackson  is introduced as ‚ÄúSteve Woodward, our data wrangler‚Äù. becominghuman.ai chatbotslife.com aijobsboard.com SciPy builds on the  NumPy  array object and is part of the NumPy stack which includes tools like  Matplotlib ,  pandas  and  SymPy , and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as  MATLAB ,  GNU Octave , and  Scilab . The NumPy stack is also sometimes referred to as the SciPy stack. [3] matplotlib  is a  plotting   library  for the  Python  programming language and its numerical mathematics extension  NumPy . It provides an  object-oriented   API  for embedding plots into applications using general-purpose  GUI toolkits  like  Tkinter ,  wxPython ,  Qt , or  GTK+ . There is also a  procedural  ‚Äúpylab‚Äù interface based on a  state machine  (like  OpenGL ), designed to closely resemble that of  MATLAB , though its use is discouraged. [2]   SciPy  makes use of matplotlib. pyplot is a matplotlib module which provides a MATLAB-like interface. [6]  matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. >>> If you like this list, you can let me know  here . <<< aijobsboard.com becominghuman.ai becominghuman.ai Stefan is the founder of  Chatbot‚Äôs Life , a Chatbot media and consulting firm. Chatbot‚Äôs Life has grown to over 150k views per month and has become the premium place to learn about Bots & AI online. Chatbot‚Äôs Life has also consulted many of the top Bot companies like Swelly, Instavest, OutBrain, NearGroup and a number of Enterprises. Big-O Algorithm Cheat Sheet:  http://bigocheatsheet.com/ Bokeh Cheat Sheet:  https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf Data Science Cheat Sheet:  https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics Data Wrangling Cheat Sheet:  https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf Data Wrangling:  https://en.wikipedia.org/wiki/Data_wrangling Ggplot Cheat Sheet:  https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Keras Cheat Sheet:  https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs Keras:  https://en.wikipedia.org/wiki/Keras Machine Learning Cheat Sheet:  https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/ Machine Learning Cheat Sheet:  https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet ML Cheat Sheet::  http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html Matplotlib Cheat Sheet:  https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY Matpotlib:  https://en.wikipedia.org/wiki/Matplotlib Neural Networks Cheat Sheet:  http://www.asimovinstitute.org/neural-network-zoo/ Neural Networks Graph Cheat Sheet:  http://www.asimovinstitute.org/blog/ Neural Networks:  https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network Numpy Cheat Sheet:  https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE NumPy:  https://en.wikipedia.org/wiki/NumPy Pandas Cheat Sheet:  https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM Pandas:  https://en.wikipedia.org/wiki/Pandas_(software) Pandas Cheat Sheet:  https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc Pyspark Cheat Sheet:  https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ Scikit Cheat Sheet:  https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet Scikit-learn:  https://en.wikipedia.org/wiki/Scikit-learn Scikit-learn Cheat Sheet:  http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html Scipy Cheat Sheet:  https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI SciPy:  https://en.wikipedia.org/wiki/SciPy TesorFlow Cheat Sheet:  https://www.altoros.com/tensorflow-cheat-sheet.html Tensor Flow:  https://en.wikipedia.org/wiki/TensorFlow"
Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,achine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Network,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  ÊôÆÈÄöËØù ,  –†—É—Å—Å–∫–∏–π ,  ÌïúÍµ≠Ïñ¥ ,  Portugu√™s ,  Ti·∫øng Vi·ªát ,  ŸÅÿßÿ±ÿ≥€å   or  Italiano . Giant update:   I‚Äôve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Are you tired of reading endless news stories about  deep learning  and not really knowing what that means? Let‚Äôs change that! This time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we‚Äôre going to explain the black magic that allows Google Photos to search your photos based on what is in the picture: Just like  Part 1  and  Part 2 , this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished! (If you haven‚Äôt already read  part 1  and  part 2 , read them now!) You might have seen  this famous xkcd comic  before. The goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years. In the last few years, we‚Äôve finally found a good approach to object recognition using  deep convolutional neural networks . That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one. So let‚Äôs do it ‚Äî let‚Äôs write a program that can recognize birds! Before we learn how to recognize pictures of birds, let‚Äôs learn how to recognize something much simpler ‚Äî the handwritten number ‚Äú8‚Äù. In  Part 2 , we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in: We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let‚Äôs modify this same neural network to recognize handwritten text. But to make the job really simple, we‚Äôll only try to recognize one letter ‚Äî the numeral ‚Äú8‚Äù. Machine learning only works when you have data ‚Äî preferably a lot of data. So we need lots and lots of handwritten ‚Äú8‚Äùs to get started. Luckily, researchers created the  MNIST data set of handwritten numbers  for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some ‚Äú8‚Äùs from the data set: The neural network we made in  Part 2  only took in a three numbers as the input (‚Äú3‚Äù bedrooms, ‚Äú2000‚Äù sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers? The answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is: To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers: The handle 324 inputs, we‚Äôll just enlarge our neural network to have 324 input nodes: Notice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an ‚Äú8‚Äù and thee second output will predict the likelihood it isn‚Äôt an ‚Äú8‚Äù. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups. Our neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone. All that‚Äôs left is to train the neural network with images of ‚Äú8‚Äùs and not-‚Äú8""s so it learns to tell them apart. When we feed in an ‚Äú8‚Äù, we‚Äôll tell it the probability the image is an ‚Äú8‚Äù is 100% and the probability it‚Äôs not an ‚Äú8‚Äù is 0%. Vice versa for the counter-example images. Here‚Äôs some of our training data: We can train this kind of neural network in a few minutes on a modern laptop. When it‚Äôs done, we‚Äôll have a neural network that can recognize pictures of ‚Äú8‚Äùs with a pretty high accuracy. Welcome to the world of (late 1980‚Äôs-era) image recognition! It‚Äôs really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic!  ‚Ä¶right? Well, of course it‚Äôs not that simple. First, the good news is that our ‚Äú8‚Äù recognizer really does work well on simple images where the letter is right in the middle of the image: But now the really bad news: Our ‚Äú8‚Äù recognizer  totally fails  to work when the letter isn‚Äôt perfectly centered in the image. Just the slightest position change ruins everything: This is because our network only learned the pattern of a perfectly-centered ‚Äú8‚Äù. It has absolutely no idea what an off-center ‚Äú8‚Äù is. It knows exactly one pattern and one pattern only. That‚Äôs not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the ‚Äú8‚Äù isn‚Äôt perfectly centered. We already created a really good program for finding an ‚Äú8‚Äù centered in an image. What if we just scan all around the image for possible ‚Äú8‚Äùs in smaller sections, one section at a time, until we find one? This approach called a sliding window. It‚Äôs the brute force solution. It works well in some limited cases, but it‚Äôs really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this! When we trained our network, we only showed it ‚Äú8‚Äùs that were perfectly centered. What if we train it with more data, including ‚Äú8‚Äùs in all different positions and sizes all around the image? We don‚Äôt even need to collect new training data. We can just write a script to generate new images with the ‚Äú8‚Äùs in all kinds of different positions in the image: Using this technique, we can easily create an endless supply of training data. More data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns. To make the network bigger, we just stack up layer upon layer of nodes: We call this a ‚Äúdeep neural network‚Äù because it has more layers than a traditional neural network. This idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play  Overwatch  can be used to train neural networks incredibly quickly. But even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn‚Äôt going to get us all the way to a solution. We need to be smarter about how we process images into our neural network. Think about it. It doesn‚Äôt make sense to train a network to recognize an ‚Äú8‚Äù at the top of a picture separately from training it to recognize an ‚Äú8‚Äù at the bottom of a picture as if those were two totally different objects. There should be some way to make the neural network smart enough to know that an ‚Äú8‚Äù anywhere in the picture is the same thing without all that extra training. Luckily‚Ä¶ there is! As a human, you intuitively know that pictures have a  hierarchy  or  conceptual structure . Consider this picture: As a human, you instantly recognize the hierarchy in this picture: Most importantly, we recognize the idea of a  child  no matter what surface the child is on. We don‚Äôt have to re-learn the idea of  child  for every possible surface it could appear on. But right now, our neural network can‚Äôt do this. It thinks that an ‚Äú8‚Äù in a different part of the image is an entirely different thing. It doesn‚Äôt understand that moving an object around in the picture doesn‚Äôt make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks. We need to give our neural network understanding of  translation invariance  ‚Äî an ‚Äú8‚Äù is an ‚Äú8‚Äù no matter where in the picture it shows up. We‚Äôll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images). Instead of feeding entire images into our neural network as one grid of numbers, we‚Äôre going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture. Here‚Äôs how it‚Äôs going to work, step by step ‚Äî Similar to our sliding window search above, let‚Äôs pass a sliding window over the entire original image and save each result as a separate, tiny picture tile: By doing this, we turned our original image into 77 equally-sized tiny image tiles. Earlier, we fed a single image into a neural network to see if it was an ‚Äú8‚Äù. We‚Äôll do the exact same thing here, but we‚Äôll do it for each individual image tile: However,  there‚Äôs one big twist : We‚Äôll keep the  same neural network weights  for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we‚Äôll mark that tile as interesting. We don‚Äôt want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this: In other words, we‚Äôve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting. The result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big: To reduce the size of the array, we  downsample  it using an algorithm called  max pooling . It sounds fancy, but it isn‚Äôt at all! We‚Äôll just look at each 2x2 square of the array and keep the biggest number: The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we‚Äôll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits. So far, we‚Äôve reduced a giant image down into a fairly small array. Guess what? That array is just a bunch of numbers, so we can use that small array as input into  another neural network . This final neural network will decide if the image is or isn‚Äôt a match. To differentiate it from the convolution step, we call it a ‚Äúfully connected‚Äù network. So from start to finish, our whole five-step pipeline looks like this: Our image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network. When solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data. The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize. For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it‚Äôs knowledge of sharp edges, the third step might recognize entire birds using it‚Äôs knowledge of beaks, etc. Here‚Äôs what a more realistic deep convolutional network (like you would find in a research paper) looks like: In this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories! So how do you know which steps you need to combine to make your image classifier work? Honestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error! Now finally we know enough to write a program that can decide if a picture is a bird or not. As always, we need some data to get started. The free  CIFAR10 data set  contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we‚Äôll also add in the  Caltech-UCSD Birds-200‚Äì2011 data set  that has another 12,000 bird pics. Here‚Äôs a few of the birds from our combined data set: And here‚Äôs some of the 52,000 non-bird images: This data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need  millions  of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data! To build our classifier, we‚Äôll use  TFLearn . TFlearn is a wrapper around Google‚Äôs  TensorFlow  deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network. Here‚Äôs the code to define and train the network: If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer. As it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn‚Äôt help, so I stopped it there. Congrats! Our program can now recognize birds in images! Now that we have a trained neural network, we can use it!  Here‚Äôs a simple script  that takes in a single image file and predicts if it is a bird or not. But to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time. That seems pretty good, right? Well‚Ä¶ it depends! Our network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things. For example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed ‚Äúnot a bird‚Äù every single time would be 95% accurate! But it would also be 100% useless. We need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at  how  it failed, not just the percentage of the time that it failed. Instead of thinking about our predictions as ‚Äúright‚Äù and ‚Äúwrong‚Äù, let‚Äôs break them down into four separate categories ‚Äî Using our validation set of 15,000 images, here‚Äôs how many times our predictions fell into each category: Why do we break our results down like this? Because not all mistakes are created equal. Imagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we‚Äôd rather have false positives than false negatives. False negatives would be the worse possible case ‚Äî that‚Äôs when the program told someone they definitely didn‚Äôt have cancer but they actually did. Instead of just looking at overall accuracy, we calculate  Precision and Recall  metrics. Precision and Recall metrics give us a clearer picture of how well we did: This tells us that 97% of the time we guessed ‚ÄúBird‚Äù, we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one! Now that you know the basics of deep convolutional networks, you can try out some of the  examples that come with tflearn  to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don‚Äôt even have to find your own images. You also know enough now to start branching and learning about other areas of machine learning. Why not learn  how to use algorithms to train computers how to play Atari games  next? If you liked this article, please consider  signing up for my Machine Learning is Fun! email list . I‚Äôll only email you when I have something new and awesome to share. It‚Äôs the best way to find out when I write more articles like this. You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I‚Äôd love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 4 ,  Part 5  and  Part 6 !"
‚ÄòI want to learn Artificial Intelligence and Machine Learning. Where can I start?‚Äô,I want to learn Artificial Intelligence and Machine Learning. Where can I start?,"I was working at the Apple Store and I wanted a change. To start building the tech I was servicing. I began looking into Machine Learning (ML) and Artificial Intelligence (AI). There‚Äôs so much going on. Too much. Every week it seems like Google or Facebook are releasing a new kind of AI to make things faster or improve our experience. And don‚Äôt get me started on the number of self-driving car companies. This is a good thing though. I‚Äôm not a fan of driving and roads are dangerous. Even with all this happening, there‚Äôs still yet to be an agreed definition of what exactly artificial intelligence is. Some argue deep learning can be considered AI, others will say it‚Äôs not AI unless it passes the Turing Test. This lack of definition really stunted my progress in the beginning. It was hard to learn something which had so many different definitions. Enough with the definitions. My friends and I were building a web startup. It failed. We gave up due to a lack of meaning. But along the way, I was starting to hearing more and more about ML and AI. ‚ÄúThe computer learns things for you?‚Äù I couldn‚Äôt believe it. I stumbled across Udacity‚Äôs Deep Learning Nanodegree. A fun character called Siraj Raval was in one of the promo videos. His energy was contagious. Despite not meeting the basic requirements (I had never written a line of Python before), I signed up. 3 weeks before the course start date I emailed Udacity support asking what the refund policy was. I was scared I wouldn‚Äôt be able to complete the course. I didn‚Äôt get a refund. I completed the course within the designated timeline. It was hard. Really hard at times. My first two projects were handed in four days late. But the excitement of being involved in one of the most important technologies in the world drove me forward. Finishing the Deep Learning Nanodegree, I had guaranteed acceptance into either Udacity‚Äôs AI Nanodegree, Self-Driving Car Nanodegree or Robotics Nanodegree. All great options. I was lost again. The classic. ‚ÄúWhere do I go next?‚Äù I needed a curriculum. I‚Äôd built a foundation with the Deep Learning Nanodegree, now it was time to figure out what was next. I didn‚Äôt plan on going back to university anytime soon. I didn‚Äôt have $100,000 for a proper Masters Degree anyway. So I did what I did in the beginning. Asked my mentor, Google, for help. I‚Äôd jumped into deep learning without any prior knowledge of the field. Instead of climbing to the tip of the AI iceberg, a helicopter had dropped me off on the top. After researching a bunch of courses, I put a list of which ones interested me the most in Trello. I knew online courses had a high drop out rate. I wasn‚Äôt going to let myself be a part of this number. I had a mission. To make myself accountable, I started sharing my learning journey online. I figured I could practice communicating what I learned plus find other people who were interested in the same things I was. My friends still think I‚Äôm an alien when I go on one of my AI escapades. I made the  Trello board public  and wrote a blog post about my endeavours. The curriculum has changed slightly since I first wrote it but it‚Äôs still relevant. I‚Äôd visit the Trello board multiple times per week to track my progress. I‚Äôm Australian. And all the commotion seemed to be happening in the US. So I did the most logical thing and bought a one-way ticket. I‚Äôd been studying for a year and I figured it was about time I started putting my skills into practice. My plan was to rock up to the US and get hired. Then Ashlee messaged me on LinkedIn, ‚ÄúHey I‚Äôve seen your posts and they‚Äôre really cool, I think you should meet Mike.‚Äù I met Mike. I told him my story of learning online, how I loved healthtech and my plans to go to the US. ‚ÄúYou may be better off staying here a year or so and seeing what you can find, I‚Äô think you‚Äôd love to meet Cameron.‚Äù I met Cameron. We had a similar chat what Mike and I talked about. Health, tech, online learning, US. ‚ÄúWe‚Äôre working on some health problems, why don‚Äôt you come in on Thursday?‚Äù Thursday came. I was nervous. But someone once told me being nervous is the same as being excited. I flipped to being excited. I spent the day meeting the  Max Kelsen  team and the problems they were working on. Two Thursday‚Äôs later, Nick, the CEO, Athon, lead machine learning engineer, and I went for coffee. ‚ÄúHow would you like to join the team?‚Äù Asked Nick. ‚ÄúSure,‚Äù I said. My US flight got pushed back a couple of months and I purchased a return ticket. Learning online, I knew it was unconventional. All the roles I‚Äôd gone to apply for had Masters Degree requirements or at least some kind of technical degree. I didn‚Äôt have either of these. But I did have the skills I‚Äôd gathered from a plethora of online courses. Along the way, I was sharing my work online. My GitHub contained all the projects I‚Äôd done, my LinkedIn was stacked out and I‚Äôd practised communicating what I learned through YouTube and articles on Medium. I never handed in a resume for Max Kelsen. ‚ÄúWe saw your LinkedIn profile.‚Äù My body of work was my resume. Regardless if you‚Äôre learning online or through a Masters Degree, having a portfolio of what you‚Äôve worked on is a great way to build skin in the game. ML and AI skills are in demand but that doesn‚Äôt mean you don‚Äôt have to showcase them. Even the best product won‚Äôt sell without any shelf space. Whether it be GitHub, Kaggle, LinkedIn or a blog, have somewhere where people can find you. Plus, having your own corner of the internet is great fun. Where do you go to learn these skills? What courses are the best? There‚Äôs no best answer. Everyone‚Äôs path will be different. Some people learn better with books, others learn better through videos. What‚Äôs more important than how you start is why you start. Start with why. Why do you want to learn these skills? Do you want to make money? Do you want to build things? Do you want to make a difference? There‚Äôs no right reason. All are valid in their own way. Start with why because having a why is more important than how. Having a why means when it gets hard and it  will  get hard, you‚Äôve got something to turn to. Something to remind you why you started. Got a why? Good. Time for some hard skills. I can only recommend what I‚Äôve tried. I‚Äôve completed courses from (in order): They‚Äôre all world-class. I‚Äôm a visual learner. I learn better seeing things being done. All of these courses do that. If you‚Äôre an absolute beginner, start with some introductory Python courses and when you‚Äôre a bit more confident, move into data science, machine learning and AI. DataCamp is great for beginners learning Python but wanting to learn it with a data science and machine learning focus. The highest level of math education I‚Äôve had was in high school. The rest I‚Äôve learned through Khan Academy as I‚Äôve needed it. There are many different opinions on how much math you need to know to get into machine learning and AI. I‚Äôll share mine. If you want to apply machine learning and AI techniques to a problem, you don‚Äôt necessarily need an in-depth understanding of the math to get a good result. Libraries such as TensorFlow and PyTorch allow someone with a bit of Python experience to build state of the art models whilst the math is taken care of behind the scenes. If you‚Äôre looking to get deep into machine learning and AI research, through means of a PhD program or something similar, having an in-depth knowledge of the math is paramount. In my case, I‚Äôm not looking to dive deep into the math and improve an algorithm‚Äôs performance by 10%. I‚Äôll leave that to people smarter than me. Instead, I‚Äôm more than happy to use the libraries available and manipulate them to help solve problems as I see fit. What a machine engineer does in practice might not be what you think. Despite the cover photos of many online articles, it doesn‚Äôt always involve working with robots that have red eyes. Here are a few questions a machine learning engineer has to ask themselves daily. I borrowed these from a  great article  by Rachel Thomas, one of the co-founders of  fast.ai , she goes into more depth in the full text. For more, I made a video of what we usually get up to on Monday‚Äôs at Max Kelsen. There‚Äôs no right or wrong way to get into ML or AI (or anything else). The beautiful thing about this field is we have access to some of the best technologies in the world, all we‚Äôve got to do is learn how to use them. You could begin by learning Python code (my favourite). You could begin by studying calculus and statistics. You could begin by learning about the philosophy of decision making. Machine learning and AI fascinate me because they meet at the intersection of all of these. The more I learn about it, the more I realise there‚Äôs plenty more to learn. And it gets me excited. Sometimes I get frustrated when my code doesn‚Äôt run. Or I don‚Äôt understand a concept. So I give up temporarily. I give up by letting myself walk away from the problem and take a nap. Or go for a walk. When I come back it feels like I‚Äôm looking at it with different eyes. The excitement comes back. I keep learning. I tell myself. I‚Äôm a learning machine. There‚Äôs so much happening in the field it can be daunting to get started. Too many options lead to no options. Ignore this. Start wherever interests you most and follow it. If it leads to a dead end, great, you‚Äôve figured out what you‚Äôre not interested in. Retrace your steps and take the other fork in the road instead. Computers are smart but they still can‚Äôt learn on their own. They need your help."
Machine Learning is Fun! Part 2,achine Learning is Fun! Part ,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 !  You can also read this article in  Italiano ,  Espa√±ol ,  Fran√ßais ,  T√ºrk√ße ,  –†—É—Å—Å–∫–∏–π ,  ÌïúÍµ≠Ïñ¥  Portugu√™s ,  ŸÅÿßÿ±ÿ≥€å ,  Ti·∫øng Vi·ªát   or  ÊôÆÈÄöËØù . Giant update:   I‚Äôve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! In  Part 1 , we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven‚Äôt already read  part 1 , read it now!). This time, we are going to see one of these generic algorithms do something really cool  ‚Äî  create video game levels that look like they were made by humans. We‚Äôll build a neural network, feed it existing Super Mario levels and watch new ones pop out! Just like  Part 1 , this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished. Back in  Part 1 , we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this: We ended up with this simple estimation function: In other words, we estimated the value of the house by multiplying each of its attributes by a  weight . Then we just added those numbers up to get the house‚Äôs value. Instead of using code, let‚Äôs represent that same function as a simple diagram: However this algorithm only works for simple problems where the result has a  linear  relationship with the input. What if the truth behind house prices isn‚Äôt so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn‚Äôt matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model? To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases: Now we have four different price estimates. Let‚Äôs combine those four price estimates into one final estimate. We‚Äôll run them through the same algorithm again (but using another set of weights)! Our new  Super Answer  combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model. Let‚Äôs combine our four attempts to guess into one big diagram: This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions. There‚Äôs a lot that I‚Äôm skipping over to keep this brief (including  feature scaling  and the  activation function ), but the most important part is that these basic ideas  click: It‚Äôs just like LEGO! We can‚Äôt model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together: The neural network we‚Äôve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it‚Äôs a  stateless algorithm . In many cases (like estimating the price of house), that‚Äôs exactly what you want. But the one thing this kind of model can‚Äôt do is respond to patterns in data over time. Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess? I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. Our model might look like this: But let‚Äôs make the problem harder. Let‚Äôs say I need to guess the  next  letter you are going to type at any point in your story. This is a much more interesting problem. Let‚Äôs use the first few words of Ernest Hemingway‚Äôs  The Sun Also Rises  as an example: Robert Cohn was once middleweight boxi What letter is going to come next? You probably guessed ‚Äôn‚Äô ‚Äî the word is probably going to be  boxing . We know this based on the letters we‚Äôve already seen in the sentence and our knowledge of common words in English. Also, the word ‚Äòmiddleweight‚Äô gives us an extra clue that we are talking about boxing. In other words, it‚Äôs easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English. To solve this problem with a neural network, we need to add  state  to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently. Keeping track of state in our model makes it possible to not just predict the most likely  first  letter in the story, but to predict the most likely  next  letter given all previous letters. This is the basic idea of a  Recurrent  Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory. Predicting the next letter in a story might seem pretty useless. What‚Äôs the point? One cool use might be auto-predict for a mobile phone keyboard: But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over ‚Äî forever? We‚Äôd be asking it to write a complete story for us! We saw how we could guess the next letter in Hemingway‚Äôs sentence. Let‚Äôs try generating a whole story in the style of Hemingway. To do this, we are going to use the  Recurrent Neural Network implementation  that  Andrej Karpathy  wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote  an excellent introduction to generating text with RNNs , You can view all the  code for the model on github . We‚Äôll create our model from the complete text of  The Sun Also Rises ‚Äî  362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway‚Äôs style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example. As we just start to train the RNN, it‚Äôs not very good at predicting letters. Here‚Äôs what it generates after a 100 loops of training: hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h‚Äôe btentmuhgehi bcgdltt. gey heho grpiahe. Ddelnss.eelaishaner‚Äù cot AAfhB ht ltny ehbih a‚Äùon bhnte ectrsnae abeahngy amo k ns aeo?cdse nh a taei.rairrhelardr er deffijha You can see that it has figured out that sometimes words have spaces between them, but that‚Äôs about it. After about 1000 iterations, things are looking more promising: hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been soms inick stalling that aid.  ‚ÄúHon‚Äôt me and acrained on .Hw‚Äôs don‚Äôt you for the roed,‚Äù In‚Äôs pair.‚Äù  ‚ÄúAlough marith him.‚Äù The model has started to identify the patterns in basic sentence structure. It‚Äôs adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there‚Äôs also still a lot of nonsense. But after several thousand more training iterations, it looks pretty good: He went over to the gate of the caf√©. It was like a country bed.  ‚ÄúDo you know it‚Äôs been me.‚Äù  ‚ÄúDamned us,‚Äù Bill said.  ‚ÄúI was dangerous,‚Äù I said. ‚ÄúYou were she did it and think I would a fine cape you,‚Äù I said.  ‚ÄúI can‚Äôt look strange in the cab.‚Äù  ‚ÄúYou know I was this is though,‚Äù Brett said.  ‚ÄúIt‚Äôs a fights no matter?‚Äù  ‚ÄúIt makes to do it.‚Äù  ‚ÄúYou make it?‚Äù  ‚ÄúSit down,‚Äù I said. ‚ÄúI wish I wasn‚Äôt do a little with the man.‚Äù  ‚ÄúYou found it.‚Äù  ‚ÄúI don‚Äôt know.‚Äù  ‚ÄúYou see, I‚Äôm sorry of chatches,‚Äù Bill said. ‚ÄúYou think it‚Äôs a friend off back and make you really drunk.‚Äù At this point, the algorithm has captured the basic pattern of Hemingway‚Äôs short, direct dialog. A few sentences even sort of make sense. Compare that with some real text from the book: There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave.  ‚ÄúSit down,‚Äù said Harvey, ‚ÄúI‚Äôve been looking for you.‚Äù  ‚ÄúWhat‚Äôs the matter?‚Äù  ‚ÄúNothing. Just looking for you.‚Äù  ‚ÄúBeen out to the races?‚Äù  ‚ÄúNo. Not since Sunday.‚Äù  ‚ÄúWhat do you hear from the States?‚Äù  ‚ÄúNothing. Absolutely nothing.‚Äù  ‚ÄúWhat‚Äôs the matter?‚Äù Even by only looking for patterns  one character at a time , our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing! We don‚Äôt have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters. For fun, let‚Äôs make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of ‚ÄúEr‚Äù, ‚ÄúHe‚Äù, and ‚ÄúThe S‚Äù: Not bad! But the  really mind-blowing part  is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking  recipes  or  fake Obama speeches . But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern. In 2015, Nintendo released  Super Mario Maker‚Ñ¢  for the Wii U gaming system. This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It‚Äôs like a virtual LEGO set for people who grew up playing Super Mario Brothers. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? First, we need a data set for training our model. Let‚Äôs take all the outdoor levels from the original Super Mario Brothers game released in 1985: This game has 32 levels and about 70% of them have the same outdoor style. So we‚Äôll stick to those. To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game‚Äôs memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game‚Äôs memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime. Here‚Äôs the first level from the game (which you probably remember if you ever played it): If we look closely, we can see the level is made of a simple grid of objects: We could just as easily represent this grid as a sequence of characters with one character representing each object: We‚Äôve replaced each object in the level with a letter: ‚Ä¶and so on, using a different letter for each different kind of object in the level. I ended up with text files that looked like this: Looking at the text file, you can see that Mario levels don‚Äôt really have much of a pattern if you read them line-by-line: The patterns in a level really emerge when you think of the level as a series of columns: So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called  feature selection ) is one of the keys of using machine learning algorithms well. To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up: Just like we saw when creating the model of Hemingway‚Äôs prose, a model improves as we train it. After a little training, our model is generating junk: It sort of has an idea that ‚Äò-‚Äôs and ‚Äò=‚Äôs should show up a lot, but that‚Äôs about it. It hasn‚Äôt figured out the pattern yet. After several thousand iterations, it‚Äôs starting to look like something: The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the ‚ÄúP‚Äùs in the data should appear in 2x2 clusters. That‚Äôs pretty cool! With a lot more training, the model gets to the point where it generates perfectly valid data: Let‚Äôs sample an entire level‚Äôs worth of data from our model and rotate it back horizontal: This data looks great! There are several awesome things to notice: Finally, let‚Äôs take this level and recreate it in Super Mario Maker: Play it yourself! If you have Super Mario Maker, you can play this level by  bookmarking it online  or by looking it up using level code  4AC9‚Äì0000‚Äì0157-F3C3 . The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‚Äòtoy‚Äô instead of cutting-edge is that our model is generated from very little data. There just aren‚Äôt enough levels in the original Super Mario Brothers game to provide enough data for a really good model. If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can‚Äôt ‚Äî because Nintendo won‚Äôt let us have them. Big companies don‚Äôt give away their data for free. As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That‚Äôs why companies like Google and Facebook need your data so badly! For example, Google recently open sourced  TensorFlow , its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate. But without Google‚Äôs massive trove of data in every language, you can‚Äôt create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your  Google Maps Location History  or  Facebook Location History  and notice that it stores every place you‚Äôve ever been. In machine learning, there‚Äôs never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often  combining multiple approaches  will give you better results than any single approach. Readers have sent me links to other interesting approaches to generating Super Mario levels: If you liked this article, please consider  signing up for my Machine Learning is Fun! email list . I‚Äôll only email you when I have something new and awesome to share. It‚Äôs the best way to find out when I write more articles like this. You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I‚Äôd love to hear from you if I can help you or your team with machine learning. Now continue on to  Machine Learning is Fun Part 3 !"
30 Amazing Machine Learning Projects for the Past Year (v.2018),0 Amazing Machine Learning Projects for the Past Year (v.2018,"For the past year , we‚Äôve compared nearly 8,800 open source Machine Learning projects to pick Top 30 (0.3% chance). This is an extremely competitive list and it carefully picks the best open source Machine Learning libraries, datasets and apps published between January and December 2017.  Mybridge AI  evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of  Github stars is 3,558. Open source projects can be useful for data scientists. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Machine Learning projects you may have missed for the past year. <Recommended Learning> A) Neural Networks Deep Learning A-Z‚Ñ¢: Hands-On Artificial Neural Networks [68,745 recommends, 4.5/5 stars] B) TensorFlow Complete Guide to TensorFlow for Deep Learning with Python [17,834 recommends, 4.6/5 stars] <Others> A) Web hosting : Get free domain name for a year. For your ‚Äòsimple‚Äô personal website or project site. (Click the numbers below. Credit given to the biggest contributor.) FastText: Library for fast text representation and classification.  [11786 stars on Github] . Courtesy of  Facebook Research ‚Ä¶‚Ä¶‚Ä¶.. [  Muse : Multilingual Unsupervised or Supervised word Embeddings, based on Fast Text. 695 stars on Github] Deep-photo-styletransfer: Code and data for paper ‚ÄúDeep Photo Style Transfer‚Äù  [9747 stars on Github] . Courtesy of Fujun Luan, Ph.D. at Cornell University The world‚Äôs simplest facial recognition api for Python and the command line  [8672 stars on Github] . Courtesy of  Adam Geitgey Magenta: Music and Art Generation with Machine Intelligence  [8113 stars on Github] . Sonnet: TensorFlow-based neural network library  [5731 stars on Github] . Courtesy of  Malcolm Reynolds  at Deepmind deeplearn.js: A hardware-accelerated machine intelligence library for the web  [5462 stars on Github] . Courtesy of Nikhil Thorat at Google Brain Fast Style Transfer in TensorFlow  [4843 stars on Github] . Courtesy of  Logan Engstrom  at MIT Pysc2: StarCraft II Learning Environment  [3683 stars on Github] . Courtesy of Timo Ewalds at DeepMind AirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research  [3861 stars on Github] . Courtesy of  Shital Shah  at Microsoft Facets: Visualizations for machine learning datasets  [3371 stars on Github] . Courtesy of Google Brain Style2Paints: AI colorization of images  [3310 stars on Github] . Tensor2Tensor: A library for generalized sequence to sequence models ‚Äî Google Research  [3087 stars on Github] . Courtesy of  Ryan Sepassi  at Google Brain Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more)  [2847 stars on Github] . Courtesy of Jun-Yan Zhu, Ph.D at Berkeley Faiss: A library for efficient similarity search and clustering of dense vectors.  [2629 stars on Github] . Courtesy of Facebook Research Fashion-mnist: A MNIST-like fashion product database  [2780 stars on Github] . Courtesy of Han Xiao, Research Scientist  Zalando Tech ParlAI: A framework for training and evaluating AI models on a variety of openly available dialog datasets  [2578 stars on Github] . Courtesy of Alexander Miller at  Facebook Research Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit  [2571 stars on Github] . Pyro: Deep universal probabilistic programming with Python and PyTorch  [2387 stars on Github] . Courtesy of Uber AI Labs iGAN: Interactive Image Generation powered by GAN  [2369 stars on Github] . Deep-image-prior: Image restoration with neural networks but without learning  [2188 stars on Github] . Courtesy of Dmitry Ulyanov, Ph.D at Skoltech Face_classification: Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV.  [1967 stars on Github] . Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind‚Äôs WaveNet and tensorflow  [1961 stars on Github] . Courtesy of Namju Kim at Kakao Brain StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation  [1954 stars on Github] . Courtesy of  Yunjey Choi  at Korea University Ml-agents: Unity Machine Learning Agents  [1658 stars on Github] . Courtesy of  Arthur Juliani , Deep Learning at Unity3D DeepVideoAnalytics: A distributed visual search and visual data analytics platform  [1494 stars on Github] . Courtesy of  Akshay Bhat , Ph.D at Cornell University OpenNMT: Open-Source Neural Machine Translation in Torch  [1490 stars on Github] . Pix2pixHD: Synthesizing and manipulating 2048x1024 images with conditional GANs  [1283 stars on Github] . Courtesy of  Ming-Yu Liu  at AI Research Scientist at Nvidia Horovod: Distributed training framework for TensorFlow.  [1188 stars on Github] . Courtesy of Uber Engineering AI-Blocks: A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models  [899 stars on Github] . Deep neural networks for voice conversion (voice style transfer) in Tensorflow  [845 stars on Github] . Courtesy of  Dabi Ahn , AI Research at Kakao Brain That‚Äôs it for Machine Learning Open Source of the Year. Visit  our publication   to find top posts for more programming skills."
Essential Math for Data Science,ssential Math for Data Scienc,"M athematics is the bedrock of any contemporary discipline of science. Almost all the techniques of modern data science, including machine learning, have a deep mathematical underpinning. It goes without saying that you will absolutely need all the other pearls of knowledge‚Äîprogramming‚Ä¶"
The best Mario Kart character according to data science,he best Mario Kart character according to data scienc,"Mario Kart was a staple of my childhood ‚Äî my friends and I would spend hours after school as Mario, Luigi, and other characters from the Nintendo universe racing around cartoonish tracks and lobbing pixelated bananas at each other. One thing that always vexed our little group of would-be speedsters was the question of which character was best. Some people swore by zippy Yoshi, others argued that big, heavy Bowser was the best option. Back then there were only eight options to choose from; fast forward to the current iteration of the Mario Kart franchise and the question is even more complicated because you can select different karts and tires to go with your character. My Mario Kart reflexes aren‚Äôt what they used to be, but I am better at data science than I was as a fourth grader, so in this post I‚Äôll use data to finally answer the question ‚ÄúWho is the best character in Mario Kart?‚Äù This is a tricky question because there are tons of potential character / kart / tire configurations now and they all have widely varying stats across a number of attributes. In general, it isn‚Äôt possible to optimize across multiple dimensions simultaneously, however some setups are undeniably worse than others. The question for an aspiring Mario Kart champion nowadays is ‚ÄúHow can I pick a character / kart / tire combination that is in some sense optimal, even if there isn‚Äôt one ‚Äòbest‚Äô option?‚Äù To answer this question we turn to one of Mario‚Äôs compatriots, the nineteenth century Italian economist Vilfredo Pareto who introduced the concept of  Pareto efficiency  and the related  Pareto frontier . The concept of Pareto efficiency applies to situations where there is a finite pool of resources and multiple competing outcomes that depend on how those resources are allocated. The ‚ÄúPareto efficient‚Äù allocations are those in which it‚Äôs impossible to improve one outcome without worsening another outcome. This is more easily explained with a picture (courtesy of  Wikipedia ). Each circle is a potential resource allocation, which in our case means a distribution of stat points across the different attributes like weight, handling, and traction (characters in Mario Kart have about the same number of total stat points, and differ only in their distribution). The position of each circle represents the outcome of that allocation on two competing dimensions, for example speed and acceleration. The allocations in red lie on the Pareto frontier: for each of these allocations, an improvement in one outcome requires a decrease in the other. Allocations in grey are not Pareto efficient because you can improve both outcomes with a different allocation of resources. Speed and acceleration are generally the two most important attributes in Mario Kart, so the goal of this analysis is to identify the character / kart / tire configurations that lie on the Pareto frontier for speed and acceleration. We‚Äôll start by examining the stats of each character, kart, and tire independently using some fan-compiled  data . One particular quirk of Mario Kart is that while there are a couple dozen characters, many of them have identical stats. From here on out, I‚Äôll refer to the character (or kart, or tire) class by the name of one of its members. For example, in the heatmap below the row labelled ‚ÄòPeach‚Äô also describes the stats for Daisy and Yoshi. The complete class memberships are listed at the end of the post in case you want to see where your favorite character lands. There are seven classes of characters. Let‚Äôs have a look at how their stats compare. The most obvious trend is the trade-off between speed and acceleration: heavy characters have good speed but poor acceleration, while light characters have snappy acceleration but a low top speed. There are variations in the other stats as well, but to a large extent, the speed and acceleration dominate the performance of a particular set up, so we‚Äôll be ignoring the rest of the stats. Karts and tires modify the base stats of the characters: the attributes of the final configuration are a sum of the character‚Äôs stats and the kart / tire modifiers. As with characters, there are dozens of karts and tires, but only a few categories with different stats. The trends here are less obvious, but they generally agree with what we saw in the character stats: improvements in speed come at the expense of acceleration, and vice versa. Our goal is to find all the configurations that have an optimal combination of speed and acceleration, so the next step is to compute the stats for each unique (character, kart, tire) combination. With a little bit of Python we can enumerate all character / kart / tire combinations and calculate their attributes by adding up the values in the figures above. Equipped with the statistics for each possible combination, we can can plot the speed vs. the acceleration of each possible setup, and identify those that lie on the Pareto frontier. According to the above chart, the optimal configurations make up a fairly small subset of the total possible setups. We can quantify this by counting all the different combinations (note that some combinations overlap in the figure). Just for fun, let‚Äôs also count up the possible combinations including all the characters, karts, and tires with identical stats. Possible combinations: 149760 Unique stat combinations: 294 Optimal combinations: 15 The optimal configurations make up just 5% of the potential unique stat configurations! Let‚Äôs have a look at what these optimal configurations look like. Unless you‚Äôre going all-in on acceleration, it looks like a heavy character is the way to go; the two heaviest character classes (Wario and Donkey Kong) account for 11/15 of the Pareto-optimal configurations. We can also look at the other main stats for each of these configurations. So there it is, if speed and acceleration are your main concerns, then one of these 15 configurations is your best bet. Sometimes an optimal configuration isn‚Äôt what you‚Äôre looking for though (say, because your roommate threatened to stop playing if there wasn‚Äôt some sort of handicap, to choose a random example). In that case, we can explore all the possible configurations with a quick  bokeh  interactive graphic. A few observations: If you‚Äôd like to see the code behind this analysis you can find it  here . And finally, in case you have a particular attachment to one of the characters (or karts / tires) you can look up which class he / she / it belongs to below. Character Classes ***************** - Baby Mario, Baby Luigi, Baby Peach, Baby Daisy, Baby Rosalina, Lemmy Koopa, Mii Light - Toad, Shy Guy, Koopa Troopa, Lakitu, Wendy Koopa, Larry Koopa, Toadette - Peach, Daisy, Yoshi - Mario, Luigi, Iggy Koopa, Ludwig Koopa, Mii Medium - Donkey Kong, Waluigi, Rosalina, Roy Koopa - Metal Mario, Pink Gold Peach - Wario, Bowser, Morton Koopa, Mii Heavy Body Classes ***************** - Standard Kart, Prancer, Cat Cruiser, Sneeker, The Duke, Teddy Buggy - Gold Standard, Mach 8, Circuit Special, Sports Coupe - Badwagon, TriSpeeder, Steel Driver, Standard ATV - Biddybuggy, Landship, Mr. Scooty - Pipe Frame, Standard Bike, Flame Ride, Varmit, Wild Wiggler - Sports Bike, Jet Bike, Comet, Yoshi Bike Tire Classes ***************** - Standard, Blue Standard, Offroad, Retro Offroad - Monster, Hot Monster - Slick, Cyber Slick - Roller, Azure Roller, Button - Slim, Crimson Slim - Metal, Gold - Wood, Sponge, Cushion"
"If you want to learn Data Science, start with one of these programming classes","f you want to learn Data Science, start with one of these programming classe","A year ago, I was a numbers geek with no coding background. After trying an online programming course, I was so inspired that I enrolled in one of the best computer science programs in Canada. Two weeks later, I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. So I dropped out. The decision was not difficult. I could learn the content I wanted to faster, more efficiently, and for a fraction of the cost. I already had a university degree and, perhaps more importantly, I already had the university experience. Paying $30K+ to go back to school seemed irresponsible. I started creating my own  data science master‚Äôs degree  using online courses shortly afterwards, after realizing it was a better fit for me than computer science. I scoured the introduction to programming landscape. I‚Äôve already taken several courses and audited portions of many others. I know the options, and what skills are needed if you‚Äôre targeting a data analyst or data scientist role. For this guide, I spent 20+ hours trying to find every single online introduction to programming course offered as of August 2016, extracting key bits of information from their syllabi and reviews, and compiling their ratings. For this task, I turned to none other than the open source Class Central community and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it‚Äôs David. I wrote this guide back in 2016. Since then, I‚Äôve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros.  My top programming course recommendation for 2023 is in there, too. datamaverickhq.com Okay, back to the guide. Each course had to fit four criteria: We believe we covered every notable course that exists and which fits the above criteria. Since there are seemingly hundreds of courses on Udemy in Python and R, we chose to consider the most reviewed and highest rated ones only. There is a chance we missed something, however. Please let us know if you think that is the case. We compiled average rating and number of reviews from Class Central and other review sites. We calculated a weighted average rating for each course. If a series had multiple courses (like Rice University‚Äôs  Part 1  and  Part 2 ), we calculated the weighted average rating across all courses. We also read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on three factors: Programming is not computer science and vice versa. There is a difference of which beginners may not be acutely aware. Borrowing  this answer  from Programmers Stack Exchange: Computer science is the study of what computers [can] do; programming is the practice of making computers do things. The course we are looking for introduces  programming  and optionally touches on relevant aspects of computer science that would benefit a new programmer in terms of awareness. Many of the courses considered, you‚Äôll notice, do indeed have a computer science portion. None of the courses, however, are strictly computer science courses, which is why something like  Harvard‚Äôs CS50x  on edX is excluded. University of Toronto‚Äôs ‚ÄúLearn to Program‚Äù series on Coursera.  LTP1: The Fundamentals  and  LTP2: Crafting Quality Code  have a near-perfect weighted average rating of 4.71 out of 5 stars over 284 reviews. They also have a great mix of content difficulty and scope for the beginner data scientist. This free, Python-based introduction to programming sets itself apart from the other 20+ courses we considered. Jennifer Campbell and Paul Gries, two associate professors in the University of Toronto‚Äôs department of computer science (which is regarded as  one of the best in the world ) teach the series. The self-paced, self-contained Coursera courses match the material in their book, ‚Äú Practical Programming: An Introduction to Computer Science Using Python 3 .‚Äù LTP1 covers 40‚Äì50% of the book and LTP2 covers another 40%. The 10‚Äì20% not covered is not particularly useful for data science, which helped their case for being our pick. The professors kindly and promptly sent me detailed course syllabi upon request, which were difficult to find online prior to the course‚Äôs official restart in September 2016. Learn to Program: The Fundamentals (LTP1) Timeline: 7 weeks Estimated time commitment: 6‚Äì8 hours per week This course provides an introduction to computer programming intended for people with no programming experience. It covers the basics of programming in Python including elementary data types (numeric types, strings, lists, dictionaries, and files), control flow, functions, objects, methods, fields, and mutability. Modules Learn to Program: Crafting Quality Code (LTP2) Timeline: 5 weeks Estimated time commitment: 6‚Äì8 hours per week You know the basics of programming in Python: elementary data types (numeric types, strings, lists, dictionaries, and files), control flow, functions, objects, methods, fields, and mutability.  You need to be good at these in order to succeed in this course. LTP: Crafting Quality Code  covers the next steps: designing larger programs, testing your code so that you know it works, reading code in order to understand how efficient it is, and creating your own types. Modules Associate professor Gries also provided the following commentary on the course structure: ‚ÄúEach module has between about 45 minutes to a bit more than an hour of video. There are in-video quiz questions, which will bring the total time spent studying the videos to perhaps 2 hours.‚Äù These videos are generally shorter than ten minutes each. He continued: ‚ÄúIn addition, we have one exercise (a dozen or two or so multiple choice and short-answer questions) per module, which should take an hour or two. There are three programming assignments in LTP1, each of which might take four to eight hours of work. There are two programming assignments in LTP2 of similar size.‚Äù He emphasized that the estimate of 6‚Äì8 hours per week is a rough guess: ‚ÄúEstimating time spent is incredibly student-dependent, so please take my estimates in that context. For example, someone who knows a bit of programming, perhaps in another programming language, might take half the time of someone completely new to programming. Sometimes someone will get stuck on a concept for a couple of hours, while they might breeze through on other concepts ‚Ä¶ That‚Äôs one of the reasons the self-paced format is so appealing to us.‚Äù In total, the University of Toronto‚Äôs  Learn to Program  series runs an estimated 12 weeks at 6‚Äì8 hours per week, which is about standard for most online courses created by universities. If you prefer to binge-study your MOOCs, that‚Äôs 72‚Äì96 hours, which could feasibly be completed in two to three weeks, especially if you have a bit of programming experience. If you already have some familiarity with programming, and don‚Äôt mind a syllabus that has a notable skew towards games and interactive applications, I would also recommend Rice University‚Äôs An Introduction to Interactive Programming in Python ( Part 1  and  Part 2 ) on Coursera. With 6,000+ reviews and the highest weighted average rating of 4.93/5 stars, this popular course is noted for its engaging videos, challenging quizzes, and enjoyable mini projects. It‚Äôs slightly more difficult, and focuses less on the fundamentals and more on topics that aren‚Äôt applicable in data science than our #1 pick. These courses are also part of the 7 course  Principles in Computing Specialization  on Coursera. The materials are self-paced and free, and a paid certificate is available. The course must be purchased for $79 (USD) for access to graded materials. The condensed course description and full syllabus are as follows: ‚ÄúThis two-part course is designed to help students with very little or no computing background learn the basics of building simple interactive applications ‚Ä¶ To make learning Python easy, we have developed a new browser-based programming environment that makes developing interactive applications in Python simple. These applications will involve windows whose contents are graphical and respond to buttons, the keyboard, and the mouse. Recommended background: A knowledge of high school mathematics is required.  While the class is designed for students with no prior programming experience, some beginning programmers have viewed the class as being fast-paced . For students interested in some light preparation prior to the start of class, we recommend a self-paced Python learning site such as codecademy.com.‚Äù Timeline: 5 weeks Estimated time commitment: 7‚Äì10 hours per week Week 0 ‚Äî statements, expressions, variables   Understand the structure of this class, and explore Python as a calculator. Week 1 ‚Äî functions, logic, conditionals  Learn the basic constructs of Python programming, and create a program that plays a variant of Rock-Paper-Scissors. Week 2 ‚Äî event-driven programming, local/global variables  Learn the basics of event-driven programming, understand the difference between local and global variables, and create an interactive program that plays a simple guessing game. Week 3 ‚Äî canvas, drawing, timers  Create a canvas in Python, learn how to draw on the canvas, and create a digital stopwatch. Week 4 ‚Äî lists, keyboard input, the basics of modeling motion  Learn the basics of lists in Python, model moving objects in Python, and recreate the classic arcade game ‚ÄúPong.‚Äù Week 5 ‚Äî mouse input, list methods, dictionaries   Read mouse input, learn about list methods and dictionaries, and draw images.   Week 6 ‚Äî classes and object-oriented programming   Learn the basics of object-oriented programming in Python using classes, and work with tiled images. Week 7 ‚Äî basic game physics, sprites   Understand the math of acceleration and friction, work with sprites, and add sound to your game. Week 8 ‚Äî sets and animation   Learn about sets in Python, compute collisions between sprites, and animate sprites. If you are set on an introduction to programming course in R, we recommend DataCamp‚Äôs series of R courses:  Introduction to R ,  Intermediate R ,  Intermediate R ‚Äî Practice , and  Writing Functions in R . Though the latter three come at a price point of $25/month, DataCamp is best in category for covering the programming fundamentals and R-specific topics, which is reflected in its average rating of 4.29/5 stars. We believe the best approach to learning programming for data science using  online courses   is to do it first through Python. Why? There is a lack of MOOC options that teach core programming principles and use R as the language of instruction. We found six such R courses that fit our testing criteria, compared to twenty-two Python-based courses. Most of the R courses didn‚Äôt receive great ratings and failed to meet most of our subjective testing criteria. The series breakdown is as follows: Estimated time commitment: 4 hours Chapters: Estimated time commitment: 6 hours Chapters: Estimated time commitment: 4 hours This follow-up course on intermediate R does not cover new programming concepts. Instead, you will strengthen your knowledge of the topics in intermediate R with a bunch of new and fun exercises. Estimated time commitment: 4 hours Chapters: Another option for R would be to take a Python-based introduction to programming course to cover the fundamentals of programming, and then pick up R syntax with an R basics course. This is what I did, but I did it with Udacity‚Äôs  Data Analysis with R . It worked well for me. You can also pick up R with our  top recommendation for a statistics class , which teaches the basics of R through coding up stats problems. Our #1 and #2 picks had a 4.71 and 4.93 star weighted average rating over 284 and 6,069 reviews, respectively. Let‚Äôs look at the other alternatives. This is the first of a six-piece series that covers the best MOOCs for launching yourself into the data science field. It will cover several other data science core competencies:  statistics , the  data science process , data visualization, and machine learning. medium.freecodecamp.com medium.freecodecamp.com The final piece will be a summary of those courses, and the best MOOCs for other key topics such as data wrangling, databases, and even software engineering. If you‚Äôre looking for a complete list of Data Science MOOCs, you can find them on Class Central‚Äôs  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ‚Äôs other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the üíö so more people will see it here on Medium. This is a condensed version of the  original article published on Class Central , where course descriptions, syllabi, and multiple reviews are included."
How To Learn Data Science If You‚Äôre Broke,ow To Learn Data Science If You‚Äôre Brok,"Over the last year, I taught myself data science. I learned from hundreds of online resources and studied 6‚Äì8 hours every day. All while working for minimum wage at a day-care. My goal was to start a career I was passionate about, despite my lack of funds. Because of this choice I have accomplished a lot over the last few months. I published my own  website , was posted in a major online data science  publication , and was given scholarships to a competitive computer science graduate  program . In the following article, I give guidelines and advice so you can make your own data science curriculum. I hope to give others the tools to begin their own educational journey. So they can begin to work towards a more passionate career in data science. When I say ‚Äúdata science‚Äù, I am referring to the collection of tools that turn data into real-world actions. These include machine learning, database technologies, statistics, programming, and domain-specific technologies. The internet is a chaotic mess. Learning from it can often feel like drinking from the fun end of a fire-hose. There are simpler alternatives that offer to sort the mess for you. Sites like  Dataquest ,  DataCamp , and  Udacity  all offer to teach you data science skills. Each creating an education program that shepherds you from topic to topic. Each requires little course-planning on your part. The problem? They cost too much, they don‚Äôt teach you how to apply concepts in a job setting, and they prevent you from exploring your own interests and passions. There are free alternatives like  edX  and  coursera  which offer one-off courses diving into specific topics. If you learn well from videos or a classroom setting, these are excellent ways to learn data science. Check out this  website  for a listing of available data science courses. There are also a few free course curricula you can use. Check out  David Venturi ‚Äôs post, or the  Open Source DS Masters  (a more traditional education plan). If you learn well from reading, look at the  Data Science From Scratch  book. This textbook is a full learning plan that can be supplemented with online resources. You can find the full book online or get a physical copy from  Amazon  ($27). These are just a few of the free resources that provide a detailed learning path for data science. There are many more. To better understand the skills you need to acquire on your educational journey, in the next section I detail a broader curriculum guideline. This is intended to be high-level, and not just a list of courses to take or books to read. Programming is a fundamental skill of data scientists. Get comfortable with the syntax of Python. Understand how to run a python program in many different ways. (Jupyter notebook vs. command line vs IDE) I took about a month to review the  Python docs , the  Hitchhiker‚Äôs Guide to Python , and coding challenges on  CodeSignal . Hint: Keep an ear out for common problem-solving techniques used by programmers. (pronounced ‚Äúalgorithms‚Äù) A prerequisite for machine learning and data analysis. If you already have a solid understanding spend a week or two brushing up on key concepts. Focus especially hard on descriptive statistics . Being able to understand a data set is a skill worth its weight in gold. Learn how to load, manipulate, and visualize data. Mastery of these libraries will be crucial to your personal projects. Quick hint: Don‚Äôt feel like you have to memorize every method or function name, that comes with practice. If you forget, Google it. Check out the  Pandas Docs ,  Numpy Docs , and  Matplotlib Tutorials . There are better resources out there, but these are what I used. Remember, the only way you will learn these libraries is by using them! Learn the theory and application of machine learning algorithms. Then apply the concepts you learn to real-world data that you care about. Most beginners start by working with toy data-sets from the  UCI ML Repository . Play around with the data and go through guided ML tutorials. The  Scikit-learn  documentation has excellent tutorials on the application of common algorithms. I also found this  podcast  to be a great (and free) educational resource behind the theory of ML. You can listen to it on your commute or while working out. Getting a job means being able to take real-world data and turn it into action. To do this you will need to learn how to use a business‚Äô computational resources to get, transform, and process data. This is the most under-taught part of the data science curriculum. Mainly because the specific tools you use depend on the industry you are going in to. However, database manipulation is a required skill set.  You can learn how to manipulate databases with code on  ModeAnalytics  or  Codecademy . You can also implement your own database (cheaply) on  DigitalOcean . Another (often) required skill is  version control .  You can acquire this skill easily by creating a  GitHub  account and using the command line to commit your code daily. When considering what other technologies to learn, it is important to think about your interests and passions. For example, if you are interested in web development, then look into the tools used by companies in that industry. There are literally thousands of web pages and forums explaining the use of common data science tools. Because of this, it is very easy to get side-tracked while learning¬†online. When you start researching a topic you need to hold your goal in mind. If you don‚Äôt, you risk getting caught up in whatever catchy link draws your eye. The solution,  get a good storage system to save interesting web-resources . This way you can save material for later, and focus on the topic that is relevant to you at the moment. If you do this right, you can make an ordered learning path that shows you what you should be focused on. You will also learn faster and avoid being distracted. Warning,  your reading list will quickly grow into the hundreds  as you explore new topics that interest you. Don‚Äôt worry, this leads us to my second piece of advice. Having a self-driven education can often feel like trying to read a never-ending library of knowledge. If you‚Äôre going to be successful in data science you need to think of your education as a lifelong process. Just remember, the process of learning is its own reward. Throughout your educational journey, you will explore your interests and discover more about what drives you . The more you learn about yourself, the more enjoyment you will get out of learning. Don‚Äôt settle for just learning a concept and then moving to the next thing. The process of learning doesn‚Äôt stop until you can apply a concept to the real world. Not every concept needs to have a dedicated project in your portfolio. But it is important to stay grounded and  remember that you are learning so you can make an impact in the world. When it comes down to it,  skepticism is one of the biggest adversities you will face when learning data science. This may come from others, or it may come from  yourself . Your portfolio is your way of showing the world that you are capable and confident in your own skills. Because of this, building a portfolio is the single most important thing you can do while studying data science. A good portfolio can land you a job and make you a more confident data scientist. Fill your portfolio with projects that you are proud of. Did you build your own web app from scratch? Did you make your own IMDB database? Have you written an interesting data analysis of healthcare data? Put it in your portfolio. Just make sure write-ups are readable, the code is well documented, and the portfolio itself looks good. This is my portfolio. A simpler method to publish your portfolio is to create a GitHub repository that includes a great ReadMe (summary page) as well as relevant project files. Here is an aesthetically pleasing, yet simple,  GitHub portfolio . For a more advanced portfolio, look into GitHub-IO to host your own free website. ( example ) Data science is a set of tools intended to make a change in the world. Some data scientists build computer vision systems to diagnose medical images, others traverse billions of data entries to find patterns in website user preferences. The applications of data science are endless, that‚Äôs why it is important to find what applications excite you. If you find topics that you are passionate about, you will be more willing to put in the work to make a great project. This leads to my favorite piece of advice in this article. When you are learning, keep your eyes open for projects or ideas that excite you. Once you have spent time learning, try to connect the dots. Find similarities between projects that fascinate you. Then spend some time researching industries that work on those types of projects. Once you find an industry that you are passionate about, make it your goal to acquire the skills and technical expertise needed in that business. If you can do this, you will be primed to turn your hard work and dedication for learning into a passionate and successful career. If you love making discoveries about the world. If you are fascinated by artificial intelligence. Then you can break into the data science industry no matter what your situation is. It won‚Äôt be easy. To motivate your own education you will need perseverance and¬†discipline. But if you are the type of person who can push yourself to improve, you are more than capable of mastering these skills on your own. After all, that‚Äôs what being a data scientist is all about. Being curious, self-driven, and passionate about finding answers."
How to Build a Data Science Portfolio,ow to Build a Data Science Portfoli,"How do you get a job in data science?  Knowing enough statistics, machine learning, programming, etc to be able to get a job is difficult. One thing I have found lately is quite a few people  may   have the required skills to get a job, but no portfolio . While a resume matters, having a portfolio of public evidence of your data science skills can do wonders for your job prospects. Even if you have a  referral ,  the ability to show potential employers what you can do instead of just telling them you can do something is important . This post will include links to where various data science professionals (data science managers, data scientists, social media icons, or some combination thereof) and others talk about what to have in a portfolio and how to get noticed. With that, let‚Äôs get started! Besides the benefit of learning by making a portfolio, a portfolio is important as it can help get you employment. For the purpose of this article, let‚Äôs define a portfolio as public evidence of your data science skills. I got this definition from  David Robinson  Chief Data Scientist at DataCamp when he was interviewed by  Marissa Gemma  on  Mode Analytics blog . He was asked about landing his first job in industry and said, The most effective strategy for me was doing public work. I blogged and did a lot of open source development late in my PhD, and these helped give public evidence of my data science skills. But the way I landed my first industry job was a particularly noteworthy example of the public work. During my PhD I was an active answerer on the programming site Stack Overflow, and an engineer at the company came across one of my answers (one explaining the intuition behind the beta distribution). He was so impressed with the answer that he got in touch with me [through Twitter], and a few interviews later  I was hired . You may think of this as a freak occurrence, but you will often find that the more active you are, the greater chance you have of something like this occuring. From  David‚Äôs blog post , The more public work you do, the higher the chance of a freak accident like that: of someone noticing your work and pointing you towards a job opportunity, or of someone who‚Äôs interviewing you having heard of work you‚Äôve done. People often forget that software engineers and data scientists also Google their issues. If these same people have their problems solved by reading your public work, they might think better of you and reach out to you. Even for an entry level role, most companies want to have people with at least a little bit of real life experience. You may have seen memes like the one below. The question is how do you get experience if you need experience to get your first job? If there is an answer, the answer is  projects . Projects are perhaps the best substitutes for work experience or as  Will Stanton  said, If you don‚Äôt have any experience as a data scientist, then you absolutely  have to  do independent projects. In fact, when  Jacqueline Nolis   interviews candidates , she wants to hear about a description of a recent problem/project that you have faced. I want to hear about a project they‚Äôve worked on recently. I ask them about how the project started, how they determined it was worth time and effort, their process, and their results. I also ask them about what they learned from the project. I gain a lot from answers to this question: if they can tell a narrative, how the problem related to the bigger picture, and how they tackled the hard work of doing something. If you don‚Äôt have some data science related work experience, the best option here is to talk about a data science project that you have worked on. Data science is such a broad field that it is hard to know what kind of projects hiring managers want to see.  William Chen , a Data Science Manager at Quora, shared his thoughts on the subject at Kaggle‚Äôs CareerCon 2018 ( video ). I love projects where people show that they are interested in data in a way that goes beyond homework assignments. Any sort of class final project where you explore an interesting dataset and find interesting results‚Ä¶ Put effort into the writeup‚Ä¶ I really like seeing really good writeups where people find interesting and novel things‚Ä¶have some visualizations and share their work. A lot of people recognize the value of creating projects, but one issue a lot of people wonder is where do you get that interesting dataset and what do you do with it.  Jason Goodman , Data Scientist at Airbnb, has a post  Advice on Building Data Portfolio Projects  where he talks about many different project ideas and has good advice on what kind of datasets you should use. He also echos one of William‚Äôs points about working with interesting data. I find that the best portfolio projects are less about doing fancy modeling and more about working with interesting data. A lot of people do things with financial information or Twitter data; those can work, but the data isn‚Äôt inherently that interesting, so you‚Äôre working uphill. One of his other points in the article is that webscraping is a great way to get interesting data. If you are interested in learning how to build your own dataset by webscraping in Python, you can see my post  here . If you are coming from academia, it is important to note that your thesis can count as a project (a very large project). You can hear  William Chen  talk about it  here . One thing I have found very common (to the point of it appearing multiple times in this blog post) in a lot of portfolio/resume advice is not to have common projects in your portfolio. Jeremie Harris  in  The 4 fastest ways not to get hired as a data scientist  said, It‚Äôs hard to think of a faster way to have your resume thrown into the ‚Äòdefinite no‚Äô pile than featuring work you did on trivial proof-of-concept datasets among your highlighted personal projects. When in doubt, here are some projects that hurt you more than they help you: * Survival classification on the  Titanic dataset . * Hand-written digit classification on the  MNIST dataset . * Flower species classification using the  iris dataset . The image below shows partial examples of classification of Titanic (A), MNIST (B), and iris (C) datasets. There aren‚Äôt a lot of ways to use these datasets to distinguish yourself from other applicants. Make sure to list novel projects. Favio Vazquez  has an  excellent article  where he talked about how he got his job as a data scientist. Of course, one of his tips is to have a portfolio. Have a portfolio. If you are looking for a serious paid job in data science do some projects with real data. If you can post them on GitHub. Apart from Kaggle competitions, find something that you love or a problem you want to solve and use your knowledge to do it. One of the other interesting findings was that you always have to keep on improving as you go through the job hunt. I applied to almost 125 jobs (for real, maybe you applied for much more), I got only like 25‚Äì30 replies. Some of them were just: Thanks but nope. And I got almost 15 interviews. I learned from each one. Got better. I had to deal with a lot of rejection. Something I was actually not prepared to. But I loved the process of getting interviewed (not all of them to be honest). I studied a lot, programmed everyday, read a lot of articles and posts. They helped a lot. As you learn more and improve yourself, your portfolio should also be updated. This same sentiment is echoed in many other advice articles. As  Jason Goodman  said, The project isn‚Äôt done when you post it publicly. Don‚Äôt be afraid to keep adding on to or editing your projects after they‚Äôre published! This advice is especially true when you are looking for a job. There are many stories of successful people like  Kelly Peng , Data Scientist at Airbnb, who really persevered and kept on working and improving. In  one of her blog posts , she went over how many places she applied for and interviewed with. Applications: 475 Phone interviews: 50 Finished data science take-home challenges: 9 Onsite interviews: 8 Offers: 2 Time spent: 6 months She clearly applied to a lot of jobs and kept on persisting. In her article, she even mentions how you need to keep on learning from your interviewing experiences. Take note of all the interview questions you got asked, especially those questions you failed to answer. You can fail again, but don‚Äôt fail at the same spot. You should always be learning and improving. One of the ways someone finds your portfolio is often through your resume so it is worth a mention. A data science resume is a place to focus on your technical skills. Your resume is a chance to succinctly represent your qualifications and fit for that particular role. Recruiters and hiring managers skim resumes very quickly, and you only have a short time to make an impression. Improving your resume can increase your chance of getting an interview. You have to make sure every single line and every single section of your resume counts. William Chen , a Data Science Manager from Quora has  9 Tips for making your data science resume .  Notice in the brief summary of his points below, that projects and portfolio are points 6, 7, 8, and arguably 9 . 2. Objective : Don‚Äôt include one. They don‚Äôt help you distinguish yourself from other people. They take away space from the more important things (skills, projects, experience etc). Cover letters are extremely optional unless you really personalize it. 3. Coursework : Do list  relevant coursework  that is applicable for the job description. 4. Skills : Don‚Äôt give numerical ratings for your skills. If you want to rate yourself on your skills, use words like proficient or familiar or things like that. You can even exclude assessments altogether. 5. Skills : Do list technical skills that the job description mentions. The order you list your skills in can suggest what you are best at. 6. Projects : Don‚Äôt list common projects or homework. They aren‚Äôt that helpful in distinguishing you from other applicants. List projects that are novel. 7. Projects :   Show results and include links. If you participated in Kaggle competition, put percentile rank as it helps the person reading your resume understand where you are in the competition. In projects sections, there is always room for links to writeups and papers as they let the hiring manager or recruiter dig in deeper (bias to real world messy problems where you learn something new). Notice that in one of the projects sections above, a person has an additional link to a blog that lets the recruiter or hiring manager find out more. This is one way to link to various parts of your portfolio from your resume. 8. Portfolio:  Fill our your online presence. The most basic is a LinkedIn profile. It is kind of like an extended resume. Github and Kaggle profiles can help show off your work. Fill out each profile and include links to other sites. Fill out descriptions for your GitHub respositories. Include links to your knowledge sharing profiles/blog (medium, quora). Data science specifically is about knowledge sharing and communicating what the data means to other people. You don‚Äôt have to do all of them, but pick a few and do it (More on this later). 9. Experience : Tailor your experience towards the job. Experience is the core of your resume, but if you don‚Äôt have work experience what do you do? Focus your resume on independent projects, like capstone projects, independent research, thesis work, or Kaggle competitions. These are substitutes for work experience if you don‚Äôt have work experience to put on your resume. Avoid putting irrelevant experience on your resume. If you want to know hear data science managers go over portfolios and resumes, here are links to Kaggle‚Äôs CareerCon 2018 ( video ,  resumes reviewed ). This is very similar to the Importance of a Portfolio section, just divided into subsections. Having a Github page, a Kaggle profile, a Stack Overflow, etc can provide support for your resume. Having online profiles filled out can be a good signal for hiring managers. As  David Robinson  phrases it, Generally, when I‚Äôm evaluating a candidate, I‚Äôm excited to see what they‚Äôve shared publicly, even if it‚Äôs not polished or finished. And sharing  anything  is almost always better than sharing nothing. The reason why data scientists like seeing public work is as  Will Stanton  said, Data scientists use these tools to share their own work and find answers to questions. If you use these tools, then you are signaling to data scientists that you are one of  them , even if you haven‚Äôt ever worked as a data scientist. A lot of Data science is about communication and presenting data so it is good to have these online profiles. Besides from the fact that these platforms help provide valuable experience, they can also help you get noticed and lead people to your resume. People can and do find your resume online through various sources (LinkedIn, GitHub, Twitter, Kaggle, Medium, Stack Overflow, Tableau Public, Quora, Youtube, etc). You will even find that different types of social media feed into eachother. A Github profile is a powerful signal that you are a competent data scientist. In the projects section of a resume, people often leave links to their GitHub where the code is stored for their projects. You can also have writeups and markdown there. GitHub lets people see what you have built and how you have built it. At some companies, hiring managers look at an applicants GitHub. It is another way to show employers you aren‚Äôt a false positive. If you take the time to develop your GitHub profile, you can be better evaluated than others. It is worth mentioning that you need to have some sort of README.md with a description of your project as a lot of  data science is about communicating results . Make sure the README.md file clearly describes what your project is, what it does, and how to run your code. Participating in Kaggle competitions, creating a kernel, and contributing to discussions are ways to show some competency as a data scientist. It is important to emphasize that Kaggle is not like an industry project as  Colleen Farrelly , mentions in this  quora question . Kaggle competitions take care of coming up with a task, acquire data for you, and clean it into some usable form. What it does is give you practice analyzing data and coming up with a model. Note that there is a good reason why  Kaggle Grandmasters continue to participate in Kaggle competitions .  Reshama Shaikh  has a post  To Kaggle Or Not  where she talked about the value of Kaggle competitions. From her post, It is true, doing one Kaggle competition does not qualify someone to be a data scientist. Neither does taking one class or attending one conference tutorial or analyzing one dataset or reading one book in data science. Working on competition(s) adds to your experience and augments your portfolio. It is a complement to your other projects, not the sole litmus test of one‚Äôs data science skillset. I completely agree with Reshama‚Äôs view on this. In particular, the point about how taking a class in something doesn‚Äôt make you an expert in something nor does it give you a job. I literally have made a course called  Python for Data Visualization  and I go into extensive depth about Pandas, Matplotlib, and Seaborn. It wont immediately give you a job or make you an immediate expert in Matplotlib or Seaborn, but it will make your knowledge greater, teach you how the libraries work, and aid in building your portfolio. Everything you do can make you more employable. Unlike a resume, which is confined by length, a LinkedIn profile allows you to describe your projects and work experience in more depth. Udacity has a  guide on making a good LinkedIn profile . An important part of LinkedIn is their search tool and for you to show up, you must have  relevant keywords   in   your profile. Recruiters often search for people on LinkedIn. LinkedIn allows you to see which companies have searched for you and who has viewed your profile. Besides companies finding you and sending you messages on your availability, LinkedIn also has many features like  Ask for a Referral .  Jason Goodman  in his article  Advice on Applying to Data Science Jobs  uses LinkedIn to  indirectly  ask for referrals. I never, never, never applied to any companies without an introduction to someone who worked at the company‚Ä¶once I was interested in a company, I would use LinkedIn to find a first- or second- degree connection at the company. I would write to that connection, asking to talk to them about their experience at the company and, if possible, whether they‚Äôd be able to connect me to someone on the Data Science team. Whenever I could, I did in-person meetings (coffee or lunch) instead of phone calls. As an aside, Trey Causey recently wrote  a great post  on how to ask for just these kinds of meetings. I would never ask for a job directly, but they would usually ask for my resume and offer to submit me as an internal referral, or put me in touch with a hiring manager. If they didn‚Äôt seem comfortable doing so...I‚Äôd just thank them for their time and move on. Notice that he doesn‚Äôt right away ask for a referral. While common job advice when applying to a company is to get a referral, it is VERY IMPORTANT to note that you still need a portfolio, experience, or some sort of proof you can do a job. Jason even mentions the importance of a portfolio in that and  other articles he has written . Aman Dalmia  learned something similar by  Interviewing at Multiple AI Companies and Startups . Networking is  NOT  messaging people to place a referral for you .  When I was starting off, I did this mistake way too often until I stumbled upon an article that talked about the importance of building a  real  connection with people by offering our help first. One other point he had is that LinkedIn is great for getting your content/portfolio out. Another important step in networking is to get your content out. For example, if you‚Äôre good at something, blog about it and share that blog on Facebook and LinkedIn .  Not only does this help others,  it helps you as well. Having some form of blog can be highly beneficial. A lot of data science is about communication and presenting data. Blogging is a way of practicing this and showing you can do this. Writing about a project or a data science topic allows you to share with the community as well as encourages you to write out your work process and thoughts. This is a useful skill when interviewing. As  David Robinson  said, A blog is your chance to practice the relevant skills. By writing a blog, you can practice communicate findings to others. It also is another form of advertising yourself. Blogs about  Using Scrapy to Build your Own Dataset , and ironically  Python Environment Management with Conda  have taught me a lot and have gotten me a lot of opportunities I would normally not have gotten. Recently, my  boxplot blog  brought me the opportunity to create my own  Python for Data Visualization course . One of the major benefits I have found is that throughout the process of people critiquing my projects and suggesting improvements (though the comments section of the blog) makes it so interviewers aren‚Äôt the first ones pointing out these same flaws. The more obvious benefit is that by making a blog you tend to read a lot more data science/machine learning blog posts and hence learn more. As for what platform to blog on, I recommend using Medium.  Manali Shinde  in her blog post  How to Construct a Data Science Portfolio from Scratch  had a really good point on why she choose Medium for her blog. I thought of creating my own website on a platform such as WordPress or Squarespace. While those platforms are amazing to host your own portfolio, I wanted a place where I would get some visibility, and a pretty good tagging system to reach greater audiences. Luckily Medium, as we know, has those options (and it‚Äôs also free). If you don‚Äôt know what to write about, I suggest you look at  David Robinson‚Äôs advice . Being active on Twitter is a great way to identify and interact with people in your field. You can also promote your blog on Twitter so that your portfolio can be that much more visible. There are so many opportunities to interact with people on twitter. One of them as  Reshama Shaikh  said in her famous blog post ‚Äú How Do I Get My First Data Science Job? ‚Äù was, David Robinson  generously offers to retweet your first data science post. With 20K+ followers, that‚Äôs an offer that can‚Äôt be refused. Twitter can be used for other things than self promotion.  Data Science Renee  has a post ‚Äú How to use Twitter to Learn Data Science (or Anything) ‚Äù that is quite insightful about taking Twitter to learn skills. One other takeaway from her article was how much her Twitter presence helped her network and get opportunities. I have been asked to be interviewed on podcasts and blogs (some of those should be coming up soon), offered contract work, and offered free admission to a conference I unfortunately couldn‚Äôt go to, but was excited to be considered for. ‚ÄúFamous‚Äù people in the industry are now coming to me to work with them in some way. Not every data science job uses Tableau or other BI tools. However, if you are applying to jobs where these tools are used, it is important to note that there are websites where you can put dashboards for public consumption. For example, if you say you are learning or know Tableau, put a couple dashboards on  Tableau Public . While a lot of companies might be okay with you learning Tableau on the job, having public evidence of your Tableau skill can help. If you want to see good examples of Tableau Public profiles, please see  Orysya Stus‚Äô  and  Brit Cava‚Äôs  profiles. Having a strong resume has long been the primary tool for job seekers to relay their skills to potential employers. These days, there is more than one way to showoff your skills and get a job. A portfolio of public evidence is a way to get opportunities that you normally wouldn‚Äôt get. It is important to emphasize that a portfolio is an iterative process. As your knowledge grows, your portfolio should be updated over time. Never stop learning or growing. Even this blog post will be updated with feedback and with increasing knowledge. If you want interview advice/guides/courses, time to check out  Brandon Rohrer‚Äôs advice on how to survive a data science interview ,  Sadat‚Äôs   interview guide , or my  15 Tips for Landing a Data Science Job course . If you want some general data science career advice, I wrote an article on it  here . If you have any questions or thoughts on the tutorial, feel free to reach out in the comments below or through  Twitter ."
"I ranked every Intro to Data Science course on the internet, based on thousands of data points"," ranked every Intro to Data Science course on the internet, based on thousands of data point","A year ago, I dropped out of one of the best computer science programs in Canada. I started creating my own  data science master‚Äôs program  using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I‚Äôm almost finished now. I‚Äôve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role.  A few months ago, I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few  coding classes  for the beginner data scientist. Then it was  statistics and probability classes . (Don‚Äôt worry if you‚Äôre unsure of what an intro to data science course entails. I‚Äôll explain shortly.) For this guide, I spent 10+ hours trying to identify every online intro to data science course offered as of January 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. For this task, I turned to none other than the open source Class Central community and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it‚Äôs David. I wrote this guide back in 2017. Since then, I‚Äôve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros.  My top intro to data science course recommendation for 2023 is in there, too. datamaverickhq.com Okay, back to the guide. Each course must fit three criteria: We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on  Udemy , we chose to consider the most-reviewed and highest-rated ones only. There‚Äôs always a chance that we missed something, though. So please let us know in the comments section if we left a good course out. We compiled average rating and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on two factors: 1.  Coverage of the data science process.  Does the course brush over or skip certain subjects? Does it cover certain subjects in too much detail? See the next section for what this process entails. 2.  Usage of common data science tools.  Is the course taught using popular programming languages like Python and/or R? These aren‚Äôt necessary, but helpful in most cases so slight preference is given to these courses. What is data science? What does a data scientist do?  These are the types of fundamental questions that an intro to data science course should answer. The following infographic from Harvard professors Joe Blitzstein and Hanspeter Pfister outlines a typical  data science process , which will help us answer these questions. Our goal with this introduction to data science course is to become familiar with the data science process. We don‚Äôt want too in-depth coverage of specific aspects of the process, hence the ‚Äúintro to‚Äù portion of the title. For each aspect, the ideal course explains key concepts within the framework of the process, introduces common tools, and provides a few examples (preferably hands-on). We‚Äôre only looking for an introduction. This guide therefore won‚Äôt include full specializations or programs like Johns Hopkins University‚Äôs  Data Science Specialization  on Coursera or Udacity‚Äôs  Data Analyst Nanodegree . These compilations of courses elude the purpose of this series: to find the best  individual  courses for each subject to comprise a data science education. The final three guides in this series of articles will cover each aspect of the data science process in detail. Several courses listed below require basic programming, statistics, and probability experience. This requirement is understandable given that the new content is reasonably advanced, and that these subjects often have several courses dedicated to them. This experience can be acquired through our recommendations in the first two articles ( programming ,  statistics ) in this Data Science Career Guide. Kirill Eremenko‚Äôs  Data Science A-Z‚Ñ¢  on Udemy is the clear winner in terms of breadth and depth of coverage of the data science process of the 20+ courses that qualified. It has a 4.5-star weighted average rating over 3,071 reviews, which places it among the highest rated and most reviewed courses of the ones considered. It outlines the full process and provides real-life examples. At 21 hours of content, it is a good length. Reviewers love the instructor‚Äôs delivery and the organization of the content. The price varies depending on Udemy discounts, which are frequent, so you may be able to purchase access for as little as $10. Though it doesn‚Äôt check our ‚Äúusage of common data science tools‚Äù box ,  the non-Python/R tool choices (gretl, Tableau, Excel) are used effectively in context. Eremenko mentions the following when explaining the gretl choice (gretl is a statistical software package), though it applies to all of the tools he uses (emphasis mine): In gretl, we will be able to do the same modeling just like in R and Python but we won‚Äôt have to code. That‚Äôs the big deal here. Some of you may already know R very well, but some may not know it at all. My goal is to show you how to build a robust model and  give you a framework that you can apply in any tool you choose . gretl will help us avoid getting bogged down in our coding. One prominent reviewer noted the following: Kirill is the best teacher I‚Äôve found online. He uses real life examples and explains common problems so that you get a deeper understanding of the coursework. He also provides a lot of insight as to what it means to be a data scientist from working with insufficient data all the way to presenting your work to C-class management. I highly recommend this course for beginner students to intermediate data analysts! Udacity‚Äôs  Intro to Data Analysis  is a relatively new offering that is part of Udacity‚Äôs popular  Data Analyst Nanodegree . It covers the data science process clearly and cohesively using Python, though it lacks a bit in the modeling aspect. The estimated timeline is 36 hours (six hours per week over six weeks), though it is shorter in my experience. It has a 5-star weighted average rating over two reviews. It is free. The videos are well-produced and the instructor (Caroline Buckey) is clear and personable. Lots of programming quizzes enforce the concepts learned in the videos. Students will leave the course confident in their new and/or improved NumPy and Pandas skills (these are popular Python libraries). The final project ‚Äî which is graded and reviewed in the Nanodegree but not in the free individual course ‚Äî can be a nice add to a portfolio. Data Science Fundamentals is a four-course series provided by IBM‚Äôs Big Data University. It includes courses titled  Data Science 101 ,  Data Science Methodology ,  Data Science Hands-on with Open Source Tools , and  R 101 . It covers the full data science process and introduces Python, R, and several other open-source tools. The courses have tremendous production value. 13‚Äì18 hours of effort is estimated, depending on if you take the ‚ÄúR 101‚Äù course at the end, which isn‚Äôt necessary for the purpose of this guide. Unfortunately, it has no review data on the major review sites that we used for this analysis, so we can‚Äôt recommend it over the above two options yet. It is free. Our #1 pick had a weighted average rating of 4.5 out of 5 stars over 3,068 reviews. Let‚Äôs look at the other alternatives, sorted by descending rating. Below you‚Äôll find several R-focused courses, if you are set on an introduction in that language. The following courses had no reviews as of January 2017. This is the third of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the  first article  and statistics and probability in the  second article . The remainder of the series will cover other data science core competencies: data visualization and machine learning. medium.freecodecamp.com medium.freecodecamp.com The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering. If you‚Äôre looking for a complete list of Data Science online courses, you can find them on Class Central‚Äôs  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ‚Äôs other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the üíö so more people will see it here on Medium. This is a condensed version of my  original article published on Class Central , where I‚Äôve included further course descriptions, syllabi, and multiple reviews."
"The best Data Science courses on the internet, ranked by your reviews","he best Data Science courses on the internet, ranked by your review","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own  data science master‚Äôs program  using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I‚Äôm almost finished now. I‚Äôve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role.   So I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few  coding classes  for the beginner data scientist. Then it was  statistics and probability classes . Then  introductions to data science . Then  data visualization .  Machine learning  was the fifth and latest guide. And now I‚Äôm back to conclude this series with even more resources. For each of the five major guides in this series, I spent several hours trying to identify every online course for the subject in question, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My goal was to identify the three best courses available for each subject and present them to you. The 13 supplemental topics ‚Äî like databases, big data, and general software engineering ‚Äî didn‚Äôt have enough courses to justify full guides. But over the past eight months, I kept track of them as I came across them. I also scoured the internet for courses I may have missed. For these tasks, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews. Since 2011,  Class Central  founder  Dhawal Shah  has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Hey, it‚Äôs David. I wrote this guide back in 2017. Since then, I‚Äôve become a professional data analyst and created courses for multiple industry-leading online education companies. Do you want to become a data analyst, without spending 4 years and $41,762 to go to university? Follow my latest  27-day curriculum  and learn alongside other aspiring data pros. datamaverickhq.com Okay, back to the guide. Each course within each guide must fit certain criteria. There were subject-specific criteria, then two common ones that each guide shared: We believe we covered every notable course that fit the criteria in each guide. There is always a chance that we missed something, though. Please let us know in each guide‚Äôs comments section if we left a good course out. We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on a variety of factors specific to each subject. The criteria in our intro to programming guide, for example: Learn to Program: The Fundamentals (LPT1)  and  Crafting Quality Code (LPT2)  by the University of Toronto via Coursera The University of Toronto‚Äôs Learn to Program series has an excellent mix of content difficulty and scope for the beginner data scientist. Taught in Python, the series has a 4.71-star weighted average rating over 284 reviews. An Introduction to Interactive Programming in Python (Part 1)  and  (Part 2)  by Rice University via Coursera Rice University‚Äôs Interactive Programming in Python series contains two of the best online courses ever. They skew towards games and interactive applications, which are less applicable topics in data science. The series has a 4.93-star weighted average rating over 6,069 reviews. R Programming Track  by DataCamp If you are set on learning R, DataCamp‚Äôs R Programming Track effectively combines programming fundamentals and R syntax instruction. It has a 4.29-star weighted average rating over 14 reviews. Foundations of Data Analysis ‚Äî Part 1: Statistics Using R  and  Part 2: Inferential Statistics  by the University of Texas at Austin via edX The courses in the UT Austin‚Äôs Foundations of Data Analysis series are two of the few with great reviews that also teach statistics and probability with a focus on coding up examples. The series has a 4.61-star weighted average rating over 28 reviews. Statistics with R Specialization  by Duke University via Coursera Duke‚Äôs Statistics with R Specialization, which is split into five courses, has a comprehensive syllabus with full sections dedicated to probability. It has a 3.6-star weighted average rating over 5 reviews, but the course it was based upon has a 4.77-star weighted average rating over 60 reviews. Introduction to Probability ‚Äî The Science of Uncertainty  by the Massachusetts Institute of Technology (MIT) via edX MIT‚Äôs Intro to Probability course by far has the highest ratings of the courses considered in the statistics and probability guide. It exclusively probability in great detail, plus it is longer (15 weeks) and more challenging than most MOOCs. It has a 4.82-star weighted average rating over 38 reviews. Data Science A-Z‚Ñ¢: Real-Life Data Science Exercises Included  by Kirill Eremenko and the SuperDataScience Team via Udemy Kirill Eremenko‚Äôs Data Science A-Z excels in breadth and depth of coverage of the data science process. The instructor‚Äôs natural teaching ability is frequently praised by reviewers. It has a 4.5-star weighted average rating over 5,078 reviews. Intro to Data Analysis  by Udacity Udacity‚Äôs Intro to Data Analysis covers the data science process cohesively using Python. It has a 5-star weighted average rating over 2 reviews. Data Science Fundamentals  by Big Data University Big Data University‚Äôs Data Science Fundamentals covers the full data science process and introduces Python, R, and several other open-source tools. There are no reviews for this course on the review sites used for this analysis. Data Visualization with Tableau Specialization  by the University of California, Davis via Coursera A five-course series, UC Davis‚Äô Data Visualization with Tableau Specialization dives deep into visualization theory. Opportunities to practice Tableau are provided through walkthroughs and a final project. It has a 4-star weighted average rating over 2 reviews. Data Visualization with ggplot2 Series  by DataCamp Endorsed by ggplot2 creator Hadley Wickham, a substantial amount of theory is covered in DataCamp‚Äôs Data Visualization with ggplot2 series. You will know R and its quirky syntax quite well leaving these courses. There are no reviews for these courses on the review sites used for this analysis. Tableau 10 Series ( Tableau 10 A-Z  and  Tableau 10 Advanced Training ) by Kirill Eremenko and the SuperDataScience Team on Udemy An effective practical introduction, Kirill Eremenko‚Äôs Tableau 10 series focuses mostly on tool coverage (Tableau) rather than data visualization theory. Together, the two courses have a 4.6-star weighted average rating over 3,724 reviews. Machine Learning  by Stanford University via Coursera Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, Stanford University‚Äôs Machine Learning covers all aspects of the machine learning workflow and several algorithms. Taught in MATLAB or Octave, It has a 4.7-star weighted average rating over 422 reviews. Machine Learning  by Columbia University via edX A more advanced introduction than Stanford‚Äôs, CoIumbia University‚Äôs Machine Learning is a newer course with exceptional reviews and a revered instructor. The course‚Äôs assignments can be completed using Python, MATLAB, or Octave. It has a 4.8-star weighted average rating over 10 reviews. Machine Learning A-Z‚Ñ¢: Hands-On Python & R In Data Science  by Kirill Eremenko and Hadelin de Ponteves via Udemy Kirill Eremenko and Hadelin de Ponteves‚Äô Machine Learning A-Z is an impressively detailed offering that provides instruction in both Python and R, which is rare and can‚Äôt be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews. Creative Applications of Deep Learning with TensorFlow  by Kadenze Parag Mital‚Äôs Creative Applications of Deep Learning with Tensorflow adds a unique twist to a technical subject. The ‚Äúcreative applications‚Äù are inspiring, the course is professionally produced, and the instructor knows his stuff. Taught in Python, It has a 4.75-star weighted average rating over 16 reviews. Neural Networks for Machine Learning  by the University of Toronto via Coursera Learn from a legend.  Geoffrey Hinton  is known as the ‚Äúgodfather of deep learning‚Äù is internationally distinguished for his work on artificial neural nets. His Neural Networks for Machine Learning is an advanced class. Taught in Octave with exercises also in Python, it has a 4.11-star weighted average rating over 35 reviews. Deep Learning A-Z‚Ñ¢: Hands-On Artificial Neural Networks  by Kirill Eremenko and Hadelin de Ponteves via Udemy Deep Learning A-Z is an accessible introduction to deep learning, with intuitive explanations from Kirill Eremenko and helpful code demos from Hadelin de Ponteves. Taught in Python, it has a 4.6-star weighted average rating over 1,314 reviews. Python Programming Track  by DataCamp, plus their individual pandas courses: DataCamp‚Äôs code-heavy instruction style and in-browser programming environment are great for learning syntax. Their Python courses have a 4.64-star weighted average rating over 14 reviews. Udacity‚Äôs Intro to Data Analysis, one of our recommendations for intro to data science courses, covers NumPy and pandas as well. R Programming Track  by DataCamp, plus their individual dplyr and data.table courses: Again, DataCamp‚Äôs code-heavy instruction style and in-browser programming environment are great for learning syntax. Their R Programming Track, which is also one of our recommendations for programming courses in general, effectively combines programming fundamentals and R syntax instruction. The series has a 4.29-star weighted average rating over 14 reviews. Introduction to Databases  by Stanford University via Stanford OpenEdx (note:  reviews  from the deprecated version on Coursera) Stanford University‚Äôs Introduction to Databases covers database theory comprehensively while introducing several open source tools. Programming exercises are challenging. Jennifer Widom,  now the Dean of Stanford‚Äôs School of Engineering , is clear and precise. It has a 4.61-star weighted average rating over 59 reviews. Importing & Cleaning Data Tracks by DataCamp: DataCamp‚Äôs Importing & Cleaning Data Tracks (one in Python and one in R) excel at teaching the mechanics of preparing your data for analysis and/or visualization. There are no reviews for these courses on the review sites used for this analysis. Data Analysis with R  by Udacity and Facebook Udacity‚Äôs Data Analysis with R is an enjoyable introduction to exploratory data analysis. The expert interviews with Facebook‚Äôs data scientists are insightful and inspiring. The course has a 4.58-star weighted average rating over 19 reviews. It also serves as a light introduction to R. The Ultimate Hands-On Hadoop ‚Äî Tame your Big Data!  by Frank Kane via Udemy, then if you want more on specific tools (all by Frank Kane via Udemy): Frank Kane‚Äôs Big Data series teaches all of the most popular big data technologies, including over 25 in the ‚ÄúUltimate‚Äù course alone. Kane shares his knowledge from a decade of industry experience working with distributed systems at Amazon and IMDb. Together, the courses have a 4.52-star weighted average rating over 6,932 reviews. Software Testing  by Udacity Software Debugging  by Udacity Version Control with Git  and  GitHub & Collaboration  by Udacity (updates to Udacity‚Äôs popular  How to Use Git & GitHub  course) Software skills are an  oft-overlooked  part of a data science education. Udacity‚Äôs testing, debugging, and version control courses introduce three core topics relevant to anyone who deals with code, especially those in team-based environments. Together, the courses have a 4.34-star weighted average rating over 68 reviews. Georgia Tech and Udacity have a  new course  that covers software testing and debugging together, though it is more advanced and not all relevant for data scientists. Building a Data Science Team  by Johns Hopkins University via Coursera Learning How to Learn: Powerful mental tools to help you master tough subjects  by Dr. Barbara Oakley and the University of California, San Diego via Coursera Mindshift: Break Through Obstacles to Learning and Discover Your Hidden Potential  by Dr. Barbara Oakley and McMaster University via Coursera Johns Hopkins University‚Äôs Building a Data Science Team provides a useful peek into data science in practice. It is an extremely short course that can be completed in a handful of hours and audited for free. Ignore its 3.41-star weighted average rating over 12 reviews, some of which were likely from paying customers. Dr. Barbara Oakley‚Äôs Learning How to Learn and Mindshift aren‚Äôt data science courses per se. Learning How to Learn, the  most popular online course ever , covers best practices shown by research to be most effective for mastering tough subjects, including memory techniques and dealing with procrastination. In Mindshift, she demonstrates how to get the most out of online learning and MOOCs, how to seek out and work with mentors, and the secrets to avoiding career ruts and general ruts in life. These are two courses that  everyone  should take. They have a 4.74-star and a 4.87-star weighted average rating over 959 and 407 reviews, respectively. Both courses are four weeks in duration. This Data Science Career Guide will continue to be updated as new courses are released and ratings and reviews for them are generated. Are you passionate about another discipline (e.g. Computer Science)? Would you like to help educate the world? If you are interested in creating a Career Guide similar in structure to this one, drop us a note at  guides@class-central.com . As for my future, I‚Äôm excited to share that I have taken a position with Udacity as a  Content Developer . That means I‚Äôll be creating and teaching courses. That also means that this guide will be updated by somebody else. I‚Äôm joining Udacity because I believe they are best positioned to create the best education product on the planet. Of all of the courses I have taken, online or at university, I learned best while enrolled a Nanodegree. They are incorporating the latest in pedagogy and production and feature an excellent project review system, upbeat instructors, and healthy student and career support teams. Though a piecewise approach like the one we took in this guide can work, a cohesive program with projects and reviews throughout is much more student-friendly. Updating the  Data Analyst Nanodegree  is my first task, which is a part of a larger effort to create a clear path of Nanodegrees for all things data. Students will soon be able to start from scratch with data basics at Udacity and progress all the way through  machine learning ,  artificial intelligence , and even  self-driving cars  if they wish. This is the final piece of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the  first article , statistics and probability in the  second article , intros to data science in the  third article , data visualization in the  fourth , and machine learning in the  fifth . medium.freecodecamp.com Here, we summarized the above five articles, and recommended the best online courses for other key topics such as databases, big data, and even software engineering. If you‚Äôre looking for a complete list of Data Science online courses, you can find them on Class Central‚Äôs  Data Science and Big Data  subject page. If you enjoyed reading this, check out some of  Class Central ‚Äôs other pieces: medium.freecodecamp.com medium.freecodecamp.com If you found this helpful, click the üíö so more people will see it here on Medium. This is a modified version of my  original article  published on Class Central, where a simple list of the courses mentioned here is also provided."
Bringing the best out of Jupyter Notebooks for Data Science,ringing the best out of Jupyter Notebooks for Data Scienc,"Reimagining what a Jupyter notebook can be and what can be done with it. Netflix  aims to provide personalized content to their 130 million viewers. One of the significant ways by which data scientists and engineers at Netflix interact with their data is through  Jupyter notebooks . Notebooks leverage the use of collaborative, extensible, scalable, and reproducible data science. For many of us, Jupyter Notebooks is the  de facto  platform when it comes to quick prototyping and exploratory analysis. However, there‚Äôs more to this than meets the eye. A lot of Jupyter functionalities sometimes lies under the hood and is not adequately explored. Let us try and explore Jupyter Notebooks‚Äô features which can enhance our productivity while working with them. The notebook is the new shell The shell is a way to interact textually with the computer. The  most popular  Unix shell is Bash( Bourne Again SHell  ). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows. Now, when we work with any Python interpreter, we need to regularly switch between the shell and the IDLE, in case we need to use the command line tools. However, the Jupyter Notebook gives us the ease to execute shell commands from within the notebook by placing an extra  ! before the commands.  Any  command that works at the command-line can be used in IPython by prefixing it with the  !  character. We can even pass values to and from the shell as follows: Notice, the data type of the returned results is not a list. Theme-ify your Jupyter Notebooks! If you are a person who gets bored while staring at the white background of the Jupyter notebook, themes are just for you. The themes also enhance the presentation of the code. You can find more about Jupyter themes  here . Let‚Äôs get to the working part. Installation List of available themes Currently, the available themes are  chesterish, grade3, gruvboxd, gruvboxl monokai, oceans16, onedork, solarizedd ,solarizedl. Extend the possibilities Notebook extensions let you move beyond the general vanilla way of using the Jupyter Notebooks. Notebook extensions (or nbextensions) are JavaScript modules that you can load on most of the views in your Notebook‚Äôs frontend. These extensions modify the user experience and interface. Installation with conda: Or with pip: Start a Jupyter notebook now, and you should be able to see an  NBextensions Tab  with a lot of options. Click the ones you want and see the magic happen. In case you couldn‚Äôt find the tab, a second small nbextension, can be located under the menu Edit . Let us discuss some of the useful extensions. Hinterland  enables code autocompletion menu for every keypress in a code cell, instead of only calling it with the tab. This makes Jupyter notebook‚Äôs autocompletion behave like other popular IDEs such as PyCharm. This extension adds a drop-down menu to the Notebook toolbar that allows easy insertion of code snippet cells into the current notebook. This extension splits the cells of the notebook and places then adjacent to each other. This extension enables to collect all running headers and display them in a floating window, as a sidebar or with a navigation menu. The extension is also draggable, resizable, collapsible and dockable. Collapsible Headings a llows the notebook to have collapsible sections, separated by headings. So in case you have a lot of dirty code in your notebook, you can simply collapse it to avoid scrolling it again and again. Autopep8 helps to reformat/prettify the contents of code cells with just a click. If you are tired of hitting the spacebar again and again to format the code, autopep8 is your savior. Make notebooks interactive Widgets  are eventful python objects that have a representation in the browser, often as a control like a slider, textbox, etc. Widgets can be used to build  interactive GUIs  for the notebooks. Let us have a look at some of the widgets. For complete details, you can visit their  Github repository . Interact The  interact  function ( ipywidgets.interact ) automatically creates a user interface (UI) controls for exploring code and data interactively. It is the easiest way to get started using IPython's widgets. Here is a list of some of the useful advanced widgets. The Play widget is useful to perform animations by iterating on a sequence of integers at a certain speed. The value of the slider below is linked to the player. The date picker widget works in Chrome and IE Edge but does not currently work in Firefox or Safari because they do not support the HTML date input field. Make Data frames intuitive Qgrid is also a Jupyter notebook widget but mainly focussed at dataframes. It uses  SlickGrid  to render pandas DataFrames within a Jupyter notebook. This allows you to explore your DataFrames with intuitive scrolling, sorting and filtering controls, as well as edit your DataFrames by double-clicking cells. The  Github Repository  contains more details and examples. Installing with pip: Installing with conda: Code is great when communicated. Notebooks are an effective tool for teaching and writing explainable codes. However, when we want to present our work either we display our entire notebook(with all the codes) or we take the help of powerpoint. Not any more. Jupyter Notebooks can be easily converted to slides and we can easily choose what to show and what to hide from the notebooks. There are two ways to convert the notebooks into slides: Open a new notebook and navigate to  View ‚Üí Cell Toolbar ‚Üí Slideshow.  A light grey bar appears on top of each cell, and you can customize the slides. Now go to the directory where the notebook is present and enter the following code: The slides get displayed at port 8000. Also, a  .html  file will be generated in the directory, and you can also access the slides from there. This would look even more classy with a themed background. Let us apply the theme ‚Äô onedork ‚Äô to the notebook and then convert it into a slideshow. These slides have a drawback i.e. you can see the code but cannot edit it. RISE plugin offers a solution. RISE is an acronym for  Reveal.js ‚Äî Jupyter/IPython Slideshow Extension . It utilized the  reveal.js  to run the slideshow. This is super useful since it also gives the ability to run the code without having to exit the slideshow. Installation 1 ‚Äî Using conda (recommended): 2 ‚Äî Using pip (less recommended): and then two more steps to install the JS and CSS in the proper places: Let us now use RISE for the interactive slideshow. We shall re-open the Jupyter Notebook we created earlier. Now we notice a new extension that says ‚ÄúEnter/Exit RISE Slideshow.‚Äù Click on it, and you are good to go. Welcome to the world of interactive slides. Refer to the  documentation  for more information. Display it right there! Why go with mere links when you can easily embed an URL, pdf, and videos into your Jupyter Notebooks using IPython‚Äôs  display  module. These were some of the features of the Jupyter Notebooks that I found useful and worth sharing. Some of them would be obvious to you while some may be new. So, go ahead and experiment with them. Hopefully, they will be able to save you some time and give you a better UI experience. Also feel free to suggest other useful features in the comments."
How I Got 4 Data Science Offers and Doubled my Income 2 Months after being Laid Off,ow I Got 4 Data Science Offers and Doubled my Income 2 Months after being Laid Of,"During this unprecedented time with the pandemic, many are finding their careers affected. This includes some of the most talented data scientists with which I have ever worked. Having shared my personal experience with some close friends to help them find a new job after being laid off, I thought it worth sharing publicly. After all, this touches more than me and my friends. Any data scientist who was laid off due to the pandemic or who is actively looking for a data science position can find something here to which they can relate, and which I hope will ultimately offer hope in your job search. So if you‚Äôve ever been stuck - in getting interviews, in interview preparation, in negotiation, anything - I‚Äôve been there, and I want to help. You can reach out to me  here  if you think I might be able to make your journey easier in any way! Here‚Äôs my story. I hope you find some useful tips and encouragement within it. In December of 2018, I was informed by my manager that I was to be laid off in January 2019. Three months before, the VP of Engineering of my then startup company had written a letter to our head of People Success. This letter explained why I was one of the top performers in the company and advocated for an increase in my salary. This helped me get a 33% increase in my salary. I was naturally feeling motivated and eager to crack the next milestone on an important project. The company‚Äôs future and my own looked bright. It was during this moment of success that I was told that I was impacted by the company-wise cost-cutting initiative. I was let go on January 15th. To be forced to start looking for a new job was daunting, to say the least. After browsing the data science job openings on the market, I soon realized my knowledge gap. What I was doing at the B2B startup (a mix of entry-level data engineering and machine learning) was simply irrelevant to many of the job requirements out there, such as product sense, SQL, stats, and more. I knew the basics but was unsure how to fill the gap towards more advanced skills. However, even that issue seemed secondary to more pressing questions, such as  how do I even get an interview ? I had a mere 1.5 years of work experience with a startup, and I lacked any statistics or computer science-related degree. More questions soon followed. What if I cannot find a job before I lose my visa status? What if the economy takes a downturn before I can find a new job? Despite my fears, there was little choice.  I had to find a new job . In the face of what felt like an overwhelming task, I needed some information to decide my next steps. After doing some research, I realized that more than half of the data science positions on the market were  product-driven positions  (‚Äòproduct analytics‚Äô), and the rest were either modeling or data engineering oriented positions. I also noted that positions other than product analytics tended to have higher requirements. For example, most modeling positions required a PhD degree, and engineering positions required a computer science background. Clearly, the requirements for different tracks varied widely, so it followed that preparation for each would differ as well. With this knowledge in hand, I made an important decision: preparing for all tracks would be both overwhelming and most likely less effective. I would need to  focus on one . I choose product analytics because, based on my background and experience, there was a higher chance that I could get interviews on this track. Of course, not everyone in data science has my exact background and experience, so below I have summarized the general requirements for three categories of data science positions at big companies. Understanding this basic breakdown saved me a lot of time, and I trust it will prove useful for others looking for a job in data science. I will add, however, that for small startups it‚Äôs possible that the interview will be less structured and require more of a mixture of all three. Product Analytics (~70% on the market) Modeling (~20% on the market) Data Engineering (~10% on the market) In light of my own experience, the rest of this post is strongly tailored towards those preparing for positions in  product analytics . Come back later to check out my post on preparation for a data engineering position. The very first thing I did once I knew I was going to be laid off was to apply widely and aggressively to other jobs. I used all the job boards I knew including  GlassDoor ,  Indeed  and  LinkedIn . I also asked everyone I knew for referrals. However, since it was almost at the end of the year, I did not receive any responses until January 2019. Asking for referrals proved to be much more effective than applying by myself.  Out of about 50 raw applications, I only got 3 interviews, but out of 18 referrals, I got 7 interviews . Overall, it was becoming obvious that I was not considered a strong candidate in this market. While the structure of interviews was different for each company, there was a general outline that most companies followed: Around half of the companies (4/10) that I‚Äôve interviewed with had a take-home assignment before or instead of a TPS. Take-home assignments consumed a lot of energy. Typically, an 8-hour take-home assignment caused me to need at least half a day to rest after submission. Because of this, I did my best to schedule the interview accordingly. There were no interviews the morning after my take-home assignment. Simply being aware of the basic structure can go a long way in making you feel more at ease and able to cope with the process of finding a new job. Going into my interviews,  every opportunity was critical   to me . Although I was aware that some people learn by interviewing, becoming better after many interviews, and typically obtaining offers for the last few companies with which they interview, I did not feel I could take this approach. When I graduated in 2017, I only received 4 interviews out of 500 raw applications. I was not expecting to get many more in 2019. Thus, my plan was to be fully prepared for each interview I got. I would let  no opportunity go to waste . One benefit of being laid off was that I could study full time for the interview. Each day I structured what I studied, focusing on two or three things per day. No more. From previous interviews, I had learned that a deep understanding allows you to give more thorough answers during interviews. It especially helps to have a depth of knowledge in an interview situation when you tend to be more nervous and anxious than usual. That is not the time when you want to try faking things. As I describe my own experience, I can‚Äôt help thinking of a  common misconception  I often hear: it‚Äôs not possible to gain the knowledge on product/experimentation without real experience. I firmly disagree. I did not have any prior experience in product or A/B testing, but I believed that those skills could be gained by reading, listening, thinking, and summarizing. After all, this is the same way we were taught things in school. Actually, as I get to know more senior data scientists I continue to learn that this method is common, even for people with years of experience. What you will be interviewed on may not be related to what you were doing at all, but you can gain the knowledge you need in ways other than job experience. Here are the basics of what you can expect. Typically, product and SQL questions were asked during a TPS. Onsite interviews included a few rounds of questions, including product sense, SQL, stats, modeling, behavior, and maybe a presentation. The next few subsections summarize the most useful resources (all freely available) I used when preparing for interviews. In general,  GlassDoor  was a good source to get a sense of company-specific problems. Once I saw those problems, I understood both what the company needed and where my gaps were in fulfilling those needs. I was then able to develop a plan to fill those gaps. The following six subsections are how I prepared for the specific content that comes up in interviews for the product analytics track. In explaining my own preparation, I hope to make the path smoother for those who come after me. Working as a data scientist at a startup, I was mainly responsible for developing and deploying machine learning models and writing spark jobs. Thus, I barely gained any product knowledge. When I saw some real interview questions on  GlassDoor , such as ‚Äúhow to measure success?‚Äù or ‚Äúhow to validate the new feature by current users‚Äô behaviors?‚Äù, I had utterly no idea how to approach such questions. At the time, they seemed far too abstract and open-ended. To learn product sense I resorted to the basic  read and summarize  strategy, using the resources listed below. All this reading helped me build up my product knowledge. As a result, I came up with a structured way (my own ‚Äò framework ‚Äô) to answer any type of product questions. I then put my knowledge and framework to the test with that all essential to learning any skill:  practice . I wrote out answers to questions involving product sense. I said my answers out loud (even recording myself with my phone), and used the recordings to finetune my answers. Soon I could not only fake it for an interview, I actually knew my stuff. Resources: The first time I took a SQL TPS I failed, and it was with a company in which I was very interested. Clearly, something needed to change. I needed to, once again, practice, and so I spent time grinding SQL questions. Eventually, I was able to complete in a day, questions that had previously taken me an entire week. Practice makes perfect! Resources: To prepare for these kinds of questions, I brushed up on elementary statistics and probability and did some coding exercises. While this may seem overwhelming (there is a lot of content for both topics), the interview questions for a product data scientist were never hard. The resources below are a great way to review. Resources: Without a CS degree, I went into the job search with limited machine knowledge. I had taken some courses during my previous job, and I reviewed my notes from these to prep for interviews. However, even though modeling questions are getting more and more frequent nowadays, the interview questions for a product data scientist mainly geared toward how to apply those models rather than the underlying math and theories. Still here are some helpful resources to bump up your machine learning skills before the interview time. Resources: Some companies required candidates to either present the take-home assignment or a project of which they are most proud. Still, other companies asked about the most impactful project during behavioral interviews. However, no matter what the form the key is to make your presentation interesting and challenging. That sounds great, but how do you do that? My main recommendation is to  think through all the details , such as high-level goals and success metrics to ETL to modeling implementation details, to deployment, monitoring, and improvement. The little things add up to make a great presentation rather than one big idea. Here are a few questions worth rethinking to help reach your ideal presentation: When presenting a project, you want to engage the audience. To make my presentations interesting, I often share  interesting findings and the biggest challenges  of the project. But the best way to make sure you are engaging is practice. Practice and practice out loud. I practiced presenting to my family to ensure my grasp of the material and ease of communication. If you can engage the people you know, an interviewer, who is required to listen, doesn‚Äôt stand a chance. While it is easy to get caught up in preparing for the technical interview questions, don‚Äôt forget that the behavioral questions are equally important. All companies I‚Äôve interviewed with had at least 1 round of behavior interviews during the onsite portions. These questions typically fall into these three categories: Behavioral questions are very important for data scientists. So be prepared! Understanding a company‚Äôs mission and core values helps answer questions in the first group. Questions like 2 and 3 can be answered by telling a story ‚Äî 3 stories were enough to answer all behavioral questions. Make sure you‚Äôve got a few good stories on hand when you walk in for an interview. Similar to product questions, I practiced a lot by saying it out loud, recording, and listening to then fine-tune my answers. Hearing a story is the best way to make sure it works. The night before an onsite interview was typically a stressful, hectic night. I always tried to cram in more technical knowledge while simultaneously reviewing my statistics notes and thinking of my framework to answer a product question. Of course, as we all learned in school, none of that was incredibly useful. The results were largely determined due to the amount of preparation before not a single night of cramming. So preparation is important, but there are some rules you can follow the day of to make sure your interview is a success. Using these rules, this was the feedback I got from onsite interviews: After receiving verbal offers, the next step was to work with recruiters to finalize the numbers. There‚Äôs only one rule here that I stick with -  ALWAYS negotiate . But how? Haseeb Qureshi has a very helpful guide on  negotiating a job offer  (with scripts!) which I followed religiously during my offer negotiation phase. Every single rule was so true. I negotiated with all companies that gave me an offer. The average increase for offers was  15% , and the highest offer was, in total value, increased by  25% . Negotiating works, so don‚Äôt be afraid to try it! After losing 11 pounds and lots of cries and screaming (job hunting is stressful and it is okay to admit that), I finally got 4 offers within 2 months of being laid off. 3 of those offers were from companies that I have never dreamed of joining:  Twitter, Lyft , and  Airbnb  (where I ultimately joined) and another offer from a healthcare startup. By the end of two frenzied months, I had received a total of 10 interviews, 4 onsite interviews, and 4 job offers, giving me a  40% TPS-to-onsite rate  and  100% onsite-to-offer rate . I was so lucky that I got lots of support and help from family and friends after being laid off, which was critical to landing a job at my dream company. It was difficult. Ironically looking for a job is also a lot of work, but everything was worth it. I wrote this blog because I know how overwhelmed I was. There is so much to prepare for interviews. I hope this post has made things clearer for other data specialists out there in need of work, and if you want more advice feel free to contact me  here . I am grateful to now be working in a great job, and I would be happy to help you get there too! Since I published this post three weeks ago, I got hundreds of questions on data science interviews. So I decided to make a series of videos to help you land your dream data science job. Check my YouTube channel if you are interested! If you like this post and want to support me‚Ä¶"
Python for Data Science: 8 Concepts You May Have Forgotten,ython for Data Science: 8 Concepts You May Have Forgotte,"If you‚Äôve ever found yourself looking up the same question, concept, or syntax over and over again when programming, you‚Äôre not alone. I find myself doing this constantly. While it‚Äôs not unnatural to look things up on StackOverflow or other resources, it does slow you down a good bit and raise questions as to your complete understanding of the language. We live in a world where there is a seemingly infinite amount of accessible, free resources looming just one search away at all times. However, this can be both a blessing and a curse. When not managed effectively, an over-reliance on these resources can build poor habits that will set you back long-term. Personally, I find myself pulling code from similar discussion threads several times, rather than taking the time to learn and solidify the concept so that I can reproduce the code myself the next time. This approach is lazy and while it may be the path of least resistance in the short-term, it will ultimately hurt your growth, productivity, and ability to recall syntax (cough,  interviews ) down the line. Recently, I‚Äôve been working through an online data science course titled  Python for Data Science and Machine Learning on Udemy  (Oh God, I sound like  that guy on Youtube ). Over the early lectures in the series, I was reminded of some concepts and syntax that I consistently overlook when performing data analysis in Python. In the interest of solidifying my understanding of these concepts once and for all and saving you guys a couple of StackOverflow searches, here‚Äôs the stuff that I‚Äôm always forgetting when working with Python, NumPy, and Pandas. I‚Äôve included a short description and example for each, however for your benefit, I will also include links to videos and other resources that explore each concept more in-depth as well. Writing out a for loop every time you need to define some sort of list is tedious, luckily Python has a built-in way to address this problem in just one line of code. The syntax can be a little hard to wrap your head around but once you get familiar with this technique you‚Äôll use it fairly often. See the example above and below for how you would normally go about list comprehension with a for loop vs. creating your list with in one simple line with no loops necessary. Ever get tired of creating function after function for limited use cases? Lambda functions to the rescue! Lambda functions are used for creating small, one-time and anonymous function objects in Python. Basically, they let you create a function,  without creating a function . The basic syntax of lambda functions is: Note that lambda functions can do everything that regular functions can do, as long as there‚Äôs just one expression. Check out the simple example below and the upcoming video to get a better feel for the power of lambda functions: Once you have a grasp on lambda functions, learning to pair them with the map and filter functions can be a powerful tool. Specifically, map takes in a list and transforms it into a new list by performing some sort of operation on each element. In this example, it goes through each element and maps the result of itself times 2 to a new list. Note that the list function simply converts the output to list type. The filter function takes in a list and a rule, much like map, however it returns a subset of the original list by comparing each element against the boolean filtering rule. For creating quick and easy Numpy arrays, look no further than the arange and linspace functions. Each one has their specific purpose, but the appeal here (instead of using range), is that they output NumPy arrays, which are typically easier to work with for data science. Arange returns evenly spaced values within a given interval. Along with a starting and stopping point, you can also define a step size or data type if necessary. Note that the stopping point is a ‚Äòcut-off‚Äô value, so it will not be included in the array output. Linspace is very similar, but with a slight twist. Linspace returns evenly spaced numbers over a specified interval. So given a starting and stopping point, as well as a number of values, linspace will evenly space them out for you in a NumPy array. This is especially helpful for data visualizations and declaring axes when plotting. You may have ran into this when dropping a column in Pandas or summing values in NumPy matrix. If not, then you surely will at some point. Let‚Äôs use the example of dropping a column for now: I don‚Äôt know how many times I wrote this line of code before I actually knew why I was declaring axis what I was. As you can probably deduce from above, set axis to 1 if you want to deal with columns and set it to 0 if you want rows. But why is this? My favorite reasoning, or atleast how I remember this: Calling the shape attribute from a Pandas dataframe gives us back a tuple with the first value representing the number of rows and the second value representing the number of columns. If you think about how this is indexed in Python, rows are at 0 and columns are at 1, much like how we declare our axis value. Crazy, right? If you‚Äôre familiar with SQL, then these concepts will probably come a lot easier for you. Anyhow, these functions are essentially just ways to combine dataframes in specific ways. It can be difficult to keep track of which is best to use at which time, so let‚Äôs review it. Concat allows the user to append one or more dataframes to each other either below or next to it (depending on how you define the axis). Merge combines multiple dataframes on specific, common columns that serve as the primary key. Join, much like merge, combines two dataframes. However, it joins them based on their indices, rather than some specified column. Check out the excellent  Pandas documentation  for specific syntax and more concrete examples, as well as some special cases that you may run into. Think of apply as a map function, but made for Pandas DataFrames or more specifically, for Series. If you‚Äôre not as familiar, Series are pretty similar to NumPy arrays for the most part. Apply sends a function to every element along a column or row depending on what you specify. You might imagine how useful this can be, especially for formatting and manipulating values across a whole DataFrame column, without having to loop at all. Last but certainly not least is pivot tables. If you‚Äôre familiar with Microsoft Excel, then you‚Äôve probably heard of pivot tables in some respect. The Pandas built-in pivot_table function creates a spreadsheet-style pivot table as a DataFrame. Note that the levels in the pivot table are stored in MultiIndex objects on the index and columns of the resulting DataFrame. That‚Äôs it for now. I hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat tricky methods, functions, and concepts you frequently encounter when using Python for data science. Personally, I know that even the act of writing these out and trying to explain them in  simple terms  has helped me out a ton. Thanks for reading! Feel free to check out some of my similar essays below and  subscribe  to my newsletter for interesting links and new content. You can follow me on Medium for more posts like this and find me on  Twitter  as well. For more on me and what I‚Äôm up to, check out  my website ."
Want to know how Deep Learning works? Here‚Äôs a quick guide for everyone.,ant to know how Deep Learning works? Here‚Äôs a quick guide for everyone,"Artificial Intelligence  (AI) and  Machine Learning  (ML) are some of the hottest topics right now. The term  ‚ÄúAI‚Äù  is thrown around casually every day. You hear aspiring developers saying they want to learn AI. You also hear executives saying they want to implement AI in their services. But quite often, many of these people don‚Äôt understand what‚Ä¶"
Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,eep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machine,"(An alternate version of this article was originally published in  the Boston Globe) On December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as  Chicago Pile-1 , it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words. Now, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it‚Äôs vital we understand this new language, and what it‚Äôs increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it. The language is a new class of machine learning known as  deep learning , and the ‚Äúwhispered word‚Äù was a computer‚Äôs use of it to seemingly out of nowhere  defeat three-time European Go champion Fan Hui , not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world‚Äôs best living Go players, if not  the  best. Imagining such a grand duel of man versus machine,  China‚Äôs top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most . What actually ended up happening when they faced off? Lee went on to lose  all but one  of their match‚Äôs five games. An AI named AlphaGo is now a better Go player than any human and has been  granted the ‚Äúdivine‚Äù rank of 9 dan . In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue. ‚ÄúAlphaGo‚Äôs historic victory is a clear signal that we‚Äôve gone from linear to parabolic.‚Äù So, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what‚Äôs happening is no game. AlphaGo‚Äôs historic victory is a clear signal that we‚Äôve gone  from linear to parabolic . Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income. This may all sound like exaggeration, so let‚Äôs take a few decade steps back, and look at what computer technology has been actively doing to human employment so far: Let the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It‚Äôs already here.  Computer technology is already eating jobs and has been since 1990. All work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn‚Äôt change, and that work can be better handled by machines. Distressingly, it‚Äôs exactly routine work that once formed the basis of the American middle class. It‚Äôs routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it‚Äôs routine cognitive work that once filled US office spaces.  Such jobs are now increasingly unavailable , leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them. If we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That‚Äôs what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to  learn . I‚Äôm a writer at heart, but my educational background happens to be in psychology and physics. I‚Äôm fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as  cognitive neuroscience . I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me. As a quick primer in the way our brains function, they‚Äôre a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It‚Äôs all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we‚Äôve been learning about how we work, we‚Äôve now begun applying to the way machines work. One of these applications is the creation of  deep neural networks  - kind of like pared-down virtual brains. They provide an avenue to machine learning that‚Äôs made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It‚Äôs not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka  big data . Big data isn‚Äôt just some buzzword. It‚Äôs information, and when it comes to information, we‚Äôre creating more and more of it every day. In fact we‚Äôre creating so much that a 2013 report by SINTEF estimated that  90% of all information in the world had been created in the prior two years . This incredible rate of data creation is even  doubling every 1.5 years  thanks to the Internet, where in 2015  every minute  we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets . Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn  to learn . Why? Imagine programming a computer to recognize a chair. You‚Äôd need to enter a ton of instructions, and the result would still be a program detecting chairs that aren‚Äôt, and  not  detecting chairs that are. So how did  we  learn to detect chairs? Our parents pointed at a chair and said, ‚Äúchair.‚Äù Then we thought we had that whole chair thing all figured out, so we pointed at a table and said ‚Äúchair‚Äù, which is when our parents told us that was ‚Äútable.‚Äù This is called reinforcement learning. The label ‚Äúchair‚Äù gets connected to every chair we see, such that certain neural pathways are weighted and others aren‚Äôt. For ‚Äúchair‚Äù to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains. The power of deep learning is that it‚Äôs a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing ‚Äúchairness‚Äù to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of ‚Äúchairness.‚Äù Next we test it with even more images. Where it‚Äôs wrong, we correct it, which further improves its ‚Äúchairness‚Äù detection. Repetition of this process results in a computer that knows what a chair is when it sees it,  for the most part as well as we can . The important difference though is that  unlike us, it can then sort through  millions  of images  within a matter of  seconds . This combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo,  Google‚Äôs DeepMind AI learned how to read and comprehend what it read  through hundreds of thousands of annotated news articles.  DeepMind also  taught itself  to play dozens of Atari 2600 video games better than humans , just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions,  attaining International Master level status in just 72 hours by repeatedly playing itself . In 2015,  an AI even passed a visual Turing test by learning to learn  in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all  major  milestones in AI. However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to  the announcement by Google of AlphaGo‚Äôs  victory, was by experts essentially, ‚Äú Maybe in another ten years .‚Äù A decade was considered a fair guess because Go is a game so complex I‚Äôll just let Ken Jennings of Jeopardy fame,  another former champion human defeated by AI , describe it: Go is famously a more complex game than chess, with its larger board, longer games, and many more pieces. Google‚Äôs DeepMind artificial intelligence team likes to say that there are more possible Go boards than atoms in the known universe, but that vastly  understates  the computational problem. There are about 10¬π‚Å∑‚Å∞ board positions in Go, and  only  10‚Å∏‚Å∞ atoms in the universe. That means that if there were as many parallel universes as there are atoms in our universe (!), then the  total  number of atoms in  all  those universes combined would be close to the possibilities on a single Go board. Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what  feels  like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and  playing itself millions of times . So the answer to when the game of Go would fall to machines wasn‚Äôt even close to ten years. The correct answer ended up being, ‚Äú Any time now. ‚Äù Any time now. That‚Äôs the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it. We need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean  nothing  humans do as a job is uniquely safe anymore. From  hamburgers  to  healthcare , machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans. Amelia  is just one AI out there currently being beta-tested in companies right  now . Created by IPsoft over the past 16 years, she‚Äôs learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she‚Äôs able to learn, she‚Äôs able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it‚Äôs been estimated that she can put 250 million people out of a job,  worldwide . Viv  is an AI coming soon from the creators of Siri who‚Äôll be our own personal assistant. She‚Äôll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she‚Äôll know we‚Äôll like best. In doing all of this for us, we‚Äôll see far fewer ads, and that means the entire advertising industry ‚Äî that industry the entire Internet is built upon ‚Äî stands to be hugely disrupted. A world with Amelia and Viv ‚Äî and the countless other AI counterparts coming online soon ‚Äî in combination with robots like  Boston Dynamics‚Äô next generation Atlas  portends, is a world where machines can do  all four   types of jobs  and that means serious societal reconsiderations. If a machine can do a job instead of a human,  should any human be forced at the threat of destitution to perform that job ? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when  jobs for many are entirely unobtainable ? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them,  where does that money go instead ? And  what does it no longer buy ?  Is it even possible that many of the jobs we‚Äôre creating don‚Äôt need to exist at all , and only do because of the incomes they provide? These are questions we need to start asking, and fast. Fortunately, people  are  beginning  to   ask   these   questions , and there‚Äôs an answer that‚Äôs building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is  universal basic income . By adopting UBI, aside from  immunizing  against the negative effects of automation, we‚Äôd also be decreasing  the risks inherent in entrepreneurship , and  the sizes of bureaucracies  necessary to boost incomes. It‚Äôs for these reasons, it has  cross-partisan support , and is even now in the beginning stages of possible implementation in countries like  Switzerland ,  Finland , the  Netherlands , and  Canada . The future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will.  The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million . That‚Äôs a net loss, not a net gain of 5 million jobs. In a frequently cited paper,  an Oxford study estimated the automation of about half of all existing jobs by 2033 . Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies ‚Äî  especially the US economy as I wrote last year about automating truck driving  ‚Äî by eliminating millions of jobs within a short span of time. And now even the White House,  in a stunning report to Congress , has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable ‚Äú duck and cover ‚Äù strategies for avoiding nuclear blasts during the Cold War. All of this is why it‚Äôs those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist  Jeremy Howard  asked ‚ÄúDo you want half of people to starve because they literally can‚Äôt add economic value, or not?‚Äù before going on to suggest, ‚ÄùIf the answer is  not , then the smartest way to distribute the wealth is by implementing a  universal basic income .‚Äù AI pioneer  Chris Eliasmith , director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, ‚ÄúAI is already having a big impact on our economies‚Ä¶ My suspicion is that more countries will have to follow Finland‚Äôs lead in exploring  basic income guarantees  for people.‚Äù Moshe Vardi  expressed the same sentiment  after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, ‚Äúwe need to rethink the very basic structure of our economic system‚Ä¶ we may have to consider instituting a  basic income guarantee .‚Äù Even Baidu‚Äôs chief scientist and founder of Google‚Äôs ‚ÄúGoogle Brain‚Äù deep learning project,  Andrew Ng , during an onstage interview at this year‚Äôs Deep Learning Summit, expressed the shared notion that  basic income  must be ‚Äúseriously considered‚Äù by governments, citing ‚Äúa high chance that AI will create massive labor displacement.‚Äù When those building the tools begin warning about the implications of their use, shouldn‚Äôt those wishing to use those tools listen with the utmost attention, especially when it‚Äôs the very livelihoods of millions of people at stake? If not then, what about when  Nobel prize winning economists  begin agreeing with them in increasing numbers? No nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let‚Äôs ask ourselves, what‚Äôs the purpose of the technologies we‚Äôre creating? What‚Äôs the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose  how  we work, and to decline any pay/hours we deem insufficient because we‚Äôre already earning the incomes that machines aren‚Äôt? What‚Äôs the big lesson to learn, in a century when machines can learn? I offer it‚Äôs that jobs are for machines, and life is for people. This article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with  a monthly patron pledge  of $1+. Are you a creative? Become a creator on Patreon . Join me in taking  the BIG Patreon Creator Pledge for basic income Special thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O‚ÄôSullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli Niinim√§ki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith. Would you like to see your name here too? Scott Santens writes about basic income on  his blog . You can also follow him here on  Medium , on  Twitter , on  Facebook , or on  Reddit  where he is a moderator for the  /r/BasicIncome  community of over 30,000 subscribers. If you feel others would appreciate this article, please click the green heart."
Thoughts after taking the Deeplearning.ai courses,houghts after taking the Deeplearning.ai course,"[ Update ‚Äî Feb 2nd 2018: When this blog post was written, only 3 courses had been released.  All 5 courses in this specialization are now out . I will have a follow-up blog post soon. ] Between a full time job and a toddler at home, I spend my spare time learning about the ideas in cognitive science & AI. Once in a while a great paper/video/course comes out and you‚Äôre instantly hooked. Andrew Ng‚Äôs new deeplearning.ai course is like that  Shane Carruth  or  Rajnikanth movie  that one yearns for! Naturally, as soon as the course was released on coursera, I registered and spent the past 4 evenings binge watching the lectures, working through quizzes and programming assignments. DL practitioners and ML engineers typically spend most days working at an abstract Keras or TensorFlow level. But it‚Äôs nice to take a break once in a while to get down to the nuts and bolts of learning algorithms and actually do back-propagation by hand. It is both fun and  incredibly useful ! Andrew Ng‚Äôs new  ad venture is a  bottom-up approach  to teaching neural networks ‚Äî powerful non-linearity learning algorithms, at a beginner-mid level. In classic Ng style, the course is delivered through a carefully chosen curriculum, neatly timed videos and precisely positioned information nuggets. Andrew picks up from where his classic ML course left off and introduces the idea of neural networks using a single neuron(logistic regression) and slowly adding complexity ‚Äî more neurons and layers. By the end of the 4 weeks(course 1), a student is introduced to all the core ideas required to build a dense neural network such as cost/loss functions, learning iteratively using gradient descent and vectorized parallel python(numpy) implementations. Andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math/coding. Lectures are delivered using presentation slides on which Andrew writes using digital pens. It felt like an effective way to get the listener to focus. I felt comfortable watching videos at 1.25x or 1.5x speed. Quizzes are placed at the end of each lecture sections and are in the multiple choice question format. If you watch the videos once, you should be able to quickly answer all the quiz questions. You can attempt quizzes multiple times and the system is designed to keep your highest score. Programming assignments are done via J upyter notebooks  ‚Äî powerful browser based applications. Assignments have a nice guided sequential structure and you are not required to write more than 2‚Äì3 lines of code in each section. If you understand the concepts like vectorization intuitively, you can complete most programming sections with just 1 line of code! After the assignment is coded, it takes 1 button click to submit your code to the automated grading system which returns your score in a few minutes. Some assignments have time restrictions ‚Äî say, three attempts in 8 hours etc. Jupyter notebooks are well designed and work without any issues. Instructions are precise and it feels like a polished product. Anyone interested in understanding what neural networks are, how they work, how to build them and the tools available to bring your ideas to life. If your math is rusty, there is no need to worry ‚Äî Andrew explains all the required calculus and provides derivatives at every occasion so that you can focus on building the network and concentrate on implementing your ideas in code. If your programming is rusty, there is a nice coding assignment to teach you numpy. But I recommend learning python first on  codecademy . Let me explain this with an analogy:  Assume you are trying to learn how to drive a car. Jeremy‚Äôs FAST.AI course  puts you in the drivers seat from the get-go. He teaches you to move the steering wheel, press the brake, accelerator etc. Then he slowly explains more details about how the car works ‚Äî why rotating the wheel makes the car turn, why pressing the brake pedal makes you slow down and stop etc. He keeps getting deeper into the inner workings of the car and by the end of the course, you know how the internal combustion engine works, how the fuel tank is designed etc.  The goal of the course is to get you driving. You can choose to stop at any point after you can drive reasonably well ‚Äî there is no need to learn how to build/repair the car. Andrew‚Äôs DL course  does all of this, but in  the complete opposite order . He teaches you about internal combustion engine first! He keeps adding  layers  of abstraction and by the end of the course you are driving like an F1 racer! The fast AI course mainly teaches you the  art of driving  while Andrew‚Äôs course primarily teaches you the  engineering behind the car . If you have not done any machine learning before this, don‚Äôt take this course first. The best starting point is Andrew‚Äôs original  ML course on coursera . After you complete that course, please try to complete part-1 of Jeremy Howard‚Äôs excellent  deep learning course . Jeremy teaches deep learning  Top-Down which is essential for absolute beginners . Once you are comfortable creating deep neural networks, it makes sense to take this  new deeplearning.ai course specialization  which fills up any gaps in your understanding of the underlying details and concepts. 2. Andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money ‚Äî the third course in the DL specialization felt incredibly useful for my role as an  architect leading engineering teams . 3. Jargon is handled well. Andrew explains that an  empirical process = trial & error  ‚Äî He is brutally honest about the reality of designing and training deep nets. At some point I felt  he might have as well just called Deep Learning as glorified curve-fitting 4. Squashes all hype around DL and AI ‚Äî Andrew makes restrained, careful comments about proliferation of AI hype in the mainstream media and by the end of the course it is pretty clear that DL is nothing like the terminator. 5.Wonderful boilerplate code that just works out of the box! 6. Excellent course structure. 7. Nice, consistent and useful notation. Andrew strives to establish a fresh nomenclature for neural nets and I feel he could be quite successful in this endeavor. 8. Style of teaching that is unique to Andrew and carries over from ML ‚Äî I could feel the same excitement I felt in 2013 when I took his original ML course. 9.The interviews with deep learning heroes are refreshing ‚Äî It is motivating and fun to hear personal stories and anecdotes. I wish that he‚Äôd said ‚Äò concretely ‚Äô  more often ! 2. Good tools are important and will help you accelerate your learning pace. I bought a digital pen after seeing Andrew teach with one. It helped me work more efficiently. 3. There is  a psychological reason  why I recommend the Fast.ai course before this one. Once you find your passion, you can learn uninhibited. 4. You just get that dopamine rush each time you score full points: 5. Don‚Äôt be scared by DL jargon ( hyperparameters = settings, architecture/topology=style etc. ) or the math symbols. If you take a leap of faith and pay attention to the lectures, Andrew shows why the symbols and notation are actually quite useful.  They will soon become your tools of choice and you will wield them with style ! Thanks for reading and best wishes! Update: Thanks for the overwhelmingly positive response! Many people are asking me to explain gradient descent and the differential calculus.  I hope this helps !"
Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,achine Learning is Fun Part 6: How to do Speech Recognition with Deep Learnin,"Update:  This article is part of a series. Check out the full series:  Part 1 ,  Part 2 ,  Part 3 ,  Part 4 ,  Part 5 ,  Part 6 ,  Part 7  and  Part 8 ! You can also read this article in  ÊôÆÈÄöËØù  ,  ÌïúÍµ≠Ïñ¥ ,  Ti·∫øng Vi·ªát ,  ŸÅÿßÿ±ÿ≥€å  or  –†—É—Å—Å–∫–∏–π . Giant update:   I‚Äôve written a new book based on these articles ! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects.  Check it out now ! Speech recognition is invading our lives. It‚Äôs built into our phones, our game consoles and our smart watches. It‚Äôs even automating our homes. For just $50, you can get an Amazon Echo Dot ‚Äî a magic box that allows you to order pizza, get a weather report or even buy trash bags ‚Äî just by speaking out loud: The Echo Dot has been so popular this holiday season that Amazon  can‚Äôt seem to keep them in stock ! But speech recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments. Andrew Ng  has long predicted that as speech recognition goes from 95% accurate to 99% accurate, it will become a primary way that we interact with computers. The idea is that this 4% accuracy gap is the difference between  annoyingly unreliable  and  incredibly useful . Thanks to Deep Learning, we‚Äôre finally cresting that peak. Let‚Äôs learn how to do speech recognition with deep learning! If you know  how neural machine translation works , you might guess that we could simply feed sound recordings into a neural network and train it to produce text: That‚Äôs the holy grail of speech recognition with deep learning, but we aren‚Äôt quite there yet (at least at the time that I wrote this ‚Äî I bet that we will be in a couple of years). The big problem is that speech varies in speed. One person might say ‚Äúhello!‚Äù very quickly and another person might say ‚Äúheeeelllllllllllllooooo!‚Äù very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text ‚Äî ‚Äúhello!‚Äù Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard. To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let‚Äôs see how it works! The first step in speech recognition is obvious ‚Äî we need to feed sound waves into a computer. In  Part 3 , we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition: But sound is transmitted as  waves . How do we turn sound waves into numbers? Let‚Äôs use this sound clip of me saying ‚ÄúHello‚Äù: Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let‚Äôs zoom in on one tiny part of the sound wave and take a look: To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points: This is called  sampling . We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That‚Äôs basically all an uncompressed .wav audio file is. ‚ÄúCD Quality‚Äù audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech. Lets sample our ‚ÄúHello‚Äù sound wave 16,000 times per second. Here‚Äôs the first 100 samples: You might be thinking that sampling is only creating a rough approximation of the original sound wave because it‚Äôs only taking occasional readings. There‚Äôs gaps in between our readings so we must be losing data, right? But thanks to the  Nyquist theorem , we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples ‚Äî as long as we sample at least twice as fast as the highest frequency we want to record. I mention this only because  nearly everyone gets this wrong  and assumes that using higher sampling rates always leads to better audio quality. It doesn‚Äôt. </end rant> We now have an array of numbers with each number representing the sound wave‚Äôs amplitude at 1/16,000th of a second intervals. We  could  feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data. Let‚Äôs start by grouping our sampled audio into 20-millisecond-long chunks. Here‚Äôs our first 20 milliseconds of audio (i.e., our first 320 samples): Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time: This recording is only  1/50th of a second   long . But even this short recording is a complex mish-mash of different frequencies of sound. There‚Äôs some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech. To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it‚Äôs component parts. We‚Äôll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a  fingerprint  of sorts for this audio snippet. Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes‚Äî C, E and G ‚Äî all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea. We do this using a mathematic operation called a  Fourier transform . It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one. The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip: But this is a lot easier to see when you draw this as a chart: If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk): A spectrogram is cool because you can actually  see  musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we‚Äôll actually feed into our neural network. Now that we have our audio in a format that‚Äôs easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the  letter  that corresponds the sound currently being spoken. We‚Äôll use a  recurrent neural network  ‚Äî that is, a neural network that has a memory that influences future predictions. That‚Äôs because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said ‚ÄúHEL‚Äù so far, it‚Äôs very likely we will say ‚ÄúLO‚Äù next to finish out the word ‚ÄúHello‚Äù. It‚Äôs much less likely that we will say something unpronounceable next like ‚ÄúXYZ‚Äù. So having that memory of previous predictions helps the neural network make more accurate predictions going forward. After we run our entire audio clip through the neural network (one chunk at a time), we‚Äôll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here‚Äôs what that mapping looks like for me saying ‚ÄúHello‚Äù: Our neural net is predicting that one likely thing I said was ‚ÄúHHHEE_LL_LLLOOO‚Äù. But it also thinks that it was possible that I said ‚ÄúHHHUU_LL_LLLOOO‚Äù or even ‚ÄúAAAUU_LL_LLLOOO‚Äù. We have some steps we follow to clean up this output. First, we‚Äôll replace any repeated characters a single character: Then we‚Äôll remove any blanks: That leaves us with three possible transcriptions ‚Äî ‚ÄúHello‚Äù, ‚ÄúHullo‚Äù and ‚ÄúAullo‚Äù. If you say them out loud, all of these sound similar to ‚ÄúHello‚Äù. Because it‚Äôs predicting one character at a time, the neural network will come up with these very  sounded-out  transcriptions. For example if you say ‚ÄúHe would not go‚Äù, it might give one possible transcription as ‚ÄúHe wud net go‚Äù. The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic. Of our possible transcriptions ‚ÄúHello‚Äù, ‚ÄúHullo‚Äù and ‚ÄúAullo‚Äù, obviously ‚ÄúHello‚Äù will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we‚Äôll pick ‚ÄúHello‚Äù as our final transcription instead of the others. Done! You might be thinking  ‚ÄúBut what if someone says ‚Äò Hullo ‚Äô? It‚Äôs a valid word. Maybe ‚ÄòHello‚Äô is the wrong transcription!‚Äù Of course it is possible that someone actually said ‚ÄúHullo‚Äù instead of ‚ÄúHello‚Äù. But a speech recognition system like this (trained on American English) will basically never produce ‚ÄúHullo‚Äù as the transcription. It‚Äôs just such an unlikely thing for a user to say compared to ‚ÄúHello‚Äù that it will always think you are saying ‚ÄúHello‚Äù no matter how much you emphasize the ‚ÄòU‚Äô sound. Try it out! If your phone is set to American English, try to get your phone‚Äôs digital assistant to recognize the world ‚ÄúHullo.‚Äù You can‚Äôt! It refuses! It will always understand it as ‚ÄúHello.‚Äù Not recognizing ‚ÄúHullo‚Äù is a reasonable behavior, but sometimes you‚Äôll find annoying cases where your phone just refuses to understand something valid you are saying. That‚Äôs why these speech recognition models are always being retrained with more data to fix these edge cases. One of the coolest things about machine learning is how simple it sometimes seems. You get a bunch of data, feed it into a machine learning algorithm, and then magically you have a world-class AI system running on your gaming laptop‚Äôs video card‚Ä¶  Right ? That sort of true in some cases, but not for speech. Recognizing speech is a hard problem. You have to overcome almost limitless challenges: bad quality microphones, background noise, reverb and echo, accent variations, and on and on. All of these issues need to be present in your training data to make sure the neural network can deal with them. Here‚Äôs another example: Did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise? Humans have no problem understanding you either way, but neural networks need to be trained to handle this special case. So you need training data with people yelling over noise! To build a voice recognition system that performs on the level of Siri, Google Now!, or Alexa, you will need a  lot  of training data ‚Äî far more data than you can likely get without hiring hundreds of people to record it for you. And since users have low tolerance for poor quality voice recognition systems, you can‚Äôt skimp on this. No one wants a voice recognition system that works 80% of the time. For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is  gold . That‚Äôs the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting  Google Now!  and  Siri  on every cell phone for free or selling $50  Alexa  units that have no subscription fee is to get you to  use them as much as possible . Every single thing you say into one of these systems is  recorded forever  and used as training data for future versions of speech recognition algorithms. That‚Äôs the whole game! Don‚Äôt believe me? If you have an Android phone with  Google Now! ,  click here to listen to actual recordings of yourself saying every dumb thing you‚Äôve ever said into it : So if you are looking for a start-up idea, I wouldn‚Äôt recommend trying to build your own speech recognition system to compete with Google. Instead, figure out a way to get people to give you recordings of themselves talking for hours. The data can be your product instead. If you liked this article, please consider  signing up for my Machine Learning is Fun! email list . I‚Äôll only email you when I have something new and awesome to share. It‚Äôs the best way to find out when I write more articles like this. You can also follow me on Twitter at  @ageitgey ,  email me directly  or  find me on linkedin . I‚Äôd love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun! Part 7 !"
How to learn Deep Learning in 6 months,ow to learn Deep Learning in 6 month,"It is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months‚Äô time. This article details out the steps to achieve that. Pre-requisites - You are willing to spend 10‚Äì20 hours per week for the next 6 months - You have some programming skills. You should be comfortable to pick up Python along the way. And cloud. (No background in Python and cloud assumed). - Some math education in the past (algebra, geometry etc).  - Access to internet and computer. Step 1 We learn driving a car ‚Äî by driving. Not by learning how the clutch and the internal combustion engine work. Atleast not initially. When learning deep learning, we will follow the same top-down approach. Do the  fast.ai  course ‚Äî  Practical Deep Learning for Coders ‚Äî Part 1 . This takes about 4‚Äì6 weeks of effort. This course has a session on running the code on cloud.  Google Colaboratory  has free GPU access. Start with that. Other options include  Paperspace ,  AWS ,  GCP ,  Crestle  and  Floydhub . All of these are great. Do not start to build your own machine. Atleast not yet. Step 2 This is the time to know some of the basics. Learn about calculus and linear algebra. For calculus,  Big Picture of Calculus  provides a good overview. For Linear Algebra, Gilbert Strang‚Äôs MIT course on  OpenCourseWare  is amazing. Once you finish the above two, read the  Matrix Calculus for Deep Learning . Step 3 Now is the time to understand the bottom-up approach to deep learning. Do all the 5 courses in the  deep learning specialisation  in Coursera. You need to pay to get the assignments graded. But the effort is truly worth it. Ideally, given the background you have gained so far, you should be able to complete one course every week. Step 4 ‚ÄúAll work and no play makes Jack a dull boy‚Äù Do a capstone project. This is the time where you delve deep into a deep learning library(eg: Tensorflow, PyTorch, MXNet) and implement an architecture from scratch for a problem of your liking. The first three steps are about understanding how and where to use deep learning and gaining a solid foundation. This step is all about implementing a project from scratch and developing a strong foundation on the tools. Step 5 Now go and do  fast.ai‚Äôs  part II course ‚Äî  Cutting Edge Deep Learning for Coders . This covers more advanced topics and you will learn to read the latest research papers and make sense out of them. Each of the steps should take about 4‚Äì6 weeks‚Äô time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning. Where to go next? Do the Stanford‚Äôs  CS231n  and  CS224d  courses. These two are amazing courses with great depth for vision and NLP respectively. They cover the latest state-of-art. And read the  deep learning book . This will solidify your understanding. Happy deep learning. Create every single day."
"The Difference Between Artificial Intelligence, Machine Learning, and Deep Learning","he Difference Between Artificial Intelligence, Machine Learning, and Deep Learnin","We‚Äôre all familiar with the term ‚ÄúArtificial Intelligence.‚Äù After all, it‚Äôs been a popular focus in movies such as The Terminator, The Matrix, and Ex Machina (a personal favorite of mine). But you may have recently been hearing about other terms like ‚ÄúMachine Learning‚Äù and ‚ÄúDeep Learning,‚Äù sometimes used interchangeably with artificial intelligence. As a result, the difference between artificial intelligence, machine learning, and deep learning can be very unclear. I‚Äôll begin by giving a quick explanation of what Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) actually mean and how they‚Äôre different. Then, I‚Äôll share how AI and the Internet of Things are inextricably intertwined, with several technological advances all converging at once to set the foundation for an AI and IoT explosion. First coined in 1956 by John McCarthy,  AI involves machines that can perform tasks that are characteristic of human intelligence . While this is rather general, it includes things like planning, understanding language, recognizing objects and sounds, learning, and problem-solving. We can put AI in two categories, general and narrow. General AI would have all of the characteristics of human intelligence, including the capacities mentioned above. Narrow AI exhibits some facet(s) of human intelligence, and can do that facet extremely well, but is lacking in other areas. A machine that‚Äôs great at recognizing images, but nothing else, would be an example of narrow AI. At its core,  machine learning is simply a way of achieving AI. Arthur Samuel coined the phrase not too long after AI, in 1959, defining it as, ‚Äúthe ability to learn without being explicitly programmed.‚Äù You see, you can get AI  without  using machine learning, but this would require building millions of lines of codes with complex rules and decision-trees. So instead of hard-coding software routines with specific instructions to accomplish a particular task, machine learning is a way of ‚Äútraining‚Äù an algorithm so that it can  learn how . ‚ÄúTraining‚Äù involves feeding huge amounts of data to the algorithm and allowing the algorithm to adjust itself and improve. To give an example, machine learning has been used to make drastic improvements to computer vision (the ability of a machine to recognize an object in an image or video). You gather hundreds of thousands or even millions of pictures and then have humans tag them. For example, the humans might tag pictures that have a cat in them versus those that do not. Then, the algorithm tries to build a model that can accurately tag a picture as containing a cat or not as well as a human. Once the accuracy level is high enough, the machine has now ‚Äúlearned‚Äù what a cat looks like. Deep learning is one of many approaches to machine learning . Other approaches include decision tree learning, inductive logic programming, clustering, reinforcement learning, and Bayesian networks, among others. Deep learning was inspired by the structure and function of the brain, namely the interconnecting of many neurons. Artificial Neural Networks (ANNs) are algorithms that mimic the biological structure of the brain. In ANNs, there are ‚Äúneurons‚Äù which have discrete layers and connections to other ‚Äúneurons‚Äù. Each layer picks out a specific feature to learn, such as curves/edges in image recognition. It‚Äôs this layering that gives deep learning its name, depth is created by using multiple layers as opposed to a single layer. I think of the relationship between AI and IoT much like the relationship between the human brain and body. Our bodies collect sensory input such as sight, sound, and touch. Our brains take that data and make sense of it, turning light into recognizable objects and turning sounds into understandable speech. Our brains then make decisions, sending signals back out to the body to command movements like picking up an object or speaking. All of the connected sensors that make up the Internet of Things are like our bodies, they provide the raw data of what‚Äôs going on in the world. Artificial intelligence is like our brain, making sense of that data and deciding what actions to perform. And the connected devices of IoT are again like our bodies, carrying out physical actions or communicating to others. The value and the promises of both AI and IoT are being realized because of the other. Machine learning and deep learning have led to huge leaps for AI in recent years. As mentioned above, machine learning and deep learning require massive amounts of data to work, and this data is being collected by the billions of sensors that are continuing to come online in the Internet of Things.  IoT makes better AI. Improving AI will also drive the adoption of the Internet of Things, creating a virtuous cycle in which both areas will accelerate drastically. That‚Äôs because  AI makes IoT useful. On the industrial side, AI can be applied to predict when machines will need maintenance or analyze manufacturing processes to make big efficiency gains, saving millions of dollars. On the consumer side, rather than having to adapt to technology, technology can adapt to us. Instead of clicking, typing, and searching, we can simply ask a machine for what we need. We might ask for information like the weather or for an action like preparing the house for bedtime (turning down the thermostat, locking the doors, turning off the lights, etc.). Shrinking computer chips and improved manufacturing techniques means cheaper, more powerful sensors. Quickly improving battery technology means those sensors can last for years without needing to be connected to a power source. Wireless connectivity, driven by the advent of smartphones, means that data can be sent in high volume at cheap rates, allowing all those sensors to send data to the cloud. And the birth of the cloud has allowed for virtually unlimited storage of that data and virtually infinite computational ability to process it. Of course, there are  one  or  two  concerns about the impact of AI on our society and our future. But as advancements and adoption of both AI and IoT continue to accelerate, one thing is certain; the impact is going to be profound. www.leverege.com üóì  This article was originally posted on  iotforall.com ."
A list of artificial intelligence tools you can use today ‚Äî for personal use (1/3), list of artificial intelligence tools you can use today ‚Äî for personal use (1/3,"A rtificial Intelligence and  the fourth industrial revolution  has made some considerable progress over the last couple of years. Most of this current progress that is usable has been developed for industry and business purposes, as you‚Äôll see in coming posts. Research institutes and dedicated, specialised companies are working toward the ultimate goal‚Ä¶"
Artificial Intelligence ‚Äî The Revolution Hasn‚Äôt Happened Yet,rtificial Intelligence ‚Äî The Revolution Hasn‚Äôt Happened Ye,"Artificial Intelligence (AI) is the mantra of the current era. The phrase is intoned by technologists, academicians, journalists and venture capitalists alike. As with many phrases that cross over from technical academic fields into general circulation, there is significant misunderstanding accompanying the use of the phrase. But this is not the classical case of the public not understanding the scientists ‚Äî here the scientists are often as befuddled as the public. The idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us ‚Äî enthralling us and frightening us in equal measure. And, unfortunately, it distracts us. There is a different narrative that one can tell about the current era. Consider the following story, which involves humans, computers, data and life-or-death decisions, but where the focus is something other than intelligence-in-silicon fantasies. When my spouse was pregnant 14 years ago, we had an ultrasound. There was a geneticist in the room, and she pointed out some white spots around the heart of the fetus. ‚ÄúThose are markers for Down syndrome,‚Äù she noted, ‚Äúand your risk has now gone up to 1 in 20.‚Äù She further let us know that we could learn whether the fetus in fact had the genetic modification underlying Down syndrome via an amniocentesis. But amniocentesis was risky ‚Äî the risk of killing the fetus during the procedure was roughly 1 in 300. Being a statistician, I determined to find out where these numbers were coming from. To cut a long story short, I discovered that a statistical analysis had been done a decade previously in the UK, where these white spots, which reflect calcium buildup, were indeed established as a predictor of Down syndrome. But I also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the UK study. I went back to tell the geneticist that I believed that the white spots were likely false positives ‚Äî that they were literally ‚Äúwhite noise.‚Äù She said ‚ÄúAh, that explains why we started seeing an uptick in Down syndrome diagnoses a few years ago; it‚Äôs when the new machine arrived.‚Äù We didn‚Äôt do the amniocentesis, and a healthy girl was born a few months later. But the episode troubled me, particularly after a back-of-the-envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide, that many of them had opted for amniocentesis, and that a number of babies had died needlessly. And this happened day after day until it somehow got fixed. The problem that this episode revealed wasn‚Äôt about my individual medical care; it was about a medical system that measured variables and outcomes in various places and times, conducted statistical analyses, and made use of the results in other places and times. The problem had to do not just with data analysis per se, but with what database researchers call ‚Äúprovenance‚Äù ‚Äî broadly, where did data arise, what inferences were drawn from the data, and how relevant are those inferences to the present situation? While a trained human might be able to work all of this out on a case-by-case basis, the issue was that of designing a planetary-scale medical system that could do this without the need for such detailed human oversight. I‚Äôm also a computer scientist, and it occurred to me that the principles needed to build planetary-scale inference-and-decision-making systems of this kind, blending computer science with statistics, and taking into account human utilities, were nowhere to be found in my education. And it occurred to me that the development of such principles ‚Äî which will be needed not only in the medical domain but also in domains such as commerce, transportation and education ‚Äî were at least as important as those of building AI systems that can dazzle us with their game-playing or sensorimotor skills. Whether or not we come to understand ‚Äúintelligence‚Äù any time soon, we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life. While this challenge is viewed by some as subservient to the creation of ‚Äúartificial intelligence,‚Äù it can also be viewed more prosaically ‚Äî but with no less reverence ‚Äî as the creation of a new branch of engineering. Much like civil engineering and chemical engineering in decades past, this new discipline aims to corral the power of a few key ideas, bringing new resources and capabilities to people, and doing so safely. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to ‚Äî ideas such as ‚Äúinformation,‚Äù ‚Äúalgorithm,‚Äù ‚Äúdata,‚Äù ‚Äúuncertainty,‚Äù ‚Äúcomputing,‚Äù ‚Äúinference,‚Äù and ‚Äúoptimization.‚Äù Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities. While the building blocks have begun to emerge, the principles for putting these blocks together have not yet emerged, and so the blocks are currently being put together in ad-hoc ways. Thus, just as humans built buildings and bridges before there was civil engineering, humans are proceeding with the building of societal-scale, inference-and-decision-making systems that involve machines, humans and the environment. Just as early buildings and bridges sometimes fell to the ground ‚Äî in unforeseen ways and with tragic consequences ‚Äî many of our early societal-scale inference-and-decision-making systems are already exposing serious conceptual flaws. And, unfortunately, we are not very good at anticipating what the next emerging serious flaw will be. What we‚Äôre missing is an engineering discipline with its principles of analysis and design. The current public dialog about these issues too often uses ‚ÄúAI‚Äù as an intellectual wildcard, one that makes it difficult to reason about the scope and consequences of emerging technology. Let us begin by considering more carefully what ‚ÄúAI‚Äù has been used to refer to, both recently and historically. Most of what is being called ‚ÄúAI‚Äù today, particularly in the public sphere, is what has been called ‚ÄúMachine Learning‚Äù (ML) for the past several decades. ML is an algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions and help make decisions. In terms of impact on the real world, ML is the real thing, and not just recently. Indeed, that ML would grow into massive industrial relevance was already clear in the early 1990s, and by the turn of the century forward-looking companies such as Amazon were already using ML throughout their business, solving mission-critical back-end problems in fraud detection and supply-chain prediction, and building innovative consumer-facing services such as recommendation systems. As datasets and computing resources grew rapidly over the ensuing two decades, it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large-scale data. New business models would emerge. The phrase ‚ÄúData Science‚Äù began to be used to refer to this phenomenon, reflecting the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, and reflecting the larger social and environmental scope of the resulting systems. This confluence of ideas and technology trends has been rebranded as ‚ÄúAI‚Äù over the past few years. This rebranding is worthy of some scrutiny. Historically, the phrase ‚ÄúAI‚Äù was coined in the late 1950‚Äôs to refer to the heady aspiration of realizing in software and hardware an entity possessing human-level intelligence. We will use the phrase ‚Äúhuman-imitative AI‚Äù to refer to this aspiration, emphasizing the notion that the artificially intelligent entity should seem to be one of us, if not physically at least mentally (whatever that might mean). This was largely an academic enterprise. While related academic fields such as operations research, statistics, pattern recognition, information theory and control theory already existed, and were often inspired by human intelligence (and animal intelligence), these fields were arguably focused on ‚Äúlow-level‚Äù signals and decisions. The ability of, say, a squirrel to perceive the three-dimensional structure of the forest it lives in, and to leap among its branches, was inspirational to these fields. ‚ÄúAI‚Äù was meant to focus on something different ‚Äî the ‚Äúhigh-level‚Äù or ‚Äúcognitive‚Äù capability of humans to ‚Äúreason‚Äù and to ‚Äúthink.‚Äù Sixty years later, however, high-level reasoning and thought remain elusive. The developments which are now being called ‚ÄúAI‚Äù arose mostly in the engineering fields associated with low-level pattern recognition and movement control, and in the field of statistics ‚Äî the discipline focused on finding patterns in data and on making well-founded predictions, tests of hypotheses and decisions. Indeed, the famous ‚Äúbackpropagation‚Äù algorithm that was rediscovered by David Rumelhart in the early 1980s, and which is now viewed as being at the core of the so-called ‚ÄúAI revolution,‚Äù first arose in the field of control theory in the 1950s and 1960s. One of its early applications was to optimize the thrusts of the Apollo spaceships as they headed towards the moon. Since the 1960s much progress has been made, but it has arguably not come about from the pursuit of human-imitative AI. Rather, as in the case of the Apollo spaceships, these ideas have often been hidden behind the scenes, and have been the handiwork of researchers focused on specific engineering challenges. Although not visible to the general public, research and systems-building in areas such as document retrieval, text classification, fraud detection, recommendation systems, personalized search, social network analysis, planning, diagnostics and A/B testing have been a major success ‚Äî these are the advances that have powered companies such as Google, Netflix, Facebook and Amazon. One could simply agree to refer to all of this as ‚ÄúAI,‚Äù and indeed that is what appears to have happened. Such labeling may come as a surprise to optimization or statistics researchers, who wake up to find themselves suddenly referred to as ‚ÄúAI researchers.‚Äù But labeling of researchers aside, the bigger problem is that the use of this single, ill-defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play. The past two decades have seen major progress ‚Äî in industry and academia ‚Äî in a complementary aspiration to human-imitative AI that is often referred to as ‚ÄúIntelligence Augmentation‚Äù (IA). Here computation and data are used to create services that augment human intelligence and creativity. A search engine can be viewed as an example of IA (it augments human memory and factual knowledge), as can natural language translation (it augments the ability of a human to communicate). Computing-based generation of sounds and images serves as a palette and creativity enhancer for artists. While services of this kind could conceivably involve high-level reasoning and thought, currently they don‚Äôt ‚Äî they mostly perform various kinds of string-matching and numerical operations that capture patterns that humans can make use of. Hoping that the reader will tolerate one last acronym, let us conceive broadly of a discipline of ‚ÄúIntelligent Infrastructure‚Äù (II), whereby a web of computation, data and physical entities exists that makes human environments more supportive, interesting and safe. Such infrastructure is beginning to make its appearance in domains such as transportation, medicine, commerce and finance, with vast implications for individual humans and societies. This emergence sometimes arises in conversations about an ‚ÄúInternet of Things,‚Äù but that effort generally refers to the mere problem of getting ‚Äúthings‚Äù onto the Internet ‚Äî not to the far grander set of challenges associated with these ‚Äúthings‚Äù capable of analyzing those data streams to discover facts about the world, and interacting with humans and other ‚Äúthings‚Äù at a far higher level of abstraction than mere bits. For example, returning to my personal anecdote, we might imagine living our lives in a ‚Äúsocietal-scale medical system‚Äù that sets up data flows, and data-analysis flows, between doctors and devices positioned in and around human bodies, thereby able to aid human intelligence in making diagnoses and providing care. The system would incorporate information from cells in the body, DNA, blood tests, environment, population genetics and the vast scientific literature on drugs and treatments. It would not just focus on a single patient and a doctor, but on relationships among all humans ‚Äî just as current medical testing allows experiments done on one set of humans (or animals) to be brought to bear in the care of other humans. It would help maintain notions of relevance, provenance and reliability, in the way that the current banking system focuses on such challenges in the domain of finance and payment. And, while one can foresee many problems arising in such a system ‚Äî involving privacy issues, liability issues, security issues, etc ‚Äî these problems should properly be viewed as challenges, not show-stoppers. We now come to a critical issue: Is working on classical human-imitative AI the best or only way to focus on these larger challenges? Some of the most heralded recent success stories of ML have in fact been in areas associated with human-imitative AI ‚Äî areas such as computer vision, speech recognition, game-playing and robotics. So perhaps we should simply await further progress in domains such as these. There are two points to make here. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited ‚Äî we are very far from realizing human-imitative AI aspirations. Unfortunately the thrill (and fear) of making even limited progress on human-imitative AI gives rise to levels of over-exuberance and media attention that is not present in other areas of engineering. Second, and more importantly, success in these domains is neither sufficient nor necessary to solve important IA and II problems. On the sufficiency side, consider self-driving cars. For such technology to be realized, a range of engineering problems will need to be solved that may have little relationship to human competencies (or human lack-of-competencies). The overall transportation system (an II system) will likely more closely resemble the current air-traffic control system than the current collection of loosely-coupled, forward-facing, inattentive human drivers. It will be vastly more complex than the current air-traffic control system, specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine-grained decisions. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction. As for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. Such an argument has little historical precedent. Did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer? Should chemical engineering have been framed in terms of creating an artificial chemist? Even more polemically: if our goal was to build chemical factories, should we have first created an artificial chemist who would have then worked out how to build a chemical factory? A related argument is that human intelligence is the only kind of intelligence that we know, and that we should aim to mimic it as a first step. But humans are in fact not very good at some kinds of reasoning ‚Äî we have our lapses, biases and limitations. Moreover, critically, we did not evolve to perform the kinds of large-scale decision-making that modern II systems must face, nor to cope with the kinds of uncertainty that arise in II contexts. One could argue  that an AI system would not only imitate human intelligence, but also ‚Äúcorrect‚Äù it, and would also scale to arbitrarily large problems. But we are now in the realm of science fiction ‚Äî such speculative arguments, while entertaining in the setting of fiction, should not be our principal strategy going forward in the face of the critical IA and II problems that are beginning to emerge. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda. It is not hard to pinpoint algorithmic and infrastructure challenges in II systems that are not central themes in human-imitative AI research. II systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent. Such systems must cope with cloud-edge interactions in making timely, distributed decisions and they must deal with long-tail phenomena whereby there is lots of data on some individuals and little data on most individuals. They must address the difficulties of sharing data across administrative and competitive boundaries. Finally, and of particular importance, II systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods. Such II systems can be viewed as not merely providing a service, but as creating  markets . There are domains such as music, literature and journalism that are crying out for the emergence of such markets, where data analysis links producers and consumers. And this must all be done within the context of evolving societal, ethical and legal norms. Of course, classical human-imitative AI problems remain of great interest as well. However, the current focus on doing AI research via the gathering of data, the deployment of ‚Äúdeep learning‚Äù infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills ‚Äî with little in the way of emerging explanatory principles ‚Äî tends to deflect attention from major open problems in classical AI. These problems include the need to bring meaning and reasoning into systems that perform natural language processing, the need to infer and represent causality, the need to develop computationally-tractable representations of uncertainty and the need to develop systems that formulate and pursue long-term goals. These are classical goals in human-imitative AI, but in the current hubbub over the ‚ÄúAI revolution,‚Äù it is easy to forget that they are not yet solved. IA will also remain quite essential, because for the foreseeable future, computers will not be able to match humans in their ability to reason abstractly about real-world situations. We will need well-thought-out interactions of humans and computers to solve our most pressing problems. And we will want computers to trigger new levels of human creativity, not replace human creativity (whatever that might mean). It was John McCarthy (while a professor at Dartmouth, and soon to take a  position at MIT) who coined the term ‚ÄúAI,‚Äù apparently to distinguish his  budding research agenda from that of Norbert Wiener (then an older professor at MIT). Wiener had coined ‚Äúcybernetics‚Äù to refer to his own vision of intelligent systems ‚Äî a vision that was closely tied to operations research, statistics, pattern recognition, information theory and control theory. McCarthy, on the other hand, emphasized the ties to logic. In an interesting reversal, it is Wiener‚Äôs intellectual agenda that has come to dominate in the current era, under the banner of McCarthy‚Äôs terminology. (This state of affairs is surely, however, only temporary; the pendulum swings more in AI than  in most fields.) But we need to move beyond the particular historical perspectives of McCarthy and Wiener. We need to realize that the current public dialog on AI ‚Äî which focuses on a narrow subset of industry and a narrow subset of academia ‚Äî risks blinding us to the challenges and opportunities that are presented by the full scope of AI, IA and II. This scope is less about the realization of science-fiction dreams or nightmares of super-human machines, and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives. Moreover, in this understanding and shaping there is a need for a diverse set of voices from all walks of life, not merely a dialog among the technologically attuned. Focusing narrowly on human-imitative AI prevents an appropriately wide range of voices from being heard. While industry will continue to drive many developments, academia will also continue to play an essential role, not only in providing some of the most innovative technical ideas, but also in bringing researchers from the computational and statistical disciplines together with researchers from other  disciplines whose contributions and perspectives are sorely needed ‚Äî notably  the social sciences, the cognitive sciences and the humanities. On the other hand, while the humanities and the sciences are essential as we go forward, we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope ‚Äî society is aiming to build new kinds of artifacts. These artifacts should be built to work as claimed. We do not want to build systems that help us with medical treatments, transportation options and commercial opportunities to find out after the fact that these systems don‚Äôt really work ‚Äî that they make errors that take their toll in terms of human lives and happiness. In this regard, as I have emphasized, there is an engineering discipline yet to emerge for the data-focused and learning-focused fields. As exciting as these latter fields appear to be, they cannot yet be viewed as constituting an engineering discipline. Moreover, we should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term ‚Äúengineering‚Äù is often  invoked in a narrow sense ‚Äî in academia and beyond ‚Äî with overtones of cold, affectless machinery, and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be. In the current era, we have a real opportunity to conceive of something historically new ‚Äî a human-centric engineering discipline. I will resist giving this emerging discipline a name, but if the acronym ‚ÄúAI‚Äù continues to be used as placeholder nomenclature going forward, let‚Äôs be aware of the very real limitations of this placeholder. Let‚Äôs broaden our scope, tone down the hype and recognize the serious challenges ahead. Michael I. Jordan Acknowledgments : There are a number of individuals whose comments during the writing of this article have helped me greatly, including Jeff Bezos, Dave Blei, Rod Brooks, Cathryn Carson, Tom Dietterich, Charles Elkan, Oren Etzioni, David Heckerman, Douglas Hofstadter, Michael Kearns, Tammy Kolda, Ed Lazowska, John Markoff, Esther Rolf, Maja Mataric, Dimitris Papailiopoulos, Ben Recht, Theodoros Rekatsinas, Barbara Rosario and Ion Stoica. And I would like to add a special thanks to Cameron Baradar at The House, who first encouraged me to contemplate writing such a piece. Bio:  Michael I. Jordan is Professor of Computer Science and Statistics at the University of California, Berkeley. He has worked for over three decades in the computational, inferential, cognitive and biological sciences, first as a graduate student at UCSD and then as a faculty member at MIT and Berkeley. One of his recent roles is as a Faculty Partner and Co-Founder at AI@The House ‚Äî a venture fund and accelerator in Berkeley. This fund aims to support not only AI activities, but also IA and II activities, and to do so in the context of a university environment that includes not only the engineering disciplines, but also the perspectives of the social sciences, the cognitive sciences and the humanities."
"How I Eat For Free in NYC Using Python, Automation, Artificial Intelligence, and Instagram","ow I Eat For Free in NYC Using Python, Automation, Artificial Intelligence, and Instagra","Living and working in the big apple comes with big rent. I, along with most other city-dwellers who live inside a crammed closet we call an apartment, look to cut costs anywhere we can. It‚Äôs no secret one way to curtail expenses, at least we‚Äôre told, is to cook at home instead of eating out all of the time. As a Hell‚Äôs Kitchen resident this is near impossible. Everywhere I look there is a sushi bar, Mexican restaurant or some delicious looking pizzeria within arm‚Äôs length that can break my willpower in the blink of an eye. I fall victim to this way more than I‚Äôd like to admit. Well,  I used  to fall victim to this ‚Äî until recently. Not wanting to give up the dining experiences I enjoyed so dearly, I decided I‚Äôd create my own currency to finance these transactions. I‚Äôve been dining at restaurants, sandwich shops, and other eateries for free ever since. I‚Äôm going to explain to you how I‚Äôm receiving these free meals from some of the best restaurants in New York City. I‚Äôll admit ‚Äî it‚Äôs rather technical and not everyone can reproduce my methodology. You‚Äôll either need a background in Data Science/Software Development or  a lot  of free time on your hands. Since I have the prior, I sit back and let my code do the work for me. Oh, and you guessed it, you‚Äôll need to know how to use Instagram as well. If you‚Äôre part of the technical audience, I will briefly go over some of the technologies and programming languages I use but I will not be providing code or anything like that. I will explain my use of logistic regression, random forests, AWS, and automation ‚Äî but not in depth. This article will be more theory based. If you‚Äôre a non-technical reader, everything here can still be done, it‚Äôs just going to take some time and effort. These methods are tedious which is why I decided to automate most of them. Now to get into it. I‚Äôll start with the answer and then go through how I got there. In today‚Äôs digital age, a large Instagram audience is considered a valuable currency. I had also heard through the grapevine that I could monetize a large following ‚Äî or in my desired case ‚Äî use it to have my meals paid for.  So I did just that. I created an Instagram page that showcased pictures of New York City‚Äôs skylines, iconic spots, elegant skyscrapers ‚Äî you name it. The page has amassed a following of over 25,000 users in the NYC area and it‚Äôs still rapidly growing. I reach out to restaurants in the area either via Instagram‚Äôs direct messaging or email and offer to post a review to my followers in return for a free entree or at least a discount. Almost every restaurant I‚Äôve messaged came back at me with a compensated meal or a gift card. Most places have an allocated marketing budget for these types of things so they were happy to offer me a free dining experience in exchange for a potential promotion. I‚Äôve ended up giving some of these meals away to my friends and family because at times I had too many queued up to use myself. The beauty of this all is that  I automated the whole thing.  And I mean 100% of it. I wrote code that finds these pictures or videos, makes a caption, adds hashtags, credits where the picture or video comes from, weeds out bad or spammy posts, posts them, follows and unfollows users, likes pictures, monitors my inbox, and most importantly ‚Äî both direct messages and emails restaurants about a potential promotion. Since its inception, I haven‚Äôt even really logged into the account. I spend zero time on it. It‚Äôs essentially a robot that operates like a human, but the average viewer can‚Äôt tell the difference. And as the programmer, I get to sit back and admire its (and my) work. I‚Äôll walk you through how I did what I did, from A all the way to Z. Some of this may seem like common sense, but when you‚Äôre automating a system to act like a human, details are important. The process can be broken down into three phases: content sharing, growth hacking, and sales & promotion. Now, none of the content my account posts is owned by me. I re-share other people's content on my page, with credit to them. If someone asks me to take down their photo, I do immediately. But since I am sourcing their page, I‚Äôve only been thanked ‚Äî never the opposite. Posting every day ‚Äî multiple times a day ‚Äî is indispensable. This is one of the main factors the Instagram algorithm uses to determine how much they are going to expose you to the public (via the ‚Äúexplore page‚Äù). Posting every day, especially at ‚Äúrush hour‚Äù times, is much harder and more monotonous than you might think. Most people give up on this task after a few weeks, and even missing a day or two can be detrimental. So, I automated the content collecting and sharing process. I first thought about setting up a picture scraper from Google Images or from Reddit to get my content. One of the biggest struggles I came across was how particular Instagram is with the sizing of the picture being posted. Ideally, it‚Äôs a ‚Äúsquare‚Äù picture, meaning its width equals its height, so it will reject an out-of-proportion post attempt. This made retrieving content very challenging. I ultimately decided to scrape directly from other Instagram feeds because the picture will come in precisely the right size as it is. It also allows me to know exactly where the picture came from, which will come in handy during the auto-crediting process. I collected a list of fifty other Instagram accounts that posted quality pictures of NYC. I then set up a scraper to go through and download media from these other accounts. In addition to the actual content, I scraped a bunch of metadata along with the picture such as the caption, the number of likes, and the location. I set the scraper to run every morning at 3:00 AM or when my inventory was empty. From this, I now have a central location with related content in the right format. Not everything someone posts on Instagram is re-sharable. A lot of the time people are trying to sell something, shouting another page, or it could flat out just be bad or unrelated content. Take these two posts as an example: The above two posts are from the same NYC-based Instagram account. The one on the left is a normal natural post in their niche ‚Äî one I would be happy re-sharing on my page. The one on the right, however, is an advertisement. Without any context, if I put this on my page it would be rather confusing and out of place. The caption got cut off, but it‚Äôs actually promoting an NYC-based app. You can see the difference in the number of likes ‚Äî 8200 vs. 1000. I need to be able to automatically weed out posts like those on the right, and re-share posts like that of the left. Therefore, I can‚Äôt just blindly re-share all the content that I scrape. And since this will be an automated process, I needed to create an algorithm that can weed out the bad from the good. The first part of my ‚Äúcleaner‚Äù has some hard-coded rules and the second is a machine learning model that refines the content even further. Cleaner Part 1 ‚Äî Hard-Coded Rules: The first thing I did was refine my inventory on some specific guidelines from the metadata. I was rather strict because there is no shortage of content for me to share. If there was even a slight red-flag, I trashed the picture. I can always scrape more content, but if my algorithm posts something spammy or inappropriate to my page there maybe thousands of people that see it before I recognize and remove it. The preliminary step was to have my algorithm look at the caption. If the text includes any text related to ‚Äúlink in bio‚Äù, ‚Äúbuy now‚Äù, ‚Äúlimited time‚Äù, or the related, I immediately have it fail the test. These are typical of posts looking to sell something rather than quality content for entertainment purposes. The next thing I looked at is if the comments were disabled. If they were, I failed the picture. Disabled comments from my experience were linked to controversial posts and not worth the risk. The final thing I looked at was if there was more than one person tagged in the picture. A lot of the times, one tag in a picture is a credit to where it came from, so I actually found that to be beneficial. But if the picture had multiple tags, It would lead to confusion when it came to crediting or what the purpose of the post even was. From these rules, I was able to get most of the spammy and undesirable posts into the trash and out of my folder. However, just because a post isn‚Äôt trying to sell something doesn‚Äôt mean it‚Äôs a good, quality post. Also, my hard-coded rules may still miss some sales-y content, so I wanted to run them through a secondary model once I was done with part one. Cleaner Part 2‚Äî Machine Learning Model: As I was going through my now-cleaner repository of pictures, I noticed there were still some lingering items that weren‚Äôt particularly desirable to post. I wasn‚Äôt going to be able to sit there and manually move out the bad ones as I planned for this to be completely automated. I wanted to run each through another test. I had a ton of metadata on each of the posts, including the number of likes, captions, time of post, and much more. My original goal was to try to predict which pictures would garner the most likes. However, the issue was that bigger accounts naturally had more likes so it wasn‚Äôt a fair barometer. My follow-up thought would be to make the response variable equal to the like ratio (number of likes/number of followers) and try to predict that. After looking at each picture and its respective ratio, I still didn‚Äôt trust the correlation. I didn‚Äôt feel those with higher ratios were necessarily the best photos. Just because an account was ‚Äúpopular‚Äù didn‚Äôt mean it had better content than a relatively unknown photographer with fewer likes. I decided to change my outlook from a regression model to a classification model and just decide if the picture is good enough to post or not ‚Äî a simple yes or no. Before even looking at any of the other metadata, I scraped a large number of photos and manually went through them, labeling them as 0 (bad) or 1 (good). This is extremely subjective, so I‚Äôm theoretically making a model to my own consciousness. However, it seems to be pretty universally agreed upon as to which content is unappealing and which is favorable. I generated my own dataset. The response variable was 0/1 (bad/good) with a large number of features. The metadata of each post gave me the following information: From these seven explanatory variables, I engineered a few more features that I thought would be useful. For example, I changed the number of comments and likes to ratios against followers. I extracted the number of hashtags from the caption and made that its own column, and did the same with the number of accounts mentioned in the caption. I cleaned up and vectorized the rest of the caption to be used in Natural Language Processing. Vectorizing is the process of removing peripheral words (‚Äúthe‚Äù, ‚Äúand‚Äù, etc.) and converting the remaining into a numeric field that can be analyzed mathematically. After all was said and done, I had the resulting data: I played around with a number of classification algorithms such as Support Vector Machines and Random Forests but landed on a basic Logistic Regression. I did this for a few reasons, first being Occam‚Äôs Razor ‚Äî sometimes the simplest answer is the right one. No matter which way I spun or re-engineered the data, logistic regression performed the best on my test set. The second and more important reason was that, unlike some other classification algorithms, I can set a threshold score while making predictions. It‚Äôs common for classification algorithms to output a binary class (in my case 0 or 1) but logistic regression actually yields a decimal between 0 and 1. For example, it may rate a post as 0.83 or 0.12. It‚Äôs common to set the threshold at 0.5 and rank everything greater than that to 1 and everything else to 0, but it would be case dependent. Since this task is critical, and there is an abundance of media available, I was extremely strict on my threshold and set it to 0.9 and rejected anything that fell below that benchmark. After I implemented my model, the inventory of pictures and videos was A) cleaned by a hard set of rules and then B) only the cream of the crop was chosen by my logistic regression algorithm. I am now able to move on to captioning and crediting each post. I now had a system of automatically gathering relevant content and removing the tangential or spammy images‚Äî but I‚Äôm not done yet. If you‚Äôve used Instagram before, you know that each post has a caption that exists under the picture or video. Being that I can‚Äôt actually see these pictures, nor do I have the time to sit there and caption them all, I needed to make a generic caption that can be used for any of them. The first thing I did was make a final template. It looked something like this: Where the three sets of {}‚Äôs needed to be filled in by my script. Let‚Äôs go through each three one-by-one. I created a text file with a number of predefined generic captions that could go with any picture. These were either quotes about NYC, broad questions, or just basic praise. Some of these included: For each post, one of my captions was randomly chosen. I have such a large list that I‚Äôm not worried about them being used too often or overlapping. So, for our example, let‚Äôs pick the first one ‚Äî ‚ÄúWho can name this spot?‚Äù. 2. Credit This was one of the harder tasks ‚Äî automatically crediting the source. What was particularly tricky was that the Instagram page that the media came from wasn‚Äôt necessarily the right person to credit. Often, that account was also re-sharing the content and crediting the owner in their caption or tagging them in the photo. I decided I would credit the page where it came from no matter what. I would then add more credits if I could possibly decipher the original owner as well. I felt I would cover all of my bases this way. Let‚Äôs take a look at this post by @likenewyorkcity on Instagram. We can see that even though he or she was the one who shared it, the real owner is @geoffrey.parry who is tagged in the picture and mentioned in the caption. Ideally, I would like my code to be able to look at this picture and return: The first part of that is easy; just inputting which account it came from. The second part was a little more challenging. I used REGEX to look for a number of keywords such as ‚Äúby‚Äù or ‚Äúphoto:‚Äù and then look for the ‚Äú@‚Äù symbol that followed right after. From there, I grabbed the username and believed that to be the second part of my credit. If none of those keywords existed in the caption, I checked if there was anyone tagged in the picture. If there was, I figured they deserved the credit. I understand this is an imperfect method, but more times than not that‚Äôs why someone was tagged and it was a risk worth taking. I very often capture exactly the right credit. In fact, many times I‚Äôve had people comment on my pictures and say ‚Äúthank you for sharing!‚Äù (I‚Äôve added an example of that below). 3. Hashtags Instagram allows you to add 30 hashtags to your picture which will then be displayed on that hashtag‚Äôs feed. I created a file with over 100 related hashes: and randomly chose 30 to add each time. I did that so after a while, I can compare which hashtags lead me to a greater number of likes. 4. Final Template After the three steps were said and done, I was able to fill in my template and have a caption that can go along with any post. Here‚Äôs an example of one of my final products: I used a generic caption that could go with any picture of NYC. I credited both the Instagram account it came from and the original source. If you look at the comments, you can see the original owner thanking me for sharing. And I added thirty hashtags to boost my post as well. I now have a central repository of relevant media and a process of generating a caption for each of these posts. Now, it‚Äôs time to do just that ‚Äî post. I spun up an EC2 instance on AWS to host my code. I chose this route because it‚Äôs more reliable than my personal machine ‚Äî it‚Äôs always on and connected to the internet and I knew it would all fit under the free-tier limits. I wrote a Python script that randomly grabs one of these pictures and auto-generates a caption after the scraping and cleaning process is completed. Using my API, I was able to write code that does the actual posting for me. I scheduled a cron-job to run around 8:00 AM, 2:00 PM, and 7:30 PM every day. At this point, I‚Äôve completely automated the content finding and posting process. I no longer have to worry about finding media and posting every day, it‚Äôs being done for me. It isn‚Äôt enough to just post ‚Äî I need to enact some methodologies to grow my following as well. And since I won‚Äôt ever be on the account myself doing any of this manually, I‚Äôll need to automate that too. The idea was to get my account exposed to an interested audience by interacting directly with those people. The interaction script that I wrote runs from 10:00 AM to 7:00 PM EST, the time range that I believed Instagram to be most active. Throughout the day, my account methodically follows, unfollows, and likes relevant users and photos in order to have the same be done back to me. Following (More Data Science) If you use Instagram, I‚Äôm sure you‚Äôve been part of this before whether you realize it or not. This method is very common for accounts that are trying to increase their following. One day you follow an interesting Instagram page in the fitness niche, and the next day you‚Äôre being followed by a bunch of bodybuilders and fitness models. This seems extremely trivial, and it is, but it‚Äôs  very effective . The issue here is that you can‚Äôt just follow willy-nilly on Instagram. Their algorithm is very very strict, so they will cut you off or even ban your account if you go overboard and follow too many accounts in one day. Additionally, you can be following at most 7,500 users at one time on Instagram. After a lot of testing, I‚Äôve found you can get away with following 400 people and unfollowing 400 people in a single day. Therefore, each follow is extremely precious. You don‚Äôt want to waste a follow on someone who is unlikely to follow you back because you only have so many users that you can follow in one day. I decided to capture the metadata of my activity and make a model to predict how likely someone would be to follow you back, so I wouldn‚Äôt waste a precious follow on someone who was unlikely to return the favor. I spent a few minutes manually gathering 20+ bigger accounts in the same niche as me. I had no initial data, so the first few weeks would be me randomly performing these actions to grow my following, but more importantly, I needed to capture as much metadata as possible so I can make my model. I cycled through these 20+ related accounts and followed the users who followed them, liked their pictures, or commented on their posts. With each follow I captured as much metadata as possible about the user into a CSV file. Some of this metadata included their follower/following ratio, if they were public or private, or if they had a profile picture or not. Every day, my script would go through this CSV and label the missing response variable, which is if they followed back or not. I gave each user two full days before labeling him or her 0, 1, or 2 ‚Äî 2 being the most desirable outcome. 0 indicated that the user did not follow back, 1 indicated that they followed back but didn‚Äôt interact with me in my last ten pictures (liking or commenting), and 2 indicated if they followed back AND interacted on one of my last ten posts. My dataset looked something like this: Before running this data through an ML model, I did some exploratory data analysis and found that: From just the above insights, I was able to refine my initial search of users. I adjusted my settings to only follow in the morning and to look primarily for females. Now I was finally able to make a machine learning model to predict the likelihood of a follow back based on a user‚Äôs metadata before interacting with them. This allows me to not waste one of my already limited daily follows on someone who has a very small chance of following me back. I chose to use the Random Forest algorithm to classify the follow back outcome. I originally was using a number of different decision trees before I had a set structure or outcome variable because I wanted to see the visual flowcharts that come along with them. The Random Forest is an enhancement of the decision tree that provides a number of tweaks to correct many of the inconsistencies in the individual trees. I was consistently seeing an accuracy of over 80% on my test data after modeling to my training data, so it was an effective model for me. I implemented this in my code on my scraped users to optimize follow usage and saw tremendous growth in my following. Unfollowing After two days, I would unfollow the people I had followed. This gave me enough time to capture if they would follow me back or not. This allowed me to collect data to continue to grow. You have to unfollow the people you follow for two reasons. The first is that you cannot be following over 7,500 people at any time. The second is because ‚Äî although artificial ‚Äî you want to have your follower/following ratio as high as possible as it is a sign of a more desirable account. This is an easy task because there aren‚Äôt any decisions that need to be made. You follow 400 people in a day, and two days later you unfollow those exact people. Liking Liking can also supplement your account. I didn‚Äôt put nearly as much effort in choosing the pictures to like as liking isn‚Äôt proven to give you that much of gain in followers compared to the following method described above. I simply gave a predefined set of hashtags, looped through their feeds, and liked the pictures in hopes those users would return the favor. At this point, I have a complete self-sustaining robotic Instagram. My NYC page, on its own, is finding relevant content, weeding out bad potential posts, generating credits and a caption, and posting throughout the day. In addition, from 7:00 AM to 10:00 PM, it is growing its presence by automatically liking, following, and unfollowing with an intrigued audience which has been further redefined by some data science algorithms. The best part is that it seems more human than most accounts in the same niche. For a month or two, I sat back and watched my product grow. I would see an increase of anywhere between 100 and 500 followers a day all the while enjoying some beautiful pictures of the city I love. I was able to go about my life; work at my job, go out with friends, see a movie‚Äî never having to worry about spending any time manually growing my page. It had the formula to do its thing while I did my thing. Once I had 20,000 followers I decided it was time to use this page to get some free meals. Again, I automated my sales pitch too. I made a direct message template which I tried to keep as generic as possible. I wanted to be able to use the same message whether it was a restaurant, a theatre, a museum, or a store. This is what I came up with: Here, I just need to impute the account name and the number of followers I have at the time of the message. My goal was to find business Instagrams and give them my pitch. A business profile is slightly different from a normal one ‚Äî it allows the user to add their email, phone number, directions, and other buttons on their page. But most importantly, they have a category label right on their profile. The above is an example of a business profile. Right under the name in the top left, it says ‚ÄúKorean Restaurant‚Äù and it has call-to-action buttons such as call, email, and directions at the top. I wrote a Python script that looks for these pages and automatically sends them a message from my account. The script takes two parameters, a starting hashtag, and a string to look for in the category label. In my case, I used the hashtag ‚ÄúManhattan‚Äù and the string ‚Äúrestaurant‚Äù. What this script does is it goes to the hashtag feed and loads a bunch of photos. It then loops through the posts until it hits one that has users tagged in the photo. If it does, it goes into the tags and checks if they are a business page. If it is, it looks at the category. If the category includes the word ‚Äúrestaurant‚Äù, it sends them my message. The nice part about business profiles is that they often have emails on their page. If they do, I automatically send them an email follow-up to my Instagram direct message. I can change the hashtag to something like #TimesSquare and I can change the string to something like ‚Äúmuseum‚Äù if my goals change down the road. If I go into my account, I will see the message that it auto-generated and sent. And if I go to my Gmail outbox, I‚Äôll see: Finally, I have a script that monitors my inbox for any responses and alerts me if so. If there is a response, I finally do some manual work and negotiate with my potential client. Along with the posting process and the growth process, this runs throughout the day without the need for any human manipulation. The results are better than you might initially imagine. I have restaurants basically throwing gift cards and free meals my way in exchange for an Instagram promotion. Due to the power of AI, automation, and data science ‚Äî I am able to sit back and relax while my code does the work for me. It acts as a source of entertainment while at the same time being my salesman. I hope this helps inspire some creativity when it comes to social media. Anyone can use these methods whether they are technical enough to automate or if they need to do it by hand. Instagram is a powerful tool and can be used for a variety of business benefits. Feel free to reach out with any questions! www.linkedin.com/in/chris-buetti www.lionize.ai LinkTree:  https://linktr.ee/chrisbuetti"
Artificial Intelligence + Blockchain = Passive Income (Forever?),rtificial Intelligence + Blockchain = Passive Income (Forever?,"The benefits of artificial intelligence are no longer limited to high-profile corporations, movie supervillains, and programmers performing fruitless experiments. What was once science fiction has now finally become reality, and when paired with blockchain technology it is no surprise that AI is making leaps and bounds. But the question that always comes to mind when a new technology comes to light is ‚ÄúCan I make money on it?‚Äù When looking at blockchain technology infused with artificial intelligence, the answer could be positive indeed! Thanks to  agency.howtotoken.com  for support in creating this topic (First platform with proven ICO contractors) You‚Äôve probably heard about  world chess champions losing to computers , or perhaps you‚Äôre familiar with  self-driving cars  ‚Äî both of these examples are fueled by artificial intelligence. We create AI to enable computers to solve problems for themselves, essentially giving them the ability to write code in response to new problems they encounter. When  paired with blockchains , AI is able to be better understood by humans, operate more efficiently, and make blockchains in general more efficient. In order for machines to learn, they need massive amounts of data to analyze, just like humans. The difference is that humans analyze their data passively, we use our five senses to take in the world and we store it away to create our belief in how the world functions. Trent McConaghy explains in  his article on Medium , ‚ÄúBecause blockchains introduced three new characteristics: People inspired by Bitcoin were happy to overlook the traditional database-centric shortcomings, because these new benefits had potential to impact industries and society at large in wholly new ways.‚Äù He continues by adding, ‚ÄúThese three new ‚Äòblockchain‚Äô database characteristics are also potentially interesting for AI applications.‚Äù Remember, machines don‚Äôt have senses, they need data, and blockchains can help them acquire that data faster and more cleanly (thanks to decentralization/shared control). Some AI may learn better than others, and by sharing IP/testing models on an exchange, there is the potential for faster growth (and profit for developers). Once AI starts learning, it becomes far easier for us to trace the path it took to a solution if that path is stored on an immutable trail. Considering that one of the primary reasons  we are creating AI in the first place is to make the lives of humans easier , one such obvious avenue is being explored by using AI and the blockchain to create both active and passive income streams. Blockchain technology provides users with the ability to create a trustless system on which they can run smart contracts, track a ledger, and more. AI and machine learning allows for hands-off programs that are designed to become smarter and more efficient over time. As  Tshilidzi Marwala and Bo Xing explain  in their University of Johannesburg paper, there are eight key ways where AI can help blockchains; four of which are pertinent to our discussion: It‚Äôs obvious to see how much of an impact AI will have on blockchains, and companies that are getting a headstart in the industry will almost certainly benefit greatly from such foresight. From increased scalability, enhanced security/privacy, and greater efficiency, AI will make it cheaper, safer, and easier to operate blockchains in general. There is a collection of examples where we can currently see AI and the blockchain working together on specific problems, such as: Trading cryptocurrencies is a risky market in the same way that the stock market can be dangerous, and one of the worst ways to increase this danger is by involving emotions. If the market starts crashing you may have a panic attack and quickly sell, only to watch the market bounce back higher than it ever was, leaving you at a loss. One simple way to prevent these emotional mistakes is by using an AI-based cryptobot to trade for you, such as  Cryptohopper  or  Autonio . Crypto bots are built on four principles: algorithms, market prediction, stop losses, and AI. Through these companies it is common for users to subscribe to ‚Äúsignals,‚Äù or external services that provide trading strategies based on the data they‚Äôve collected and analyzed using AI. If the average trader removes themselves (and their emotions) from the equation, and instead base their trading on strategies generated by artificial intelligence, they are far more likely to succeed. As we‚Äôve mentioned before in a different article,  musicians in today‚Äôs market struggle to earn their keep .  Musiclife  is a company that is working hard to change that by using blockchain technology infused with artificial intelligence. On the Musiclife platform, artists are automatically paid through smart contracts every time their music is streamed. This alone helps to ensure that the artists make more money for their work compared to traditional streaming platforms. But the real kicker here is Musiclife‚Äôs pricing model. Using an AI pricing method, the music price through the platform automatically adjusts based on current market playback data, so the more popular a song is the more money the artist will make. Given the potential of the AI + blockchain music streaming industry, it comes as no surprise to see big names like Spotify throwing their hat into the ring and  buying Niland , a startup focused on using AI for user-preference predictions. For those of us that aren‚Äôt musicians or professional cryptocurrency traders, however, these technologies may feel a bit detached from our daily lives. Luckily, there is a third option for making money in this industry, both during our lives and even potentially long after we‚Äôve passed. A legacy used to be something that only the rich and powerful could afford to think about, but as decentralization technology helps to disrupt the status quo the possibility of creating lasting change is opening up to everyone. Whether your goal is to provide for your family after your death, or to have the work you‚Äôve dedicated your life to carry on after your passing, avatars may be what you seek. Created by  EverLife , avatars are created using a combination of artificial intelligence and blockchain technology that gives you a platform where you can create your digital legacy. By harnessing a combination of machine learning, smart contracts, teachable skills, and secure avatar-to-avatar communications, there are a few key things you can do with your avatar: When creating avatars, EverLife chose to implement AI into their systems to ensure growth and that expansion would continue long after user input ceased. They understood that blockchains alone wouldn‚Äôt be sufficient, which is the same conclusion that  Tshilidzi Marwala and Bo Xing come to : ‚ÄúThe DNA of the 4IR (Industrial Revolution) is AI, while blockchain represents one of the most disruptive technologies that may transform the whole economic system. Though blockchain holds various promises, this technology is still in its infancy.‚Äú What EverLife, Marwala, and Xing are saying is that although the blockchain may be a groundbreaking technology, only by intertwining it with artificial intelligence will we truly see its full potential. As Matt Turck  points out in his article , the last 15 years was defined by social, mobile, and cloud-based services. Turck then goes on to say that ‚Äú[t]here‚Äôs a rationale for making the argument that AI, the blockchain, and the Internet of Things is the new social, mobile, and cloud. Those trends are still very much emerging, but their potential impact is massive.‚Äù In the same way that social, mobile, and cloud development spawned many huge companies that exist today (Spotify, Airbnb, Twitter, Facebook, Uber, etc), one could speculate that the next 10‚Äì15 years will see a surplus of AI, blockchain, and IoT-based companies succeeding. Will our lives benefit from this? The better question would be to ask if our lives benefitted from all of the companies created during the last 10‚Äì15 years. We find ourselves currently standing on the precipice of change ‚Äî a shift in the major industries is coming before us that has the potential of leading to sweeping innovations across a wide spectrum of our lives. The only question left is whether or not you, the average crypto-enthusiast, will seize the opportunity to make a profit during these changing times. Kirill Shilov  ‚Äî Founder of Geekforge.io and Howtotoken.com. Interviewing the top 10,000 worldwide experts who reveal the biggest issues on the way to technological singularity. Join my  #10kqachallenge:   GeekForge Formula ."
The fourth industrial revolution: a primer on Artificial Intelligence (AI),he fourth industrial revolution: a primer on Artificial Intelligence (AI,"‚ÄúThe last 10 years have been about building a world that is mobile-first. In the next 10 years, we will shift to a world that is AI-first.‚Äù (Sundar Pichai, CEO of  Google , October 2016) From Amazon and Facebook to Google and Microsoft, leaders of the world‚Äôs most influential technology firms are highlighting their enthusiasm for Artificial Intelligence (AI). But what is AI? Why is it important? And why now? While there is growing interest in AI, the field is understood mainly by specialists. Our goal for this primer is to make this important field accessible to a broader audience. We‚Äôll begin by explaining the meaning of ‚ÄòAI‚Äô and key terms including ‚Äòmachine learning‚Äô. We‚Äôll illustrate how one of the most productive areas of AI, called ‚Äòdeep learning‚Äô, works. We‚Äôll explore the problems that AI solves and why they matter. And we‚Äôll get behind the headlines to see why AI, which was invented in the 1950s, is coming of age today. As venture capitalists, we look for emerging trends that will create value for consumers and companies. We believe AI is an evolution in computing as, or more, important than the shifts to mobile or cloud computing. ‚ÄúIt‚Äôs hard to overstate,‚Äù Amazon CEO Jeff Bezos wrote, ‚Äúhow big of an impact AI is going to have on society over the next 20 years.‚Äù We hope this guide cuts through the hype and explains why ‚Äî whether you‚Äôre a consumer or executive, entrepreneur or investor ‚Äî this emerging trend will be important for us all. Interested in more venture capital insights?  Sign up  for our blog posts. For a map of 226 AI startups in the UK,  explore this post . Are you an AI entrepreneur?  Get in touch . Coined in 1956 by Dartmouth Assistant Professor John McCarthy, ‚ÄòArtificial Intelligence‚Äô (AI) is a general term that refers to hardware or software that exhibits behaviour which appears intelligent. In the words of Professor McCarthy, it is ‚Äúthe science and engineering of making intelligent machines, especially intelligent computer programs.‚Äù Basic ‚ÄòAI‚Äô has existed for decades, via rules-based programs that deliver rudimentary displays of ‚Äòintelligence‚Äô in specific contexts. Progress, however, has been limited ‚Äî because algorithms to tackle many real-world problems are too complex for people to program by hand. Complicated activities including making medical diagnoses, predicting when machines will fail or gauging the market value of certain assets, involve thousands of data sets and non-linear relationships between variables. In these cases, it‚Äôs difficult to use the data we have to best effect ‚Äî to ‚Äò optimise‚Äô  our predictions. In other cases, including recognising objects in images and translating languages, we can‚Äôt even develop rules to describe the  features  we‚Äôre looking for. How can we write a set of rules, to work in all situations, that describe the appearance of a dog? What if we could transfer the difficulty of making complex predictions ‚Äî the  data optimisation  and  feature specification  ‚Äî from the programmer to the program? This is the promise of modern artificial intelligence. Machine learning (ML)  is a sub-set of AI. All machine learning is AI, but not all AI is machine learning (Figure 1, above). Interest in ‚ÄòAI‚Äô today reflects enthusiasm for machine learning, where advances are rapid and significant. Machine learning lets us tackle problems that are too complex for humans to solve by shifting some of the burden to the algorithm. As AI pioneer Arthur Samuel wrote in 1959, machine learning is the ‚Äòfield of study that gives computers the ability to learn without being explicitly programmed.‚Äô The goal of most machine learning is to develop a prediction engine for a particular use case. An algorithm will receive information about a domain (say, the films a person has watched in the past) and weigh the inputs to make a useful prediction (the probability of the person enjoying a different film in the future). By giving ‚Äòcomputers the ability to learn‚Äô, we mean passing the task of optimisation ‚Äî of weighing the variables in the available data to make accurate predictions about the future ‚Äî to the algorithm . Sometimes we can go further, offloading to the program the task of specifying the features to consider in the first place. Machine learning algorithms learn through training. An algorithm initially receives examples whose outputs are known, notes the difference between its predictions and the correct outputs, and tunes the weightings of the inputs to improve the accuracy of its predictions until they are optimised. The defining characteristic of machine learning algorithms, therefore, is that  the quality of their predictions improve with experience . The more data we provide (usually up to a point), the better the prediction engines we can create (Figures 2 and 3, below. Note that the size of data sets required are highly context dependent ‚Äî we cannot generalise from the examples below.) There are more than 15 approaches to machine learning, each of which uses a different algorithmic structure to optimise predictions based on the data received. One approach ‚Äî ‚Äò deep learning‚Äô  ‚Äî is delivering breakthrough results in new domains and we explore this below. But there are many others which, although they receive less attention, are valuable because of their applicability to a broad range of usage cases. Some of the most effective machine learning algorithms beyond deep learning include: Each approach has its advantages and disadvantages and combinations may be used (an ‚Äò ensemble‚Äô  approach). The algorithms selected to solve a particular problem will depend on factors including the nature of the available data set. In practice, developers tend to experiment to see what works. Use cases of machine learning vary according to our needs and imagination. With the right data we can build algorithms for myriad purposes including: suggesting the products a person will like based on their prior purchases; anticipating when a robot on a car assembly line will fail; predicting whether an email was mis-addressed; estimating the probability of a credit card transaction being fraudulent; and many more. Even with general machine learning ‚Äî random forests, Bayesian networks, support vector machines and more ‚Äî it‚Äôs difficult to write programs that perform certain tasks well, from understanding speech to recognising objects in images. Why? Because we can‚Äôt specify the features to optimise in a way that‚Äôs practical and reliable. If we want to write a computer program that identifies images of cars, for example, we can‚Äôt specify the features of a car for an algorithm to process that will enable correct identification in all circumstances. Cars come in a wide range of shapes, sizes and colours. Their position, orientation and pose can differ. Background, lighting and myriad other factors impact the appearance of the object. There are too many variations to write a set of rules. Even if we could, if wouldn‚Äôt be a scalable solution. We‚Äôd need to write a program for every type of object we wanted to identify. Enter  deep learning (DL) , which has revolutionised the world of artificial intelligence. Deep learning is a sub-set of machine learning ‚Äî one of the more than 15 approaches to it. All deep learning is machine learning, but not all machine learning is deep learning (Figure 4, below). Deep learning is useful because it avoids the programmer having to undertake the tasks of  feature specification  (defining the features to analyse from the data) or  optimisation  (how to weigh the data to deliver an accurate prediction) ‚Äî the algorithm does both. How is this achieved? The breakthrough in deep learning is to  model the brain, not the world . Our own brains learn to do difficult things ‚Äî including understanding speech and recognising objects ‚Äî not by processing exhaustive rules but through practice and feedback. As a child we experience the world (we see, for example, a picture of a car), make predictions (‚Äòcar!‚Äô) and receive feedback (‚Äòyes!‚Äô). Without being given an exhaustive set of rules, we learn through training. Deep learning uses the same approach. Artificial, software-based calculators that approximate the function of neurons in a brain are connected together. They form a ‚Äò neural network ‚Äô which receives an input (to continue our example, a picture of a car); analyses it; makes a determination about it and is informed if its determination is correct. If the output is wrong, the connections between the neurons are adjusted by the algorithm, which will change future predictions. Initially the network will be wrong many times. But as we feed in millions of examples, the connections between neurons will be tuned so the neural network makes correct determinations on almost all occasions. Practice makes (nearly) perfect. Using this process, with increasing effectiveness we can now: Deep learning is not well suited to every problem. It typically requires large data sets for training. It takes extensive processing power to train and run a neural network. And it has an ‚Äòexplainability‚Äô problem ‚Äî it can be difficult to know how a neural network developed its predictions. But by freeing programmers from complex feature specification, deep learning has delivered successful prediction engines for a range of important problems. As a result, it has become a powerful tool in the AI developer‚Äôs toolkit. Given its importance, it‚Äôs valuable to understand the basics of how deep learning works. Deep learning involves using an artificial ‚Äò neural network ‚Äô ‚Äî a collection of ‚Äòneurons‚Äô (software-based calculators) connected together. An artificial neuron has one or more inputs. It performs a mathematical calculation based on these to deliver an output. The output will depend on both the ‚Äò weights ‚Äô of each input and the configuration of ‚Äòinput-output function‚Äô in the neuron (Figure 5, below). The input-output function can vary. A neuron may be: A neural network is created when neurons are connected to one another; the output of one neuron becomes an input for another (Figure 6, below). Neural networks are organised into multiple layers of neurons (hence ‚Äòdeep‚Äô learning). The ‚Äòinput layer‚Äô receives information the network will process ‚Äî for example, a set of pictures. The ‚Äòoutput layer‚Äô provides the results. Between the input and output layers are ‚Äòhidden layers‚Äô where most activity occurs. Typically, the outputs of each neuron on one level of the neural network serve as one of the inputs for each of the neurons in the next layer (Figure 7, below). Let‚Äôs consider the example of an image recognition algorithm ‚Äî say, to recognise human faces in pictures. When data are fed into the neural network, the first layers identify patterns of local contrast ‚Äî ‚Äòlow level‚Äô features such as edges. As the image traverses the network, progressively ‚Äòhigher level‚Äô features are extracted ‚Äî from edges to noses, from noses to faces (Fig. 8, below) At its output layer, based on its training the neural network will deliver a probability that the picture is of the specified type (human face: 97%; balloon 2%; leaf 1%). Typically, neural networks are trained by exposing them to a large number of labelled examples. Errors are detected and the weights of the connections between the neurons tuned by the algorithm to improve results. The optimisation process is extensively repeated, after which the system is deployed and unlabelled images are assessed. The above is a simple neural network but their structure can vary and most are more complex. Variations include connections between neurons on the same layer; differing numbers of neurons per layer; and the connection of neuron outputs into the previous levels of the network (‚Äòrecursive‚Äô neural networks). Designing and improving a neural network requires considerable skill. Steps include structuring the network for a particular application, providing a suitable training set of data, adjusting the structure of the network according to progress, and combining multiple approaches. AI is important because it tackles profoundly difficult problems, and the solutions to those problems can be applied to sectors important to human wellbeing ‚Äî ranging from health, education and commerce to transport, utilities and entertainment. Since the 1950s, AI research has focused on five fields of enquiry: AI is valuable because in many contexts, progress in these capabilities offers revolutionary, rather than evolutionary, capabilities. Example applications of AI include the following; there are many more. In the coming years, machine learning capabilities will be employed in  almost all sectors in a wide variety of processes . Considering a single corporate function ‚Äî for example, human resource (HR) activity within a company ‚Äî illustrates the range of processes to which machine learning will be applied: Over time we expect the adoption of machine learning to become  normalised . Machine learning will become a part of a developer‚Äôs standard toolkit, initially improving existing processes and then reinventing them. The  second-order consequences  of machine learning will exceed its immediate impact. Deep learning has improved computer vision, for example, to the point that autonomous vehicles (cars and trucks) are viable. But what will be their impact? Today, 90% of people and 80% of freight are transported via road in the UK. Autonomous vehicles alone will impact: AI research began in the 1950s; after repeated false dawns, why is now the inflection point? The effectiveness of AI has been transformed in recent years due to the development of new algorithms, greater availability of data to inform them, better hardware to train them and cloud-based services to catalyse their adoption among developers. While deep learning is not new ‚Äî the specification for the first effective, multi-layer neural network was published in 1965 ‚Äî evolutions in deep learning algorithms during the last decade have transformed results. Our ability to recognise objects within images has been transformed (Figure 9, below) by the development of  convolutional neural networks  (CNN). In a design inspired by the visual cortexes of animals, each layer in the neural network acts as a filter for the presence of a specific pattern. In 2015, Microsoft‚Äôs CNN-based computer vision system identified objects in pictures more effectively (95.1% accuracy) than humans (94.9% accuracy). ‚ÄúTo our knowledge,‚Äù they wrote, ‚Äúour result is the first to surpass human level performance.‚Äù Broader applications of CNNs include video and speech recognition. Progress in speech and handwriting recognition, meanwhile, is improving rapidly (Figure 10, bel0w) following the creation of  recurrent neural networks  (RNNs). RNNs have feedback connections that enable data to flow in a loop, unlike conventional neural networks that ‚Äòfeed forward‚Äô only. A powerful new type of RNN is the ‚ÄòLong Short-Term Memory‚Äô (LSTM) model. With additional connections and memory cells, RNNs ‚Äòremember‚Äô the data they saw thousands of steps ago and use this to inform their interpretation of what follows ‚Äî valuable for speech recognition where interpretation of the next word will be informed by the words that preceded it. From 2012, Google used LSTMs to power the speech recognition system in Android. Just six weeks ago, Microsoft engineers reported that their system reached a word error rate of 5.9% ‚Äî a figure roughly equal to that of human abilities for the first time in history. Graphical Processing Units (GPUs)  are specialised electronic circuits that are slashing the time required to train the neural networks used for deep learning. Modern GPUs were originally developed in the late 1990s to accelerate 3D gaming and 3D development applications. Panning or zooming cameras in 3D environments makes repeated use of a mathematical process called a matrix computation. Microprocessors with serial architectures, including the CPUs that power today‚Äôs computers, are poor suited to the task. GPUs were developed with massively parallel architectures (the Nvidia M40 has 3,072 cores) to perform matrix calculations efficiently. Training a neural network makes extensive use of matrix computations. It transpired, therefore, that GPUs useful for 3D gaming were well suited to accelerate deep learning. Their effect has been considerable; a simple GPU can offer a 5x improvement in training time for a neural network, while gains of 10x or much greater are possible on larger problems. When combined with software development kits tuned for widely used deep learning frameworks, the improvements in training speed can be even greater (Figure 11, below). The neural networks used for deep learning typically require large data sets for training ‚Äî from a few thousand examples to many millions. Fortunately, data creation and availability has grown exponentially. Today, as we enter the ‚Äòthird wave‚Äô of data,  humanity produces 2.2 exabytes (2,300 million gigabytes) of data every day ; 90% of all the world‚Äôs data has been created in the last 24 months. The ‚Äòfirst wave‚Äô of data creation, which began in the 1980s and involved the creation of documents and transactional data, was catalysed by the proliferation of internet-connected desktop PCs. To this, a ‚Äòsecond wave‚Äô of data has followed ‚Äî an explosion of unstructured media (emails, photos, music and videos), web data and meta-data resulting from ubiquitous, connected smartphones. Today we are entering the ‚Äòthird age‚Äô of data, in which machine sensors deployed in industry and in the home create additional monitoring-, analytical- and meta-data. Given that much data created today is transmitted via the internet for use, ballooning internet traffic serves as a proxy for the enormous increase in humanity‚Äôs data production. While as a species we transferred 100GB of data per day in 1992, by 2020 we will be transferring 61,000GB per second (Figure 12, below ‚Äî note the logarithmic scale). Beyond increases in the availability of general data, specialist data resources have catalysed progress in machine learning. ImageNet, for example, is a freely available database of over 10 million hand-labelled images. Its presence has supported the rapid development of object classification deep learning algorithms. Developers‚Äô use of machine learning is being catalysed by the provision of cloud-based machine learning infrastructure and services from the industry‚Äôs leading cloud providers. Google, Amazon, Microsoft and IBM all offer  cloud-based infrastructure  (environments for model-building and iteration, scalable ‚ÄòGPUs-as-a-service‚Äô and related managed services) to reduce the cost and difficulty of developing machine learning capabilities. In addition, they offer a burgeoning range of  cloud-based machine learning services  (from image recognition to language translation) which developers can use directly in their own applications. Google Machine Learning offers easily accessible services for: vision (object identification, explicit content detection, face detection and image sentiment analysis); speech (speech recognition and speech-to-text); text analysis (entity recognition, sentiment analysis, language detection and translation); and employee job searching (opportunity surfacing and seniority-based matching). Microsoft Cognitive Services includes more than 21 services within the fields of vision, speech, language, knowledge and search. The public‚Äôs  interest in AI has increased six-fold in the last five years  (Figure 13, below), with a still greater increase in the number of investments in AI companies by venture capital firms (Figure 14, below). We have entered a virtuous circle, in which progress in machine learning is attracting investment, entrepreneurship and awareness. The latter, in turn, are catalysing further progress. The benefits of machine learning will be numerous and significant. Many will be visible, from autonomous vehicles to new methods of human-computer interaction. Many will be less apparent, but enable more capable and efficient day-to-day business processes and consumer services. As with any paradigm shift, at times inflated expectations will exceed short-term potential. We expect a period of disillusionment regarding AI at some point in the future, to be followed by a longer and lasting recognition of its value as machine learning is used to improve and then reimagine existing systems. Historically, industrial revolutions transformed production and communication through new sources of power and transmission. The first industrial revolution used steam power to mechanise production in the 1780s. The second used electricity to drive mass production in the 1870s. The third used electronics and software to automate production and communication from the 1970s. Today, as software eats the world, our primary source of value creation is the processing of information. By enabling us to do so more intelligently, machine learning will yield benefits both humble and historic. We‚Äôll be publishing more AI and venture capital insights. To avoid missing out,  Sign up  for our blog posts. For a map of 226 AI startups in the UK and key trends shaping the market,  explore this post . If you‚Äôre an AI entrepreneur,  Get in touch ."
Google is redefining mobile with artificial intelligence,oogle is redefining mobile with artificial intelligenc,"At Google‚Äôs annual developer conference in Mountain View today, the company took the wraps off the  next version of Android . It‚Äôs not named yet, so simply called ‚ÄòP‚Äô but what  is  clear is that Google is executing on its clear lead in AI across every surface it develops for. Today, we saw how Google is redefining mobile with machine learning at its core. Let‚Äôs take a quick look at how it‚Äôs redefining Android, and what that means for the future of mobile."
Machine Learning (ML) vs. Artificial Intelligence (AI) ‚Äî Crucial Differences,achine Learning (ML) vs. Artificial Intelligence (AI) ‚Äî Crucial Difference,"October 15, 2018, by  Roberto Iriondo  ‚Äî Last updated: October 31, 2021 sponsors.towardsai.net R ecently, a report was released regarding the misuse of companies claiming to use artificial intelligence [ 29 ] [ 30 ] on their products and services. According to  the Verge  [ 29 ], 40% of European startups claiming to use AI don‚Äôt use the technology. Last year,  TechTalks , also stumbled upon such misuse by companies claiming to use machine learning and advanced artificial intelligence to gather and examine thousands of users‚Äô data to enhance user experience in their products and services [ 2 ] [ 33 ]. Unfortunately, there‚Äôs still much confusion within the public and the media regarding what genuinely is artificial intelligence [ 44 ] and what exactly is machine learning [ 18 ]. Often the terms are being used as synonyms. In other cases, these are being used as discrete, parallel advancements, while others are taking advantage of the trend to create hype and excitement to increase sales and revenue [ 2 ] [ 31 ] [ 32 ] [ 45 ]. üìö Check out our editorial recommendations on the  best machine learning books . üìö Below we go through some main differences between AI and machine learning. Quoting Interim Dean at the School of Computer Science at CMU, Professor and Former Chair of the Machine Learning Department at Carnegie Mellon University,  Tom M. Mitchell : A scientific field is best defined by the central question it studies. The field of Machine Learning seeks to answer the question: ‚ÄúHow can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes? [ 1 ]‚Äù Machine learning  (ML) is a branch of artificial intelligence, and as defined by Computer Scientist and machine learning pioneer [ 19 ] Tom M. Mitchell: ‚Äú Machine learning is the study of computer algorithms that allow computer programs to automatically improve through experience. ‚Äù [ 18 ] ‚Äî ML is one of the ways we expect to achieve AI. Machine learning relies on working with small to large datasets by examining and comparing the data to find common patterns and explore nuances. contribute.towardsai.net For instance, if you provide a machine learning model with many songs that you enjoy, along with their corresponding audio statistics (dance-ability, instrumentality, tempo, or genre). Then, it oughts to be able to automate (depending on the supervised machine learning model used) and generate a recommender system [ 43 ] as to suggest you with music in the future that (with a high percentage of probability rate) you‚Äôll enjoy, similarly as to what Netflix, Spotify, and other companies do [ 20 ] [ 21 ] [ 22 ]. In a simple example, if you load a machine learning program with a considerable large dataset of x-ray pictures along with their description (symptoms, items to consider, and others), it oughts to have the capacity to assist (or perhaps automatize) the data analysis of x-ray pictures later on. The machine learning model looks at each picture in the diverse dataset and finds common patterns found in pictures with labels with comparable indications. Furthermore, (assuming that we use an acceptable ML algorithm for images), when you load the model with new pictures, it compares its parameters with the examples it has gathered before to disclose how likely the pictures contain any of the indications it has analyzed previously. The type of machine learning from our previous example, called ‚Äú supervised learning ,‚Äù where supervised learning algorithms try to model relationships and dependencies between the target prediction output and the input features, such that we can predict the output values for new data based on those relationships, which it has learned from previous datasets  [15]  fed. Unsupervised learning , another type of machine learning, is the family of machine learning algorithms, which have main uses in pattern detection and descriptive modeling. These algorithms do not have output categories or labels on the data (the model trains with unlabeled data). Reinforcement learning , the third popular type of machine learning, aims at using observations gathered from the interaction with its environment to take actions that would maximize the reward or minimize the risk. In this case, the reinforcement learning algorithm (called the agent) continuously learns from its environment using iteration. A great example of reinforcement learning is computers reaching a super-human state and beating humans on computer games  [3] . Machine learning can be dazzling, particularly its advanced sub-branches, i.e., deep learning and the various types of neural networks. In any case, it is ‚Äúmagic‚Äù (Computational Learning Theory)  [16] , regardless of whether the public, at times, has issues observing its internal workings. While some tend to compare deep learning and neural networks to the way the human brain works, there are essential differences between the two [ 2 ]  [4]  [ 46 ]. Artificial intelligence, on the other hand, is vast in scope. According to Andrew Moore [ 6 ] [ 36 ] [ 47 ], Former-Dean of the School of Computer Science at  Carnegie Mellon University , ‚ÄúArtificial intelligence is the science and engineering of making computers behave in ways that, until recently, we thought required human intelligence.‚Äù That is a great way to define AI in a single sentence; however, it still shows how broad and vague the field is. Fifty years ago, a chess-playing program was considered a form of AI [ 34 ] since game theory and game strategies were capabilities that only a human brain could perform. Nowadays, a chess game is dull and antiquated since it is part of almost every computer‚Äôs operating system (OS) [ 35 ]; therefore, ‚Äúuntil recently‚Äù is something that progresses with time [ 36 ]. Assistant Professor and Researcher at CMU  Zachary Lipton  clarifies on Approximately Correct  [7] , the term AI ‚Äúis aspirational, a moving target based on those capabilities that humans possess but which machines do not.‚Äù AI also includes a considerable measure of technology advances that we know. Machine learning is only one of them. Prior works of AI utilized different techniques. For instance, Deep Blue, the AI that defeated the world‚Äôs chess champion in 1997, used a method called tree search algorithms  [8]  to evaluate millions of moves at every turn [ 2 ] [ 37 ] [ 52 ] [ 53 ]. As we know it today, AI is symbolized with Human-AI interaction gadgets by Google Home, Siri, and Alexa, by the machine-learning-powered video prediction systems that power Netflix, Amazon, and YouTube. These technological advancements are progressively becoming essential in our daily lives. They are intelligent assistants who enhance our abilities as humans and professionals ‚Äî making us more productive. In contrast to machine learning, AI is a moving target [ 51 ], and its definition changes as its related technological advancements turn out to be further developed  [7] . Possibly, within a few decades, today‚Äôs innovative AI advancements ought to be considered as dull as flip-phones are to us right now. The term ‚Äúartificial intelligence‚Äù came to inception in 1956 by a group of researchers, including Allen Newell and Herbert A. Simon  [9] . Since then, AI‚Äôs industry has gone through many fluctuations. In the early decades, there was much hype surrounding the industry, and many scientists concurred that human-level AI was just around the corner. However, undelivered assertions caused a general disenchantment with the industry along with the public and led to the AI winter, a period where funding and interest in the field subsided considerably [ 2 ] [ 38 ] [ 39 ] [ 48 ]. Afterward, organizations attempted to separate themselves from the term AI, which had become synonymous with unsubstantiated hype and used different names to refer to their work. For instance, IBM described Deep Blue as a supercomputer and explicitly stated that it did not use artificial intelligence  [10] , while it did [ 23 ]. During this period, various other terms, such as big data, predictive analytics, and machine learning, started gaining traction and popularity [ 40 ]. In 2012, machine learning, deep learning, and neural networks made great strides and found use in a growing number of fields. Organizations suddenly started to use the terms ‚Äúmachine learning‚Äù and ‚Äúdeep learning‚Äù for advertising their products [ 41 ]. Deep learning began to perform tasks that were impossible to do with classic rule-based programming. Fields such as speech and face recognition, image classification, and natural language processing, which were at early stages, suddenly took great leaps [ 2 ] [ 24 ] [ 49 ], and in March 2019‚Äìthree of the most recognized deep learning pioneers won a Turing award thanks to their contributions and breakthroughs that have made deep neural networks a critical component to nowadays computing [ 42 ]. Hence, to the momentum, we see a gearshift back to AI. For those who are used to the limits of old-fashioned software, the effects of deep learning almost seemed like ‚Äúmagic‚Äù  [16] . Especially since a fraction of the fields that neural networks and deep learning are entering were considered off-limits for computers, and nowadays, machine learning and deep learning engineers are earning high-level salaries, even when they are working at non-profit organizations, which speaks to how hot the field is [ 50 ]  [11] . Sadly, this is something that media companies often report without profound examination and frequently go along with AI articles with pictures of crystal balls and other supernatural portrayals. Such deception helps those companies generate hype around their offerings [ 27 ]. Yet, down the road, as they fail to meet the expectations, these organizations are forced to hire humans to make up for their so-called AI  [12] . In the end, they might end up causing mistrust in the field and trigger another AI winter for the sake of short-term gains [ 2 ] [ 28 ]. I am always open to feedback, please share in the comments if you see something that may need revisited. Thank you for reading! The author would like to extensively thank  Ben Dickson , Software Engineer and Tech Blogger, for his kindness to allow me to rely on his expertise and storytelling, along with several members of the AI Community for the immense support and constructive criticism in preparation of this article. DISCLAIMER:  The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University nor other companies (directly or indirectly) associated with the author(s). These writings do not intend to be final products but rather a reflection of current thinking and a catalyst for discussion and improvement. You can find me on  my website ,  Medium ,  Instagram ,  Twitter ,  Facebook ,  LinkedIn . Introduction to Machine Learning |  Matt Gormley  |  School of Computer Science , Carnegie Mellon University |  http://www.cs.cmu.edu/~mgormley/courses/10601/ AI for Everyone | Andrew Ng | Coursera |  https://www.coursera.org/learn/ai-for-everyone Machine Learning Course | Google |  https://developers.google.com/machine-learning/crash-course/ Intro to Machine Learning | Udacity |  https://www.udacity.com/course/intro-to-machine-learning‚Äìud120 Machine Learning Training | Amazon Web Services |  https://aws.amazon.com/training/learning-paths/machine-learning/ Introduction to Machine Learning | Coursera |  https://www.coursera.org/learn/machine-learning [1] The Discipline of Machine learning | Tom M. Mitchell |  http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf [2] Why the difference between AI and machine learning matters | Ben Dickson | TechTalks |  https://bdtechtalks.com/2018/10/08/artificial-intelligence-vs-machine-learning/ [3] Types of Machine Learning Algorithms You Should Know | David Fumo | Towards Data Science |  https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861 [4] Watch our AI system play against five of the world‚Äôs top Dota 2 Professionals | Open AI |  https://openai.com/five/ [5] Differences between Neural Networks and Deep Learning | Quora |  https://www.quora.com/What-is-the-difference-between-Neural-Networks-and-Deep-Learning [6] What Machine Learning Can and Cannot Do | WSJ |  https://blogs.wsj.com/cio/2018/07/27/what-machine-learning-can-and-cannot-do/ [7] Carnegie Mellon Dean of Computer Science on the Future of AI | Forbes |  https://www.forbes.com/sites/peterhigh/2017/10/30/carnegie-mellon-dean-of-computer-science-on-the-future-of-ai [8] From AI to Ml to AI: On Swirling Nomenclature & Slurried Thought | Zachary C. Lipton | Approximately Correct |  http://approximatelycorrect.com/2018/06/05/ai-ml-ai-swirling-nomenclature-slurried-thought/ [9] Tree Search Algorithms | Introduction to AI |  http://how2examples.com/artificial-intelligence/tree-search [10] Reinventing Education Based on Data and What Works, Since 1955 | Carnegie Mellon University |  https://www.cmu.edu/simon/what-is-simon/history.html [11] Does Deep-Blue use AI? | Richard E. Korf | University of California |  https://www.aaai.org/Papers/Workshops/1997/WS-97-04/WS97-04-001.pdf [12] Artificial Intelligence: Salaries Heading Skyward | Stacy Stanford | Machine Learning Memoirs |  https://medium.com/mlmemoirs/artificial-intelligence-salaries-heading-skyward-e41b2a7bba7d [13] The rise of ‚Äòpseudo-AI‚Äô: how tech firms quietly use humans to do bots‚Äô work | The Guardian |  https://www.theguardian.com/technology/2018/jul/06/artificial-intelligence-ai-humans-bots-tech-companies [14] Simplify Machine Learning Pipeline Analysis with Object Storage | Western Digital |  https://blog.westerndigital.com/machine-learning-pipeline-object-storage/ [15] Dr. Andrew Moore Opening Keynote | Artificial Intelligence and Global Security Initiative |  https://youtu.be/r-zXI-DltT8 [16] The 50 Best Public Datasets for Machine Learning | Stacy Stanford |  https://medium.com/datadriveninvestor/the-50-best-public-datasets-for-machine-learning-d80e9f030279 [17] Computational Learning Theory | ACL |  http://www.learningtheory.org/ [18] Machine Learning Definition | Tom M. Mitchell| McGraw-Hill Science/Engineering/Math; (March 1, 1997), Page 1 | h ttp://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html [19] For pioneering contributions and leadership in the methods and applications of machine learning. |  ‚ÄúProf. Tom M. Mitchell .‚Äù  National Academy of Engineering . Retrieved October 2, 2011. [20] Recommender System | Wikipedia |  https://en.wikipedia.org/wiki/Recommender_system [21] Spotify‚Äôs ‚ÄúThis Is‚Äù playlists: the ultimate song analysis for 50 mainstream artists | James Le |  https://towardsdatascience.com/spotifys-this-is-playlists-the-ultimate-song-analysis-for-50-mainstream-artists-c569e41f8118 [22] How recommender systems make their suggestions | Bibblio |  https://medium.com/the-graph/how-recommender-systems-make-their-suggestions-da6658029b76 [23] Deep Blue | Science Direct Assets |  https://www.sciencedirect.com/science/article/pii/S0004370201001291 [24] 4 great leaps machine learning made in 2015 | Sergar Yegulalp |  https://www.infoworld.com/article/3017250/4-great-leaps-machine-learning-made-in-2015.html [25] Limitations of Deep Learning in AI Research | Roberto Iriondo | Towards Data Science |  https://towardsdatascience.com/limitations-of-deep-learning-in-ai-research-5eed166a4205 [26] Forty percent of ‚ÄòAI startups‚Äô in Europe don‚Äôt use AI, claims report | The Verge |  https://www.theverge.com/2019/3/5/18251326/ai-startups-europe-fake-40-percent-mmc-report [27] This smart toothbrush claims to have its very own ‚Äòembedded AI‚Äô | The Verge | h ttps://www.theverge.com/circuitbreaker/2017/1/4/14164206/smart-toothbrush-ara-ai-kolibree [28] The Coming AI Autumn | Jeffrey P. Bigham |  http://jeffreybigham.com/blog/2019/the-coming-ai-autumnn.html [29] Forty percent of ‚ÄòAI startups‚Äô in Europe don‚Äôt use AI, claims report | The Verge |  https://www.theverge.com/2019/3/5/18251326/ai-startups-europe-fake-40-percent-mmc-report [30] The State of AI: Divergence | MMC Ventures |  https://www.mmcventures.com/wp-content/uploads/2019/02/The-State-of-AI-2019-Divergence.pdf [31] Top Sales & Marketing Priorities for 2019: AI and Big Data, Revealed by Survey of 600+ Sales Professionals | Business Wire |  https://www.businesswire.com/news/home/20190129005560/en/Top-Sales-Marketing-Priorities-2019-AI-Big [32] Artificial Intelligence Beats the Hype With Stunning Growth | Forbes |  https://www.forbes.com/sites/jonmarkman/2019/02/26/artificial-intelligence-beats-the-hype-with-stunning-growth/#4e8507391f15 [33] Misuse of AI can destroy customer loyalty: here‚Äôs how to get it right | Compare the Cloud |  https://www.comparethecloud.net/articles/misuse-of-ai-can-destroy-customer-loyalty-heres-how-to-get-it-right/ [34] Timeline of Artificial Intelligence | Wikipedia |  https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence#1950s [35] Computer Chess | Wikipedia |  https://en.wikipedia.org/wiki/Computer_chess [36] Artificial Intelligence at Carnegie Mellon University |Machine Learning Department at Carnegie Mellon University |  https://www.youtube.com/watch?v=HH-FPH0vpVE [37] Search Control Methods in Deep Blue | Semantic Scholar |  https://pdfs.semanticscholar.org/211d/7268093b4dfce8201e8da321201c6cd349ef.pdf [38] Is Winter Coming? | University of California, Berkeley |  https://pha.berkeley.edu/2018/12/01/is-winter-coming-artificial-intelligence-in-healthcare/ [39] AI Winter | Wikipedia |  https://en.wikipedia.org/wiki/AI_winter [40] A Very Short History of Data Science | Forbes |  https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/#3c828f2055cf [41] Deep Learning Revolution | Wikipedia |  https://en.wikipedia.org/wiki/Deep_learning#Deep_learning_revolution [42] Turing Award Winners 2018 | ACM |  https://amturing.acm.org/byyear.cfm [43] Recommender System | Wikipedia |  https://en.wikipedia.org/wiki/Recommender_system [44] The discourse is unhinged: how the media gets AI alarmingly wrong | The Guardian |  https://www.theguardian.com/technology/2018/jul/25/ai-artificial-intelligence-social-media-bots-wrong [45] Retailers moved from AI hype to reality in 2018 | iXtenso |  https://ixtenso.com/technology/retailers-moved-from-ai-hype-to-reality-in-2018.html [46] Deep Learning & The Human Brain, Inspiration not Imitation | Imaginea |  https://www.imaginea.com/sites/deep-learning-human-brain-inspiration-not-imitation/ [47] Carnegie Mellon Dean of Computer Science on the Future of AI | Forbes |  https://www.forbes.com/sites/peterhigh/2017/10/30/carnegie-mellon-dean-of-computer-science-on-the-future-of-ai/#164487aa2197 [48] History of AI Winters | Actuaries Digital |  https://www.actuaries.digital/2018/09/05/history-of-ai-winters/ [49] Recent Advances in Deep Learning: An Overview | Arxiv |  https://arxiv.org/pdf/1807.08169.pdf [50] Tech Giants Are Paying Huge Salaries for Scarce AI Talent | New York Times |  https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html [51] Artificial Intelligence is a Moving Target | AnswerRocket |  https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html [52] Search Control Methods in Deep Blue | Semantic Scholar |  https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html [53] Search Tree Algorithms on Deep Blue | Stanford University |  http://stanford.edu/~cpiech/cs221/apps/deepBlue.html  DDI"
How Quantitative UX Research Differs from Data Analytics,ow Quantitative UX Research Differs from Data Analytic,"When we introduce ourselves as quantitative UX researchers, people often get curious about the ‚Äúquantitative‚Äù part of our title. One question they ask is how our work is similar to, or different from, the work of product analysts or product data scientists. We hope this article will provide a comprehensive answer. The confusion is understandable. For one thing, we both love data. We‚Äôre trained to conduct quantitative analyses with variety of different data sources, including experiments, surveys, and logged behaviors. Many of us, across both groups, come from quantitative disciplines such as social psychology, statistics, computer science, and economics, which help us drive insights and elevate our teams‚Äô understanding of product usage through large amounts of data. At Facebook, the Data Analytics and the UX Research team share the same mission of explaining phenomena we observe in data. We both focus on making meaningful and interpretable inferences about data, relationships between variables, and explanations for changes or patterns in the data. (By contrast, machine learning engineers and those in other big data roles focus on predicting unknowns as accurately as possible.) Both groups use the same primary tool: statistics. We write code in data analysis software like R, Python, or SPSS. We spend a lot of time exploring and visualizing data to drive our hypothesis generation. We visualize more complex relationships of data points using libraries in R and Python. We also need to have knowledge of distributed data storage systems like Presto and Hive (some SQL knowledge is often sufficient to work with those tools). Most of our work ends up, hopefully, in presentations that clearly and concisely communicate our results.    However, there are also fundamental differences between the two roles. Here are 4 major distinctions between quantitative UX research and data analytics. (Note: these differences are influenced by our work at Facebook and may not all apply everywhere.) For a technology company to perform well, it has to focus relentlessly on both improving business metrics and delighting its users. Insights about how and why metrics are changing help the company build better products and grow their business value. Understanding users ‚Äî their motivations, their experiences, and how the product fits into their life ‚Äî is also critically important.   Quantitative UX research delivers insights about people. UX researchers often approach research projects with questions such as: What are the human motivations for using these products? How do people perceive and use the product? How do they react emotionally and physically to it? What do they like and dislike about specific features? What role does the product play in their daily life?    Data scientists, on the other hand, often start by asking questions related to how the product is performing, or is expected to perform, in the marketplace. How does a product feature change behavioral metrics, such as clicks or time spent? How much adoption of the product did we receive on various devices? Which features are used and which ones are abandoned?    Despite the different motivating problems, data scientists and quantitative UX researchers have similar workflows as they collect and analyze data in order to discover important interactions and relationships between technology and people. User actions tell us about what is happening ‚Äî for example, how many times they clicked on something, how often they come back to the app, or how much time they spent on the site.    User intents, on the other hand, are about the relationship between people who use the product and (un)available product features. For example, out of 10 clicks, how many of those clicks were out of interest and how many were due to frustration? What brings users back to the app and how do they feel about it? How much of the time they spent on the site was time well-spent, and how much was spent looking for something they couldn‚Äôt find or trying to figure out a feature that wasn‚Äôt intuitive?    Data scientists are less concerned with such questions than with with metrics and collective performance based on user actions or lack of action. They‚Äôre interested in the timing, variety, and magnitude of users‚Äô signals ‚Äî things like views, clickthrough rates, time spent, and churn.   UX researchers, including quantitative ones, are mainly interested in understanding how people use our products, what problems they may have, and what works differently for them. Quantitative researchers seek to gain insights about the intent of people‚Äôs product usage through patterns in the data they collected. They also try to measure quality of experience using self-reported data (surveys) or behavioral data.    While data scientists are more concerned about how many people used a new feature and what they did afterward, UX researchers aim to understand how many people used the feature in various contexts, what motivates them to use the feature, and how they felt about the experience. Like data scientists, quantitative UX researchers may use a multitude of statistical tools to gather insights from data. While the main suite of tools used by the two groups are roughly similar, each group uses the tools differently, since they‚Äôre pursuing different goals.   Compared to UX researchers, data scientists are more often motivated to improve the predictive accuracy of their models. (The most accurate models are black box machine learning models, which are hard to interpret by their nature.)    UX researchers are more often motivated by inference. In many cases, we‚Äôre not looking to predict future phenomena but to better understand the factors underlying experience or behavior. That‚Äôs why UX researchers more often use social science models that are more interpretable but have lower predictive accuracy. In data science, activity logs are the primary source of data. Quantitative UX researchers use a combination of log data and self-reported survey data. Depending on the research questions, we may use only one source of data or combine multiple methods of data collection and analysis.   Analyzing survey data requires a different methodology than analyzing log data. To accurately make sense of survey data, the quantitative researcher must consider, and model, the survey design and data collection process. Survey data is therefore typically analyzed with some form of regression, in which   survey design elements are incorporated into the model. For activity log data, Data Scientists typically consider these elements of data collection to be ignorable.   Log data is frequently several orders of magnitude larger than survey data. As a result, overfitting and algorithm speed are critical issues in learning from log data that typically don‚Äôt arise in analyzing survey data. While regression models are also used in analyzing log data, Data Scientists frequently use methods involving  regularization   in order to handle issues introduced by data size. Despite the real differences between data science and quantitative UX research, there are undoubtedly many cases where the two roles are almost interchangeable. But they can also be highly complementary, taking full advantage of a diverse range of backgrounds and skills. In fact, at Facebook, some of the most impactful, satisfying, and fulfilling research projects are collaborations between the two. The need for both groups shows no signs of fading. The number and variety of meaningful research questions about the relationship between technology and people ensure that data science and quantitative UX research will continue to exist in parallel, driving the business and the technology forward to serve people better. While we‚Äôve shared some of the ways Quantitative UX Research and Data Analytics are similar or different from each other, our observations are influenced by the context of our organization and the nature of our work. We welcome further discussions about these two roles in other organizations, and fields. What kind of problems do you solve as a Quantitative UX Researcher or a Data Scientist? We‚Äôd love to hear from you ‚Äî please comment below! Authors:   Mary Nguyen , Researcher at Facebook;  Saide Bakshi , Researcher at Facebook; and Alex Whitworth, Data Scientist at Facebook  (from left to right) Illustrator:  Drew Bardana"
Building a Successful Modern Data Analytics Platform in the Cloud,uilding a Successful Modern Data Analytics Platform in the Clou,"I worked with dozens of companies migrating their legacy data warehouses or analytical databases to the cloud. I saw the difficulty to let go of the monolithic thinking and design and to benefit from the modern cloud architecture fully. In this article, I‚Äôll share my pattern for a scalable, flexible, and cost-effective data analytics platform in the AWS cloud, which was successfully implemented in these companies."
Top 6 Data Analytics Tools in 2021,op 6 Data Analytics Tools in 202,"When it comes to data analytics tools, we always have questions. What is the difference between so many data analysis tools? Which is better? Which one should I study? Although this is a commonplace topic, it is really important, and I have been working hard to pursue the answer to this ultimate problem. If you go online to search for relevant information in this area, it is difficult to see a fair point of view. Because the reviewers who evaluate a certain data analytics tool may be from a different perspective, with some personal feelings. Today, let us put aside these personal feelings. And I am trying to talk objectively with you about my personal views on data analysis tools on the market, for your reference. I have chosen a total of 6 tools in three types. Next let me introduce them one by one. With a variety of powerful features such as form creation, PivotTable, VBA, etc., Excel‚Äôs system is so large that no analytics tool can surpass it, ensuring that people can analyze data according to their needs. However, some people may think that they are very proficient in computer programming languages, and disdain to use Excel as a tool because Excel can‚Äôt handle big data. But think about it, do the data we use in our daily life exceed the limit of big data? In my opinion, Excel is a versatile player. It works best for small data, and with plugins it can handle millions of data. To sum up, based on the powerful features of Excel and its user scale, my opinion is that it is an indispensable tool. If you want to learn data analysis,  Excel  is definitely the first choice. Business intelligence is born for data analysis, and it is born at a very high starting point. The goal is to shorten the time from business data to business decisions and use data to influence decisions. The product goal of Excel is not like this. Excel can do a lot of things. You can use Excel to draw a curriculum, make a questionnaire, or use it as a calculator, or even use it for drawing. If you master VBA, you can also make a small game. But these are not really data analysis functions. But BI tools are specialized in data analysis. Take the common BI tools such as Power BI, FineReport, and Tableau for example. You will find that they are designed according to the data analysis process. First, data processing, data cleaning, and then data modeling, finally  data visualization  that uses presentation of charts to identify problems and influence decision-making. These are the only way for data analysis, and there are some pain points of employees in this process. For example, the repetitive and low value-added work of cleaning data can be simplified with BI tools. If the amount of data is large, the traditional tool Excel cannot complete the PivotTable. If we use Excel to do graphical displays, it will take a lot of time to edit the chart, including color and font settings. These pain points are where BI tools can bring us change and value. Now let‚Äôs compare the three popular BI tools on the market: Power BI, FineReport, and Tableau. The core essence of Tableau is actually the PivotTable and PivotChart of Excel. It can be said that Tableau is keenly aware of this feature of Excel. It entered the BI market earlier and carried forward this core value. From the perspective of development history and current market feedback, Tableau is better at visualization. I don‚Äôt think this is because of how cool its charts are, but its design, color, and user interface give us a simple and fresh feeling. This is indeed like Tableau‚Äôs own propaganda, investing a lot of academic energy to study what kind of charts people like, how to give users the ultimate experience in operation and visual. As Tableau advertises, their team puts a lot of academic energy into researching what kind of charts people like, and how to give users the ultimate experiencein terms of operation and vision. In addition, Tableau has added data cleaning function and more intelligent analysis functions. This is also a predictable product development advantage for Tableau. The advantage of Power BI lies in its business model and data analysis capabilities. Power BI was previously a plug-in for Excel, and its development was not ideal. So it got out of Excel and developed into a BI tool. As a latecomer, Power BI has iterative updates every month and catches up very fast. Power BI currently has three licensing methods: Power BI Free, Power BI Pro, and Power BI Premium. Just like Tableau, the features of its free version are not complete. But they are almost enough for personal use. And the data analysis of Power BI is powerful. Its PowerPivot and DAX language allow me to implement complex advanced analysis in a way similar to writing formulas in Excel. What makes FineReport unique is that its self-service data analysis is very suitable for business users. With a simple drag and drop operation, you can design various styles of reports with FineReport and easily build a data decision analysis system. FineReport can directly connect to all kinds of databases, and it is convenient and quick to customize various styles to make weekly, monthly, and annual reports. Its format is similar to the interface of Excel. The features include report creation, report permission assignment, report management, data entry, etc. In addition, the visualization function of FineReport is also very prominent, providing a variety of  dashboard templates  and a number of self-developed visual plug-in libraries. In terms of price, the personal version of FineReport is completely free and all features are open. R  and  Python  are the third type of tools I want to talk about. Although softwarelike Excel and BI tools have been designed with the utmost effort to consider the most application scenarios of data analysis, they are essentially customized. If the software doesn‚Äôt design a feature, or develop a button for a feature, chances are that you won‚Äôt be able to complete your work with them. The programming language is different for this. It is very powerful and flexible. You can write code to do anything you want. For example, R and Python are the indispensable tools for data scientists. From a professional perspective, they are definitely powerful than Excel and BI tools. So what are the application scenarios that R and Python can realize, while it is difficult for Excel and BI tools to achieve? In terms of R language, it is best at statistical analysis, such as normal distribution, using algorithm to classify clusters, and regression analysis. This kind of analysis is like using data as an experiment. It can help us answer the following questions. For example, the distribution of data is a normal distribution, a triangular distribution or other types of distribution? What is the discrete situation? Is it within the statistical controllable range we want to achieve? What is the magnitude of the effect of different parameters on the results? And there is also hypothetical simulation analysis. If a certain parameter changes, how much impact will it bring? For example, we want to predict the behavior of a consumer. How long will he stay in our store? How much will he spend? We can find out his personal credit status and make a loan amount based on his online consumption record. Or we can push different items based on his browsing history on the web page. This also involves the current popular concepts of machine learning and artificial intelligence. The above comparison illustrates the difference between several softwares. What I want to summarize is that what is real is reasonable. Excel, BI tools or programming languages have overlapping functions, but they are also complementary tools. The value of each depends on the kind of application being developed and situation at hand. Before you choose a data analytics tool, you must first understand your own work: whether you will use the application scenarios I just mentioned. Or think about your career direction: whether it is toward the data science or business analysis. 8 Best Reporting Tools& Software To Improve Your Business 5 Most Popular Business Intelligence (BI) Tools in 2021 9 Data Visualization Tools That You Cannot Miss in 20 21 Top 16 Types of Chart in Data Visualization Top 10 Map Types in Data Visualization A Step-by-Step Guide to Making Sales Dashboards"
Data Analytics with Python by Web scraping: Illustration with CIA World Fact-book,ata Analytics with Python by Web scraping: Illustration with CIA World Fact-boo,"In  a data science project, almost always the most time consuming and messy part is the data gathering and cleaning. Everyone likes to build a cool deep neural network (or XGboost) model or two and show off one‚Äôs skills with cool 3D interactive plots. But the models need raw data to start with and they don‚Äôt come easy and clean. Life, after all, is not Kaggle where a zip file full of data is waiting for you to be unpacked and modeled :-) But why gather data or build model anyway ? The fundamental motivation is to answer a business or scientific or social question.  Is there a trend ?  Is this thing related to that ?  Can the measurement of this entity predict the outcome for that phenomena ? It is because answering this question will validate a hypothesis you have as a scientist/practitioner of the field. You are just using data (as opposed to test tubes like a chemist or magnets like a physicist) to test your hypothesis and prove/disprove it scientifically.  That is the ‚Äòscience‚Äô part of the data science. Nothing more, nothing less‚Ä¶ Trust me, it is not that hard to come up with a good quality question which requires a bit of application of data science techniques to answer. Each such question then becomes a small little project of your which you can code up and showcase on a open-source platform like Github to show to your friends. Even if you are not a data scientist by profession, nobody can stop you writing cool program to answer a good data question. That showcases you as a person who is comfortable around data and one who can tell a story with data. Let‚Äôs tackle one such question today‚Ä¶ Is there any relationship between the GDP (in terms of purchasing power parity) of a country and the percentage of its Internet users? And is this trend similar for low-income/middle-income/high-income countries? Now, there can be any number of sources you can think of to gather data for answering this question. I found that an website from CIA (Yes, the ‚ÄòAGENCY‚Äô), which hosts basic factual information about all countries around the world, is a good place to scrape the data from. So, we will use following Python modules to build our database and visualizations, Let‚Äôs talk about the program structure to answer this data science question. The  entire boiler plate code is available here  in my  Github repository . Please feel free to fork and star if you like it. Here is how the  front page of the CIA World Factbook  looks like, We use a simple urllib request with a SSL error ignore context to retrieve this page and then pass it on to the magical BeautifulSoup, which parses the HTML for us and produce a pretty text dump. For those, who are not familiar with the BeautifulSoup library, they can watch the following video or read this  great informative article on Medium . So, here is the code snippet for reading the front page HTML, Here is how we pass it on to BeautifulSoup and use the  find_all  method to find all the country names and codes embedded in the HTML. Basically, the idea is to  find the HTML tags named ‚Äòoption‚Äô . The text in that tag is the country name and the char 5 and 6 of the tag value represent the 2-character country code. Now, you may ask how would you know that you need to extract 5th and 6th character only? The simple answer is that  you have to examine the soup text i.e. parsed HTML text yourself and determine those indices . There is no universal method to determine this. Each HTML page and the underlying structure is unique. This step is the essential scraping or crawling as they say. To do this,  the key thing to identify is how the URL of each countries information page is structured . Now, in general case, this is may be hard to get. In this particular case, quick examination shows a very simple and regular structure to follow. Here is the screenshot of Australia for example, That means there is a fixed URL to which you have to append the 2-character country code and you get to the URL of that country‚Äôs page. So, we can just iterate over the country codes‚Äô list and use BeautifulSoup to extract all the text and store in our local dictionary. Here is the code snippet, For good measure, I prefer to serialize and  store this data in a  Python pickle object   anyway. That way I can just read the data directly next time I open the Jupyter notebook without repeating the web crawling steps. This is the core text analytics part of the program, where we take help of  regular expression  module  to find what we are looking for in the huge text string and extract the relevant numerical data. Now, regular expression is a rich resource in Python (or in virtually every high level programming language). It allows searching/matching particular pattern of strings within a large corpus of text. Here, we use very simple methods of regular expression for matching the exact words like ‚Äú GDP ‚Äî per capita (PPP): ‚Äù and then read few characters after that, extract the positions of certain symbols like $ and parentheses to eventually extract the numerical value of GDP/capita. Here is the idea illustrated with a figure. There are other regular expression tricks used in this notebook, for example to extract the total GDP properly regardless whether the figure is given in billions or trillions. Here is the example code snippet.  Notice the multiple error-handling checks placed in the code . This is necessary because of the supremely unpredictable nature of HTML pages. Not all country may have the GDP data, not all pages may have the exact same wordings for the data, not all numbers may look same, not all strings may have $ and () placed similarly. Any number of things can go wrong. It is almost impossible to plan and write code for all scenarios but at least you have to have code to handle the exception if they occur so that your program does not come to a halt and can gracefully move on to the next page for processing. One thing to remember is that all these text analytics will produce dataframes with slightly different set of countries as different types of data may be unavailable for different countries. One could use a  Pandas left join  to create a dataframe with intersection of all common countries for which all the pieces of data is available/could be extracted. After all the hard work of HTML parsing, page crawling, and text mining, now you are ready to reap the benefits ‚Äî eager to run the regression algorithms and cool visualization scripts! But wait, often you need to clean up your data (particularly for this kind of socio-economic problems) a wee bit more before generating those plots. Basically, you want to filter out the outliers e.g. very small countries (like island nations) who may have extremely skewed values of the parameters you want to plot but does not follow the main underlying dynamics you want to investigate. A few lines of code is good for those filters. There may be more  Pythonic  way to implement them but I tried to keep it extremely simple and easy to follow. The following code, for example, creates filters to keep out small countries with < 50 billion of total GDP and low and high income boundaries of $5,000 and $25,000 respectively (GDP/capita). We use  seaborn regplot  function  to create the scatter plots (Internet users % vs. GDP/capita) with linear regression fit and 95% confidence interval bands shown. They look like following. One can interpret the result as There is a strong positive correlation between Internet users % and GDP/capita for a country. Moreover, the strength of correlation is significantly higher for low-income/low-GDP countries than the high-GDP, advanced nations.  That could mean access to internet helps the lower income countries to grow faster and improve the average condition of their citizens more than it does for the advanced nations . This article goes over a demo Python notebook to illustrate how to crawl webpages for downloading raw information by HTML parsing using BeautifulSoup. Thereafter, it also illustrates the use of Regular Expression module to search and extract important pieces of information what the user demands. Above all, it demonstrates how or why there can be no simple, universal rule or program structure while mining messy HTML parsed texts. One has to examine the text structure and put in place appropriate error-handling checks to gracefully handle all the situations to maintain the flow of the program (and not crash) even if it cannot extract data for all those scenarios. I hope readers can benefit from the provided Notebook file and build upon it as per their own requirement and imagination. For more web data analytics notebooks,  please see my repository. If  you have any questions or ideas to share, please contact the author at  tirthajyoti[AT]gmail.com . Also you can check author‚Äôs  GitHub repositories  for other fun code snippets in Python, R, or MATLAB and machine learning resources. If you are, like me, passionate about machine learning/data science, please feel free to  add me on LinkedIn  or  follow me on Twitter."
Data Analytics is hard‚Ä¶ Here‚Äôs how you can excel,ata Analytics is hard‚Ä¶ Here‚Äôs how you can exce,"I really enjoyed working in data. Since my sophomore year, I already knew that I want to use technology as a way to solve real business problems. It gives me meaning to work and I hope it does for you as well. Every day I would squeeze every bit of my time to read and learn. I would sign up for the local Data Hackathon and organize Analytics related events to learn from industry leaders. I spoke in conferences and university events to mentor my juniors on how to succeed as a data analyst in large tech companies. I imagined myself as a fellow student to influence as many of my juniors as possible to learn and give back to communities. If you say that this end goal might sound too noble, you are right! In fact, as a Christian, I learnt that time is precious, the mother earth is a terminal where we only reside temporarily. Nothing in the world is lasting including time. So we need to create and deliver as much value as possible to prepare for the everlasting life with God ahead. For me, my religion and data career has given me meanings to excel, to learn and to contribute. After my admittance at Google as a Data Analyst to develop better ML Models to fight abuse, I have received many requests to share my life journey and tips for my juniors to ponder on. Therefore, I really hoped that this blog could fulfill that demand to give you the starting points to be successful as a data professional. As what we have known, data science and analytics have become a fast moving industry with the highest growth over the past few years. Just within a few years many universities started providing specializations in Data Science with thousands of sign ups from all around the world. Even in Singapore, there were no such programs 5 years ago, but now, the entrance of this degree has become as hard as getting into law and business school. However, despite the rising trend, there are so many uncertainties about where the excitement is going. The job market for data scientist is becoming more and more saturated and delusional.  Many startups are starting to realize that they are moving too fast in data science and start laying off their data scientists. Therefore, to secure your future, you will need to mature quickly and differentiate yourself from your peers. Just like a war, You would need to start preparing your armory. The best way, is to contribute more: learn, write, speak, specialize, and chill more Keep experimenting with your learning styles (kinesthetic, auditory, visual). When I was learning, my friends always gave me lists of curated Machine Learning materials. But after reading and listening to lots of videos, I realized that I was a kinesthetic type (learnt by doing) and I retained very few from listening. Knowing that, I created my own projects which I documented on Github.  Surprisingly, these projects had become the key for me to get into Visa and Google . Similarly, do not blindly follow the conventional learning materials that your friends suggest. Do your due diligence and always have a trial and error mindset. Soon, you will find your best learning style to boost your skills. For me, I usually use many different kinds of sources from Youtube and research paper. Personally I enjoyed  Sentdex  and  Computerphile . Highly recommend for you to watch these videos and even better, reproduce them. Furthermore, I am also taking a part time  online master degree at Georgia Tech  which exposed me to deeper technical rigor of machine learning and statistics. Always experiment, trial and error for you to learn quickly about this exciting industry. Write articles, share your codes to github, even better create a Youtube channel. During my university time, I started writing online tutorials for my juniors to tackle their university projects. Once I formed those tutorials, I would set up some small sessions to nurture a study group and shared some machine learning related models. By sharing, I had fun and learnt to articulate my thoughts. Similarly, I also believe you will learn more as you write more. Every time you write some projects, you will be able to reflect on a certain model/test/code review. It will allow you to close your gap of knowledge and figure out how to up skill and find better solutions. So far, I find  Medium  as one of the best channels to write on. It gives a sleek and standardized look for everyone to write, which free you the hassle to deal with the visual layout. Everything in Medium is already optimized for you to read and write. Even better, you can also sign up for the metered paywall. This will give the opportunity for curators to distribute your articles and improve your readership. So far, I have released 30 articles with a few producing $300 USD per article. Honestly, I think the best benefit of writing is that you get the chance to articulate your learning. It does not matter whether you blog online or even make a youtube channel. The goal is to maximize your time to learn and promote yourself. Teach your fellow peers or any conferences out there! When you speak, you are distributing your knowledge for others to use. You are promoting both yourself and your company. In the long run, you would be the more valuable data scientist because you have inspired your juniors to do the same. You will differentiate yourself with your peers. Furthermore, speaking will give you further meaning to learn. The more you learn the more you speak. Similarly the more you speak, the more you learn. Initially, I would send emails to university, data conference, and youth groups to see if I could share some of my writings in data science. I have been fortunate that a few students groups at Singapore Management University (SMU) and National University of Singapore (NUS) have been receptive to my requests. When I speak about my knowledge, I not only inspire my juniors but also learn from them to communicate my thoughts well. Try to dig deep into a certain data science technique to complement your strengths (Business, Social Science, Sales, etc) Have your own mindset and stick with it. Most of the common misconceptions are that business students would lose to IT students due to the needs of technical expertise. This is not accurate. A lot of superstar analysts I know come from various backgrounds such as Social Science, Business, and Economics. They use analytics to complement their expertise. Therefore, stay calm and leverage your strengths rather than indulging on the latest Kaggle‚Äôish analytics trends such as Random Forest, XGBoost, etc For example, if you come from a finance background, you can develop your own stocks analysis project. If you come from operations and inventory management, you can focus on JIT (Just In Time) analytics to minimize bottlenecks and maximize efficiency. The sky‚Äôs the limit when you use analytics to make data driven analysis on your domain. For me, I come from a software development background. But I used business analytics to communicate my skills by developing a simple product. One day, I asked my friends who are finance students how they analysed stocks. Their answers surprised me as they used most time to copy and paste each data from Google Finance into an excelsheet and analysed them. As a response, I created a simple program to web scrape and generate intrinsic values using Value Investing methodology.  This has become my flagship product that I share with my fellow students at SMU. Avoid burnout and have fun while hacking Personal relationship is important in life. Therefor keep hacking with your friends. Use your Saturdays to learn and Sundays to chill. Furthermore, Join a learning minded companies. Visa and Google are a few of them. The best thing about working in tech companies is that you get paid to learn critical skills that would be in demand in the years ahead. For me, I am very happy to avoid burning out by chilling with my friends at church. Whenever I have free time, I would work out.  I would train for marathon, triathlon, and even cycling trip overseas (recently 515 km all over 3 days at Java Island Indonesia) . It is very busy, but it is very fulfilling and I would re-energize once the new Monday starts. Soli Deo Gloria. I really hope this has been a great read and a source of inspiration for you to develop and innovate. Please  Comment  out below for suggestions and feedback. Just like you, I am still learning how to become a better Data Scientist and Engineer. Please help me improve so that I could help you better in my subsequent article releases. Thank you and Happy coding :) Vincent Tatan is a Data and Technology enthusiast with relevant working experiences from Google LLC, Visa Inc. and Lazada to implement microservice architectures, business intelligence, and analytics pipeline projects . Vincent is a native Indonesian with a record of accomplishments in problem-solving with strengths in Full Stack Development, Data Analytics, and Strategic Planning. He has been actively consulting SMU BI & Analytics Club, guiding aspiring data scientists and engineers from various backgrounds, and opening up his expertise for businesses to develop their products . Vincent also opens up his 1 on 1 mentorship service on  BestTop  and 10to8 to coach how you can land your dream Data Analyst/Engineer Job at Google, Visa or other large tech companies.  book your appointment with him here  if you are looking for mentorship. Lastly, please reach out to Vincent via  LinkedIn ,  Medium  or  Youtube Channel"
Advice to a Data Analytics Newbie.,dvice to a Data Analytics Newbie,"Data Analysts are recognized as some of the most sought after tech talents with increasing demand and companies are prepared to pay extremely competitive salaries. It‚Äôs no surprise that more and more people are looking to switch into the field from other fields including non-technical fields. However, making a career switch isn‚Äôt just about the salary specs and job security but you also want to be certain that it is a role you will excel in. What do Data Analysts do? The job description of a data analyst as well as the tools needed to get the job done varies from company to company so for now you only need to get familiar with the general roles and tools. As a newbie to the Data analytics field, you may get overwhelmed with overload of information, as there are a lot of available information on the internet. I believe setting both big and small learning goals is a technique that will help you make progress. Here are some tips that have helped me since I started my journey in July 2021 and I hope you find them useful as you start your own journey. If you answered YES to the above questions, you‚Äòd be an excellent data analyst! 3. Join Data communities:  The importance of community in the tech space cannot be over emphasized, as a collaborative effort is necessary for technological advancement. There are communities on different platforms (LinkedIn, Discord, slack, WhatsApp, Twitter, etc.) that you can join to stay motivated, learn from others and ask for help when stuck. I would recommend joining ‚ÄòThe All about Data‚Äô community. It is an online data community of over 6000 members from varying data fields, ready to assist you. You can join the community on  Twitter ,  Facebook  and  discord . 4. Get mentors:  Mentors can   influence and help you directly and indirectly on your journey. You can follow and even reach out to some professionals for tips and recommendations. Here is a list of 10 data professionals that you can follow online that constantly contribute to the growth of data community, 5 .  Love knowledge:  As an aspiring data analyst, you should be hungry for knowledge and have a ready-to-learn mindset. Choosing this career path means, you‚Äôve agreed and signed up for a life of consistent learning and growing. Data Analytics is still developing so even the professionals are still learning new things. Are you up for the task? 6. Have a ‚ÄòGrowth over Goal‚Äô mindset : Some days you will meet and exceed your goals, other days you may not but you are still learning and growing regardless. 7. Go slow and steady:  Going fast and hard leads to burnout but going steady ensures you make consistent progress. There is a lot to learn, so do not try to learn everything so fast. Draw up a learning curriculum for the road map you want to follow and move at a reasonable pace. Learning is a lifelong process. Below is a list of tools and skills to include in your data analytics learning curriculum. 8.   Understand the process:  Data analytics is much more than the tools involved. In as much as we all want to master the tools, knowledge about the data analysis process is important. I‚Äôd recommend taking the Coursera Google Data Analytics professional course, it explains the analysis process from asking the right questions to giving actionable recommendations from the results. (Check out my  article  on how to get financial aid for the course.) 9.   Self-Development is important:  Don‚Äôt just focus on the technical skills, learn other skills like time management, proper planning, presentation, communication, etc. You can read good self-development books and work hard towards becoming your best self. These skills will help you stand out. 10. Share your work online:  The feedback and recommendations you get online will help you grow and improve your skills. However, before sharing your work online with professionals, share with your friends and make the suggested corrections before sharing with a professional for review as they are busy and get a lot of requests daily. Some of the mentors I mentioned earlier, selflessly make out time to review projects for newbies. Follow me on  Twitter  for more data related information."
The Surprising Link between a Company‚Äôs Growth and its Data Analytics Strategy,he Surprising Link between a Company‚Äôs Growth and its Data Analytics Strateg,"As  a Business Intelligence Analyst, I often wondered, ‚ÄúHow does a company‚Äôs Data Strategy drive growth?‚Äù Typically, vendors would provide the same kinds of answers: Yawn. We‚Äôve all heard these pitches before and working in BI I knew there‚Äôs more to better decisions than just more data, more dashboards and more visualizations, especially since everyone just looks at pivot tables. Not to mention getting to insights faster doesn‚Äôt ensure you‚Äôll get to the  correct  insight. Faster does not equal better. The other thing to notice is that a lot of contemporary wisdom is just ‚Äúindustry jargon du jour‚Äù, which changes almost constantly. I wanted to go ‚Äúbeneath the veneer‚Äù to uncover the fundamental relationship between a company‚Äôs maturity with respect to its data strategy, and the effect of data maturity on growth. What I found shocked me: Your current Data Strategy may actually be  impeding  your growth. How could this be?  To understand how this is possible, we first need a definition of ‚ÄúData Maturity.‚Äù Measuring Maturity with the  Dell Data Maturity Model I‚Äôm sure every business student has seen this picture (or one very similar to it): What I wanted to see was another curve that quantified the maturity over time of a company‚Äôs Data Strategy, superimposed upon this growth curve. What I needed first was a way to measure Data Maturity, and then plot that over time. To do this I drew from the teachings of a leading tech company: Dell. CIO.com has an  article  in which they define the ‚ÄúDell Data Maturity Model‚Äù (or DDMM). Given Dell‚Äôs cachet this seemed like a good place to start. For anyone who doesn‚Äôt eat, breathe or sleep the industry jargon these descriptions may be a little abstract. So, let me build on this by illustrating the  pains  a company feels just before it transitions from one stage of the DDMM to the next. If you find yourself nodding along with one set of pains more than any other you have a good idea of where you are in the Dell Data Maturity Model. You may have noticed the gap between  Data Proficient  and  Data Savvy  is much wider as compared to the other stages. This wasn‚Äôt a formatting mistake. There are so many challenges involved in making that particular leap that, naturally, the gap is wider. Most companies never make the transition. In fact, a lot of businesses erroneously begin investing in becoming  Data Driven  (AI, Machine Learning, Data Mining, etc.) by hiring Data Scientists before achieving the  Data Savvy  stage. As a result, most Data Scientists are frustrated that they spend so much time cleaning and plumbing data because the foundation was never laid. In Dell‚Äôs Model, Data Maturity grows in steps or stages. But the one dimension missing from this diagram is  time . That is, how long does a company typically spend in each stage of the DDMM? This is an important piece that we‚Äôll dive into right after we talk about the different stages of Growth. Stage 1: The Startup Phase The company has only a few people, anywhere from 1 to 3 employees and a few systems. At this phase you‚Äôre building a car and trying to drive it at the same time. Growth will be slow and data-driven decision-making usually takes the form of ad-hoc spreadsheets. Stage 2: The Scale-up Phase This stage is all about trying to scale-out your production and sales efforts and just get product out the door. People are buying as fast as you‚Äôre producing, and you have little concern for inefficiencies as you skip over bumps in the road. Think of it this way: Your company is a brand-new sports car and you‚Äôre driving around with the parking-brake up and a parachute strapped to the back. But it doesn‚Äôt matter. Your growth engine is so powerful you‚Äôre just blazing past everyone else. But, a big enough pot-hole or bump in the road, and all that friction catches up with you. Stage 3: The Enterprise Phase The fun, youthful years of exponential growth have come to a close. It‚Äôs inevitable but all high performing companies eventually settle into Sustainable Growth. This isn‚Äôt a bad thing because your brand recognition also makes you King of the Hill. It just means growth over time will be linear, instead of exponential, and inefficiencies play a bigger role in slowing down growth. Why Data Maturity Lags Growth over Time This is where the rubber meets the road. How long does a company spend in each of these Data Maturity stages, and when   does a ‚Äútransition‚Äù occur relative to growth? Let‚Äôs quickly define a ‚Äútransition‚Äù as a step-wise improvement in a company‚Äôs Data Maturity. These are typically tied to events like hiring a BI Consultant, eventually a BI Team, or building a Data Warehouse. The purchase of technology is typically a part of all these transitions. To track Data Maturity against Growth over time, we combine a few things: In my experience, these things don‚Äôt just neatly line up when you overlay them, and that‚Äôs the reason we find an interesting pattern in most companies. If you add time to the Dell Data Maturity Model you end up with a  Data Maturity Curve  which roughly illustrates how long most successful companies stay in each stage of the DDMM. After overlaying revenue growth, in my experience, you see this pattern emerge: Now, as you can see, Data Maturity grows in steps, typically the result of transitions caused by ‚Äúprecipitating events‚Äù (more on that below). The Data Maturity Curve isn‚Äôt so much a ‚Äúcurve‚Äù as it is a series of steps. If, for the purpose of illustration, we fit a curve to those steps and overlaid that, we‚Äôd see the following: While I understand that (cross)correlation does not imply causation, what I expected to see was that as companies implement data assets, technologies or strategies, its Data Maturity Curve would lead ahead of its Growth Curve. And while higher Data Maturity doesn‚Äôt necessarily cause higher Growth, a leading Data Maturity Curve would at least have been in-line with my expectations. What I‚Äôve found is the opposite. The maturity of a company‚Äôs Data Strategy (its Data Maturity Curve) often  lags  its Growth Curve . Time and time again I have found this to be true of companies. A business‚Äôs Data Strategy (the different stages of the DDMM) is always behind the current state of growth. So, if you‚Äôre an executive leading a growing company, and reading this article right now, your company has already outgrown its current data strategy. Why is this? Because data-related pains don‚Äôt suddenly appear overnight. Unlike any other pain that physically prevents you from doing business (POS failures, internet outages, etc.), Data Pains are subtle, pernicious, chronic pains that compound. It‚Äôs a lot like lower back pain, something many of us just learn to live with. It‚Äôs there. It sucks. But we get on with our day. Startups in the Data Aware stage barely feel it. But if left untreated that pain can show up in the worst and most unexpected ways, and you will pay good money to make it go away. Anyone who‚Äôs spent an evening lying on the couch due to back pain will understand this. It‚Äôs these sorts of ‚Äúprecipitating events‚Äù that motivate step-wise improvements in a company‚Äôs Data Maturity. The above graphs are meant to illustrate a specific point: The maturity of a company‚Äôs data strategy  lags  company growth. So, you‚Äôre almost always behind the curve when it comes to your Data Strategy. But the keen reader should realize that, although we have graphed pain (and maturity) over time, they are not causally linked to time. They‚Äôre linked to  growth . This is the key distinction here. The pains themselves are tied to growth, not time, technology or the data itself. Untreated pains compound not over  time  but over  growth . Likewise,  new  pains occur due to growth, not the passage of time. You may even call them Data Growing Pains. Data-related pains tend to become unbearable at certain levels of growth. In the above graph we use revenue as a measurement of growth, but this is also very true of headcount, number of locations, countries in which the company does business, number of SKUs, and so on. This chart shows approximately where data pains become almost unbearable for  Apparel  businesses: Different industries will have different milestones in terms of revenue or headcount at which data pains become unbearable, but the general shape of the Pain vs. Growth curve will be the same. The pain will drop soon after a company undergoes a ‚Äútransition‚Äù to move from one stage of maturity to the next. What happens if you never invest in your Data Strategy? Some executives believe the first step of data integration ‚Äúis a showstopper,‚Äù leaving them stuck at a nascent level of Data Maturity despite their growth. This is a  bad  situation to be in, because not only will you inherit all the new problems associated with growth, the original problems which went unsolved just get worse. Fortunately, most executives realize the need for Business Intelligence.  Unfortunately , due to a lack of available technology, industry hype around Big Data and AI, or poor guidance from industry experts, most companies never cross the chasm between  Data Proficient  (having a BI system) and  Data Savvy  (Decision Support System enabled Centre of Excellence). In fact, some companies try to skip the  Data Savvy  stage entirely and go straight to  Data Driven  (using Data Science). As a result, most companies‚Äô Pain v. Growth curve actually looks like this: The silver lining here is that, while the pain will continue to worsen with growth, it doesn‚Äôt get exponentially worse. Most companies think that the solution here is to invest in AI and Big Data, and while they certainly have the data quantity to do this, they don‚Äôt have the foundations in place to act  nimbly  on the insights (or opportunities) that come from Big Data or AI. It‚Äôs also important to realize that if a company never expands beyond a certain level of growth, it‚Äôll never feel the (new) pains that accompany that (new) growth. If the business never adds countries, stores, SKUs, warehouses, customer types and so on, they‚Äôll never need to transition to a higher stage of Data Maturity. This is important for any executive who believes he or she can simply ‚Äúdeal with it later‚Äù and ‚Äúfocus on other priorities.‚Äù You cannot delay your data pains unless you also plan on delaying your company‚Äôs growth. In a future post we‚Äôll describe what these pains look like and how they manifest in the day-to-day operations of a business. How Process can be a Competitive Advantage You may be thinking, ‚ÄúOK, Company Growth causes Data Pains which, when solved, result in step-wise maturity in our Data Strategy. But who cares? We‚Äôre still growing in spite of that.‚Äù Maybe you‚Äôll continue to grow. Maybe not. The question you should be asking is, ‚ÄúHow much money are we leaving on the table by not solving these problems?‚Äù Another question to ask is, ‚ÄúHow long before these inefficiencies start to catch up with us?‚Äù and ‚ÄúWill it cause a premature slow-down in growth?‚Äù The best question to ask is, ‚ÄúHow bad is this problem for our larger competitors?‚Äù Dashboarding and Business Intelligence have become commoditized and easily accessible for most companies. This is a good thing, and especially beneficial to companies on the smaller side. Tech giants are filing patents on hardware and software that make it faster and more economical to crush Big Data and incorporate AI. This is useful to ultra-big enterprises. But what about everyone in-between? If you review the pains listed in this article, you‚Äôll notice a lot of them relate to the  process  of using data to make decisions. Collection, cleaning and integration play a (very big) part in the early stages of growth, but process, people and business unit integration play an equally big part in later stages. While most of the data-industry is focusing on the sexy things like AI and Big Data, the truth is most companies will have problems with the  process  of making decisions with the data they  currently  have long before they are ready for AI and Big Data. Many large enterprises never truly make the leap between  Data Proficient  to  Data Savvy , with leadership choosing instead to hire Data Scientists and invest in sexier technologies. This presents a huge opportunity for Scaleups. When you think about patents you think about technology as a competitive advantage. Not many people think about  process , which is as equally valid a thing to patent. And while I‚Äôm not saying you need to run out and file a patent on how you make decisions with data, what I  am  saying is that if you can nail a process that allows you to be nimble with your data early in your growth curve then you have a huge advantage over larger companies that didn‚Äôt. A Scaleup‚Äôs major competitive advantage is (still) having the ability to jump into a meeting room, pull up some numbers and make decisions. If a company can retain that agility throughout its growth it can continue to run circles around its larger rivals. The net effect is reducing the lag between the Growth Curve and the Data Maturity Curve and, ultimately, the data pains associated with growth. In future we‚Äôll dive deeper into  how  this can be done. Key Takeaways This story is published in  Noteworthy , where thousands come every day to learn about the people & ideas shaping the products we love. Follow our publication to see more product & design stories featured by the  Journal  team."
A 22-week curriculum to learn Data Analytics in 2022 + FREE Resources, 22-week curriculum to learn Data Analytics in 2022 + FREE Resource,"It‚Äôs always day one in tech! As the entire world countdowns to welcome the new year with hope and aspirations, I sit in front of my laptop making a roadmap to upskill and get job ready in 2022. In this roadmap each week discusses a different topic and has roughly 4‚Äì5 hours of content so you can spend the rest of the time building things and not get stuck in tutorial hell. Always remember tech is NOT a pen paper industry, get your hands dirty, add those projects and build a portfolio. SQL is the meat and potatoes of data analytics and is used in almost any job in the data industry. A great tutorial for the same is - mode.com Any one of the data visualization tool is a must have. You can choose any but I prefer Tableau because of it‚Äôs simplicity. Excel is a very important tool for Data Analysts. Be sure to take it all in with VBA, macros and learn while building. Statistics is vast and immensely useful when it comes to data and it‚Äôs better to know how to whip out all the nitty gritty of the data in a giphy with it. Here‚Äôs a free tutorial by Udacity: classroom.udacity.com Python is a helpful tool to automate things and one can do almost everything ( visualization , data cleaning , wrangling , modelling ) with it‚Äôs vast set of libraries. Here is a free tutorial by Udemy : https://www.udemy.com/course/master-data-analysis-with-python-intro-to-pandas/ Although you need just one data viz tool, I consider the basics of one more a plus. Google Analytics is a great tool for data collection and analysis, and while it is not a must I consider it a win win. Google has a free course and certification for it . analytics.google.com This is not a must have and you can totally skip it. I read about it and found it‚Äôs a great tool to generate CRM reports a for Business Analysts, so I added it to the list. https://trailhead.salesforce.com/en/users/strailhead/trailmixes/build-your-business-analyst-career-on-salesforce A very important and useful application widely used by almost any firm. I just felt it required extra attention. Here‚Äôs a free course by Udacity - classroom.udacity.com As much as we hate DSA it is still an integral part of the hiring process for many organizations. It‚Äôs better to be safe and practice it at least in a familiar language Python. Here‚Äôs a free course by Google for it: classroom.udacity.com Now that you are familiar with all the tools it is necessary to know to get data and work with it. Internet is the biggest source of data and being able to get it out of any website puts you ahead. Numpy is one of the most important libraries , it helps you transform and utilize numerical data to your liking. Python has a lot of useful libraries. One of the essential ones for a Data Analyst is matplotlib which is used for data visualization. With Big Data emerging as the next big thing, analysts who can leverage it to gain insights would be extremely beneficial to the companies. Here‚Äôs a free SQL course on Google‚Äôs BigQuery platform: www.kaggle.com Business not just want to gain insights from existing data but also want to forecast the unknown. Machine learning comes into play here and Kaggle‚Äôs platform with it‚Äôs micro courses are the best place to get started. www.kaggle.com All of this would make you familiar with all the skills required to start a career in data. But all of this by itself is not enough, you need to practice build projects and network. Happy new year! A blog on projects dropping soon. Stay tuned!"
Architecting a Successful Modern Data Analytics Platform in the Cloud,rchitecting a Successful Modern Data Analytics Platform in the Clou,"After we discussed the concepts for  Building a Successful Modern Data Analytics Platform in the Cloud , it is time to architect it. This post will review the reference architectures for our scalable, flexible and robust design in both Amazon Web Services (AWS) and Microsoft Azure. I worked in the last 20 years with hundreds of companies to harness the ever-evolving technology tools to bring to life business ideas. Some companies were startups born to the cloud age‚Ä¶"
You Should Master Data Analytics First Before Becoming a Data Scientist,ou Should Master Data Analytics First Before Becoming a Data Scientis,"While it may seem obvious at first to state that knowing Data Analytics before learning Data Science is key, it might surprise you then how many people jump right into Data Science without the right foundation of analyzing and presenting data. There are certain benefits to having either an internship, entry-level position, or any position really in Data Analytics beforehand. It is also important to note that this form of experience can be acquired by completing online courses and specializations in Data Analytics. That being said, if you already have a formal education in Data Science, you might already be learning the foundation of Data Analytics in one course only ‚Äî most likely, which is why it is essential to add a few Data Analytics-focused learnings into your portfolio. However, the best way is to have some sort of Data Analytics practiced with other people as you will see below when I discuss the top four benefits of mastering Data Analytics before learning Data Science. As you specialize in Data Analytics, it is no surprise that you would become efficient at exploring data. As a Data Scientist, this is usually the first step of the Data Science process, so if you skip practicing this step, your model could result in error, confusion, and misleading results. You must keep in mind that garbage in creates garbage out. Just because you throw a dataset at a Machine Learning algorithm does not mean it will answer the business question at hand. You will have to find anomalies in the data, aggregations, missing values, transformations, preprocessing, and much more. Understanding the data first is of course important so being a master at Data Analysis is crucial. There are a few Python ( and R as wel l) libraries that help do this automatically. However, I often find, with large datasets that they take way too long and can cause your kernel to crash and you have to restart. That is why it is important to have a manual eye at the data too. That being said, there is a large dataset mode for the library that I will present below that can skip some of the expensive and longer-lasting computations. The parameter for this situation is within the profile report of the Pandas Profiling library:  minimal=True . Here is one particular library that is plenty easy to use: Pandas profiling  [3], can be viewed in your Jupyter Notebook. Some of the unique features of this library include, but are not limited to type inference, unique values, missing values, descriptive statistics, frequent values, histograms, text analysis, and file as well as image analysis. Other than this library, overall, there are countless ways to practice exploratory data analysis, so if you have not already, find a course and master analyzing data. Data Scientists can often learn complex Machine Learning algorithms pretty quickly in their education, skipping the important part of communicating with stakeholders to achieve a goal and articulate the Data Science process. If you have not noticed already, you will have to become a master at translating a business use case into a Data Science model. A Product Manager or other stakeholder will not come up to you and ask you to create a supervised Machine Learning algorithm with 80% accuracy. What they will do is tell you about some data, and what problem they keep seeing, you will have little guidance on Data Science, which of course is expected, because that is your job. You will have to come up with the idea of regression, classification, clustering, boosting, bagging, etc. You will have to work with them as well in order to set up success criteria ‚Äî for example, what does 100 RMSE mean ‚Äî and how can you address and translate it to meaningful business problems to stakeholders. So, how can you learn collaboration? Working as a Data Analyst beforehand often requires plenty of collaboration more often than that of a Data Scientist. You will create metrics, make visualizations, and develop analytical insights from working with others almost daily or at least weekly as a Data Analyst. This practice is vital in becoming a better Data Scientist as we have learned from above. Benefits of stakeholder collaboration practice through Data Analytics roles: As you can see collaborating with stakeholders is an important part of both the Data Analyst and Data Scientist positions. As a Data Scientist, you will have to perform feature engineering, where you will isolate key features that contribute to the prediction of your model. In school or wherever you learned Data Science, you may have a perfect dataset that is already made for you, but in the real world, you will have to use SQL to query your database to start finding the necessary data. In addition to the columns that you already have in your tables, you will have to make new ones ‚Äî usually, these are new features that can be aggregated metrics like  clicks per user , for example. As a Data Analyst, you will practice SQL the most, and as a Data Scientist, it can be frustrating if all you know is Python or R ‚Äî and you can not rely on Pandas all the time, and as a result, you cannot even start the model building process without knowing how to efficiently query your database. Similarly, the focus on analytics can allow you to practice creating subqueries and metrics like the one stated above so that you can add a few to at least, say 100, new features that are completely created from you that could be more important than the base data that you have now. Benefits of feature creation: A Data Analyst usually will master visualizations because they have to present findings in a way that is easily digestible for others in the company. Having a complex table full of values can be confusing and frustrating to read, so having the ability to highlight important metrics, insights, and results is extremely beneficial to know as a Data Scientist, too. Similarly, when you are finished with your complex Machine Learning algorithm that you have utilized to build your final model, you will be excited to share your results; however, stakeholders will need to know only the highlights and key takeaways. The best way to do this process through visualization, and here are some of the key ways to create those visualizations: Of course, there are more, but here are the ones I often see used the most. By articulating insights and results through visualizations, you also help yourself to learn the process and takeaways better. So the question is, should you become a Data Analyst first before becoming a Data Scientist?  I say yes ‚Äî or at least some form of it, whether that be an internship, job, a similar job like that of a Business Analyst, or becoming certified in a Data Analytics course. In addition to the four benefits that I have discussed above, another one to highlight is that it could certainly help you to land a job as a Data Scientist if you have the title or experience of Data Analytics on your resume. To summarize, here are some of the important benefits to becoming a master in Data Analytics first before becoming a Data Scientist: I hope you found my article both interesting and useful. Please feel free to comment down below if you have become a Data Analyst first in some way before becoming a Data Scientist. Has it helped you in your Data Science career now? Do you agree or disagree, and why? Please feel free to check out my profile and other articles, as well as reach out to me on LinkedIn. [1] Photo by  NEW DATA SERVICES  on  Unsplash , (2018) [2] Photo by  Lukas Blazek  on  Unsplash , (2017) [3] Pandas,  Pandas Profiling , (2021) [4] Photo by  DocuSign  on  Unsplash , (2021) [5] Photo by  Myriam Jessier  on  Unsplash , (2020) [6] Photo by  William Iven  on  Unsplash , (2015)"
5 Quick and Easy Data Visualizations in Python with Code, Quick and Easy Data Visualizations in Python with Cod,"Want to be inspired? Come join my  Super Quotes newsletter . üòé Data Visualization is a big part of a data scientist‚Äôs jobs. In the early stages of a project, you‚Äôll often be doing an Exploratory Data Analysis (EDA) to gain some insights into your data. Creating visualizations really helps make things clearer and easier to understand, especially with larger, high dimensional datasets. Towards the end of your project, it‚Äôs important to be able to present your final results in a clear, concise, and compelling manner that your audience, whom are often non-technical clients, can understand. Matplotlib is a popular Python library that can be used to create your Data Visualizations quite easily. However, setting up the data, parameters, figures, and plotting can get quite messy and tedious to do every time you do a new project. In this blog post, we‚Äôre going to look at 5 data visualizations and write some quick and easy functions for them with Python‚Äôs Matplotlib. Just before we jump in, check out the  AI Smart Newsletter   to read the latest and greatest on AI, Machine Learning, and Data Science! Scatter plots are great for showing the relationship between two variables since you can directly see the raw distribution of the data. You can also view this relationship for different groups of data simple by colour coding the groups as seen in the first figure below. Want to visualise the relationship between three variables? No problemo! Just use another parameters, like point size, to encode that third variable as we can see in the second figure below. All of these points we just discussed also line right up with the first chart. Now for the code. We first import Matplotlib‚Äôs pyplot with the alias ‚Äúplt‚Äù. To create a new plot figure we call  plt.subplots()  . We pass the x-axis and y-axis data to the function and then pass those to  ax.scatter()  to plot the scatter plot. We can also set the point size, point color, and alpha transparency. You can even set the y-axis to have a logarithmic scale. The title and axis labels are then set specifically for the figure. That‚Äôs an easy to use function that creates a scatter plot end to end! Line plots are best used when you can clearly see that one variable varies greatly with another i.e they have a high covariance. Lets take a look at the figure below to illustrate. We can clearly see that there is a large amount of variation in the percentages over time for all majors. Plotting these with a scatter plot would be extremely cluttered and quite messy, making it hard to really understand and see what‚Äôs going on. Line plots are perfect for this situation because they basically give us a quick summary of the covariance of the two variables (percentage and time). Again, we can also use grouping by colour encoding. Line charts fall into the ‚Äúover-time‚Äù category from our first chart. Here‚Äôs the code for the line plot. It‚Äôs quite similar to the scatter above. with just some minor variations in variables. Histograms are useful for viewing (or really discovering)the distribution of data points. Check out the histogram below where we plot the frequency vs IQ histogram. We can clearly see the concentration towards the center and what the median is. We can also see that it follows a Gaussian distribution. Using the bars (rather than scatter points, for example) really gives us a clearly visualization of the relative difference between the frequency of each bin. The use of bins (discretization) really helps us see the ‚Äúbigger picture‚Äù where as if we use all of the data points without discrete bins, there would probably be a lot of noise in the visualization, making it hard to see what is really going on. The code for the histogram in Matplotlib is shown below. There are two parameters to take note of. Firstly, the  n_bins  parameters controls how many discrete bins we want for our histogram. More bins will give us finer information but may also introduce noise and take us away from the bigger picture; on the other hand, less bins gives us a more ‚Äúbirds eye view‚Äù and a bigger picture of what‚Äôs going on without the finer details. Secondly, the  cumulative  parameter is a boolean which allows us to select whether our histogram is cumulative or not. This is basically selecting either the Probability Density Function (PDF) or the Cumulative Density Function (CDF). Imagine we want to compare the distribution of two variables in our data. One might think that you‚Äôd have to make two separate histograms and put them side-by-side to compare them. But, there‚Äôs actually a better way: we can overlay the histograms with varying transparency. Check out the figure below. The Uniform distribution is set to have a transparency of 0.5 so that we can see what‚Äôs behind it. This allows use to directly view the two distributions on the same figure. There are a few things to set up in code for the overlaid histograms. First, we set the horizontal range to accommodate both variable distributions. According to this range and the desired number of bins we can actually computer the width of each bin. Finally, we plot the two histograms on the same plot, with one of them being slightly more transparent. Bar plots are most effective when you are trying to visualize categorical data that has few (probably < 10) categories. If we have too many categories then the bars will be very cluttered in the figure and hard to understand. They‚Äôre nice for categorical data because you can easily see the difference between the categories based on the size of the bar (i.e magnitude); categories are also easily divided and colour coded too. There are 3 different types of bar plots we‚Äôre going to look at: regular, grouped, and stacked. Check out the code below the figures as we go along. The regular barplot is in the first figure below. In the  barplot()  function,  x_data  represents the tickers on the x-axis and  y_data  represents the bar height on the y-axis. The error bar is an extra line centered on each bar that can be drawn to show the standard deviation. Grouped bar plots allow us to compare multiple categorical variables. Check out the second bar plot below. The first variable we are comparing is how the scores vary by group (groups G1, G2, ... etc). We are also comparing the genders themselves with the colour codes. Taking a look at the code, the  y_data_list  variable is now actually a list of lists, where each sublist represents a different group. We then loop through each group, and for each group we draw the bar for each tick on the x-axis; each group is also colour coded. Stacked bar plots are great for visualizing the categorical make-up of different variables. In the stacked bar plot figure below we are comparing the server load from day-to-day. With the colour coded stacks, we can easily see and understand which servers are worked the most on each day and how the loads compare to the other servers on all days. The code for this follows the same style as the grouped bar plot. We loop through each group, except this time we draw the new bars on top of the old ones rather than beside them. We previously looked at histograms which were great for visualizing the distribution of variables. But what if we need more information than that? Perhaps we want a clearer view of the standard deviation? Perhaps the median is quite different from the mean and thus we have many outliers? What if there is so skew and many of the values are concentrated to one side? That‚Äôs where boxplots come in. Box plots give us all of the information above. The bottom and top of the solid-lined box are always the first and third quartiles (i.e 25% and 75% of the data), and the band inside the box is always the second  quartile  (the  median ). The whiskers (i.e the dashed lines with the bars on the end) extend from the box to show the range of the data. Since the box plot is drawn for each group/variable it‚Äôs quite easy to set up. The  x_data  is a list of the groups/variables. The Matplotlib function  boxplot()  makes a box plot for each column of the  y_data  or each vector in sequence  y_data ; thus each value in  x_data  corresponds to a column/vector in  y_data . All we have to set then are the aesthetics of the plot. There are your 5 quick and easy data visualisations using Matplotlib. Abstracting things into functions always makes your code easier to read and use! I hope you enjoyed this post and learned something new and useful."
15 Stunning Data Visualizations (And What You Can Learn From Them),5 Stunning Data Visualizations (And What You Can Learn From Them,"We‚Äôre drowning in data. Everyday,  2.5 quintillion bytes of data are created . This is the equivalent of 90% of the world‚Äôs information‚Äìcreated in the last two years alone. Now this is what we call ‚Äúbig data.‚Äù But where does it come from? Everywhere, from sensors and social media sites to digital images and videos. We have more data than we know what to do with, so it‚Äôs time now to organize and make sense of it all. This is where data visualization comes into the picture. In the seismic shift awaiting us, referred by some as the  Industrial Revolution of Data , we have to get better and more efficient at creating innovative data visualization that make the complex easy to understand. In the hopes of inspiring your own work, we‚Äôve compiled 15 data visualizations that will not only blow your mind, they will also give you a clearer understanding of what makes a good visualization‚Äìand what makes a bad one. The interactive piece  The Daily Routines of Famous Creative People  is a perfect example of a data visualization that combines all the necessary ingredients of an effective and engaging piece: It combines reams of data into a single page; it uses color to easily distinguish trends; it allows the viewer to get a global sense of the data; it engages users by allowing them to interact with the piece; and it is surprisingly simple to understand in a single glance. The Year in News  is a good example of how an expertly executed data visualization can reveal patterns and trends hiding beneath the surface of mountains of data. By analyzing 184.5 million Twitter mentions, Echelon Insights was able to provide a bird‚Äôs eye view of what America was talking about in 2014. This U.S. age pyramid created by the Pew Research Center is a noteworthy example of how shifts and trends over time can be effectively communicated through the use of well-executed animation. Not only does this type of data visualization pack a whole lot of information into a single visual‚Äì23 bar charts were combined into a single GIF composite‚Äìit can be easily shared on social media and embedded anywhere. With so many data visualizations out there nowadays, it can be hard to find a unique angle that hasn‚Äôt been explored already. Not the case, though, with this wonderfully imaginative series of infographics created by designer Marion Luttenberger. Using real-life images as the basis of his infographics, Luttenberger was able to craft an entire annual report for an organization that provides aid to drug addicts in Austria‚Äìand still communicate the organization‚Äôs mission clearly and effectively. An effective way to communicate complex ideas is by using symbols and metaphors. Take, for example, this ambitious data visualization of the Internet created by Ruslan Enikeev. By using the metaphor of planets in a solar system, Enikeev is able to create a ‚ÄúMap of the Internet‚Äù that helps users visualize the relative reach and influence of every site out there. The amount of website traffic, for example, is represented by the size of the circle on the map. One of the great strengths of data visualizations is their unsurpassed ability to put isolated pieces of information into a bigger context. The goal of this insightful interactive piece by Nikon is to give users a sense of the size of objects, both big and small, by using comparisons. Next to the Milky way, for example, a common object such as a ball or a car seem smaller than we ever imagined. Another hallmark of an effective data visualization is its ability to summarize a ton of information and, in the process, save you time and effort. This data visualization, for instance, represents 100 years of the evolution of rock in a single page. Not only does it simplify information for you‚Äìcondensing a century‚Äôs worth of information into a piece that can be viewed in less than a minute‚Äìit also provides actual audio samples for each genre, from electronic blues to dark metal. As humans, we cannot help but see the Universe and life from our own self-centered and completely unique point-of-view. This data visualization, however, gives us some perspective on our own lives‚Äìand the events of the current day‚Äìby placing them in the larger context of time, from the current year to the current millennium. In line with the objective of making the complex easy to understand, this infographic provides a visual representation of a coffee bean‚Äôs journey, from bean to cup. By breaking the process down into parts, this data visualization does its job of giving the reader bite-sized pieces of information that are easily digestible. A good infographic will not only do the hard work of digesting complex data, it may also stimulate readers‚Äô imagination by allowing them to conjure up different hypothetical situations and possibilities, as is done in this example. By presenting an interactive, game-like experience, this infographic quickly engages the user and keeps them interested from beginning to end. This infographic takes dense material, such as indicators and figures, and presents it in a beautiful, clean and captivating format. Not only is the design deceptively simple and functional, it also provides the user with many options for interacting with the graphic, such as adding countries, indicators and type of relation. An effective data visualization not only conveys information in a convincing manner, it also narrates a story worth telling. This piece, for example, tells the story of every known drone strike and victim in Pakistan. By distilling information into an easily understandable visual format, this infographic dramatically brings to light disturbing facts that should not go unnoticed. This data visualization not only has all of the previous qualities mentioned, it also allows the user to have direct access to all the original raw data (view link on bottom right corner). Also, by using bubbles shaped in accordance with the size of the data breach, the viewer can get a solid overview of the data breach ‚Äúlandscape.‚Äù And if viewers want to get into the details of the information, they can also go as deep or as superficially as they want by navigating the different filters and raw data. In line with the global trend of democratizing access to information and empowering users, this data visualization does an excellent job of demystifying the process of balancing the national budget. By placing budget balancing in the hands of everyday users, this project taps into the power of collective thinking to solve big problems. This piece goes beyond a common data visualization to become an educational and interactive minisite. By combining enough data and information to fill an encyclopedia into a single interactive application, this data visualization becomes a useful classroom tool for students learning about wind, ocean and weather conditions. Interactive data visualizations are unique in that they appeal to several of the five senses: to the  sense of hearing  through audio, the  sense of sight  through stunning visuals and the  sense of touch  through the interactive experience of clicking, hovering and scrolling through content. While you may think that data visualizations are too costly and time consuming to produce on your own, you can explore several free tools out there that allow non-designers and non-programmers to create their own interactive content. Visme, for example, is an online tool that allows you to create interactive charts, graphs and maps. You can try it for free  here . And if you want to receive additional tips and guides on becoming a better visual communicator, don‚Äôt forget to sign up for our weekly newsletter below. The  original version of this post  first appeared on  Visme‚Äôs Visual Learning Center ."
"Web Scraping, Regular Expressions, and Data Visualization: Doing it all in Python","eb Scraping, Regular Expressions, and Data Visualization: Doing it all in Pytho","As with most interesting projects, this one started with a simple question asked half-seriously: how much tuition do I pay for five minutes of my college president‚Äôs time? After a chance pleasant discussion with the president of my school ( CWRU ), I wondered just how much my conversation had cost me. My search led to  this article , which along with my president‚Äôs salary, had this table showing the salaries of private college presidents in Ohio: While I could have found the answer for my president, (SPOILER ALERT, it‚Äôs $48 / five minutes), and been satisfied, I wanted to take the idea further using this table. I had been looking for a chance to practice  web scraping  and  regular expressions  in Python and decided this was a great short project. Although it almost certainly would have been faster to manually enter the data in Excel, then I would not have had the invaluable opportunity to practice a few skills! Data science is about solving problems using a diverse set of tools, and web scraping and regular expressions are two areas I need some work on (not to mention that making plots is always fun). The result was a very short ‚Äî but complete ‚Äî project showing how we can bring together these three techniques to solve a data science problem. The complete code for this project is available as a  Jupyter Notebook  on  Google Colaboratory  (this is a new service I‚Äôm trying out where you can share and collaborate on Jupyter Notebooks in the cloud. It feels like the future!) To edit the notebook, open it up in Colaboratory, select file > save a copy in drive and then you can make any changes and run the Notebook. While most data used in classes and textbooks just appears ready-to-use in a clean format, in reality, the world does not play so nice. Getting data usually means getting our hands dirty, in this case pulling (also known as scraping) data from the web. Python has great tools for doing this, namely the  requests  library for retrieving content from a webpage, and  bs4  (BeautifulSoup) for extracting the relevant information. These two libraries are often used together in the following manner: first, we make a GET request to a website. Then, we create a Beautiful Soup object from the content that is returned and parse it using several methods. The resulting soup object is quite intimidating: Our data is in there somewhere, but we need to extract it. To select our table from the soup, we need to find the right  CSS selectors . One way to do this is by going to the webpage and inspecting the element. In this case, we can also just look at the soup and see that our table resides under a  <div>  HTML tag with the attribute  class = ""entry-content""  . Using this info and the  .find  method of our soup object, we can pull out the main article content. This returns another soup object which is not quite specific enough. To select the table, we need to find the  <ul>  tag (see above image). We also want to deal with only the text in the table, so we use the  .text  attribute of the soup. We now have the exact text of the table as a string, but clearly is it not of much use to us yet! To extract specific parts of a text string, we need to move on to regular expressions. I don‚Äôt have space in this article (nor do I have the experience!) to completely explain regular expressions, so here I only give a brief overview and show the results. I‚Äôm still learning myself, and I have found the only way to get better is practice. Feel free to go over  this notebook  for some practice, and check out the Python  re   documentation  to get started (documentation is usually dry but  extremely  helpful). The basic idea of regular expressions is we define a pattern (the ‚Äúregular expression‚Äù or ‚Äúregex‚Äù) that we want to match in a text string and then search in the string to return matches. Some of these patterns look pretty strange because they contain both the content we want to match and special characters that change how the pattern is interpreted. Regular expressions come up all the time when parsing string information and are a vital tool to learn at least at a basic level! There are 3 pieces of info we need to extract from the text table: First up is the name. In this regular expression, I make use of the fact that each name is at the start of a line and ends with a comma. The code below creates a regular expression pattern, and then searches through the string to find all occurrences of the pattern: Like I said, the pattern is pretty complex, but it does exactly what we want! Don‚Äôt worry about the details of the pattern, but just think about the general process: first define a pattern, and then search a string to find the pattern. We repeat the procedure with the colleges and the salary: Unfortunately the salary is in a format that no computer would understand as numbers. Fortunately, this gives us a chance to practice using a Python  list comprehension  to convert the string salaries into numbers. The following code illustrates how to use string slicing,  split  , and  join , all within a list comprehension to achieve the results we want: We apply this transformation to our salaries and finally have the all info we want. Let‚Äôs put everything into a  pandas  dataframe. At this point, I manually insert the information for my college (CWRU) because it was not in the main table. It‚Äôs important to know when it‚Äôs more efficient to do things by hand rather than writing a complicated program (although this whole article kind of goes against this point!). This project is indicative of data science because the majority of time was spent collecting and formatting the data. However, now that we have a clean dataset, we get to make some plots! We can use both  matplotlib  and  seaborn  to visualize the data. If we aren‚Äôt too concerned about aesthetics, we can use the built in dataframe plot method to quickly show results: To get a better plot we have to do some work. Plotting code in Python, like regular expressions, can be a little complex, and it takes some practice to get used to. Mostly, I learn by building on answers on sites like Stack Overflow or by reading  official documentation . After a bit of work, we get the following plot (see notebook for the details): Much better, but this still doesn‚Äôt answer my original question! To show how much students are paying for 5 minutes of their president‚Äôs time we can convert salaries into $ / five minutes assuming 2000 work hours per year. This is not necessarily a publication-worthy plot, but it‚Äôs a nice way to wrap up a small project. The most effective way to learn technical skills is by doing. While this whole project could have been done manually inserting values into Excel, I like to take the long view and think about how the skills learned here will help in the future. The process of learning is more important than the final result, and in this project we were able to see how to use 3 critical skills for data science: Now, get out there and start your own project and remember: it doesn‚Äôt have to be world-changing to be worthwhile. I welcome feedback and discussion and can be reached on Twitter  @koehrsen_will ."
D3 is not a Data Visualization Library,3 is not a Data Visualization Librar,"D3.js is an incredibly successful library yet there‚Äôs a disconnect between what people think D3 does and what it actually does. As a result, learning D3 is intimidating and confusing. By better understanding its structure and separating it into more manageable pieces, it can be easier to choose which parts of the library to learn and which parts to avoid ‚Äî key lessons not only for D3 novices but for expert users like myself that might want to reexamine how they use D3. I wrote  a book about D3 (twice) , so I‚Äôm sure you‚Äôre thinking that this is some kind of clickbait title and that I‚Äôm going to make a subtle play on words or say something like  D3 is not a Data Visualization Library it is THE Data Visualization Library . But no, along with key functionality that lets you do data visualization, D3 also consists of other functionality that is only tangentially related to data visualization. You might be surprised by that given what the D3 home page looks like. But much of D3 has little to do with graphics and many of the parts that do aren‚Äôt necessary to learn to create effective data visualization. We can take the  D3 API page  and visualize it as a hierarchy by graphically nesting the functions into the sections and subsections described in the documentation. Here‚Äôs how  d3-selection  looks using this method: Along with doing this for the entire docs, I grouped the functionality into a few broad semantic categories. Obviously, this method of visualizing the API doesn‚Äôt account for true complexity because some sections have many small nearly duplicate functions whereas others only have a few complex functions but it gives a reasonable graphical overview. The size and complexity of the library has always made it difficult to teach and almost every lesson and book (mine included) focuses first on establishing the JQuery part of the library, which is all about DOM manipulation to create and manipulate elements on a web page. But take a look at that diagram above. If you want to learn how to use D3 for data visualization you don‚Äôt need to learn anything on the right hand side and you almost certainly don‚Äôt need to learn everything on the left. In fact, if you do, you might be setting yourself up for a worse chance at success in the long run. I‚Äôll explain as I walk through the areas of the D3 API. So much of what you read in D3 tutorials focuses on its DOM manipulation functionality. This includes the  select/enter/exit/update  pattern you‚Äôve probably seen a thousand times but also convenience functions for dragging, zooming, dispatching events and even using Fetch (the D3 flavor of which is unsurprisingly called  d3-fetch ). There are useful and interesting functions (like  d3-zoom ) along with particular D3 flavors of existing ES2015 functionality if you prefer the D3 way of writing code. But this whole section is unnecessary if you‚Äôre using something like Vue to create your DOM elements and actively conflicts with other forms of DOM management necessitating hacks or mixed systems. If you‚Äôre  working in a team environment , it‚Äôs far more likely the rest of your team will be more familiar with other methods of DOM management than D3, so there are good reasons to avoid this aspect of D3 altogether. It stands to reason why this is so tied up with data visualization: You can‚Äôt visualize things unless they actually exist (in the DOM) but because it is so prominent in the API it comes across to new audiences as if D3 can only be used if you use D3 to manipulate the DOM. It also makes sense given that when D3 was originally being developed (v3 was released in 2013). There‚Äôs another reason for selections and that‚Äôs tied to animation, which I‚Äôll get into below. There‚Äôs another significant portion of the library that exposes a host of functions that are a part of the data transformation, cleaning and formatting process. Some of them, like the  min  and  max  functions in  d3-array  are syntactic sugar for operations that can be handled with vanilla JavaScript. Others, like  set  and  map  are D3 flavors that aren‚Äôt quite the same as ES2015  Set  and  Map . The formatting utilities are there if you like python style numerical formatting, something I find less intuitive than  numeral , as well as time formatting, which in all my experience with D3 I‚Äôve never used preferring  moment  or (before Moment and now more recently) vanilla JavaScript time formatting options. There are, additionally, some interpolation functions that are used internally to power the scales and color functionality, as well as a host of random number generators. You need to know how to measure and format data to do data visualization but there are many ways to do that, often times in the process leading up to creating the dataset. There are also other libraries that might be more commonly used in industry or native ES2015 functions that larger teams will have a better chance of understanding than D3 particular functions. d3-transition  is one of the most convenient ways to animate graphical elements on the web and is intimately tied to  d3-selection . But I‚Äôve found that animation in data visualization, while impressive and eye-catching, is a much lower priority than understanding how different chart types present data, how to properly design charts, understanding form in data visualization, and other aspects of data visualization practice. Given that you need to use D3 selections to get access to animation via  d3-transition  it‚Äôs a heavy investment that might be better spent on leveling up in other ways. Another aspect of animation that makes it optional as far as learning to do it with D3 is that there are other libraries that do good animation, like  GSAP , as well as animation solutions that are native to the method you (or your team) are using for DOM management. We‚Äôre getting closer to data visualization here. Parsing data like CSVs and its various flavors (tab-delimited, comma-delimited) is an important part of data visualization and I haven‚Äôt seen a library that does it so easily as  d3-dsv . But much of my production data comes processed and available as JavaScript data structures and doesn‚Äôt require processing CSVs.  d3-quadtree  is an amazingly fun and useful library for spatial search but its direct application is pretty rare in the real world (under the hood quadtrees are used for things like the network diagrams built with  d3-force ). Binning and nesting functions from  d3-collection  and  d3-array  also fall into this category. The real meat of D3 for data visualization is in its functions for decorating data with drawing instructions, creating new drawable data from source data, generating SVG paths and creating data visualization elements (like an axis) in the DOM from your data and methods. It‚Äôs useful to separate the graphical functionality of D3 into  generators ,  components  and  layouts . Since layouts and generators don‚Äôt produce DOM elements but only raw material for creating DOM elements, you still need to pass the data they create to the DOM,  a process I describe using React in an earlier article . Understanding not only how to use layouts and generators but also how they work is key to understanding data visualization. Because most complex data visualization is combinatorial, understanding how these different functions can interact to produce a different kind of chart is key to understanding how to design data visualization well. It‚Äôs unfortunate that people learning D3 spend so much time learning its DOM management functionality and have only a superficial understanding of the actual data visualization functionality. d3-shape  has a bunch of really valuable functions that draw SVG paths from arrays of data that are useful in themselves and as models of how you can build your own generators. Its built-in canvas rendering functionality is nice but there‚Äôs an existing solution for that in vanilla JavaScript: Path2D . Besides the graphical functions,  scales  and  interpolators  don‚Äôt create graphics but are key to projecting graphics into visual space (whether within the boundaries of a chart or with the color, stroke-width or other channels used to communicate visually). Scales don‚Äôt just interpolate numbers to numbers but transform from one mode to another (such as with quantizing scales) and expose convenience functions like calculating ticks suitable for axes. d3-hierarchy  has hierarchical layouts (like dendrograms, treemaps and sunbursts) but also a hierarchical data structure that provides convenient ways to slice up and analyze hierarchical data. d3-force  has a simple constraint-based force-directed layout that is effective and generic enough to be used in most network visualization. d3-color  and  d3-color-schemes  are not the only way to deal with color and if you‚Äôre just starting out, I‚Äôd suggest working with  chroma.js  rather than D3‚Äôs color utilities. The axis and brush functions in D3 suffer from the same problem that  d3-selection  does in that they reach into the DOM and create elements themselves. That means they‚Äôre harder to integrate into an application that‚Äôs using Vue or React to manage the DOM. Some functionality, like the canvas-to-SVG of  d3-path , can be useful in some corner cases, but the real value of learning D3 comes from learning the different visualization methods, like hulls, contours, voronoi polygons and chord diagrams. Understanding how they take different forms of data and derive drawing instructions can help you to understand how data can be transformed for your applications and how those transformations interplay. While definitely a part of the data visualization area of D3,  d3-geo  is likely too specific and not as easily integrated across different design cases as the broader data visualization pieces. Its popularity is well justified as D3 has become something of a playground for neogeographers experimenting with cartograms, raster reprojection and other geographic geekery. It consists of a million projections, an entire projection streaming system, functions for translating GeoJSON to paths, finding centroids and bounding boxes (use  d3-polygon  if you want this for generic geometries). A lot of spherical math and spherical shapes and steradians. D3 lets you make simple choropleth maps easily but with a high learning curve if you‚Äôre not a GIS professional or a map nerd. Given the innovations happening in the WebGL mapping space, if you‚Äôre looking to make maps you might want to first explore  kepler.gl  or Mapbox. People have a hard time learning D3. If you‚Äôre expected to learn the DOM manipulation part first it can be a barrier especially when you‚Äôre working on a project where that‚Äôs already being handled. The point of learning D3 is to learn how to create data visualization products, and that‚Äôs really just part of the library that you can focus on instead of the supplemental functionality. It limits the combinatorial quality of D3 if people think of it as a self-contained ecosystem. Data visualization and charts are about mapping data attributes to visual features. That should be the focus of anyone trying to learn a library that does data visualization, not DOM management. There are lots of tools for managing the DOM and all of them can integrate the data visualization functions in D3, which will only increase the amount of sophisticated data visualization being done. You might find using D3 for everything to be particularly suitable for your practice. That‚Äôs great! You should buy my book, since it explains how to use all those bits. But I‚Äôve found that this is often not the case with people coming to learn D3 and as a result the data visualization community loses out on contributions from developers who really just wanted to do data visualization. For them, I hope I‚Äôve done a decent job of defining where they should look and what they should focus on when it comes to learning D3. For those who have already mastered D3, I hope this has helped describe just how complex and eclectic and opinionated the library can appear to outsiders (And I didn‚Äôt even get into all the function chaining‚Ä¶). Let‚Äôs not assume that the way we learned or use D3 is the only way to do it, even if that means evolving our practice."
The Architecture of,he Architecture o,"Multilayered Storytelling through ‚ÄúInfo-spatial‚Äù Compositions Information Design  is playing an increasingly critical role in everyday journalism. The movement from word and picture to ‚Äúwords within diagrams‚Äù is building a new form of truth-telling and storytelling ‚Äî and with it, a new journalistic aesthetic. At  Accurat  we‚Äôve been working side-by-side with the newsroom of  Corriere della Sera  for more than 2 years, designing a series of of exploratory data-visualizations originally published for  La Lettura , their Sunday cultural supplement. We aim to deliver rich visual narratives able to maintain the complexity of the data while making this complexity more accessible and understandable. Simply put, we publish compound and complex stories told through data visualizations. Within this collaboration we always aimed at  opening new perspectives in the newspaper-editorial field , and higher aim at educating readers‚Äô eyes to  get familiar with new visual ways  to convey  the richness of the data stories we are telling  rather than simplifying them. This article will elucidate and share our  design method , based upon layering multiple sub-narratives over a main construct, through a  dissection of the spatial build-up of the visualizations ; it draws from different contributions I published on the   Parsons Journal for Information Mapping,  on the recently published  ‚ÄúNew challenges for data design: articles & interviews‚Äù  Springer book, and by talks I delivered at the  Eyeo Festival ,   Resonate Festival  and  Visualized Conference . La Lettura  is the Sunday cultural supplement of Corriere della Sera, the highest circulation newspaper in Italy. The supplement is conceived as a  collection of long-form articles  about cultural and sociological phenomena, new media and communication related topics. The aim of the issue is to provide readers with  a product they can read throughout their week,  with deep essays usually written by sociologists, professional writers, art and literature critics, historians, philosopher or modern thinkers. We here try to  push the boundaries of what visualization can do  on data with multiple attributes and a high density: we are not going for simple diagrams to express basic concepts,  we somehow instead embrace complexity . We intend to deliver rich visual narratives able to maintain the nuances of the data but still making all this different aspects and flavors more accessible and understandable. For each story we consider and pursue a  topic  we believe may be of particular interest to explore, ranging from current affairs to historical or cultural issues. Sometimes choices are driven by a fascination we have, sometimes by a compelling dataset we find and we would start from, other times we choose to present events and topics that are hot at the moment. We then  analyze and compare different kinds of datasets  trying to identify and reveal a central story, hopefully a not-so-expected one. We start from a question or an intuition we have and work from here , then try to  put the information in context  and find some further facts and materials to potentially correlate. Every time we aim at moving away from mere quantity in order to pursue a qualitative transformation of raw statistical material into something that will provide new knowledge: unexpected parallels, not common correlation or  secondary tales, to enrich the main story with . In this respect our work here cannot be considered data-visualization in the pure sense: we are not just providing insight into numbers but into social issues or other qualitative aspects as well. In addition, since we publish on a full-spread format within the cultural Sunday supplement of the highest circulation Italian newspaper,  the leading narrative and the visual ways through which we display information have to be both catchy and attractive : once the first attention of the audience is ‚Äúcaught‚Äù by the aesthetic features of the image, the presentation of the information must be clear as might be expected. The clarity does not need to come all at once, however; we also like the idea of  providing several and consequent layers of exploration on the multiple dataset we analyze . We call it a  ‚Äúnon-linear storytelling‚Äù  where people can get lost in singular elements, minor tales, and last-mile textual elements within the greater visualization. To achieve this  multi-layered storytelling  with data even when visualizations are static and printed,  everything depends on the concept of layering, establishing hierarchies and making them clear : this is the case for both the data analysis (the stories we desire to tell), and the visual composition (i.e. the main architecture and the aesthetic value we desire to present), inviting readers to ‚Äúget lost‚Äù within the narrative(s) and engage at deeper levels. Our design method, based upon layering multiple sub-narratives over a main construct prescribes this specific phenomenon. The following is a dissection of how we build our pieces , where the editorial process of selecting, analyzing, comparing, building hierarchies, etc., is in direct conjunction to the visual development of the layers. 1. Composing the main architecture of the visualization Composing the main architecture: this acts as the formalized base through which the main story will be mapped and displayed, upon this, one will see the most relevant patterns emerging from the story: the essential ‚Äúmap‚Äù that conceptually identifies where we are. This base is essentially a matrix or pattern that will serve as our organizer. It may be composed of cells, or distances, or other interrelated multiples. 2. Positioning singular elements within the main framework. This process will test the effectiveness of the main architecture; the placement of elements reveals or confirms weaknesses and strengths, which may lead to modification of the main architecture. 3. Constructing shaped elements of dimensionality and form Constructing shaped elements of dimensionality and form (essentially polygons) with quantitative and qualitative parameters and positioning these within the main architecture. As these elements have form they must also be identified through colors according to opportunities to establish categorizations, thus advancing clarity and relationships that serve to enhance the story. 4. Elucidating internal relationships between elements (if any) These links, directives, and qualifiers serve to give the story a comprehensive texture and correlate dependencies within the story. 5. Labeling and identifying Through the addition of explanatory labels and short texts we provide requisite last mile clarity throughout the presentation. 6. Supplementing the greater story through the addition of minor or tangential tales elements. We consider this a very important step to contextualize the phenomena in a wider world. These components link the story to external ideas, other times, or other places. Elements that are rendered here may come from very diverse sources ‚Äî analysis that is undertaken once we have strongly established the core story. These elements, which may take the form of small images, textual components, graphic symbols, etc., are to be located where they best help to enrich the overall comprehension: they must not distract from the main story. 7. Providing small visual explanations such as a legend or key that assists readers and the general public who may not be familiar with norms of data visualization. These are composed to enlighten the layered idea of the visualization, often constructed as miniatures of the layers themselves. The process usually involves simplification of the general architecture (e.g. the x and y axes, base timelines, or map components) as well as minimal explicit shapes, colors, and dimensions of singular elements. These explanations also provide units of measurement for distances and volumes, etc. 8. Fine-tuning and stylizing of elements‚Äô shapes, colors, and weights to make hierarchies pop out. By visually highlighting the most relevant elements and lightening the other background layers of information, we should be able to allow information to be selectively and sequentially revealed, helping readers discover stories by themselves and recognizing the patterns or interrelationships from one element within the story to another. The final fine-tuning of the piece is the  necessary effort required to please readers‚Äô eyes: a well-balanced image where negative space and light elements play their role aesthetically. Is the process always so linear? Obviously, the answer is no. It is  a constant iteration of explorations and a mixture of different approaches  we can start to resolve the design problem with; with the constant goal in mind to allow people understand the stories, or, better said:  see the stories. Whenever the main purpose of visualizations is to open readers‚Äô eyes to  new knowledge  and to  reveal something new about the world , or to engage and entertain the audience about a topic,  it‚Äôs impractical to avoid a certain level of visual complexity indeed. The world is complex, compound, rich in information that can be combined in endless ways, therefore catching new points of view or discovering something that you didn‚Äôt know before often cannot happen at a glance: this process of ‚Äúrevelation‚Äù often needs and require an  in-depth investigation of the context. Consequently, we like to think at these kind of data visualizations we presented as visual ways to convey the  richness , the involvement and feelings (being engagement or concern) that we experience in our everyday lives rather than simplifications of the world. One of the important challenges for data-visualization design nowadays is to experiment on and find  proper ways to express the data complexity,  and more broadly the complexity and the multiple possible interpretations and contextualization of any phenomena in the contemporary world; which in opposition to reductions  require the comprehension of the relations between the whole and the parts, at any time . And, as in the physical world,  aesthetics  plays an important role in shaping people‚Äôs reactions and responses to any products, acting as the bridge between it and people‚Äôs emotion and feeling. Since the goal here is neither to visualize data for rapid decision making processes, nor are we representing information for scientific purposes, the  opportunity to experiment with new visual metaphors is wide open, and the exploratory nature of this work is clear. We can, every time, try to push forward how we can ‚Äúcompose‚Äù data visualizations that achieve (in our idea) aesthetic beauty and elegance through new visual metaphors, intentionally avoiding typical and already tested styles of representation. To us, elegance is not only beauty and prettiness, our intention is to make things not just understandable but also  appealing , conveying the information in an effective way but also  catching the attention  (the eyes!) of the particular audience we are creating it for, as a trigger to their curiosity, and to the willingness to explore the piece. Many times standard visual models like bar charts, scatter plots, regular timelines and maps are the best way to convey data and messages indeed. To us, that doesn‚Äôt mean they are ‚Äúan end‚Äù , though. We here simply believe that keeping on exploring the realm of possibilities in the representation of information, could lead up to  refining and perfecting the core of this ‚Äúscience‚Äù, of this field,  even passing through failures and mistakes. We are moving fast towards  infinite possibilities for data analysis and display,  with theoretical models, abstract formal languages, and open knowledge developed in other disciplines more than available to get inspirations from. Experimental visualizations design should always aim at  balancing conventions and familiar forms people are comfortable with, and novelty:  truly imaginative visuals able to attract individuals into the exploration, able  to transform the strange of any visual experiment we include into the known , and ultimately able to invite readers to explore the richness of the stories lying behind. With  this body of work  we try to test and to understand  what works and what doesn‚Äôt , what people like and what they don‚Äôt, and how we can even think of educating readers‚Äô eyes to some new visual metaphors and models. Drawing the parallel with arts such as painting and music through centuries we know how much these disciplines have been able to constantly re-invent themselves even when a reinvention was not strictly necessary, opening new worlds and possibilities.  The interesting question to us here is:   how far we can go? Accurat  is a data-driven research, design and innovation firm. We help our clients understand, communicate and leverage their data assets through static and interactive data visualizations, interfaces and analytical tools that provide comprehension, insight and engagement. We have offices in Milan and New York. A very special and sincere thank to Professor  William Bevington  who made me think critically about what I was doing and about my mental process, and who knew it all well before me, helping me shaping all this and putting it down on paper."
20 ideas for better data visualization,0 ideas for better data visualizatio,"Applications we design are becoming increasingly data-driven. The need for quality data visualization is high as ever. Confusing and misleading graphics are all around us, but we can change this by following these simple rules. Choosing the wrong chart type, or defaulting to the most common type of data visualization could confuse users or lead to data misinterpretation. The same data set can be represented in many ways, depending on what users would like to see. Always start with a review of your data set and user interview. You can learn more on how to pick the right representation for your data, and how to design effective dashboards in my article about  Dashboard design . When using horizontal bars, plot  negatives values on the left side  and  positive on the right side  of a baseline. Do not plot negative and positive values on the same side of the baseline. Truncation leads to misrepresentation. On the example below, looking at the chart on the left, you can quickly conclude that value B is more than 3 times greater than D when in reality the difference is far more marginal. Starting at zero baseline ensures that users get a much more accurate representation of data. For line charts always limiting the y-axis scale to start at zero may render the chart almost flat. As the main goal for a line chart is to represent the trend, it's important to adapt the scale based on the data set for a given period and keep the line occupying two-thirds of the y-axis range. The line chart is composed of ‚Äúmarkers‚Äù that are connected by lines, often used to visualize a trend in data over intervals of time ‚Äî a time series. This helps to illustrate how values change over time and works really well with short time intervals, but when data updates infrequently this may cause confusion. Ex. Using a line chart to represent yearly revenue, if values are updated monthly will open the chart to interpretation. Users may assume the lines connecting the ‚Äúmarkers‚Äù are representing actual values when in reality true revenue numbers at that specific time are unknown.  In such scenarios using a vertical bar chart can be a better option. Smoothed line charts may be visually pleasing but they misrepresent the actual data behind them, also excessively thick lines obscure the real ‚Äúmarkers‚Äù positions. Often, to save space for your visualization, you may be inclined to use dual-axis charts when there are two data series with the same measure, but different magnitudes. Not only are those charts hard to read, but they also represent a comparison between 2 data series in completely misleading way. Most users will not pay close attention to the scales and just scan the chart, drawing wrong conclusions. A pie chart is one of the most popular and often misused charts. In most cases, a bar chart is a much better option. But if you decided on a pie chart here are a few recommendations on how to make it work: Without proper labeling, no matter how nice is your graph ‚Äî it won‚Äôt make sense. Labeling directly on the chart is super helpful for all viewers. Consulting the legend requires time and mental energy to link the values and corresponding segments. Putting the values on top of slices may cause multiple problems, from readability issues to challenges with thin slices. Instead, add black labels with clear links to each segment. There are several ways commonly accepted in ordering pie slices: The same recommendation holds true for many other charts. Do not default to alphabetical sorting. Place the largest values on top (for horizontal bar charts) or left (for vertical bar charts) to ensure the most important values take the most prominent space, reducing the eye movements, and time required to read a chart. A pie chart in general is not the easiest chart to read, as it's very hard to compare similar values. When we take the middle out and create a donut chart, we free as space to display additional information but sacrifice clarity, taken to extremes it renders the chart useless. Unnecessary styling is not only distracting, it may cause misinterpretation of the data and users making false impressions. You should avoid: Color is an integral part of effective data visualization, consider those 3 color palette types when designing: A  Qualitative  color palette works best for the display of categorical variables. Colors assigned should be distinct to ensure accessibility. A  Sequential  color palette works best for numeric variables that need to be placed in a specific order. Using hue or lightness or a combination of both, you create a continuous color set. A  Divergent  color palette is a combination of two sequential palettes with a central value in the middle(usually zero). Often divergent color palettes will communicate positive and negative values. Make sure color also matches the notion of ‚Äúnegative‚Äù and ‚Äúpositive‚Äù performance. Check out a handy tool ‚Äî  ColorBrewer  that can help you generate various color palettes. According to the National Eye Institute, about 1 in 12 humans are color blind. Your charts are only successful if they are accessible to a broad audience. Make sure typography is communicating information and helping users focus on data, rather than distracting from it. This simple trick will ensure users will be able to scan the chart much more effectively, without straining their neck) If your task is to add interactive charts to web and mobile projects, one of the first questions you should ask is what charting library will we use? Modern charting libraries have many of the previously mentioned interactions and rules baked in. Designing based on a defined library will ensure ease of implementation and will give you a ton of interaction ideas. Help users explore by changing parameters, visualization type, timeline. Draw conclusions to maximize value and insight. In the example below, you can see the IOS Health app using a combination of various kinds of data presentation to its benefit. For all who would like to learn more about this topic, I highly recommend reading  ‚ÄúThe Wall Street Journal Guide to Information Graphics: The Dos and Don‚Äôts of Presenting Data, Facts, and Figures‚Äù by Dona M. Wong . Many of the ideas in this article are inspired by this book."
11 Javascript Data Visualization Libraries for 2019,1 Javascript Data Visualization Libraries for 201,"We live in an era of data explosion, when nearly every application we develop uses or leverages data to improve the experience delivered to the users. Sometimes, the best feature we can deliver to our users is the data itself. However, table and number charts are often tiring to read and it can be hard to draw actionable insights from large data tables. Instead, we can use neat data visualization methods that leverage the brain‚Äôs ability to identify and process data in a visual way. To help you get started and easily add beautiful data visualization to your favorite application, here are some of the best Javascript data visualization libraries around in 2019 (unranked). Feel free to comment and add your own suggestions and insights! Tip : Use  Bit  to quickly reuse UI components between your apps. Empower your team with a cloud-library to speed your app development. It‚Äôs free. https://bit.dev At 80k stars D3.js is probably the most popular and extensive Javascript data visualization library out there. D3 is built for manipulating documents based on data and bring data to life using HTML, SVG, and CSS. D3‚Äôs emphasis on web standards gives you the capabilities of modern browsers without coupling to a proprietary framework, combining visualization components and a data-driven approach to DOM manipulation. It allows you to bind arbitrary data to a Document Object Model (DOM), and then apply data-driven transformations to the document. Here‚Äôs a great  example gallery . Note: some say D3  isn‚Äôt a data visualization library  at all‚Ä¶ :) An extremely popular (40k stars) library of open source HTML 5 charts for responsive web applications using the canvas element. V.2 provides mixed chart-types, new chart axis types, and beautiful animations. Designs are simple and elegant with 8 basic chart types, and you can combine the library with  moment.js  for time axis. You can also check out the library  on cdnjs . This incredibly popular library (45K stars; 1K contributors) in built for creating 3d animations using WebGL. The project‚Äôs flexible and abstract nature means it‚Äôs also useful for visualizing data in  2 or 3  dimensions. For example You can also use  this designated module  for 3D graph visualization with WebGL, or try  this online playground . Interesting choice to consider. github.com Baidu‚Äôs Echarts project (30k stars) is an interactive charting and visualization library for the browser. It‚Äôs written in pure JavaScript and is based on the  zrender  canvas library. It supports rendering charts in the form of Canvas, SVG (4.0+), and VML In addition to PC and mobile browsers, ECharts can also be used with node-canvas on node for efficient server-side rendering (SSR). Here‚Äôs a link to the full  example gallery , where each example can be played with (and themed) in an interactive playground. github.com Highcharts JS is a 8K stars and widely popular JavaScript charting library based on SVG, with fallbacks to VML and canvas for old browsers. It claims to eb used by 72 out of the world‚Äôs 100 largest companies, which makes it (probably) the most popular JS charting API in the world (Facebook, Twitter). github.com MetricsGraphics.js (7k stars) is a library optimized for visualizing and laying out time-series data. It‚Äôs relatively small (80kb minified), and provides a narrow yet elegant selection of line charts, scatterplots, histograms, bar charts and data tables, as well as features like rug plots and basic linear regression. Here‚Äôs a link to an  interactive example gallery . github.com Recharts is a chart library build with React and D3 that lets you deploy as declarative React components. The library provides native SVG support, lightweight dependency tree (D3 submodules) is highly customizable via component props. You can find live examples in the docs website. github.com A 10k stars Javascript ‚Äúvector library‚Äù for working with vector graphics in the web. The library uses the SVG W3C Recommendation and VML as a base for creating graphics, so every graphical object is also a DOM object and you can attach JavaScript event handlers. Rapha√´l currently supports Firefox 3.0+, Safari 3.0+, Chrome 5.0+, Opera 9.5+ and Internet Explorer 6.0+. github.com At 8k stars C3 is a D3-based reusable chart library for web applications. The library provides classes to every element so you can define a custom style by the class and extend the structure directly by D3. It also provides a variety of APIs and callbacks to access the state of the chart. By using them, you can update the chart even after it‚Äôs rendered. Take a look at  these examples . github.com React-vis  (4k stars) is Uber‚Äôs set of React components for visualizing data in a consistent way, including line/area/bar charts, heat maps, scatterplots, contour plots, hexagon heatmaps and much more. The library does not require any previous knowledge with D3 or any other data-vis library, and provides low-level modular building-block components such as X/Y axis. A great match for working with  Bit   and a very useful library to consider. github.com React virtualized  (12k stars) is a set of React components for efficiently rendering large lists and tabular data. ES6, CommonJS, and UMD builds are available with each distribution and the project supports a Webpack 4 workflow. Note that  react ,  react-dom  must be specified as peer dependencies in order to avoid version conflicts. Give it a try. github.com Victory  is a collection of React composable React components for building interactive data visualization, built by Formidable Labs and with over 6k stars. Victory uses the same API for web and React Native applications for easy cross-platform charting. An elegant and flexible way to leverage React components in favor of practical data visualization. I like it. These libraries is a neat combination  with  Bit  when using individual components, to share and sync them across apps. At 2k stars Carto is a Location Intelligence & Data Visualization tool for discovering insights underlying location data. You can upload geospatial data (Shapefiles, GeoJSON, etc) using a web form and visualize it in a dataset or on a map, search it using SQL, and apply map styles using CartoCSS. Here are a bunch of  video demos  to help you get the idea and get started. github.com At over 5K stars Raw is a connection link between spreadsheets and data visualization built to create custom vector-based visualizations on top of the  d3.js  library. It works with tabular data (spreadhseets and comma-separated values) as well as with copied-and-pasted texts from other applications. Based on the SVG format, visualizations can be edited with vector graphics applications for further refinements, or directly embedded into web pages. Here‚Äôs an  example gallery  to explore before diving in. github.com At over 11k stars Metabase is a rather quick and simple way to create data dashboards without knowing SQL (but with SQL Mode for analysts and data pros). You can create canonical  segments and metrics , send data to Slack (and view data in Slack with  MetaBot ) and more. Probably a great tool to visualize data internally for your team, although some maintenance might be required. github.com At nearly 2k stars tauCharts is a D3-based and data-focused charting library. The library provides a declarative interface for fast mapping of data fields to visual properties, and its architecture allows you to build  facets  and extend chart behavior with reusable plugins. It also looks pretty good, right? github.com Note that some of these are unmaintained. github.com github.com github.com github.com github.com github.com github.com github.com github.com github.com github.com github.com blog.bitsrc.io blog.bitsrc.io blog.bitsrc.io"
"Data Visualization with Bokeh in Python, Part III: Making a Complete Dashboard","ata Visualization with Bokeh in Python, Part III: Making a Complete Dashboar","Creating an interactive visualization application in Bokeh Sometimes I learn a data science technique to solve a specific problem. Other times, as with Bokeh, I try out a new tool because I see some cool projects on Twitter and think: ‚ÄúThat looks pretty neat. I‚Äôm not sure when I‚Äôll use it, but it could come in handy.‚Äù Nearly every time I say this, I end up finding a use for the tool. Data science requires knowledge of many different skills and you never know where that next idea you will use will come from! In the case of Bokeh, several weeks after trying it out, I found a perfect use case in my work as a data science researcher. My  research project  involves increasing the energy efficiency of commercial buildings using data science, and, for a  recent conference , we needed a way to show off the results of the many techniques we apply. The usual suggestion of a powerpoint gets the job done, but doesn‚Äôt really stand out. By the time most people at a conference see their third slide deck, they have already stopped paying attention. Although I didn‚Äôt yet know Bokeh very well, I volunteered to try and make an interactive application with the library, thinking it would allow me to expand my skill-set and create an engaging way to show off our project. Skeptical, our team prepared a back-up presentation, but after I showed them some prototypes, they gave it their full support. The final interactive dashboard was a stand-out at the conference and will be adopted by our team for future use: While not every idea you see on Twitter is probably going to be helpful to your career, I think it‚Äôs safe to say that knowing more data science techniques can‚Äôt possibly hurt. Along these lines, I started this series to share the capabilities of  Bokeh , a powerful plotting library in Python that allows you to make interactive plots and dashboards. Although I can‚Äôt share the dashboard for my research, I can show the basics of building visualizations in Bokeh using a publicly available dataset. This third post is a continuation of my Bokeh series, with  Part I focused on building a simple graph,  and  Part II showing how to add interactions to a Bokeh plot . In this post, we will see how to set up a full Bokeh application and run a local Bokeh server accessible in your browser! This article will focus on the structure of a Bokeh application rather than the plot details, but the full code for everything can be found on  GitHub.  We will continue to use the  NYCFlights13 dataset , a real collection of flight information from flights departing 3 NYC airports in 2013. There are over 300,000 flights in the dataset, and for our dashboard, we will focus primarily on exploring the arrival delay information. To run the full application for yourself, make sure you have Bokeh installed ( using  pip install bokeh ),  download the  bokeh_app.zip  folder  from GitHub, unzip it, open a command window in the directory, and type  bokeh serve --show bokeh_app . This will set-up a  local Bokeh server  and open the application in your browser (you can also make Bokeh plots available publicly online, but for now we will stick to local hosting). Before we get into the details, let‚Äôs take a look at the end product we‚Äôre aiming for so we can see how the pieces fit together. Following is a short clip showing how we can interact with the complete dashboard: Here I am using the Bokeh application in a browser (in Chrome‚Äôs fullscreen mode) that is running on a local server. At the top we see a number of tabs, each of which contains a different section of the application. The idea of a dashboard is that while each tab can stand on its own, we can join many of them together to enable a complete exploration of the data. The video shows the range of charts we can make with Bokeh, from histograms and density plots, to data tables that we can sort by column, to fully interactive maps. Besides the range of figures we can create in Bokeh, another benefit of using this library is interactions. Each tab has an interactive element which lets users engage with the data and make their own discoveries. From experience, when exploring a dataset, people like to come to insights on their own, which we can allow by letting them select and filter data through various controls. Now that we have an idea of the dashboard we are aiming for, let‚Äôs take a look at how to create a Bokeh application. I highly recommend  downloading the code  for yourself to follow along! Before writing any code, it‚Äôs important to establish a framework for our application. In any project, it‚Äôs easy to get carried away coding and soon become lost in a mess of half-finished scripts and out-of-place data files, so we want to create a structure beforehand for all our codes and data to slot into. This organization will help us keep track of all the elements in our application and assist in debugging when things inevitably go wrong. Also, we can re-use this framework for future projects so our initial investment in the planning stage will pay off down the road. To set up a Bokeh application, I create one parent directory to hold everything called  bokeh_app  . Within this directory, we will have a sub-directory for our data (called  data ), a sub-directory for our scripts ( scripts ), and a  main.py  script to pull everything together. Generally, to manage all the code, I have found it best to keep the code for each tab in a separate Python script and call them all from a single main script. Following is the file structure I use for a Bokeh application, adapted from the  official documentation . For the flights application, the structure follows the general outline: There are three main parts:  data ,  scripts , and  main.py,  under one parent bokeh_app  directory. When it comes time to run the server, we tell Bokeh to serve the  bokeh_app  directory and it will automatically search for and run the  main.py  script. With the general structure in place, let‚Äôs take a look at  main.py  which is what I like to call the executive of the Bokeh application (not a technical term)! The  main.py  script is like the executive of a Bokeh application. It loads in the data, passes it out to the other scripts, gets back the resulting plots, and organizes them into one single display. This will be the only script I show in its entirety because of how critical it is to the application: We start out with the necessary imports including the functions to make the tabs, each of which is stored in a separate script within the  scripts  directory. If you look at the file structure, notice that there is an  __init__.py  file in the  scripts  directory. This is a completely blank file that needs to be placed in the directory to allow us to import the appropriate functions using relative statements (e.g.  from scripts.histogram import histogram_tab  ). I‚Äôm not quite sure why this is needed, but it works (here‚Äôs the  Stack Overflow answer  I used to figure this out). After the library and script imports, we read in the necessary data with help from the  Python  __file__  attribute . In this case, we are using two pandas dataframes (  flights  and  map_data  ) as well as US states data that is included in Bokeh. Once the data has been read in, the script proceeds to delegation: it passes the appropriate data to each function, the functions each draw and return a tab, and the main script organizes all these tabs in a single layout called  tabs . As an example of what each of these separate tab functions does, let‚Äôs look at the function that draws the  map_tab . This function takes in  map_data  (a formatted version of the flights data) and the US state data and produces a map of flight routes for selected airlines: We covered interactive plots in Part II of this series, and this plot is just an implementation of that idea. The overall structure of the function is: We see the familiar  make_dataset ,  make_plot , and  update  functions used to  draw the plot with interactive controls . Once we have the plot set up, the final line returns the entire plot to the main script. Each individual script (there are 5 for the 5 tabs) follows the same pattern. Returning to the main script, the final touch is to gather the tabs and add them to a single document. The tabs appear at the top of the application, and much like tabs in any browser, we can easily switch between them to explore the data. After all the set-up and coding required to make the plots, running the Bokeh server locally is quite simple. We open up a command line interface (I prefer Git Bash but any one will work), change to the directory containing  bokeh_app  and run  bokeh serve --show bokeh_app . Assuming everything is coded correctly, the application will automatically open in our browser at the address  http://localhost:5006/bokeh_app . We can then access the application and explore our dashboard! If something goes wrong (as it undoubtedly will the first few times we write a dashboard) it can be frustrating to have to stop the server, make changes to the files, and restart the server to see if our changes had the desired effect. To quickly iterate and resolve problems, I generally develop plots in a Jupyter Notebook. The Jupyter Notebook is a great environment for Bokeh development because you can create and test fully interactive plots from within the notebook. The syntax is a little different, but once you have a completed plot, the code just needs to be slightly modified and can then be copied and pasted into a standalone  .py  script. To see this in action, take a look at the  Jupyter Notebook  I used to develop the application. A fully interactive Bokeh dashboard makes any data science project stand out. Oftentimes, I see my colleagues do a lot of great statistical work but then fail to clearly communicate the results, which means all that work doesn‚Äôt get the recognition it deserves. From personal experience, I have also seen how effective Bokeh applications can be in communicating results. While making a full dashboard is a lot of work (this one is over 600 lines of code!) the results are worthwhile. Moreover, once we have an application, we can quickly share it using GitHub and if we are smart about our structure, we can re-use the framework for additional projects. The key points to take away from this project are applicable to many data science projects in general: That‚Äôs all for this post and for this series, although I plan on releasing additional stand-alone tutorials on Bokeh in the future. With libraries like Bokeh and plot.ly it‚Äôs becoming easier to make interactive figures and having a way to present your data science results in a compelling manner is crucial. Check out this  Bokeh GitHub repo  for all my work and feel free to fork and get started with your own projects. For now, I‚Äôm eager to see what everyone else can create! As always, I welcome feedback and constructive criticism. I can be reached on Twitter  @koehrsen_will ."
Data Mining Reveals Fundamental Pattern of Human Thinking,ata Mining Reveals Fundamental Pattern of Human Thinkin,"Back in 1935, the American linguist George Zipf made a remarkable discovery. Zipf was curious about the‚Ä¶"
How to improve the process of Data Mining through implementation of common Rules?,ow to improve the process of Data Mining through implementation of common Rules,"Web Data Mining Companies  have started to hit a point in providing  web data mining services  where running out of the box is quite effective, however, after long experimenting only. In noteworthy of mentioning, for the beginners of  Data Mining Services , there are common rules that must be implemented in order to improve the process. These rules may be taken to skip sometimes; on the contrary, the application of these basic rules can assist the Data Miner in the complex extraction of Data. Web Data Mining Services  come out effortless with the use of tools, but at this juncture, you will learn how rule implementation works. The complexity of Business in the context of Data Extraction and pulling out benefits may sound cool and challenging but this requires experience and intellect in the field. Rule one states that in case, you have not handled any complex business Data mining projects in the past, don‚Äôt take hold of a complex project in the first place. Start with the resolution of common business issues through data mining. These business issues could be cross-selling opportunities, customer‚Äôs loyalty, and fraud identification/detection. Rule2: Leaving specific categories of Data is not an option More often than not,  Data Mining service  providers leave upon the business data that does not interest them. This is not an option if you want the outcome to be beneficial and add-on to business. While preparation of model, never skip any Data extracted that might make a difference in the predictions and plans. Overlooking the other side of the coin is an injustice to the process. Rule3: Stick to Sampling Strategy In noteworthy of mentioning, sometimes the use of advanced tools and techniques result in the creation of complexities as the unwanted populations‚Äô database comes in the way. Sticking to a specific strategy can help in the management of activities and outcomes in the long run. Loginworks Softwares  recommend the professionals of  data mining service  to put hands off advanced tools prior to expertise in simpler data mining techniques. Connect with source url:- https://www.loginworks.com/data-mining-services-various-type/"
BENIFITS ASSOCIATED WITH DATA MINING,ENIFITS ASSOCIATED WITH DATA MININ,"Data has been used from time immemorial by various companies to manage their operations.Data is needed by various organizations strategically aimed at expanding their business operations, reduction of costs, improve their marketing force and above all improve profitability. Data mining is aimed at the creation of information assets and uses them to leverage their objectives. In this article, we discuss some of the common questions asked about the  data mining technology .  Some of the questions we have addressed include: Data Mining Defined Data mining can be regarded as a new concept in the enterprise decision support system, usually abbreviated as DSS. It does more than complementing and interlocking with the DSS capabilities that may involve reporting and query. It can also be used in on-line analytical processing (OLAP), traditional statistical analysis and data visualization. The technology comes up with tables, graphs, and reports of the past business history. We may define data mining as modeling of hidden patterns and discovering data from large volumes of data.It is important to note that data mining is very different from other retrospective technologies because it involves the creation of models. By using this technology, the user can discover patterns and use them to build models without even understanding what you are after. It gives an explanation why the past events happened and even predicting what is likely to happen. Some of the information technologies that can be linked to data mining include neural networks, fuzzy logic, rule induction and genetic algorithms. In this article, we do not cover those technologies but focus on how data mining can be used to meet your business needs and you can translate the solutions thereafter into dollars. Setting Your Business Solutions and Profits One of the common questions asked about this technology is; what role can data mining play for my organization? At the start of this article, we described some of the opportunities that can be associated with the use of data. Some of those benefits include cost reduction, business expansion, sales and marketing, and profitability. In the following paragraphs, we look into some of the situations where companies have used data mining to their advantage. Business Expansion Equity Financial Limited wanted to expand their customer base and also attract new customers. They used the Loan Check offer to meet their objectives. Initiating the loan, a customer had to go to any branch of Equity branch and just cash the loan. Equity introduced a $6000 LoanCheck by just mailing the promotion to their existing customers. The equity database was able to track about 400 characteristics of every customer. The characteristics were about loan history of the customer, their active credit cards, current balance on the credit cards and if they could respond to the loan offer. Equity used data mining to shift through 400 customer features and also finding the significant ones. They used the data and build a model based on the response to the Loan Check offer. They then integrated this model to 500,000 potential customers from a credit bureau. They then selectively mailed the most potential customers that were determined by the data mining model. At the end of the process, they were able to generate a total of $2.1M in extra net income from 15,000 new customers. Reduction of Operating Costs Empire is one of the largest insurance companies in the country. In order to compete with other insurance companies, it has to offer quality services and at the same time reducing costs.Therefore it has to attack costs that may in form of fraud and abuse. This demands a considerable investigation skills and use of data management technology. The latter calls for data mining application that can profile every physician in their network based on claims records of every patient in their data warehouse. The application is able to detect subtle deviations on the physician behavior that are linked to her/her peer group. The deviations are then reported to the intelligence and fraud investigators as ‚Äúsuspicion index.‚Äù With this effort derived from data mining, the company was able to save $31M, $37M, and $41M in the first three years respectively from frauds. Sales Effectiveness and Profitability In this case, we look into pharmaceutical sector. Their sales representatives have a wide range of assortment tools they use in promoting various products to physicians. Some of the tools include product samples, clinical literature, dinner meetings, golf outings, teleconferences and many more. Therefore getting to know the promotions methods that are ideal for a particular physician is of valuable importance and it is likely to cost the company a lot of dollars in sales call and thereby more lost revenue. Through  data mining , a drug maker was able to link eight months of promotional activity based on corresponding sales found in their database. They then used this information to build a predictive model for each physician.The model revealed that for the six promotional alternatives, only three had a significant impact. Then they used the knowledge found in the data mining models and thereby customizing the ROI. Looking at those two case studies, then ask yourself, was data mining necessary? Getting Started All the cases presented above have revealed how data mining was used to yield results to the various businesses. Some of the results led to increased revenue and increased customer base. Others can be regarded as bottom-line improvements that impacted on cost savings and also improved productivity.In the next few paragraphs we try to answer the question; how can my company get started and start realizing the benefits of data mining. The right time to start your data mining project is now. With the emergence of specialized data mining companies, starting the process has been simplified and the costs greatly reduced. Data mining project can offer important insights into the field and also aggregate the idea of creating a data warehouse. In this article, we have addressed some of the common questions regarding data mining, what are the benefits associated with the process and how a company can get started. Now, with this knowledge your company should start with a pilot project and then continue building a data mining capability in your company; to improve profitability, market your products more effectively, expand your business and also reduce costs Connect with orignal sources:- https://www.loginworks.com/blogs/255-benefits-associated-with-data-mining/"
Cambridge Analytica: the Geotargeting and Emotional Data Mining Scripts,ambridge Analytica: the Geotargeting and Emotional Data Mining Script,"Last year, Michael Phillips, a data science intern at Cambridge Analytica, posted the following scripts to a set of ‚Äúwork samples‚Äù on his  personal GitHub account . The Github profile,  MichaelPhillipsData  is still around. It contains a selection of Phillips‚Äô coding projects. Two of the ‚Äúcommits‚Äù ‚Äî still online today ‚Äî appear to be scripts that were used by Cambridge Analytica around the election. One of them even lists his email address. The rest of his current work, Phillips notes on his Github profile, he unfortunately ‚Äúcannot share.‚Äù archive.is The first of Phillips‚Äô two election data processing Github scripts is titled  GeoLocation.py ,* a list-completing and enrichment tool that can be used to: archive.is ‚Äúcomplete an array of addresses with accurate latitudes and longitudes using the completeAddress functionIncludes another function compareAPItoSource for testing APIs with source latitude longitudes.‚Äù Phillips describes the geolocation list completion script as performing the following tasks (to enrich clients‚Äô personal information files): ‚ÄúEssentially what it does is: For each address in the addresses file, try to get an accurate lng/lat quickly (comparing available datafrom Aristotle/IG to the zip code file data to determine accuracy), but if we can‚Äôt, we fetch it from ArcGIS.‚Äù >Don‚Äôt miss the line item called ‚ÄúVoter_ID‚Äù The second ‚Äúwork-related‚Äù script sitting on Phillips‚Äô Github repo is called  Twitteranalysis.py . archive.is Phillips offers a quick starter for how the Twitter sentiment-mining code works: For starters, we will just get sentiment from textBlob for tweets containing keywords like ‚ÄúTrump‚Äù, ‚ÄúCarson‚Äù, ‚ÄúCruz‚Äù, ‚ÄúBern‚Äù, Bernie‚Äù, ‚Äúguns‚Äù, ‚Äúimmigration‚Äù, ‚Äúimmigrants‚Äù, etc. Twitteranalysis.py  also finds the Twitter  user IDs  amongst the tweet sample it collects in order to ‚Äú retrieve all the user‚Äôs recent tweets and favorites .‚Äù Looking in more detail, it then: As a real-time social media mining tool which uses common tools like tweepy and matplotlib, this doesn‚Äôt appear to be science fiction or extremely complex. However, this is not what makes the code interesting as a key  research , political  evidence , and  cultural  object. The most fascinating part of the Twitter sentiment-miner that Phillips‚Äô posted is how it appears to pull users‚Äô IDs and find their ‚Äúrecent tweets‚Äù and favorites to  expand  the company‚Äôs corpus of keywords around specific objects of election ‚Äúoutrage‚Äù sentiment (ie, immigration, border control, etc.). Looking below, nearly all ‚Äú sentiments ‚Äù within the lines of code involve ‚Äúhot-button‚Äù 2016 election topics such as  abortion ,  citizenship ,  naturalization ,  guns , the  NRA ,  liberals ,  Obama , and  Planned Parenthood. See for yourself, here‚Äôs the actual code: #each  sentiments  list will have tuples: ( sentiment ,  tweetID ) #note: could include many more keywords like ‚Äú feelthebern ‚Äù for example, but need neutral keywords to get true sentiments.  feelthebern  would be a biased term. In any case, here are the ‚Äúsentiments‚Äù the script was set to look for via Twitter‚Äôs API: hillarySentiments  = [] hillaryKeywords = [‚Äòhillary‚Äô, ‚Äòclinton‚Äô, ‚Äòhillaryclinton‚Äô] trumpSentiments  = [] trumpKeywords = [‚Äòtrump‚Äô, ‚Äòrealdonaldtrump‚Äô] cruzSentiments  = [] cruzKeywords = [‚Äòcruz‚Äô, ‚Äòtedcruz‚Äô] bernieSentiments  =[] bernieKeywords = [‚Äòbern‚Äô, ‚Äòbernie‚Äô, ‚Äòsanders‚Äô, ‚Äòsensanders‚Äô] obamaSentiments  = [] obamaKeywords = [‚Äòobama‚Äô, ‚Äòbarack‚Äô, ‚Äòbarackobama‚Äô] republicanSentiments  = [] republicanKeywords = [‚Äòrepublican‚Äô, ‚Äòconservative‚Äô] democratSentiments  = [] democratKeywords = [‚Äòdemocrat‚Äô, ‚Äòdems‚Äô, ‚Äòliberal‚Äô] gunsSentiments  = [] gunsKeywords = [‚Äòguns‚Äô, ‚Äògun‚Äô, ‚Äònra‚Äô, ‚Äòpistol‚Äô, ‚Äòfirearm‚Äô, ‚Äòshooting‚Äô] immigrationSentiments  = [] immigrationKeywords = [‚Äòimmigration‚Äô, ‚Äòimmigrants‚Äô, ‚Äòcitizenship‚Äô, ‚Äònaturalization‚Äô, ‚Äòvisas‚Äô] employmentSentiments  = [] emplyomentKeywords = [‚Äòjobs‚Äô, ‚Äòemployment‚Äô, ‚Äòunemployment‚Äô, ‚Äòjob‚Äô] inflationSentiments  = [] inflationKeywords = [‚Äòinflate‚Äô, ‚Äòinflation‚Äô, ‚Äòprice hike‚Äô, ‚Äòprice increase‚Äô, ‚Äòprices rais‚Äô] minimumwageupSentiments  = [] minimumwageupKeywords = [‚Äòraise minimum wage‚Äô, ‚Äòwage increase‚Äô, ‚Äòraise wage‚Äô, ‚Äòwage hike‚Äô] abortionSentiments  = [] abortionKeywords = [‚Äòabortion‚Äô, ‚Äòpro-choice‚Äô, ‚Äòplanned parenthood‚Äô] governmentspendingSentiments  = [] governmentspendingKeywords = [‚Äògov spending‚Äô, ‚Äògovernment spending‚Äô, ‚Äògov. spending‚Äô, ‚Äòexpenditure‚Äô] taxesupSentiments  = [] taxesupKeywords = [‚Äòraise tax‚Äô, ‚Äòtax hike‚Äô, ‚Äòtaxes up‚Äô, ‚Äòtax up‚Äô, ‚Äòincrease taxes‚Äô, ‚Äòtaxes increase‚Äô, ‚Äòtax increase‚Äô] taxesdownSentiments  = [] taxesdownKeywords = [‚Äòlower tax‚Äô, ‚Äòtax cut‚Äô, ‚Äòtax slash‚Äô, ‚Äòtaxes down‚Äô, ‚Äòtax down‚Äô, ‚Äòdecrease taxes‚Äô, ‚Äòtaxes decrease‚Äô, ‚Äòtax decrease‚Äô] Drilling down to the list of terms that are linked to each election sentiment keyword (in the code as  #(nameOfTuple,  sentimentList ,  keywordList  ),  we can see: personSentimentList  = [(‚Äòhillary‚Äô, hillarySentiments, hillaryKeywords), (‚Äòtrump‚Äô, trumpSentiments, trumpKeywords), (‚Äòcruz‚Äô, cruzSentiments, cruzKeywords), (‚Äòbernie‚Äô, bernieSentiments, bernieKeywords), (‚Äòobama‚Äô, obamaSentiments, obamaKeywords)] issueSentimentList  = [(‚Äòguns‚Äô, gunsSentiments, gunsKeywords), (‚Äòimmigration‚Äô, immigrationSentiments, immigrationKeywords), (‚Äòemployment‚Äô, employmentSentiments, emplyomentKeywords), (‚Äòinflation‚Äô, inflationSentiments, inflationKeywords), (‚Äòminimum wage up‚Äô, minimumwageupSentiments, minimumwageupKeywords), (‚Äòabortion‚Äô, abortionSentiments, abortionKeywords), (‚Äògovernment spending‚Äô, governmentspendingSentiments, governmentspendingKeywords), (‚Äòtaxes up‚Äô, taxesupSentiments, taxesupKeywords), (‚Äòtaxes down‚Äô, taxesdownSentiments, taxesdownKeywords) ] Phillips also provides a snippet of code ‚Äúfor taking random twitter IDs‚Äù to create a Twitter ‚Äú control group.‚Äù  This part of the code appears to ‚Äúskim the most recent tweets that have mentioned one of our [Cambridge Analytica‚Äôs pre-defined] keywords.‚Äù Phillips explains in the notes within his code about the practicalities of sentiment mining‚Ää‚Äî‚Ääthis is not big data (ie, ‚Äúall the tweets‚Äù) that were being sought out: ‚Äúit turned out that skimming all of the tweets found very very few occurances of keywords since ‚Äútwitter is such a global/multilingual platform.‚Äù Next, Phillips provides a snippet to parse * any * text that CA was ‚Äúlooking for through  non-tweets  (like transcripts of some sort),‚Äù noting that the tool is set up to ‚Äúfind sentiment and  adds  [it] to the respective keywords‚Äô data list‚Äù: Interesting functionality, indeed. The lines of code then follow with a function that Phillips states: ‚Äúgoes through tweets of each user, looks for keywords, and if the keyword is there, we find the sentiment for that tweet and add it to the sentiment data list‚Äù Finally, the code compiles the collected and refined Twitter data into a set. Phillips describes: ‚Äúcompiles the sentiment data for each keyword group into an easier to work with format (dataframe) ‚Ä¶ it is only meaningful if compared with a control group, since keyword selection is impossible to employ neutrally.‚Äù The final output of   the   Twitteranalysis.py  is a list of tweets and Twitter users (via user IDs) from a pre-defined set of keywords ( abortion ,  NRA ,  Hillary ,  Obama , lower  taxes ,  guns ,  immigration ,  liberals , etc.). All relate to #Election2016 campaign issues. Also, this code appears to be extensible ‚Äî it can be used outside of Twitter, such as to mine the transcripts and recorded text from  focus   groups  and  survey   respondents . These scripts normally wouldn‚Äôt be  that  interesting. But provided both were added by a Cambridge Analytica intern (at least at the time) and contain a running dialog of what the tools do, how they work, and why they were built‚Äî and the fact that they are *still* available on Github ‚Äî I thought I‚Äôd share. Almost every pronoun used in the script walkthroughs (see the archived Github links) is  inclusive  and  plural  ‚Äî ‚Äúour,‚Äù ‚Äúwe,‚Äù ‚Äúwe‚Äôre,‚Äù etc. Also, reference to convert to format ‚Äúto put into the  neural   network .‚Äù Wait, there‚Äôs  one  more thing. When Phillips committed his original  Twitteranalysis.py  script, he accidentally  left the  working  Twitter API keys in the code  (via the consumer  key  and consumer ‚Äú secret ‚Äù). This contains the alphanumeric strings which are used for the developer account to access data from Twitter‚Äôs API. Interestingly, on Feb 23, 2017 (yes,  2017 ), Phillips  removed  the API keys: Two days later, another Github user added a comment about Phillip‚Äôs mistake: Was this API key Cambridge Analytica‚Äôs? Or SCL‚Äôs ? While both scripts‚Äî the first including Phillips‚Äô  @cambridgeanalytica.org  email address, clearly are voter data and election related, from the commentary in the script, it‚Äôs not clear who the API key belonged to. This might have been Phillips‚Äô own account. Regardless, this code shows the  inner workings  of client voter file geo-data ‚Äúenrichment‚Äù and presumably automated  voter database processing  for clients by Cambridge Analytica. This code also provides the proof in showing once and for all how Twitter users‚Äô emotional reactions and real-time discussions even favorites/likes (pulled from the API) are  mined  in real time and used to create  test   phrases,  establish  control   groups , and apparently provide sets of  future  terms around keywords related to political campaign issues. The fact that Cambridge Analytica was using this kind of code to mine emotional responses that surface from users‚Äô ‚Äúrecent tweets‚Äù from a defined set of 2016 presidential campaign ‚Äútrigger words‚Äù is interesting. medium.com I‚Äôm confident Phillips provided this data in earnest, as he includes an excellent working description in the purposes and uses of these scripts. He was an CA intern who wanted to show his work to get a job in the future. Yet, this is part of the arsenal of tools used by Cambridge Analytica to  geolocate American voters  and harness American‚Äôs  real-time emotional sentiment  (see example below for Instagram targeting). I‚Äôd argue the question of the  ownership  of Cambridge Analytica ‚Äî a foreign business previously registered in the United States as a  foreign  corporation (SCL Elections ) just became a bit more relevant. Foreign  influence‚Äî‚Ääsound familiar? And that fact that a  working Twitter developer API key ‚Ää‚Äî‚Ääpossibly one of Cambridge Analytica‚Äôs own‚Ää‚Äî‚Ääwas left sitting on GitHub by a data  intern  for anyone to use is, well, another story. The code will likely be removed soon, so it‚Äôs available here: data.world medium.com"
Your Ultimate Data Mining & Machine Learning Cheat Sheet,our Ultimate Data Mining & Machine Learning Cheat Shee,"There are several areas of data mining and machine learning that will be covered in this cheat-sheet: Train-test-split  is an important part of testing how well a model performs by training it on designated training data and testing it on designated testing data. This way, the model‚Äôs ability to generalize to new data can be measured. In  sklearn , both lists, pandas DataFrames, or NumPy arrays are accepted in  X  and  y  parameters. Training a standard supervised learning model  takes the form of an import, the creation of an instance, and the fitting of the model. sklearn  classifier models  are listed below, with the branch highlighted in blue and the model name in orange. sklearn  regressor models  are listed below, with the branch highlighted in blue and the model name in orange. Evaluating model performance  is done with train-test data in this form: sklearn  metrics  for classification and regression are listed below, with the most commonly used metric marked in green. Many of the grey metrics are more appropriate than the green-marked ones in certain contexts. Each have their own advantages and disadvantages, balancing priority comparisons, interpretability, and other factors. Before clustering, the data needs to be standardized (information for this can be found in the Data Transformation section). Clustering is the process of creating clusters based on point distances. Training and creating a K-Means clustering model  creates a model that can cluster and retrieve information about the clustered data. Accessing the labels  of each of the data points in the data can be done with: Similarly, the label of each data point can be stored in a column of the data with: Accessing the cluster label of new data  can be done with the following command. The  new_data  can be in the form of an array, a list, or a DataFrame. Accessing the cluster centers of each cluster  is returned in the form of a two-dimensional array with: To  find the optimal number of clusters , use the silhouette score, which is a metric of how well a certain number of clusters fits the data. For each number of clusters within a predefined range, a K-Means clustering algorithm is trained and its silhouette score saved to a list ( scores ).  data  is the  x  that the model is trained on. After the scores are saved to the list  scores , they can be graphed out or computationally searched for to find the highest one. Dimensionality reduction is the process of expressing high-dimensional data in a reduced number of dimensions such that each one contains the most amount of information. Dimensionality reduction may be used for visualization of high-dimensional data or to speed up machine learning models by removing low-information or correlated features. Principal Component Analysis , or PCA, is a popular method of reducing the dimensionality of data by drawing several orthogonal (perpendicular) vectors in the feature space to represent the reduced number of dimensions. The variable  number  represents the number of dimensions the reduced data will have. In the case of visualization, for example, it would be two dimensions. Fitting the PCA Model : The  .fit_transform  function automatically fits the model to the data and transforms it into a reduced number of dimensions. Explained Variance Ratio : Calling  model.explained_variance_ratio_  will yield a list where each item corresponds to that dimension‚Äôs ‚Äúexplained variance ratio‚Äù, which essentially means the percent of information in the original data represented by that dimension. The sum of the explained variance ratios is the total percent of information retained in the reduced dimensionality data. PCA Feature Weights : In PCA, each newly creates feature is a linear combination of the former data‚Äôs features. These   linear weights can be accessed with  model.components_ , and are a good indicator for feature importance (a higher linear weight indicates more information represented in that feature). Linear Discriminant Analysis  (LDA, not to be commonly confused with Latent Dirichlet Allocation) is another method of dimensionality reduction. The primary difference between LDA and PCA is that LDA is a supervised algorithm, meaning it takes into account both  x  and  y . Principal Component Analysis only considers  x  and is hence an unsupervised algorithm. PCA attempts to maintain the structure (variance) of the data purely based on distances between points, whereas LDA prioritizes clean separation of classes. Feature Importance is the process of finding the most important feature to a target. Through PCA, the feature that contains the most information can be found, but feature importance concerns a feature‚Äôs impact on the target. A change in an ‚Äòimportant‚Äô feature will have a large effect on the  y -variable, whereas a change in an ‚Äòunimportant‚Äô feature will have little to no effect on the  y -variable. Permutation Importance  is a method to evaluate how important a feature is. Several models are trained, each missing one column. The corresponding decrease in model accuracy as a result of the lack of data represents how important the column is to a model‚Äôs predictive power. The  eli5  library is used for Permutation Importance. In the data that this Permutation Importance model was trained on, the column  lat  has the largest impact on the target variable (in this case the house price). Permutation Importance is the best feature to use when deciding which to remove (correlated or redundant features which actually confuse the model, marked by negative permutation importance values) in models for best predictive performance. SHAP  is another method of evaluating feature importance, borrowing from game theory principles in Blackjack to estimate how much value a player can contribute. Unlike permutation importance,  SH apley  A ddative Ex P lanations use a more formulaic and calculation-based method towards evaluating feature importance. SHAP requires a tree-based model (Decision Tree, Random Forest) and accommodates both regression and classification. PD(P) Plots , or partial dependence plots, are a staple in data mining and analysis, showing how certain values of one feature influence a change in the target variable. Imports required include  pdpbox  for the dependence plots and  matplotlib  to display the plots. Isolated PDPs : the following code displays the partial dependence plot, where  feat_name  is the feature within  X  who will be isolated and compared to the target variable. The second line of code saves the data, whereas the third constructs the canvas to display the plot. The partial dependence plot shows the effect of certain values and changes in the number of square feet of living space on the price of a house. Shaded areas represent confidence intervals. Contour PDPs : Partial dependence plots can also take the form of contour plots, which compare not one isolated variable but the relationship between two isolated variables. The two features that are to be compared are stored in a variable  compared_features . The relationship between the two features shows the corresponding price when only considering these two features. Partial dependence plots are chock-full of data analysis and findings, but be conscious of large confidence intervals. Standardizing or scaling  is the process of ‚Äòreshaping‚Äô the data such that it contains the same information but has a mean of 0 and a variance of 1. By scaling the data, the mathematical nature of algorithms can usually handle data better. The  transformed_data  is standardized and can be used for many distance-based algorithms such as Support Vector Machine and K-Nearest Neighbors. The results of algorithms that use standardized data need to be ‚Äòde-standardized‚Äô so they can be properly interpreted.  .inverse_transform()  can be used to perform the opposite of standard transforms. Normalizing  data puts it on a 0 to 1 scale, something that is, similarly to standardized data, makes the data mathematically easier to use for the model. While normalizing doesn‚Äôt transform the shape of the data as standardizing does, it restricts the boundaries of the data. Whether to normalize or standardize data depends on the algorithm and the context. Box-cox transformations  involve raising the data to various powers to transform it. Box-cox transformations can normalize data, make it more linear, or decrease the complexity. These transformations don‚Äôt only involve raising the data to powers, but also to fractional powers (square rooting) and logarithms. For instance, consider data points situated along the function  g ( x ). By applying the logarithm box-cox transformation, the data can be easily modelled with linear regression. sklearn  automatically determines the best series of box-cox transformations to apply to the data to make it better resemble a normal distribution. Because of the nature of box-cox transformation square-rooting, box-cox transformed data must be strictly positive (normalizing the data before hand can take care of this). For data with negative data points as well as positive ones, set  method = ‚Äòyeo-johnson‚Äô  for a similar approach to making the data more closely resemble a bell curve. Be sure to bookmark this page for easy reference if you find it helpful. Often, data mining and analysis will require visualization ‚Äî feel free to check out another cheat sheet for visualization. While you‚Äôre creating visualizations and performing machine learning operations, you may want to take a look at the data manipulation and cleaning cheat sheet. medium.com medium.com medium.com"
Data Mining Has Revealed Previously Unknown Russian Twitter Troll Campaigns,ata Mining Has Revealed Previously Unknown Russian Twitter Troll Campaign,By Emerging Technology from the arXiv
Data Mining Tools,ata Mining Tool,"Huge amount of data generated every second and it is necessary to have knowledge of different tools that can be utilized to handle this huge data and apply interesting data mining algorithms and visualizations in quick time. Data Mining  is the set of methodologies used in analyzing data from various dimensions and perspectives, finding previously unknown hidden patterns, classifying and grouping the data and summarizing the identified relationships. The tasks of data mining are twofold: Four most useful data mining techniques: For doing quick analysis on data using any data mining technique it is important to have hands on knowledge of different tools. All the tools mentioned below has its own peculiarity in terms of implementation and each has its own merits. It all boils down to the requirement of task. Most important thing is to know that tools exist which can immensely enhance the efficiency of a data scientist or a student working on some project, so that you can focus more the things that matter that is is gaining useful insights and making projections. It also takes the pain of implementing any standard algorithm from scratch but at the same time gives you the power to modify the code of tool (open source ) as per requirements. There are many tools apart from mentioned below and I encourage you to check that out as well. The list I have provided are the one that are most common and used widely in leading companies as well as academia. Also most of them are open source == Awesome :) . Rapid Miner This is very popular since it is a ready made, open source, no-coding required software, which gives advanced analytics. Written in Java, it incorporates multifaceted data mining functions such as data pre-processing, visualization, predictive analysis, and can be easily integrated with WEKA and R-tool to directly give models from scripts written in the former two. Besides the standard data mining features like data cleansing, filtering, clustering, etc, the software also features built-in templates, repeatable work flows, a professional visualisation environment, and seamless integration with languages like Python and R into work flows that aid in rapid prototyping. Download  |  Site  |  Tutorial Weka Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes. Download  |  Site  |  Tutorial Orange Python users playing around with data sciences might be familiar with Orange. It is a Python library that powers Python scripts with its rich compilation of mining and machine learning algorithms for data pre-processing, classification, modelling, regression, clustering and other miscellaneous functions. Orange also comes with a visual programming environment and its workbench consists of tools for importing data, and dragging and dropping widgets and links to connect different widgets for completing the workflow. Download  |  Site  |  Tutorial R R is a free software environment for statistical computing and graphics written in C++. R Studio is IDE specially designed for R language.It is one of the leading tools used to do data mining tasks and comes with huge community support as well as packaged with hundreds of libraries built specifically for data mining. Download  |  Site  |  Tutorial Knime Primarily used for data preprocessing ‚Äî i.e. data extraction, transformation and loading, Knime is a powerful tool with GUI that shows the network of data nodes. Popular amongst financial data analysts, it has modular data pipe lining, leveraging machine learning, and data mining concepts liberally for building business intelligence reports. Download  |  Site  |  Tutorial Rattle Rattle, expanded to ‚ÄòR Analytical Tool To Learn Easily‚Äô, has been developed using the R statistical programming language. The software can run on Linux, Mac OS and Windows, and features statistics, clustering, modelling and visualisation with the computing power of R. Rattle is currently being used in business, commercial enterprises and for teaching purposes in Australian and American universities. Download  |  Site  |  Tutorial Tanagra TANAGRA is a free open source data mining software for academic and research purposes. It proposes several data mining methods from exploratory data analysis, statistical learning, machine learning and databases area. TANAGRA is more powerful, it contains some supervised learning but also other paradigms such as clustering, factorial analysis, parametric and non parametric statistics, association rule, feature selection and construction algorithms.The main purpose of Tanagra project is to give researchers and students an easy-to-use  data mining software , conforming to the present norms of the software development in this domain (especially in the design of its GUI and the way to use it), and allowing to analyse either real or synthetic data. Download  |  Site  |  Tutorial XL Miner XLMiner is the only comprehensive data mining add-in for Excel, with neural nets, classification and regression trees, logistic regression, linear regression, Bayes classifier, K-nearest neighbors, discriminant analysis, association rules, clustering, principal components, and more. XLMiner provides everything you need to sample data from many sources ‚Äî PowerPivot, Microsoft/IBM/Oracle databases, or spreadsheets; explore and visualize your data with multiple linked charts; preprocess and ‚Äòclean‚Äô your data, fit data mining models, and evaluate your models‚Äô predictive power. The drawback of XL Miner is that is paid add in for excel but there is 15 day free trial option. The software has great features and its integration in excel makes life easier. Download  |  Site  |  Tutorial . ‚Äú The goal is to turn data into information, and information into insight.‚Äù‚Ää‚Äî‚ÄäCarly Fiorina (  former executive, president, and chair of Hewlett-Packard Co.) . [1]  http://opensourceforu.com/2017/03/top-10-open-source-data-mining-tools/ [2]  https://thenewstack.io/six-of-the-best-open-source-data-mining-tools/ [3]  http://blog.galvanize.com/four-data-mining-techniques-for-businesses-that-everyone-should-know/ [4]  http://www.rdatamining.com/resources/tools [5]  https://www.invensis.net/blog/data-processing/12-data-mining-tools-techniques/ [6]  https://www.predictiveanalyticstoday.com/top-free-data-mining-software/ [7]  https://www.kdnuggets.com/software/index.html"
Data Mining in Brief,ata Mining in Brie,"Data mining is a very popular topic nowadays. Unlike a few years ago, everything is bind with data now and we are capable of handling these kinds of large data well. By collecting and inspecting these data, people were able to discover some patterns. Even the whole data set is a junk, there are some hidden patterns that can be extracted by combining multiple data sources to provide valuable insights. This is called as  Data Mining. Data mining is often combined with various sources of data including enterprise data that is secured by an organization and has privacy issues and sometimes multiple sources are integrated including third party data, customer demographics and financial data etc. The amount of data available is a critical factor here. Since we are going to discover patterns in sequential or non-sequential data, correlations, to determine if the amount of obtained data is of good quality, as much as data available is good. Let‚Äôs start with an example. Assume we got some data related to login logs for a web application. As a whole, this set of data has no value. It may contain the username of a user, login timestamp, time spent to log out, activities have done etc. If we take an overview look at this, it is a whole mess. But we can analyze this to do extract some useful information. For example, this data can be used to find out a regular habit of a particular user. Further, it will help to find out the peak hours of the system. This extracted information can be used to increase the efficiency of the system and make more user-friendly. However, data mining is not a simple task. It takes a certain amount of time and it requires a special procedure as well. The basic steps of data mining are follows There are different kinds of models associated with data mining In  Descriptive Modeling , it detects the similarities between the collected data and the reasons behind them. This is very important in constructing the final conclusion from the data set. Predictive Modeling  is used to analyze the past data and predict the future behavior. Past data give some kind of hint about the future. With the significant development of web, text mining has added as a related discipline to data mining. It is required to process, filter and analyze data properly to create such predictive models. Data mining is useful in many ways. For marketing, it can be applied effectively. Using data mining we can analyze the behavior of customers and we can do advertising by getting more close to them. It will help to identify trends of customers for goods in the market and it allows the retailer to understand the purchase behavior of a buyer. In education domain we can identify the learning behaviors of students and the learning institutions can upgrade their modules and courses accordingly. We can use data mining to solve natural disasters as well. If we can collect some information, we can use them to predict things like land sliding, rainfall, tsunami etc. There are much more applications in data mining nowadays. They can vary from very simple things like marketing to very complex domains like making environmental disaster predictions etc. Thanks for reading‚Ä¶ Cheers!"
Data Mining Reveals How The ‚ÄúDown-Vote‚Äù Leads To A Vicious Circle Of Negative Feedback,ata Mining Reveals How The ‚ÄúDown-Vote‚Äù Leads To A Vicious Circle Of Negative Feedbac,"In the 1930s, the American psychologist Burrhus Skinner popularised the notion of operant conditioning, the notion that an individual‚Äôs future behaviour is determined by the punishments and rewards he or she has received in the past. It means that specific patterns of behaviour can be induced by punishing unwanted actions while rewarding those that are desired. And it certainly works with rats and pigeons. This idea has since become one of the foundations of behavioural psychology and is an important driver of the way online social networks are designed and operate. Many have systems that allow people to like, or up-vote, certain types of content while disliking, or down-voting, others. An up-vote can be thought of as a reward designed to encourage while the down-vote is a punishment designed to discourage. In theory, this should guide contributors towards producing better content that is more likely to be rewarded. At least, that‚Äôs what the theory of operant conditioning predicts. But does that actually happen on real social networks? Today, we find out thanks to the work of Justin Cheng at Stanford University in Palo Alto and a couple of buddies. These guys have measured how up-voting and down-voting influences the behaviour of a large number of contributors to different social networks. And they say that the results are far from reassuring. The evidence is that a contributor who is down-voted produces lower quality content in future that is valued even less by others on the network. What‚Äôs more, people are more likely to down-vote others after they have been down voted themselves. The result is a vicious spiral of increasingly negative behaviour that is exactly the opposite of the intended effect. Cheng and co began by compiling a dataset of the comments associated with news articles on four online communities: CNN.com, a general news site; Breitbart.com, a political new site; IGN.com, a computer gaming news site; and Allkpop.com, a Korean entertainment site. The data includes 1.2 million threads with 42 million comments and 114 million votes from 1.8 million different users. These guys conducted a survey on Amazon‚Äôs Mechanical Turk asking people to rate the quality of comments from these communities and then worked out the how the percentage of up-votes the post received correlated with the human evaluation. This confirmed that the percentage of up-votes is indeed a good measure of the quality of a post. Cheng and co then built a machine learning algorithm that predicts a post‚Äôs quality by examining the words it contains. They trained the algorithm on half of the posts in the community and found that its ratings correlated well with the subjective opinions of humans. So the algorithm is an automatic way of rating the quality of every post in their dataset. Then came the actual experiment. They used the machine learning algorithm to find posts of equal quality but with a twist: they matched posts into pairs in which one had been positively received by the community while the other had been negatively received. In other words, one of these posts received more up votes while the other received more down votes. They then assessed the future output of the authors of these posts to measure the effect of positive and negative evaluations. The results are something of an eye-opener. ‚ÄúWe find that negative feedback leads to significant behavioural changes that are detrimental to the community,‚Äù say Cheng and co. ‚ÄúNot only do authors of negatively-evaluated content contribute more, but also their future posts are of lower quality, and are perceived by the community as such,‚Äù they say. And it gets worse: ‚ÄúThese authors are more likely to subsequently evaluate their fellow users negatively, percolating these effects through the community.‚Äù By contrast, positive feedback does not appear to influence authors much at all. It does not encourage them to write more and does not improve the quality of their posts. Curiously, authors that receive no feedback, are more likely to leave the community entirely. ‚ÄúSurprisingly, our findings are in a sense exactly the opposite than what we would expect under the operant conditioning framework,‚Äù say Cheng and co. That points to an obvious strategy for improving the quality of comments on any social network site. Clearly, providing negative feedback to ‚Äúbad‚Äù users does not appear to be a good way of preventing undesired behaviour. So how can unwanted behaviour be stopped? ‚ÄúGiven that users who receive no feedback post less frequently, a potentially effective strategy could be to ignore undesired behaviour and provide no feedback at all,‚Äù say Cheng and co. That‚Äôs an interesting study that provides a fascinating insight into the complex nature of social interactions. And it backs up certain kinds of real life experience. Anyone with children will know that it is possible to unintentionally reward bad behaviour with the increased attention that it generates. So sometimes it‚Äôs better to ignore unwanted behaviour than to focus on it. But at the same time, parents also need an effective way of stepping in and actively preventing more serious incidents of undesirable behaviour when necessary. The work of Cheng and co clearly suggested ignoring bad behaviour is an effective way of discouraging it, and one that social network sites might profitably explore. But at the same time, these sites will need a way to step in and actively prevent certain types of behaviour when necessary. What‚Äôs needed now, of course, is a test of this idea. There are certainly social networks that allow up voting but not down voting (Medium being one of them). An interesting question is whether it results in the same rich tapestry of opinion that clearly flourishes on social network sites that allow both types of voting. In other words, does this kind of manipulation have other consequences that Cheng and co have not yet accounted for. Clearly, there‚Äôs interesting work ahead in teasing these things apart. Ref:  arxiv.org/abs/1405.1429  : How Community Feedback Shapes User Behavior Follow the Physics arXiv Blog on Twitter at  @arxivblog , on  Facebook  and by hitting the Follow button below"
A Beginner‚Äôs Guide to Data Engineering ‚Äî Part I, Beginner‚Äôs Guide to Data Engineering ‚Äî Part ,"The more experienced I become as a data scientist, the more convinced I am that data engineering is one of the most critical and foundational skills in any data scientist‚Äôs toolkit. I find this to be true for both evaluating project or job opportunities and‚Ä¶"
A Beginner‚Äôs Guide to Data Engineering ‚Äî Part II, Beginner‚Äôs Guide to Data Engineering ‚Äî Part I,"In  A Beginner‚Äôs Guide to Data Engineering  ‚Äî  Part I ,  I explained that an organization‚Äôs analytics capability is built layers upon layers. From collecting raw data and building data warehouses to applying Machine Learning, we‚Ä¶"
A Beginner‚Äôs Guide to Data Engineering ‚Äî The Series Finale, Beginner‚Äôs Guide to Data Engineering ‚Äî The Series Final,"From  Part I   of this series, we learned that a robust data foundation is an important prerequisite for pursuing analytics, be it business intelligence, experimentation, or machine learning. From  Part II , we delved deeply into‚Ä¶"
Functional Data Engineering ‚Äî a modern paradigm for batch data processing,unctional Data Engineering ‚Äî a modern paradigm for batch data processin,"Batch data processing  ‚Äî historically known as ETL ‚Äî  is extremely challenging . It‚Äôs time-consuming, brittle, and often unrewarding. Not only that, it‚Äôs hard to operate, evolve, and troubleshoot. In this post, we‚Äôll explore how applying the  functional programming paradigm  to  data engineering  can bring a lot of  clarity  to the process. This post distills fragments of wisdom accumulated while working at Yahoo, Facebook, Airbnb and Lyft, with the perspective of well over a decade of data warehousing and data engineering experience. Let‚Äôs start with a quick primer/refresher on what functional programming is about, from the  functional programming Wikipedia page : In  computer science ,  functional programming  is a  programming paradigm  ‚Äî a style of building the structure and elements of  computer programs  ‚Äî that treats  computation  as the evaluation of  mathematical functions  and avoids changing- state  and  mutable  data. It is a  declarative programming  paradigm, which means programming is done with  expressions [1]  or declarations [2]  instead of  statements . In functional code, the output value of a function depends only on the  arguments  that are passed to the function, so calling a function f twice with the same value for an argument x produces the same result f(x)each time; this is in contrast to  procedures  depending on a  local  or  global state , which may produce different results at different times when called with the same arguments but a different program state. Eliminating  side effects , i.e., changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming. Functional programming brings  clarity . When functions are ‚Äúpure‚Äù ‚Äî meaning they do not have side-effects ‚Äî they can be written, tested, reasoned-about and debugged in isolation, without the need to understand external context or history of events surrounding its execution. As ETL pipelines grow in complexity, and as data teams grow in numbers, using methodologies that provide clarity isn‚Äôt a luxury, it‚Äôs a necessity. Reproducibility and replicability are foundational to the  scientific method . The greater the claim made using analytics, the greater the scrutiny on the process should be. In order to be able to trust the data, the process by which results were obtained should be transparent and reproducible. While business rules evolve constantly, and while corrections and adjustments to the process are more the rule than the exception, it‚Äôs important to insulate compute logic changes from data changes and have control over all of the moving parts. Let‚Äôs take an example where some raw data provided by some other company has been identified as faulty, and the decision has been made to re-import corrected data and reprocess a certain time-range to correct the issue. Now we know that all of the data structures derived from that table, for that time range, need to be reprocessed, or  backfilled . In theory, what we want to do is to apply the exact same compute scheme as we did on the original pass, on top of the new, corrected data. Note that it‚Äôs also important for the related datasets used in this computation to be identical as they were at the time of the original computation. For example, if one of the downstream processes joins to a dimension to enrich the data, we‚Äôd want for that dimension to be identical to how it was when computing the original process. If we cannot provide this guarantee, it implies that performing the same computation, for the same period may yield different results each time it is computed. Similarly, if we want to change a business rule and perform a backfill all of the downstream computation, we need the guarantee that the change of logic is the only moving part. We need that same guarantee that the blocks of data used in the computation are identical to the ones used when re ran the original process, or in other words, that the sources have have not been altered. To put it simply, immutable data along with versioned logic are key to reproducibility. To make batch processing functional, the first thing is to avoid any form of side-effects outside the scope of the task. A pure task should be  deterministic  and  idempotent , meaning that it will produce the same result every time it runs or re-runs. This requires forcing an overwrite approach, meaning re-executing a pure task with the same input parameters should overwrite any previous output that could have been left out from a previous run of the same task. Having idempotent tasks is vital for the  operability  of pipelines. When tasks fail, or when the compute logic needs to be altered for whatever reason, we need the certainty that re-running a task is safe and won‚Äôt lead to double-counting or any other form of bad state. Without this assumption, making decisions about what needs to be re-executed requires the full context and understanding of all potential side-effects of previous executions. Note that contrarily to a pure-function, the pure-task is typically not ‚Äúreturning‚Äù an object in the programming sense of the term. In the context of a SQL  ELT-type approach  which has become common nowadays, it is likely to be simply overwriting a portion of a table (partition). While this may look like a side-effect, you can think of this output as akin to the immutable object that a typical pure-function would return. Also, for the sake of simplicity and as a best practice, individual tasks should target a single output, the same way that a pure-function shouldn‚Äôt return a set of heterogenous objects. In some cases it may appear difficult to avoid side-effects. One potential solution is to re-think the size of the unit of work. Can the task become pure if it encompasses other related tasks? Can it be purified by breaking down into a set of smaller tasks? It‚Äôs also important that all transitional states within the pure-task are insulated much like locally scoped variables in pure-functions. Multiple instances of the pure task should be fully independent in their execution. If temporary tables or dataframes are used, they should be implemented in a way that task instances cannot interfere with one another so that they can be parallelized. One of the way to enforce the functional programming paradigm is to use immutable objects. While purely functional languages will prevent mutations by enforcing the use of immutable objects, it‚Äôs possible to use functional practices even in a context where the exposed objects are in fact mutable. While your database of choice may allow you to update, insert, and delete at will, it doesn‚Äôt mean you should. DML operations like UPDATE, APPEND and DELETE are performing mutations that ripple in side-effects. Thinking of partitions as immutable blocks of data and systematically overwriting partitions is the way to make your tasks functional. A pure task should always fully overwrite a partition as its output. Your data store may not have support for physical partitions, but that does not prevent you from taking a functional approach. To work around this limitation, one option is to implement your own partitioning scheme by using different physical tables as the output of your pure tasks that can be UNIONed ALL into views that act as logical tables. Results may vary depending on how smart your database optimizer is. Alternatively, it‚Äôs also possible to logically partition a table and to systematically DELETE prior to INSERTing using a partitioning key that reflects the parameters used to instantiate the task. Note that where a TRUNCATE PARTITION is typically a ‚Äúfree‚Äù metadata operation, a DELETE operation may be expensive and that should be taken into considerations. If you decide to go that route, note that on many database engines it may more efficient to check whether the DELETE operation is needed first to avoid unnecessary locking. The staging area is the conceptual loading dock of your data warehouse, and while in the case a real physical retail-type warehouse you‚Äôd want to use a transient staging area and keep it unobstructed, in most modern data warehouse you‚Äôll want to accumulate and persist all of your source data there, and keep it unchanged forever. Given a persistent immutable staging area and pure tasks, in theory it‚Äôs possible to recompute the state of the entire warehouse from scratch (not that you should), and get to the exact same state. Knowing this, the retention policy on derived tables can be shorter, knowing that it‚Äôs possible to backfill historical data at will. Business logic and related computations tend to change over time, sometimes in retroactive fashion, sometimes not. When retroactive, you can think of the change as making existing downstream partitions ‚Äúdirty‚Äù and it calls for them to be recomputed. When non-retroactive, the ETL logic should capture the change and apply the proper logic to the corresponding time range. Logic that changes over time should always be captured inside the task logic and be applied conditionally when instantiated. This means that ideally the logic in source control describes how to build the full state of the data warehouse throughout all time periods. For example, if a change is introduced as a new rule around how to calculate taxes for 2018 that shouldn‚Äôt be applied prior to 2018, it would be dangerous to simply update the task to reflect this new logic moving forward. If someone else was to introduce an unrelated change that required ‚Äúbackfilling‚Äù 2017, they would apply the 2018 rule to 2017 data without knowing. The solution here is to apply conditional logic within the task with a certain effective date, and depending on the slice of data getting computed, the extra tax logic would apply where needed. Also note that in many cases business rules changes over time are best expressed with data as opposed to code. In those cases it‚Äôs desirable to store this information in ‚Äúparameter tables‚Äù using effective dates, and have logic that joins and apply the right parameters for the facts being processed. An oversimplified example of this would be a ‚Äútaxation rate‚Äù table that would detail the rates and effective periods, as opposed to hard-coded rates wrapped in conditional blocks of code. By nature, dimensions and other referential data is slowly changing, and it‚Äôs tempting to model this with UPSERTs (UPdating what has changed and inSERTing new dimension members) or to apply other  slowly-changing-dimension methodology  to reflect changes while keeping track of history. But how do we model this in a functional data warehouse without mutating data? Simple. With  dimension snapshots  where a new partition is appended at each ETL schedule. The dimension table becomes a collection of dimension snapshots where each partition contains the full dimension as-of a point in time.  ‚ÄúBut only a small percentage of the data changes every day, that‚Äôs a lot of data duplication!‚Äù . That‚Äôs right, though typically dimension tables are negligible in size in proportion to facts. It‚Äôs also an elegant way to solve SCD-type problematic by its simplicity and reproducibility. Now that storage and compute are dirt cheap compared to engineering time, snapshoting dimensions make sense in most cases. While the traditional type-2 slowly changing dimension approach is conceptually sound and may be more computationally efficient overall, it‚Äôs cumbersome to manage. The processes around this approach, like managing surrogate keys on dimensions and performing surrogate key lookup when loading facts, are error-prone, full of mutations and hardly reproducible. In case of very large dimensions, mixing the snapshot approach along with SCD-type methodology may be reasonable or necessary. Though in general it‚Äôs sufficient to have the current attribute only in the current snapshot, and denormalize dimension attributes into fact table where the attribute-at-the-time-of-the-event is important. For the rare cases where attribute-at-the-time-of-the-event importance was not foreseen and denormalized into fact upfront, you can always run a more expensive query that joins the facts to their time-relative dimension snapshots as opposed to the latest snapshot. You could almost think about the ‚Äúdimension snapshotting‚Äù approach as a type of further  denormalization  that has similar tradeoffs as other types of denormalization. Another modern approach to store historical values in dimensions is to use complex or nested data types. For example if we wanted to keep track of the customer‚Äôs state over time for tax purposes, we could introduce a ‚Äústate_history‚Äù column as a map where keys are effective dates and values are the state. This allows for history tracking without altering the grain of the table like a type-2 SCD requires, and is much more dynamic than a type-3 SCD since it doesn‚Äôt require creating a new column for every pice of information. Clarity around the unit of work is also important, and the unit of work should be directly aligned to output to a single partition. This makes it trivial to map each logical table to a task, and each partition to a task instance. By being strict in this area it makes it easy to directly map each partitions in your data store to its corresponding compute logic. For example, in this Airflow ‚Äútree view‚Äù where squares represents task instances of a DAG of tasks over time, it‚Äôs comforting to know that each row represents a task that corresponds to a table, and that each cell is a task instance that corresponds to a partition. It‚Äôs easy to track down the log file that correspond to any given partition. Given that tasks are idempotent, re-running any of these cells is a safe operation. This set of rule makes everything easier to maintain since a clear scope on the units of work make everything clear and maintainable. As a side note, Airflow was designed with functional data engineering in mind, and provides a solid framework to orchestrate idempotent, pure-tasks at scale. Full disclosure is due here, the author of this post happens to be the creator of Airflow, so no wonder the tooling works well with the methodology prescribed here. While you may think of your workflow as a directed acyclic graph (DAG) of tasks, and of your data lineage as a graph made of tables as nodes, the functional approach allows you to conceptualize a more accurate picture made out of task instances and partitions. In this more detailed graph, we move away from individual rows or cells being the ‚Äúatomic state‚Äù that can be mutated to a place where partitions are the smallest unit that can be changed by tasks. This data lineage graph of partitions is much easier to conceptualize to the alternative where any row could have been computed by any task. With this partition-lineage graph, things become clearer. The lineage of any given row can be mapped to a specific task instance through its partition, and by following the graph upstream it‚Äôs possible to understand the full lineage as a set of partitions and related task instances. A common pattern that leads to increased ‚Äústate complexity‚Äù is when a partition depends on a previous partition on the same table. This leads to growing complexity linearly over time. If for instance, computing the latest partition of your user dimension uses the previous partition of that same table, and the table has 3 years of history with daily snapshot, the depth of the resulting graph grows beyond a thousand and the complexity of the graph grows linearly over time. Strictly speaking, if we had to reprocess data from a few months back, we may need to reprocess hundreds of partition using a DAG that  cannot be parallelized . Given that backfills are common and that past dependencies lead to high-depth DAGs with limited parallelization, it‚Äôs a good practice to avoid modeling using past-dependencies whenever possible. In cases where cumulative-type metrics are necessary (think live-to-date customer spending as part of a customer dimension for instance), one option is to model the computation of this metric in a specialized framework or somewhat independent model optimized for that purpose. A cumulative computation framework could be a topic for an entire other blog post, let‚Äôs leave this out of scope for this post. Late arriving facts can be problematic with a strict immutable data policy. Unfortunately late arriving data is fairly common, especially in given the popularity of mobile phones and occasional instability of networks. To bring clarity around this not-so-special case, the first thing to do is to dissociate the event‚Äôs time from the event‚Äôs reception or processing time. Where late arriving facts may exist and need to be handled, time partitioning should always be done on event processing time. This allows for landing immutable blocks of data without delays, in a predictable fashion. This effectively brings in two tightly related time dimensions to your analytics and allows to do intricate analysis specific to late-arriving facts. Knowing when events were reported in relation to when they occurred is useful. It allows for showing figures like ‚Äútotal sales in February as of today‚Äù, ‚Äútotal sales in February as of March 1st‚Äù or ‚Äúhow much sales adjustments on February since March 1st‚Äù. This effectively provides a time machine that allows you to understand what reality looked like at any point in time. When defining your partitioning scheme based on event-processing time, it means that your data is not longer partitioned on event time, and means that your queries that will typically have predicates on event time won‚Äôt be able to benefit from  partition pruning  (where the database only bothers to read a subset of the partitions). It‚Äôs clearly an expensive tradeoff. There are a few ways to mitigate this. One option is to partition on both time dimensions, this should lead to a relatively low factor of partition multiplication, but raises the complexity of the model. Another option is to author queries that apply predicates on both event time and on a relatively wider window on processing time where partition pruning is needed. Also note that in some cases, read-optimized stores may not suffer much from the inability the database optimizer to skip partitions through pruning as execution engine optimizations can kick in to reach comparable performance. For example, if your engine is processing ORC or Parquet files, the execution will be limited to reading the file header before moving on to the next file. Rules are meant to be broken and in some cases it‚Äôs rational to do so. A common pattern we‚Äôve observed is to trade perfect immutability guarantees in exchange for earlier SLAs (Service License Agreement on acceptable delays make data available). Let‚Äôs take an example where we want to compute aggregates that depend on the user dimension, but that this user dimension usually lands very late in the day. Perhaps we care more about our aggregate landing early in the day than we care about accuracy. Given this preference, it‚Äôs possible to join onto the latest partition available at the time the other dependencies are met. Note that this can easily limited to a specified time range (say 2‚Äì3 partitions) to insure a minimum level of accuracy. Of course this means that re-processing the aggregation table later in the future may lead to slightly different results. While this post may be a first in formalizing the functional approach to data engineering, these practices aren‚Äôt experimental or new by any means. Large, data driven, analytically mature corporations have been applying these methods for years, and reputable tooling that prescribes this approach has been widely adopted in the industry. It‚Äôs clear that the functional approach contrasts with with decades of data-warehousing literature and practices. It‚Äôs arguable that this new approach is more aligned with how modern read-optimized databases function as they tend to use immutable blocks or segments and typically don‚Äôt allow for row-level mutations. This methodology also skews on treating storage as cheap and engineering time as expensive, duplicating data for the sake of clarity and reproducibility. At the time where Ralph Kimball authored  The Datawerouse Toolkit , databases used for warehousing were highly mutable, and data teams were small and highly specialized. Things have changed quite a bit since then. Data is larger, read-optimized stores are typically built on top of immutable blocks, we‚Äôve seen the rise of distributed systems, and the number and proportion of people partaking in the ‚Äúanalytics process‚Äù has exploded. More importantly, given that this methodology leads to more manageable and maintainable workloads, it empowers larger teams to push the boundaries of what‚Äôs possible further by using reproducible and scalable practices."
Data engineering in 2020,ata engineering in 202,"It is incredible how fast data processing tools and technologies are evolving. And with it, the nature of the data engineering discipline is changing as well. Tools I am using today are very different from what I used ten or even five years ago, however, many lessons learned are still relevant today. I have started to work in the data space long before  data engineering became a thing  and  data scientist became the sexiest job of the 21st century . I ‚Äòofficially‚Äô became a big data engineer six years ago, and I know firsthand the challenges developers with a background in ‚Äútraditional‚Äù data development have going through this journey. Of course, this transition is not easy for software engineers either, it is just different. Even though technologies keep changing ‚Äî and that‚Äôs the reality for anyone working in the tech industry ‚Äî some of the skills I had to learn are still relevant, but often overlooked by data developers who are just starting to make the transition to data engineering. These usually are the skills that software developers often take for granted. In this post, I will talk about the evolution of data engineering and what skills ‚Äútraditional‚Äù data developers might need to learn today (Hint: it is not Hadoop). Data teams before the Big Data craze were composed of business intelligence and ETL developers. Typical BI / ETL developer activities involved moving data sets from location A to location B (ETL) and building the web-hosted dashboards with that data (BI). Specialised technologies existed for each of those activities, with the knowledge concentrated within the IT department. However, apart from that, BI and ETL development had very little to do with software engineering, the discipline which was maturing heavily at the beginning of the century. As the data volumes grew and interest in data analytics increased, in the past ten years, new technologies were invented. Some of them died, and others became widely adopted, that in turn changed demands in skills and teams‚Äô structures. As modern BI tools allowed analysts and business people to create dashboards with minimal support from IT teams, data engineering became a new discipline, applying software engineering principles to ETL development using a new set of tools. Creating a data pipeline may sound easy, but at big data scale, this meant bringing together a dozen different technologies (or more!). A data engineer had to understand a myriad of technologies in-depth, pick the right tool for the job and write code in Scala, Java or Python to create resilient and scalable solutions. A data engineer had to know their data to be able to create jobs which benefit from the power of distributed processing. A data engineer had to understand the infrastructure to be able to identify reasons for failed jobs. Conceptually, many of those data pipelines were typical ETL jobs ‚Äî collecting data sets from a number of data sources, putting them in a centralised data store ready for analytics and transforming them for business intelligence or machine learning. However, ‚Äútraditional‚Äù ETL developers didn‚Äôt have the necessary skills to perform these tasks in the Big Data world. I have reviewed many articles describing what skills data engineers should have. Most of them advise learning technologies like Hadoop, Spark, Kafka, Hive, HBase, Cassandra, MongoDB, Oozie, Flink, Zookeeper, and the list goes on. While I agree that it won‚Äôt hurt to know these technologies, I find that in many cases today, in 2020, it is enough to ‚Äúknow about them‚Äù ‚Äî what particular use cases they are designed to solve, where they should or shouldn‚Äôt be used and what are the alternatives. Rapidly evolving cloud technology has given rise to a huge range of cloud-native applications and services in recent years. In the same way as modern BI tools made data analysis more accessible to the wider business several years ago, modern cloud-native data stack simplifies data ingestion and transformation tasks. I do not think that technologies like Apache Spark will become any less popular in the next few years as they are great for complex data transformations. Still, the high rate of adoption of cloud data warehouses such as Snowflake and Google BigQuery indicates that there are certain advantages they provide. One of them is that Spark requires highly specialised skills, whereas ETL solutions on top of cloud data platforms are heavily reliant on SQL skills even for big data ‚Äî such roles are much easier to fill. BI / ETL developers usually possess a strong understanding of database fundamentals, data modelling and SQL. These skills are still valuable today and mostly transferable to a modern data stack ‚Äî which is leaner and easier to learn than the Hadoop ecosystem. Below are three areas I often observe ‚Äútraditional‚Äù data developers having gaps in their knowledge because, for a long time, they didn‚Äôt have tools and approaches software engineers did. Understanding and fixing those gaps will not take a lot of time, but might make a transition to a new set of tools much smoother. SQL code is a code, and as such, software engineering principles should be applied. I am a big fan of  DBT  ‚Äî an open-source tool which brings software engineering best practices to SQL world and simplifies all these steps. It does  much more  than that so I strongly advise to check it out. 2. Good understanding of the modern cloud data analytics stack We tend to stick with the tools we know because they often make us more productive. However, many challenges we are facing are not unique, and often can be solved today more efficiently. It might be intimidating trying to navigate in the cloud ecosystem at first. One workaround is to learn from other companies‚Äô experiences. Many successful startups are very open about their data stack and the lessons they learnt on their journey. These days, it is common to adopt a version of a cloud data warehouse and several other components for data ingestion (such as Fivetran or Segment) and data visualisation. Seeing a few architectures is usually enough to get a 10,000-foot view and know what to research further when needed ‚Äî e.g. dealing with events or streaming data might be an entirely new concept. 3. Knowing a programming language in addition to SQL As much as I love Scala, Python seems to be a safe bet to start with today. It is reasonably easy to pick up, loved by data scientists and supported pretty much by all components of cloud ecosystems. SQL is great for many data transformations, but sometimes it is easier to parse complex data structure with Python before ingesting it into a table or use it to automate specific steps in the data pipeline. This is not an exhaustive list, and different companies might require different skills, what brings me to my last point ‚Ä¶ Data processing tools and technologies have evolved massively over the last few years. Many of them have evolved to the point where they can easily scale as the data volume grows while working well with the ‚Äúsmall data‚Äù too. That can significantly simplify both the data analytics stack and the skills required to use it. Does it mean that the role of a data engineer is changing? I think so. It doesn‚Äôt mean it gets easier ‚Äî business demands grow as technology advances. However, it seems that this role might become more specialised or split into a few different disciplines. New tools allow data engineers to focus on core data infrastructure, performance optimisation, custom data ingestion pipelines and overall pipeline orchestration. At the same time, data transformation code in those pipelines can be owned by anyone who is comfortable with SQL. For example,  analytics engineering  is starting to become a thing. This role sits at the intersection of data engineering and data analytics and focuses on data transformation and data quality. Cloud data warehouse engineering is another one. Regardless of whether the distinction in job titles will become widely adopted or not, I believe that ‚Äútraditional‚Äù data developers possess many fundamental skills to be successful in many data engineering related activities today ‚Äî strong SQL and data modelling are some of them. By understanding the modern cloud analytics data stack and how different components can be combined together, learning a programming language and getting used to version control, this transition can be reasonably seamless."
Data Engineering Roadmap For 2021,ata Engineering Roadmap For 202,"Maybe it‚Äôs the 6 figure salaries, the opportunity to work with cool technology or people are finally learning that data engineering is where everything starts in the data field. Whatever the reason, people are noticing."
5 Data Engineering Projects To Add to Your Resume, Data Engineering Projects To Add to Your Resum,"All signs point towards an auspicious future for data engineering. Dice‚Äôs 2020 tech jobs report cites data engineering as the fastest-growing field in 2020, increasing by a staggering 50%,  while data science roles only increased by 10% . You can rest assured that the influx of‚Ä¶"
The New Data Engineering Stack,he New Data Engineering Stac,"This article was last updated on 22 July 2021. R emember when the software development industry realised that a single person could take on multiple technologies glued tightly with each other and came up with the notion of a Full Stack Developer ‚Äî someone who does data modelling, writes backend code and also does front end work. Something similar has happened to the data industry with the birth of a Data Engineer almost half a decade ago. For many, the Full Stack Developer remains  a mythical creature  because of the never-ending list of technologies that cover frontend, backend, and data. A complete Data Engineer, on the other hand, doesn‚Äôt sound as far-fetched or mythical. One of the reasons for that could be that visualisation (business intelligence) has become a massive field in its own right. A Data Engineer is supposed to build systems to make data available, make it useable, move it from one place to another, and so on. Although many companies want their data engineers to do visualisations, it is not a common practice. Still, the BI skillset is definitely good to have for a Data Engineer. Here, I am going to talk about  the technologies which are too important  to ignore. You don‚Äôt have to master all of them. That‚Äôs not possible, anyway. It is important to be aware and somewhat skilled at most of these technologies to do good things in the data engineering space. Don‚Äôt forget that newer technologies will keep coming, and older technologies, at least some of them, will keep moving out. The philosophy of listing these technologies comes from a simple idea borrowed from the investing world ‚Äî  where the world is going . Even with the bursting on the scene of many unconventional databases, the first thing that comes to mind when we talk about databases is still relational databases and SQL. All relational databases, more or less, work in the same way. Internal implementation differs, obviously. It‚Äôs more than enough to be skilled in one or two of the four major relational databases ‚Äî  Oracle ,  MySQL , Microsoft SQL Server, and PostgreSQL. I haven‚Äôt heard of a company that works without relational databases, no matter how advanced and complex their systems are. The Big Four ‚Äî Oracle, MySQL, MS SQL Server, PostgreSQL. This website  maintains the database engine ratings for all kinds of databases‚ÄîHead over to see what kind of databases companies are using these days. OLTP-based relational databases are, by definition, meant for transactional loads. For analytical loads, data lakes, data warehouses, data marts, there‚Äôs another list of databases. In theory, you can create data warehouses using OLTP databases, but at scale, it never ends well. Been there, done that. Data warehouses have a different set of database management systems, the most popular out of which are Google BigQuery, Amazon Redshift, Snowflake, Azure Data Warehouse,   etc. The choice of a data warehouse usually defaults to the cloud service provider a company is using. For instance, if a company‚Äôs infrastructure is on AWS, they‚Äôd surely want to use Amazon Redshift as their data warehouse to reduce friction. The Big Four ‚Äî BigQuery, Redshift, Snowflake, Azure DW. Having said that, there are good chances that the future of the cloud will not be  a cloud.  It will probably be  multi-cloud , which means companies would be able to choose their data warehouses almost irrespective of where their existing infrastructure is without worrying too much about inter-cloud friction. Different use cases require different solutions. Geospatial data requires geospatial databases like PostGIS; time-series data sometimes requires specialised time-series databases like InfluxDB or TimescaleDB. Document-oriented databases, key-value stores have made their place in the database ecosystem by offering something that relational databases had struggled to offer for the longest period of time, i.e., the ability to store, retrieve and analyse semi-structured and unstructured data efficiently. The Big Eight ‚Äî MongoDB, InfluxDB, neo4j, Redis, Elasticsearch, CosmosDB, DynamoDB, Cloud Datastore. Then there are graph databases, in-memory data stores, and full-text search engines ‚Äî which are solutions for particular problems. It isn‚Äôt easy to choose from hundreds of databases, but these are the major ones. The ones I have left out are probably close seconds of these eight. towardsdatascience.com With the mainstreaming of cloud computing with cloud service providers like AWS, Azure, and Google Cloud, infrastructure has been democratised to a great degree. Smaller companies don‚Äôt have to worry about CapEx incurred from infrastructure anymore. It couldn‚Äôt have been more of a blessing for data engineering that a host of exceptional services by all the major providers are available, which charge on a pay-what-you-use basis. Companies have moved to the serverless computing model where the infrastructure is up only when the compute & memory are needed. Persistent storage is a separate service. The Big Three ‚Äî Google Cloud, Azure, AWS. For a data engineer, it‚Äôs important to know all the major data-related cloud services provided by at least one of the three cloud providers. We‚Äôll take the example of AWS. If you‚Äôre a Data Engineer who‚Äôs supposed to be working on AWS, you should know about S3 & EBS (for storage), EC2 & EMR (for computing & memory), Glue & Step Functions & Lambda (for orchestration) and more. The same goes for other cloud providers. aws.amazon.com. Airflow has been the obvious choice for an orchestrator in the last two to three years for more engineering-centric teams. Cloud platforms have their own orchestrators. For instance, with AWS, you can use a mix of Glue, Step Functions and Lambda. Google Cloud offers implemented a fully-managed cloud version of Airflow called Cloud Composer. Azure also offers similar services. The Big One ‚Äî Airflow. cloud.google.com. Some of the old school orchestration, workflow engines and ETL tools have adapted well and are still relevant. For instance, Talend is still used widely as an orchestrator. This brings us to the much-dreaded ETL. All things considered, SQL has been the best option for doing ETL to date. Recently, many other technologies like Spark have come into space where more compute & memory gives you quicker results by exploiting the principles of MPP computing. azure.microsoft.com. Traditionally, ETL has been done mostly using proprietary software, but those days are long gone now. More open-source toolkits are available in the market to be used by the community. There‚Äôs also a host of fully managed ETL solutions provided by companies dedicated to data integration and ETL. Some of them are Fivetran, Panoply and Stitch. Most of these tools are purely scheduled or triggered SQL statements getting data from one database and inserting it into another. This is easily achievable by using Airflow (or something similar). The Big Two ‚Äî SQL, Spark. fivetran.com. Fishtown Analytics‚Äôs dbt  is one of the only tools that concentrate on solving the Transformation layer problems in the ETL. The fact that dbt is completely SQL-based makes it so attractive to use. I‚Äôm looking forward to having cloud dbt services by the major cloud providers. Something might already be in the works. The DevOps space has split into three in the past couple of years ‚Äîcore DevOps, DataOps and DevSecOps. Data Engineers are expected to know their infrastructure now. This means that whatever stack they are using, they should resolve operational issues concerning the infrastructure ‚Äî databases, data pipelines, data warehouses, orchestrators, storage, etc. For provisioning infrastructure and maintenance, there are several cloud platform-independent tools like Pulumi and Terraform are available in the market. Platform-specific tools like CloudFormation (for AWS) have also seen wide acceptance. The Big Two ‚Äî Terraform, Pulumi. If you have drunk the kool-aid of a multi-cloud future, it‚Äôs better to know at least one of the two aforementioned Infrastructure-as-Code tools. IaC comes with its own benefits like the ease of implementing immutable infrastructure, increased speed of deployment and so on. towardsdatascience.com Whether deploying infrastructure or SQL scripts, or Spark code, continuous integration and continuous deployment pipeline are standard ways to do it. Gone are the days (or gone should be the days) when engineers used to have access to the machines, and they‚Äôd log in to a database and execute the DDL for a stored procedure on the database server. The Big Four ‚Äî Jenkins, AWS CodePipeline, Google Cloud Build, Azure DevOps. Many have realised the risk of doing that, unfortunately after many years of having suffered from unintended human errors. The whole point of the data engineering exercise is to make the data available to data scientists, data analysts, and business people. Without proper testing, any project is at risk of catastrophic failure. Manual testing of data is highly inefficient, and, honestly, it isn‚Äôt doable at scale. The Big Two ‚Äî Pytest, JUnit. So the best way out is to automate the tests. Any of the automation test frameworks available for testing backend code also works for testing Data Engineering components. You can also use tools like dbt for automation testing. Otherwise, widely used tools like Cucumber, Gherkin for BDD are available. Pytest, JUnit and others can also be used. I have already written about source control for SQL. I don‚Äôt want to repeat all the information I had shared in the other piece, so I am just sharing the link here. towardsdatascience.com Source control everything. The pipelines, the database DDLs, the orchestrator code, test cases. Everything. Although Python should be the obvious answer to  which language data engineers use , there is a host of technologies built on Java & Scala. The whole Hadoop ecosystem is based on Java. Talend, the orchestrator + ETL tool, is also written in Java. Not everyone is required to know both languages, though. Most widely used technologies have a wrapper for the other language to make the product more acceptable. The most common example of this is PySpark which allows Data Engineers to use Python to interact with Spark. The Big Three ‚Äî SQL, Python, Java. The same can be said for SQL. If there was one language that data engineers should understand, it is SQL. After all, it is the language data speak. towardsdatascience.com A Data Engineer is not just an ETL person now. They‚Äôre not just a database person either. A Data Engineer is an amalgamation of all the things we have talked about in this piece and maybe some more. Again, remember that mastery of all these technologies is impossible, but one can certainly be aware and skilled in some of these. That‚Äôs what is the need of the hour. And this will probably be the case for the next couple of years. P.S.  ‚Äî Some of the readers have shared some ideas in the comments, some technologies that I might have missed and some general disagreements about  where the world is going  in data engineering. Thanks, everyone, for the suggestions! I‚Äôll update the article based on the comments wherever relevant as soon as possible."
Data Scientists Without Data Engineering Skills Will Face the Harsh Truth,ata Scientists Without Data Engineering Skills Will Face the Harsh Trut,"You have probably read an article about the difference between a data scientist and a data engineer. I always thought the distinction was clear. Data engineers make the data ready for use and then data scientists work on that data. However, my opinion on this distinction has changed dramatically after I started working as a data scientist. Everything in data science starts with data. Your machine learning model is just as good as the data fed into it. Garbage in, garbage out! A data scientist cannot do some magic to create a valuable product without proper data. The proper data is not always readily available for data scientists. In most cases, it will the responsibility of the data scientist to convert the raw data to a proper format. Unless you work for a big tech company that has separate teams of data engineers and data scientists, you should possess the ability and skills to handle some data engineering tasks. These tasks cover a broad range of operations and I will elaborate on this in the remaining part of the article. I would like to state my opinion on the relationship between the job of a data engineer and a data scientist. A data engineer is a data engineer. A data scientist should be both a data scientist and a data engineer. It may seem like an arguable statement. However, I would like to emphasize that my opinion was different before I started working as a data scientist. I used to think of data engineers and data scientists as separate entities. In the remaining part of the article, I will try to explain what I mean by a data scientist should be both a data scientist and a data engineer. For instance, data engineers do a set of operations known as ETL (extract, transform, load). It covers the procedures for collecting data from one or more sources, apply some transformations, and then load into a different source. I would definitely not be surprised if a data scientist is expected to perform ETL operations. Data science is still evolving and most companies do not have clearly separated data engineer and data scientist roles. As a result, a data scientist should be able to perform some data engineering tasks. If you expect to only work on running machine learning algorithms with ready-to-use data, you will face the harsh truth soon after you start working as a data scientist. You may have to write some stored procedures in SQL to preprocess the client data. It is also possible that you receive the client data from a few different sources. It will be your job to extract and combine them. Then, you will need to load them into a single source. In order to write efficient stored procedures, you need extensive SQL skills. The transform part of ETL procedures involves in many data cleaning and manipulation steps. SQL may not be the best choice if you work with large-scale data. Distributed computing is a better alternative in such cases. Therefore, a data scientist should also be familiar with distributed computing. Your best friend in distributed computing might be Spark. It is an analytics engine used for large-scale data processing. We can distribute both data and computations over clusters to achieve a substantial performance increase. If you are familiar with Python and SQL, you won‚Äôt have hard time getting used to Spark. You can use Spark features with PySpark which is a Python API for Spark. When it comes to work with clusters, the optimal environment is the cloud. There are various cloud providers but AWS, Azure, and Google Cloud Platform (GCP) lead the way. Although the PySpark code is the same for all cloud providers, how you setup the environment and create clusters change between them. They allow for creating clusters using both scripts or the user interface. Distributed computing over clusters is a whole different world. It is nothing like doing analysis in your computer. It has very different dynamics. Evaluating cluster performance and choosing the optimal number of workers for a cluster will be your predominant concerns. Long story short, data processing will be a substantial part of your job as a data scientist. By substantial, I mean more than 80% of your time. Data processing is not just cleaning and manipulating the data. It also involves ETL operations which are thought to be the job of a data engineer. I strongly recommend getting familiar with ETL tools and concepts. It would of great help if you have a chance to practice them. It would be a naive assumption to think you will only work on machine learning algorithms as a data scientist. It is an important task too but it will only consume a small part of your time. Thank you for reading. Please let me know if you have any feedback."
Data Engineering 101: Writing Your First Pipeline,ata Engineering 101: Writing Your First Pipelin,"One of the main roles of a data engineer can be summed up as getting data from point A to point B. We often need to pull data out of one system and insert it into another. This could be for various purposes. This includes analytics, integrations, and machine learning. But in order to get that data moving, we need to use what are known as ETLs/Data pipelines. These are processes that pipe data from one data system to another. One question we need to answer as data engineers is how often do we need this data to be updated. This is where the question about batch vs. stream comes into play. These are the two main types of ETLs/ELTs that exist. For a very long time, almost every data pipeline was what we consider a batch pipeline. This means that the pipeline usually runs once per day, hour, week, etc. There‚Äôs some specific time interval, but the data is not live. Batch jobs refers to the data being loading in chunks or batches rather than right away. Thus the term batch jobs as the data is loaded in batches. Compare this to streaming data where as soon as a new row is added into the application database it‚Äôs passed along into the analytical system. This is usually done using various forms of Pub/Sub or event bus type models. All these systems allow for transactional data to be passed along almost as soon as the transaction occurs. Some might ask why we don‚Äôt just use streaming for everything. Isn‚Äôt it better to have live data all the time? In some regard this is true. But oftentimes creating streaming systems is technically more challenging, and maintaining it is also difficult. Whereas while batch jobs run at normal intervals could fail, they don‚Äôt need to be fixed right away because they often have a few hours or days before they run again. In comparison, a streaming system is live all the time. Failures and bugs need to be fixed as soon as possible. For now, we‚Äôre going to focus on developing what are traditionally more batch jobs. Besides picking your overall paradigm for your ETL, you will need to decide on your ETL tool. If you just want to get to the coding section, feel free to skip to the section below. But we can‚Äôt get too far in developing data pipelines without referencing a few options your data team has to work with. There are plenty of data pipeline and workflow automation tools. Let‚Äôs break them down into two specific options. Drag and drop vs. frameworks. Drag and drop options offer you the ability to know almost nothing about code ‚Äî this would be like SSIS and Informatica. These are great for people who require almost no custom code to be implemented. Although many of these tools offer custom code to be added, it kind of defeats the purpose. If your team is able to write code, we find it more beneficial to write pipelines using frameworks as they often allow for better tuning. Although Informatica is pretty powerful and does a lot of heavy lifting as long as you can foot the bill. Even so, many people rely on code-based frameworks for their ETLs (some companies like Airbnb and Spotify have developed their own). These frameworks are often implemented in Python and are called  Airflow and Luigi . Both of these frameworks can be used as workflows and offer various benefits. But for now, let‚Äôs look at what it‚Äôs like building a basic pipeline in Airflow and Luigi. In later posts, we will talk more about design. But for now, we‚Äôre just demoing how to write ETL pipelines. In order to make pipelines in Airflow, there are several specific configurations that you need to set up. There is a set of arguments you want to set, and then you will also need to call out the actual DAG you are creating with those default args. See the config below. This is just the base of your DAG. You can set things like how often you run the actual data pipeline ‚Äî like if you want to run your schedule daily, then use the following code parameters. For example, you can use schedule_interval=‚Äô@daily‚Äô . Or you can use cron instead, like this:  schedule_interval=‚Äô0 0 * * *‚Äô . Once you have set up your baseline configuration, then you can start to put together the operators for Airflow. Operators are essentially the isolated tasks you want to be done. This could be extracting data, moving a file, running some data transformation, etc. For example, if you look below we are using several operators. These include the PythonOperator and BashOperator. This allows you to run commands in Python or bash and create dependencies between said tasks. We do go a little more in-depth on Airflow pipelines here. But this is the general gist of it. You can continue to create more tasks or develop abstractions to help manage the complexity of the pipeline. But the usage above of the Airflow operators is a great introduction. Now onto Luigi. Luigi is another workflow framework that can be used to develop pipelines. In some ways, we find it simpler, and in other ways, it can quickly become more complex. The reason we personally find Luigi simpler is because it breaks the main tasks into three main steps. These can be seen in what Luigi defines as a ‚ÄúTask.‚Äù Within a Luigi Task, the class three functions that are the most utilized are requires(), run(), and output(). What do each of these functions do in Luigi? The requires() is similar to the dependencies in airflow. You are essentially referencing a previous task class, a file output, or other output. For example: In this case, the requires function is waiting for a file to land. But it could also wait for a task to finish or some other output. Not every task needs a requires function. But it can be used to reference a previous task that needs to be finished in order for the current task to start. But tasks do need the run() function. The run() function is essentially the actual task itself. What do you want to get done? For example: The output of a task is a target, which can be a file on the local filesystem, a file on Amazon‚Äôs S3, some piece of data in a database, etc. You can see the slight difference between the two pipeline frameworks. Airflow is wrapped up in one specific operator whereas Luigi is developed as a larger class. At the end of the day, this slight difference can lead to a lot of design changes in your pipeline. Personally, we enjoy Airflow due to a larger community. However, in many ways, Luigi can have a slightly lower bar to entry as far as figuring it out. There aren‚Äôt a lot of different operators that can be used. Instead, you decide what each task really does. This can allow a little more freedom but also a lot more thinking through for design and development. So in the end, you will have to pick what you want to deal with. Regardless of the framework you pick, there will always be bugs in your code. Good luck, and thanks for reading!"
"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture, and Unifield architecture"," big data architectures, Data Streaming, Lambda architecture, Kappa architecture, and Unifield architectur","Although data analysis is hidden behind the business system, it has a very important role. The results of data analysis play a pivotal role in decision-making and business development. With the development of big data technology, the exposure of proper terms such as data mining and data exploration is‚Ä¶"
Enterprise Data Architecture,nterprise Data Architectur,"Job ad: ‚ÄúWanted: truck driver to drive a trailer load of tropical plants from Atlanta to St. Petersburg. Must know optimal planting conditions, desired soil characteristics, drought tolerance, and disease resistance of each of the 65 plant species on board.‚Äù Why data architecture?"
Big Data Architecture in Data Processing and Data Access,ig Data Architecture in Data Processing and Data Acces,"I started my career as an Oracle database developer and administrator back in 1998. Over the past 20+ years, it has been amazing to see how IT has been evolving to handle the ever growing amount of data, via technologies including relational OLTP (Online Transaction Processing) database, data warehouse, ETL (Extraction, Transformation and Loading) and OLAP (Online Analytical Processing) reporting, big data and now AI, Cloud and IoT. All these technologies were enabled by the rapid growth in computational power, particular in terms of processors, memory, storage, and networking speed. The objective of this article is to summarize, first, the underlying principles on how to handle large amounts of data and, second, a thought process that I hope can help you get a deeper understanding of any emerging technologies in the data space and come up with the right architecture when riding on current and future technology waves. In a data pipeline, data normally go through 2 stages: Data Processing and Data Access. For any type of data, when it enters an organization (in most cases there are multiple data sources), it is most likely either not clean or not in the format that can be reported or analyzed directly by the eventual business users inside or outside of the organization. Data Processing is therefore needed first, which usually includes data cleansing, standardization, transformation and aggregation. The finalized data is then presented in the Data Access layer ‚Äî ready to be reported and used for analytics in all aspects. Data Processing is sometimes also called Data Preparation, Data Integration or ETL; among these, ETL is probably the most popular name. Data processing and data access have different goals, and therefore have been achieved by different technologies. Data Processing for big data emphasizes ‚Äúscaling‚Äù from the beginning, meaning that whenever data volume increases, the processing time should still be within the expectation given the available hardware. The overall data processing time can range from minutes to hours to days, depending on the amount of data and the complexity of the logic in the processing. On the other hand, data access emphasizes ‚Äúfast‚Äù response time on the order of seconds. On a high level, the scalability of data processing has been achieved mostly by parallel processing, while fast data access is achieved by optimization of data structure based on access patterns as well as increased amounts of memory available on the servers. In order to clean, standardize and transform the data from different sources, data processing needs to touch every record in the coming data. Once a record is clean and finalized, the job is done. This is fundamentally different from data access ‚Äî the latter leads to repetitive retrieval and access of the same information with different users and/or applications. When data volume is small, the speed of data processing is less of a challenge than compared to data access, and therefore usually happens inside the same database where the finalized data reside. As the data volume grows, it was found that data processing has to be handled outside of databases in order to bypass all the overhead and limitations caused by the database system which clearly was not designed for big data processing in the first place. This was when ETL and then Hadoop started to play a critical role in the data warehousing and big data eras respectively. The challenge of big data processing is that the amount of data to be processed is always at the level of what hard disk can hold but much more than the amount of computing memory that is available at a given time. The fundamental way of efficient data processing is to break data into smaller pieces and process them in parallel. In another word, scalability is achieved by first enabling parallel processing in the programming such that when data volume increases, the number of parallel processes will increase, while each process continues to process similar amount of data as before; second by adding more servers with more processors, memory and disks as the number of parallel processes increases. Parallel processing of big data was first realized by data partitioning technique in database systems and ETL tools. Once a dataset is partitioned logically, each partition can be processed in parallel. Hadoop HDFS (Highly Distributed File Systems) adapts the same principle in the most scalable way. What HDFS does is partition the data into data blocks with each block of a constant size. The blocks are then distributed to different server nodes and recorded by the meta-data store in the so called Names node. When a data process kicks off, the number of processes is determined by the number of data blocks and available resources (e.g., processors and memory) on each server node. This means HDFS enables massive parallel processing as long as you have enough processors and memory from multiple servers. Currently Spark has become one of the most popular fast engine for large-scale data processing in memory. Does it make sense? While memory has indeed become cheaper, it is still more expensive than hard drives. In the big data space, the amount of big data to be processed is always much bigger than the amount of memory available. So how does Spark solve it? First of all, Spark leverages the total amount of memory in a distributed environment with multiple data nodes. The amount of memory is, however, still not enough and can be costly if any organization tries to fit big data into a Spark cluster. Let‚Äôs consider what type of processing Spark is good for. Data processing always starts with reading data from disk to memory, and at the end writing the results to disks. If each record only needs to be processed once before writing to disk, which is the case for a typical batch processing, Spark won‚Äôt yield advantage compared to Hadoop. On the other hand, Spark can hold the data in memory for multiple steps for data transformation while Hadoop cannot. This means Spark offers advantages when processing iteratively on the same piece of data multiple times, which is exactly what‚Äôs needed in analytics and machine learning. Now consider the following: since there could be tens or hundreds of such analytics processes running at the same time, how to make your processing scale in a cost effective way? Clearly, simply relying on processing in memory cannot be the full answer, and distributed storage of big data, such as Hadoop, is still an indispensable part of the big data solution complementary to Spark computing. Another hot topic in data processing area is Stream processing. It offers great advantage in reducing processing speed because at a given point of time it only needs to process small amount of data whenever the data arrives. However, it is not as versatile as batch processing in 2 aspects: the first is that the input data needs to come in a ‚Äústream‚Äù mode, and the second is that certain processing logic that requires aggregation across time periods still need to be processed in batch afterwards. Lastly Cloud solutions provide the opportunity to scale the distributed processing system in a more dynamic fashion based on data volume, hence, the number of parallel processes. This is hard to achieve on premise within an enterprise because new servers need to be planned, budgeted and purchased. If the capacity is not planned well, the big data processing could be either limited by the amount of hardware, or extra purchase leads to wasted resources without being used. Processing on Cloud gains the big advantage of infrastructure elasticity which can give more guarantee to achieve the best scale in a more cost effective fashion. As compared to data processing, data access has very different characteristics, including: Given the above principles, there have been several milestones in the past 2 decades that reflect how to access the ever increasing amount of data while still returning the requested data within seconds: Below table gives some popular examples of each database type, but not intent to give a full list. Note that a database may combine more than 1 technologies. For example, Redis is a NoSQL database as well as in memory. In addition, data retrieval from Data Warehouse and Columnar Storages leverages parallel processes to retrieve data whenever applicable. Because there could be many choices of different types of databases depending on data content, data structure and retrieval patterns by users and/or applications, Data Access is an area an organization needs to evolve quickly and constantly. It should be also common to have different types of databases or tools at the same time for different purposes. As we can see, a big distinction between data processing and data access is that data access ultimately comes from customers‚Äô and business‚Äôs needs, and choosing the right technology drives future new product developments and enhances users experience. On the other hand, data processing is the core asset of a company, and processing in scale and producing good quality of data is the essential enabler for a company to grow with its data. Many companies experience the stalking of their data processing system when data volume grows, and it is costly to rebuild a data processing platform from scratch. The principle of parallel data processing and scalability need to be carefully thought through and designed from the beginning. Data Processing also goes hand in hand with data management and data integration ‚Äî all 3 are essential for the success of any data intensive organization. Furthermore, every organization is now facing many choices of big data solutions from both open source communities and third-party vendors. A clear understanding of the differences between data processing and data access can enable IT and business leaders to not only build a solid data architecture, but also make the right decisions to expand and modernize it at a steady pace."
Modern Unified Data Architecture,odern Unified Data Architectur,"Today, most business value is derived from the analysis of data and products powered by data, rather than the software itself. The data generated by several application silos are combined and greatly enhanced to provide a better customer experience. Deriving value from the data includes building a unified data architecture and a collaborative effort of data engineering and data science teams. Data engineering involves building and maintaining the data infrastructure and data pipelines, and Data science involves transforming crude data into something useful and deriving insights through analytical and ML workloads. Modern unified data architecture includes infrastructure, tools and technologies that create, manage and support data collection, processing, analytical and ML workloads. Building and operating the data architecture in an organization require deployments to cloud and colocations, use of several technologies (open source and proprietary) and languages (python, sql, java), and involves different skilled resources (engineers, scientists, analysts, admins). It is cost-effective to have a centralized data infrastructure to avoid duplication of data and efforts as well as to maintain a single source of truth in the organization for efficient usage. Many organizations generate, process and store massive amounts of data regularly for business analysis and operations. The challenges faced by big data analytical and processing applications are summarized as 3Vs, 5Vs, 7Vs or even more. In this article, I consider the 7 key challenges of modern data architectures: As shown in Figure 1., these challenges are surfaced at different stages as the data flows through the modern big data architectures. To address these challenges, the separation of data ingestion, processing, storage, ML modeling and consumers into separate isolated components makes it possible to independently repair, scale or replace resources in these stages without impacting others. Data Producers Data producers generate data in a variety of ways in structured, unstructured or semi-structured format. Data producers can be transactional applications and operational systems that generate relational data, or they can be social media mobile apps, IoT devices, clickstreams or log files that generate non-relational data. The data sources can have different data mutation rates ‚Äî data that comes from OLTP transaction applications experience heavy write operations and data that arrive from other OLAP systems can experience heavy read but low write operations. Data produced from relational databases typically have static schemas whereas distributed non-relational data stores have dynamic schemas. Data produced by dissimilar systems arrive in different formats such as json, csv, parquet, avro etc. Data Ingestion The huge volume of data generated by the providers is ingested into big data system through various techniques such as batch ingestion, micro-batches, change data capture, publish-subscribe, sync-async, and stream ingestion. Both push and pull mechanism of data extraction is employed along with features such as ordering, message delivery guarantees, delivery confirmation, message retention, message aging and watermarking. The data architecture should effectively handle the performance, throughput, failure rate requirements and avoid throttling in the system. Data ingestion through massive batch processing is used for complex processing and deep analysis; real-time streaming is used for quick feedback and anomaly detections. Typically, batch ingestions at scheduled intervals have predictable workloads and on-the-fly batch ingestions have unpredictable workloads. For stream ingestions, the data should be query-able as soon as it enters the system and provides immediate actionable insights. Data Processing Data processing involves various methods such as cleansing, profiling, validating, enriching, and aggregating datasets. It involves data modeling and mapping source-destination schemas. The data architecture should support both schema enforcement to avoid inadvertent changes (schema-on-write) and at the same time offer flexibility to modify schemas (schema-on-read) as the requirements evolve. For slow-moving datasets, batch processing techniques are employed to churn large datasets, perform complex transformations and generate deep insights. Previously, batch processing used to be long-running jobs, but lower latencies are possible by the use of distributed massive parallel processing engines such as Spark. For fast-moving datasets, real-time streaming techniques such as aggregating and filtering on rolling time-windows are employed to generate immediate insights by the use of Spark streaming or Flink. Languages such as python, java, scala and sql are predominantly used for data processing. Previously, Lambda/Kappa architectures provide unified analytics but separate paths for batch and real-time processing resulting in duplicate resources and effort. However, with modern architectures through the use of frameworks such as databricks, it is possible to combine batch and real-time processing into a single path. As the count and complexity of data processing jobs increases, complex DAGs (Directed Acyclic Graphs) and efficient pipelines can be built using workflow tools such as Airflow, Nifi, Luigi, etc. along with virtualization container services such as Docker or Kubernetes. As data velocity changes, processing jobs should scale elastically to handle data bursts and data accelerations due to a sudden spike in usage or demand. Data Storage The data architecture should effectively manage the massive amounts of data processed and stored in the system through distributed storage, object stores and purpose-built storage options (nosql db, columnar db, timeseries db etc. ). Previously multi-cluster distributed Hadoop systems have combined storage and compute at each node. However, modern solutions decouple storage from compute so the same data can be analyzed with variety of compute engines. Decoupled storage employs efficient columnar data indexing and compression techniques. Centralized storage avoids duplication of data copies distributed across multiple systems and provides better access control to users. Cloud data lakes are essential components in any modern data solutions and store unlimited amounts of data. The fundamental challenge with data lakes are they are typically append-only and updating records is hard. Delta lakes and HUDI solutions solve this challenge by bringing ACID properties to data lakes. The performance of data processing is improved through properly configuring settings such as partition, vacuuming, compaction, shuffling, etc. ML Modeling After the datasets are prepared by the pipelines built by the data engineers, the data scientists will perform further curation, validation and labeling the data for feature engineering and model building. Scaling out data preparation is not the same as scaling out ML models. Scaling out ML is hard and training models are typically not multithreaded. Once the ML models are trained then the models are deployed at scale on multiple nodes, and the inference endpoints are generated to provide predictions. The MLOps and DevOps will help the data engineers and data scientists to manage and automate the end-to-end ML workflows. The modern data architecture supports MLOps practices to enable automation and traceability of model training, testing, hyper-parameter updates and experiments so that ML models are deployed in production at scale. For tracking experiments and deploying ML models, open-source tools such as MLflow or Kubeflow are used. Deploying a model to the production is not the end. The models are continuously monitored for any drifts in data and model accuracy. When any decline in model quality is detected, then the data received by the model are captured and compared with the training datasets. The models are retrained, redeployed to production and inference endpoints are updated again, and this process continues for the ML lifecycle. The effectiveness of model deployments to production are improved using shadow deployments, canary deployments and A/B testing. Data Consumers At the end of the data and ML pipelines, the value of the data and data architecture is derived by the data consumers, harnessing data through analytical services, data science, and operational products. After all the processing, crunching and mining of data is performed, the goal is to provide actionable insights of value through interactive exploratory analysis, reports, visualization, data science and statistical modeling, so business can make evidence-based data-driven decisions. Depending upon the analytical maturity of the use cases, descriptive, predictive and prescriptive analysis are performed. Rich support for languages, query engines and libraries are available for analysis. Typical languages used for analysis are sql, python and R. Big data query engines such as hive, spark sql, cassandra cql, impala etc and Search engines such as Solr, Lucene, Elastic Search etc., are used. Data scientists use libraries such as pandas, matplotlib, numpy, scipy, scikitlearn, tensorflow, pytorch etc. Data powered applications support live operations of the business through APIs and microservices from the data platform. The data products can use the APIs that are built upon the data stores to provide enriched information or they can be referential ML endpoints that provide predictions and recommendations. Figure 2 shows the various open-source and proprietary products available at each stage in building modern data architecture. Metadata Management Metadata management includes data cataloging, data relationship and data lineage techniques. Data cataloging offers smarter data scanning methods to automatically deduce data structures and mappings. It describes the data traits such as quality, lineage and profile statistics of a dataset. The data architecture should allow users to append tagging and keywords to easily search data assets. Data architecture can provide enhanced features such as automatically exposing correlations, data corruptions, joins, relationships and predictions within the data. As the variety and number of datasets increases, the metadata of big data applications can itself become so large that the data architecture should include search tools for data discovery and serve as data inventory. Scalable metadata management is required for democratized data access and self-service management. Data Quality & Integrity Data Quality is to ensure an accurate, complete and consistent record of data is maintained over its entire flow through different pipeline stages as well as its lifecycle. This ensures that the data is reliable and trustworthy for planning, decision making and operations. To ensure integrity of the data, we need to have full traceability and lineage information when the data enters the system and through all stages till the data reaches the consumer end points. Several basic techniques can be employed to validate the data integrity between source and destination datasets at each processing step such as comparing rowCounts, nullCounts, uniqueCounts, and md5 checkSums. Data corruptions can be detected and corrected by ensuring that referential integrity, entity relations and constraints of datasets are defined and met. Data integrity is maintained by providing selective update access only for authorized users and services, establishing data governance policies and employing data stewards. Data Security The data architecture should provide stringent security, compliance, privacy and protection mechanisms for data in all the different layers. Only authenticated and authorized users or services can access the data. PII information should be masked and hashed out. Modern data architectures provide automatic anonymization when patterns such as email, ssn, and credit card are detected. Data encryption methods are applied for data at rest and for data in transit. Observability and site reliability engineering methods are employed for auditing and alert mechanisms. Modern data solutions utilize CI/CD and DevOps to manage and automate deployments and changes to the system by including build systems like Jenkins, configuration management systems like Puppet or Chef, and containers such as Dockers or Kubernetes. The major cloud providers (AWS, Azure and Google) offer end-to-end solutions to build unified integrated data architecture. In Figure 3, each of the stages is mapped to the services offered by the major cloud providers. The big data unified architecture has a plethora of tools and technologies available today and this is an area where rapid changes are happening. Each of these tools and technologies has certain strengths that make them the right choice for a particular scenario, however, they could be a terrible selection for a different use case. Hence for tool selection, understanding your organization‚Äôs use case and requirements are important, to begin with. Then follows the evaluation and experimentation of tools with clear and time-bound goals, before picking the right tool. In this space, open-source technologies and services offered by major cloud providers (AWS, Azure, GCP) are generally preferred rather than being locked to proprietary vendor solutions. This space it continuously evolving, so identifying the right technologies, and being flexible to change and iterate are important to meet your business needs and build a competitive advantage. References :"
Fundamentals of Data Architecture to Help Data Scientists Understand Architectural Diagrams Better,undamentals of Data Architecture to Help Data Scientists Understand Architectural Diagrams Bette,"Within a company using data to derive business value, although you may not be appreciated with your data science skills all the time, you always are when you manage the data infrastructure well. Everyone wants the data stored in an accessible location, cleaned up well, and updated regularly. Backed up by these unobtrusive but steady demands, the salary of a data architect is equally high or even higher than that of a data scientist. In fact, based on the salary research conducted by PayScale ( https://www.payscale.com/research/US/Country=United_States/Salary ) shows the US average salary of Data Architect is  $121,816 , while that of Data Scientist is  $96,089 . Not to say all data scientists should change their job, there would be a lot of benefits for us to learn at least the fundamentals of data architecture. Actually, there is one simple (but meaningful) framework that will help you understand any kinds of real-world data architectures. ‚ÄúData Lake‚Äù, ‚ÄúData Warehouse‚Äù, and ‚ÄúData Mart‚Äù are typical components in the architecture of data platform. In this order, data produced in the business is processed and set to create another data implication. Three components take responsibility for three different functionalities as such: For more real-world examples beyond this bare-bone-only description, enjoy googling ‚Äúdata architecture‚Äù to find a lot of data architecture diagrams. Because different stages within the process have different requirements. In the data lake stage, we want the data is close to the original, while the data warehouse is meant to keep the data sets more structured, manageable with a clear maintenance plan, and having clear ownership. In the data warehouse, we also like the database type to be analytic-oriented rather than transaction-oriented. On the other hand, data mart should have easy access to non-tech people who are likely to use the final outputs of data journeys. Differently-purposed system components tend to have re-design at separate times. Then, configuring the components loosely-connected has the advantage in future maintenance and scale-up. Roughly speaking, data engineers cover from data extraction produced in business to the data lake and data model building in data warehouse as well as establishing ETL pipeline; while data scientists cover from data extraction out of data warehouse, building data mart, and to lead to further business application and value creation. Of course, this role assignment between data engineers and data scientists is somewhat ideal and many companies do not hire both just to fit this definition. Actually, their job descriptions tend to overlap. Last but not the least, it should be worth noting that this three-component approach is conventional one present for longer than two decades, and new technology arrives all the time. For example,  ‚Äú Data Virtualization ‚Äù  is an idea to allow one-stop data management and manipulation interface against data sources, regardless of their formats and physical locations. Now, we understood the concept of three data platform components. Then, what tools do people use? Based on  this ‚ÄúData Platform Guide‚Äù  (in Japanese) , here‚Äôre some ideas: There are the following options for data lake and data warehouse. ETL happens where data comes to the data lake and to be processed to fit the data warehouse. Data arrives in real-time, and thus ETL prefers event-driven messaging tools. A workflow engine is used to manage the overall pipelining of the data, for example, visualization of where the process is in progress by a flow chart, triggering automatic retry in case of error, etc. The following tools can be used as data mart and/or BI solutions. The choice will be dependent on the business context, what tools your company is familiar with (e.g. are you Tableau person or Power BI person?), the size of aggregated data (e.g. if the data size is small, why doesn‚Äôt the basic solution like Excel or Google Sheets meet the goal?), what data warehouse solution do you use (e.g. if your data warehouse is on BigQuery, Google DataStudio can be an easy solution because it has natural linkage within the Google circle), and etc. When the data size stays around or less than tens of megabytes and there is no dependency on other large data set, it is fine to stick to spreadsheet-based tools to store, process, and visualize the data because it is less-costly and everyone can use it. Once the data gets larger and starts having data dependency with other data tables, it is beneficial to start from cloud storage as a one-stop data warehouse. (When the data gets even larger to dozens of terabytes, it can make sense to use on-premise solutions for cost-efficiency and manageability.) In this chapter, I will demonstrate a case when the  data is stored in Google BigQuery as a data warehouse.  BigQuery data is processed and stored in real-time or in a short frequency. The end-user still wants to see daily KPIs on a spreadsheet on a highly aggregated basis. This means  data mart can be small and fits even the spreadsheet solution . Instead of Excel, let‚Äôs use  Google Sheets  here because it can be in the same environment as the data source in BigQuery. Oh, by the way, do not think about running the query manually every day.  Try to find a solution to make everything running automatically without any action from your side. In this case study, I am going to use a sample table data which has records of NY taxi passengers per ride, including the following data fields: The sample data is stored in the BigQuery as a data warehouse. Technically yes, but at the moment this is only available through  Connected Sheets and you need an account of G Suite Enterprise, Enterprise for Education, or G Suite Enterprise Essentials account . Connected Sheets allows the user to manipulate BigQuery table data almost as if they play it on spreadsheet. See the GIF demonstration  in this page on ‚ÄúBenCollins‚Äù blog post. Connected Sheets also allows automatic scheduling and refresh of the sheets , which is a natural demand as a data mart. Although it demonstrates itself as a great option, one possible issue is that owing G Suite account is not very common. For more details about the setups, see  this blog post from ‚ÄúBenCollins‚Äù . To extract data from BigQuery and push it to Google Sheets, BigQuery alone is not enough, and we need a help of server functionality to call the API to post a query to BigQuery, receive the data, and pass it to Google Sheets. The server functionality can be on a server machine, external or internal of GCP (e.g. ‚ÄòCompute Engine‚Äô instance on GCP; or ‚ÄòEC2‚Äô instance on AWS). The code run can be scheduled using  unix-cron job . But one downside here is that it takes maintenance work and cost on the instance and is too much for a small program to run. ‚ÄòGoogle Cloud Functions‚Äô is a so-called ‚Äúserverless‚Äù solution to run code without the launch of a server machine. Putting code in Cloud Functions and setting a trigger event (e.g. scheduled timing in this case study, but also can be HTML request from some internet users), GCP automatically manages the run of the code. There are two steps in the configuration of my case study using NY taxi data. Step 1: Set up scheduling ‚Äî set Cloud Scheduler and Pub/Sub to trigger a Cloud Function. Here, ‚ÄúPub/Sub‚Äù is a messaging service to be subscribed by Cloud Functions and to trigger its run every day at a certain time. ‚ÄúCloud Scheduler‚Äù is functionality to kick off something with user-defined frequency based on  unix-cron format . Combining these two, we can create regular messages to be subscribed by Cloud Function. See  this official instruction on how to do  it. Here are screenshots from my GCP set-up. Step 2: Set up code ‚Äî prepare code on Cloud Functions to query BigQuery table and push it to Google Sheets. The next step is to set up Cloud Functions. In Cloud Functions, you define 1) what is the trigger (in this case study, ‚Äúcron-topic‚Äù sent from Pub/Sub, linked to Cloud Scheduler which pulls the trigger every 6 am in the morning) and 2) the code you want to run when the trigger is detected. See  this official instruction  for further details, and here are screenshots from my set-up. The code to run has to be enclosed in a function named whatever you like (‚Äúnytaxi_pubsub‚Äù in my case.) The code content consists of two parts: part 1 to run a query on BigQuery to reduce the original BigQuery table to KPIs and save it as another data table in BigQuery, as well as make it a Pandas data frame, and part 2 to push the data frame to Sheets. Here‚Äôre the codes I actually used. Importantly, the authentication to BigQuery is automatic as long as it resides within the same GCP project as Cloud Function (see  this page  for explanation.) Yet, this is not the case about the Google Sheets, which needs at least a procedure to share the target sheet through Service Account. See  the description in gspread library  for more details. Finally, I got the aggregated data in Google Sheets like this: This sheet is automatically updated every morning, and as the data warehouse is receiving new data through ETL from the data lake, we can easily keep track of the NY taxi KPIs the first thing every morning. In a large company who hires data engineers and/or data architects along with data scientists, a primary role of data scientists is not necessarily to prepare the data infrastructure and put it in place, but knowing at least getting the gist of data architecture will benefit well to understand where we stand in the daily works. Data Lake -> Data Warehouse -> Data Mart is a typical platform framework to process the data from the origin to the use case. Separating the process into three system components has many benefits for maintenance and purposefulness. There are many options in the choice of tools. They are to be wisely selected against the data environment (size, type, and etc.) and the goal of the business. Finally in this post, I discussed a case study where we prepared a small size data mart on Google Sheets, pulling out data from BigQuery as a data warehouse. With the use of Cloud Scheduler and Pub/Sub, the update was made to be automatic."
Modern Data Architecture Models,odern Data Architecture Model,"Data has come a long way, starting from the 1640s when the term ‚Äúdata‚Äù had its first use, to the 21st century, where AI has become integral to everyday life. As you can imagine, several software and hardware developments have co-evolved with data, bringing us to the here and now. One of the early challenges in data was ingesting it ‚Äî how the data was to be used and the needs it served, weren‚Äôt nearly as interesting. The use cases were extremely narrow, mostly defaulting to basic business reporting. Today however, the focus has shifted from ingesting data to making it accessible in a way that would support a plethora of applications ‚Äî parameters of accuracy, timeliness, reliability and trust at a massive scale, are paramount. The challenges today in data ‚Äî a consequence of its scale and speed ‚Äî are in the areas of data discoverability, governance and reliability. The market is flooded with tools for every data problem conceivable. But, is there a guiding philosophy on how to bring these multitude of tools together, or how to stitch the different roles in an org with these tools? What we need are data architectures that can provide directional guidance, allow for weighing trade-offs, are domain-agnostic and at the same time don‚Äôt put us at the risk of building something that quickly becomes obsolete. Data architecture is a relatively new term. In fact, one of the first references to data architecture is the mention of Data mesh as a model in this  article  in April, 2020. So, is data mesh the only model or one of many? A search will show  Data Fabric  and  Data Mesh  as two popular candidates for data architecture models. If you are looking for a short form introduction to the two models; Think of data fabric as a convergence of the modern data tools, stitched together to collect disparate data and move it within a system in a multi-hop manner. The objectives being, data discoverability, accessibility and management ‚Äî for varied consumers and use cases. Data Mesh then, is the next step in the evolution of data architectures; brining in aspects of product management and decentralization to data. Contrasting one with the other, Data fabric allows for ingestion of data from any source, for any use case ‚Äî without gating it for quality during ingestion ‚Äî trust in data, data integrity are addressed through layers that logically come after ingestion. Whereas, Data mesh places strong emphasis on data quality and data being treated as a product, even before it can become part of the data ecosystem. Is one better than the other? Which one renders itself better to implementation? Here‚Äôs the long form of the two models. How should data move in a system, What characteristics should data retain and shed as it moves? In the Data fabric architecture, data follows a set of steps that determine its flow. The first step takes data through an integration phase. In the integration phase, data is ingested and then cleaned, transformed and loaded into storage. Then, there is the data quality phase where quality assessment is performed on the stored data. This data is then made available for different use cases through a combination of a data lake and a data warehouse, Typical use cases are BI, analytics and machine learning. Data governance policies are defined for the ingested data and a data catalog is used for discoverability. The above functions are mostly centralized ‚Äî a team of data specialists are designing and implementing the different stages in the fabric and also setting up policies and access controls. Simply put, Data Fabric is how most data ecosystems move, store and access data today. The beauty of data fabric as an architecture model is the flexibility it offers ‚Äî not all components are a must, there are multiple vendors with off-the-shelf solutions that can collect and process data from  any  source and  any  use case. Riding on the shift of software systems towards distributed domain design, data mesh is built on the principles of distributed architecture. There are three major components in a data mesh ‚Äî  Decentralized Domain ownership of data  and the resulting  Data products ,  Self-serve data infrastructure  and  Federated Governance . Data Mesh has been designed to derive value from data at scale, in complex environments ‚Äî complex not in data volume or velocity but in the number of use cases and the diversity of data sources. Since the complexity in not only technical, this architecture is modelled as a socio-technical construct. Domain owned data is probably the most critical shift in going from Data Fabric to a Mesh. The idea is quite simple ‚Äî Who better to own and provide data for use, than the teams generating the very data? In this paradigm, business domains decide what data is useful and should be exposed for different use cases within the org. If that is true, are these the teams also building methods and tools to serve this data? No ‚Äî This requires skills that the domains are not expected to have and is instead delegated to the data infrastructure that builds a self-serve data platform. Domains serve their data as a product ‚Äî a product that meets well-defined standards that ensure interoperability with data from other domains. This data product lives as a node on the mesh. This is how the concept of ETLs is done with in the Data Mesh paradigm. Decentralized domain data ownership is the highlight in this architecture. Ownership of design and deployment of the infrastructure that serves data, however is centralized ‚Äî with the data platform team. Naturally, there arises a need for a body that balances these aspects, delineates decisions that lie localized with each domain from the decisions that are considered global. This group is the federated governance group that is carved out of both the data platform team and individual domains. Both the architecture models attempt to solve the problem of getting value from data at scale ‚Äî while making data secure, accessible and easy to use and interpret. In a Data Fabric, a dataset gains value by being onboarded, catalogued and made available through a standardized set of governance rules. In a Data Mesh, a dataset gains value because of its usability as determined by its consumers (data scientists/data analysts). In a Data fabric, there is standardization in how data is cleaned, labeled and checked for quality. In a Data Mesh, the decision on how data is to be made consumption ready i.e the pre-processing steps lies with the domains that own the data. In a Data Fabric, the onus of understanding the data, interoperability of data sets generated by different services becomes a joint responsibility of the data engineering team and consumers of data ‚Äî the analysts and the scientists. In a Data Mesh, it is the responsibility of the teams serving their data, to understand how the data could be used to generate value and design it in a way that meets the needs of the consumers. Data Fabric addresses and recommends solutions to the fundamental questions on ingestion and use of data. Data Mesh as a model, can become a solution when the fabric hits a wall on issues around data ownership and data quality. Also, an important pre-requisite for the Data Mesh architecture to be successful is domain oriented software architecture and teams in an organization. All things considered, it is a good idea for a data org to get started with the data fabric paradigm and adopt principles from data mesh as their data, their needs and the complexity of their data systems evolve."
Evolution of Data Architectures,volution of Data Architecture,"The separation of data from business operations and various analytical workloads (BI, Data Science, Cognitive Solutions, etc.) is as old as IT systems and business applications are. As analytical workloads are resource intensive, they need to be separated from the IT systems that run business operations so that operational workloads run smoothly without any resource constraints, thereby ensuring a positive customer experience. Our dependency on big data and business analytics has significantly increased over the years, with its market size  expected  to reach USD 684.12 billion by 2030. Globally, various industries invest in analyzing their massive volumes of data and creating effective data strategies. Data architectures are frameworks for how data strategies are supported through IT infrastructures. As the foundation of data strategies, data architectures play an essential role in effective strategy implementation. The evolution of data architectures over the years has accordingly shaped the effectiveness of data strategy. Data models, architectures, and storage have evolved with time, catering to diverse analytical workloads. In this article, we will introduce various data architectures that have evolved to meet continuously growing analytical needs. Each of these evolutions deserves a book to describe the complete details that cannot be produced in one article. However, the purpose is to describe high-level details of each of them here and point to additional literature available. Let‚Äôs begin. In the early days, an operational data store (ODS) was developed to cater to decision support systems, mainly targeting operational users who needed predefined reports. ODS stores only current data (6 months typically) for operational reporting and tactical decision making, such as a bank teller. It decides whether to offer an overdraft facility, increase the credit limit, etc., for a customer standing in the queue. The arrival of Business Intelligence tools has broadened the analytics user base to cover senior executives who prefer summary information in a graphical representation. Data marts and dimensional modeling techniques like star/snowflake schemas have been developed to support this user base. Data Marts are typically used for descriptive and diagnostic analytics focusing on specific subject areas, helping users to understand what happened, what is happening, and why, and also to conduct what-if analysis. While ODS and Data Marts serve two sets of different analytical users, they tend to get limited to specific functional areas. Enterprise Data Warehouses (EDW) have been developed to cater to the needs of cross-functional analysis. Data warehouses store historical data to find long-term patterns in the data. They have been designed with ER and dimensional modeling techniques depending on the organizational preferences. A typical enterprise data analytics architecture looks like this at this stage: ODS, Data Marts, and EDW implemented with traditional RDBMS such as Db2, Oracle, and SQL Server serve the purpose of canned reports and executive dashboards that could be delivered in batch mode as per predefined schedules, typically daily. For ad-hoc reports and interactive analysis, they have severe performance constraints. To serve these needs, multi-dimensional databases (MDDBs) such as Oracle Express, Cognos Power Play, Hyperion Essbase, etc., have been developed. These databases have been used for data marts for specific subject area analytics such as financial planning & budgeting, accounting, procurement, sales, marketing, etc., due to the size limitation of MDDBs (each cube could typically hold 2 GB of data). Users could perform interactive analysis with drill up/down/through, what-if analysis, and scenario planning with these MDDBs, though limited to a specific functional area. Analytical applications have to process data at an aggregate level to find new patterns. Traditional RDBMS like DB2, Oracle, and SQL Server that run-on general-purpose/commodity hardware lag behind in meeting the demands of these analytical workloads. DBMS like Teradata, appliances like Netezza, Neoview, Parallel Data Warehouse, and SAP HANA came into the market to address those needs. They run on special purpose hardware that uses massively parallel architecture and in-memory processing, giving a required performance boost. These appliances have been used to implement a flavor of Enterprise Data Warehouse. However, except for Teradata, all other appliance technologies have minimal success. ODS, EDW, and Data Marts deal with enterprise structured data only. They cannot process and analyze semi-structured (JSON, XML, etc.) and unstructured data (text, documents, images, videos, audio, etc.). In addition, they were developed before the cloud came into existence. Hence, there was tight integration between storage and computing resources. As a result, these resources had to be planned for peak load on the application, which will be underutilized most of the time when the load on the application is not high. With the arrival of big data technologies, another variant of data architecture came into existence, the data lake. While the purpose of the data lake is similar to that of EDW or data marts, it also caters to semi-structured and unstructured data. It is a more prominent implementation on cloud infrastructures such as AWS S3, Azure ADLS, or Google‚Äôs GCS. While data warehouses and data marts are built with a predefined purpose, a data lake is a raw storage of all types of data (at the lowest possible storage cost), which can be processed for specific purposes by spinning of a data warehouse, data marts, or data pipeline for data science and cognitive science applications. Since a data lake holds raw data, it does not require schema when writing, unlike data warehouses and data marts that need pre-defined schema when loading data into them. The low cost, object storage, and open format features of a data lake make it popular, as opposed to expensive and proprietary data warehouses. However, data lakes come with their own challenges, such as: This is what a typical data lake architecture looks like: Data warehouse and data lake architectures are centralized implementations that limit the scalability and availability of data for consumption. These implementations take a long time, limit domain understanding of the data, and are more technology-oriented than end user-oriented. They are designed and owned by data engineering specialists who are not readily available in large numbers, which is also a limitation of scalability and democratization of data for analysis. These data engineers are far away from business applications that generate the data; hence, it lacks business context and meaning of data. Data Mesh architecture/concept has been developed to address these challenges. In this approach, data is organized as data products along with various functional/subject areas or domains. They are owned by those responsible for business applications, so they understand the business context, meaning, and usage of the data. These data product owners take help from data engineers to design and distribute analytical data products. There will be a catalog of these analytical data products, which every consumer in the organization can see, understand the context of, use any given data product, and interpret accordingly. Core principles of Data Mesh are essentially: Data Mesh is still an approach for data architecture. There are no products available in the market yet that implement this architecture. Data Fabric is also trying to solve the same problems that Data Mesh is trying to do. However, their approaches are quite different. While Data Mesh is a domain and a business-oriented distributed approach, Data Fabric is a centralized meta data-driven and technology-centric approach. Data Fabric is developed with metadata, catalog, logical data model, and data delivery APIs. Part of the data is virtualized, while the remaining data is centralized, just like a data warehouse. It is complemented with centrally managed data life cycle management policies, such as: SAP Data Intelligence, IBM‚Äôs Cloud Pak for Data, Oracle Coherence, and Talend Data Fabric are some of the products available in this space. Denodo is another product that is more about data virtualization technology which is a core part of the data fabric approach. In the Data Lake architecture, each type of analytical workload requires its own data pipeline due to different data access requirements, leading to inconsistent understanding and usage of the same data. It also introduces one more layer of data storage in between analytical applications (consumers) and business applications (sources) that generate data. First, data has to come into the data lake and then move to the consuming applications, which could reduce the value of key insights by the time they are acted upon. Data lake does not support transactional applications and has many other limitations, as described in the section above. Lakehouse architecture is trying to address these issues by having a common interface for all types of data analytics workloads. It supports ACID properties of transactional applications. It essentially combines the advantages of both data warehouse and data lake architectures while addressing the challenges of both. Data architectures have been evolving to meet the growing demands for various analytical and cognitive workloads, leveraging innovations in cloud and big data technologies. Depending on where the organization stands on the data analytics maturity, the variety of data it holds, and the kind of analytical workloads it requires, a specific type of data architecture can be chosen. While the Lakehouse architecture holds the promise of the best of all worlds, it is new and yet to mature for broader adoption. Data architectures are at the core of all business data strategies; hence, paying attention to them is crucial. With the right data architectures for your specific use case, you can ensure the successful implementation of data strategies. References 1.   What is Operational Data Stores? 2.  Data Warehouse Concepts: Kimball vs. Inmon Approach 3.  What is a multidimensional database? 4.  Data Warehouse Appliance Vendors and Products 5.  Introduction to Data Lakes 6.  Data Mesh Principles and Logical Architecture 7.  Using Data Fabric Architecture to Modernize Data Integration 8.  Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics krtrimaIQ Cognitive Solutions is an agile start-up with the attitude and power of an enterprise, focused on applying Data Science, Cognitive Science and NextGen BI to build the Intelligent Enterprise. In Sanskrit,  ‚Äúkrtrima‚Äù  means  ‚Äúartificial‚Äù , and  ‚ÄúIQ‚Äù  is the  Intelligent Quotient .  To know more about what we do, check out our  website , and follow us on  LinkedIn , and  Twitter ."
What is the Data Architecture We Need?,hat is the Data Architecture We Need,"In the new era of Big Data and Data Sciences, it is vitally important for an enterprise to have a centralized data architecture aligned with business processes, which scales with business growth and evolves with technological advancements. A successful data architecture provides clarity about every aspect of the data, which enables data scientists to work with trustable data efficiently and to solve complex business problems. It also prepares an organization to quickly take advantage of new business opportunities by leveraging emerging technologies and improves operational efficiency by managing complex data and information delivery throughout the enterprise. When compared with information architecture, system architecture, and software architecture, data architecture is relatively new. The role of Data Architects has also been nebulous and has fallen on the shoulders of senior business analysts, ETL developers, and data scientists. Nonetheless, I will use Data Architect to refer to those data management professionals who design data architecture for an organization. When talking about architecture, we often think about the analogy with building architecture. A conventional building architect plans, designs, and reviews the construction of a building. The design process involves working with the clients to fully gather the requirements, understanding the legal and environmental constraints of the location, and working with engineers, surveyors and other specialists to ensure the design is realistic and within the budget. The complexity of the job is indeed very similar to the role of a data architect. However, there are a few fundamental differences between the two architect roles: Given all these differences, a data architect could still learn from building architects and, in particular, take their top-down approach to improve data architecture design. In many organizations, there has been a lack of systematic, centralized, end-to-end data architecture designs. Below lists some of the main reasons: With these shortfalls, we often see a company with disjointed data systems and gaps between teams and departments. The disparities lead to the poor performance of the systems with many hand-offs, a long time to troubleshoot when a production data issue arises, a lack of accountability to reach the right solution across systems, and a lack of capability to assess the impact of a change. Lastly, the disjointed systems could cause tremendous effort to analyze and research when migrated or re-engineered to the next-gen platform. Given all these, a successful enterprise needs to have a top-down coherent data architecture designed based on the business processes and operations. In particular, just like what a building architect does, an enterprise data architect needs to build a blueprint at the conceptual and logical level first, before applying the technologies to the detailed application designs and implementations. In modern IT, business processes are supported and driven by data entities, data flows, and business rules applied to the data. A data architect, therefore, needs to have in-depth business knowledge, including Financial, Marketing, Products, and industry-specific expertise of the business processes, such as Health, Insurance, Manufacturers, and Retailers. He or she can then properly build a data blueprint at the enterprise level by designing the data entities and taxonomies that represent each business domain, as well as the data flow underneath the business process. In particular, the following areas need to be considered and planned at this conceptual stage: This conceptual level of design consists of the underlying data entities that support each business function. The blueprint is crucial for the successful design and implementation of Enterprise and System architectures and their future expansions or upgrades. In many organizations, this conceptual design is usually embedded in the business analysis driven by the individual project without guidance from the perspective of enterprise end-to-end solutions and standards. This level of design is sometimes called data modeling by considering which type of database or data format to use. It connects the business requirements to the underlying technology platforms and systems. However, most organizations have data modeling designed only within a particular database or system, given the siloed role of the data modeler. A successful data architecture should be developed with an integrated approach, by considering the standards applicable to each database or system, and the data flows between these data systems. In particular, the following 5 areas need to be designed in a synergistic way: The naming conventions and data integrity The naming conventions for data entities and elements should be applied consistently to each database. Also, the integrity between the data source and its references should be enforced if the same data have to reside in multiple databases. Ultimately, these data elements should belong to a data entity in the conceptual design in the data architecture, which can then be updated or modified synergistically and accurately based on business requirements. Data archival/retention policies The data archival and retention policies are often not considered or established until every late-stage on Production, which caused wasted resources, inconsistent data states across different databases, and poor performance of data queries and updates. To enforce the data integrity, data architects should define the data archival and retention policy in the data architecture based on Operational standards. Privacy and security information Privacy and security become an essential aspect of the logical database design. While the conceptual design has defined which data component is sensitive information, the logical design should have the confidential information protected in a database with limited access, restricted data replication, particular data type, and secured data flows to protect the information. Data Replications Data Replication is a critical aspect to consider for three objectives: 1) High availability; 2) Performance to avoid data transferring over the network; 3) De-coupling to minimize the downstream impact. Excessive data replications, however, can lead to confusion, poor data quality, and poor performance. Any data replication should be examined by data architect and applied with principles and disciplines. Data Flows and Pipelines How data flows between different database systems and applications should be clearly defined at this level. Again, this flow is consistent with the flow illustrated in the business process and data architect conceptual level. Besides, the frequencies of the data ingestion, data transformations in the pipelines, and data access patterns against the output data should be considered in an integrated view in the logical design. For example, if an upstream data source comes in real-time, while a downstream system is mainly used for data access of aggregated information with heavy indexes (e.g., expensive for frequent updates and inserts), a data pipeline needs to be designed in between to optimize the performance. As data architecture reflects and supports the business processes and flow, it is subject to change whenever the business process is changed. As the underlying database system is changed, the data architecture also needs to be adjusted. The data architecture, therefore, is not static but needs to be continuously managed, enhanced, and audited. Data governance, therefore, should be adopted to ensure that enterprise data architecture is designed and implemented correctly as each new project is being kicked off. Within a successful data architecture, a conceptual design based on the business process is the most crucial ingredient, followed by a logical design that emphasizes consistency, integrity, and efficiency across all the databases and data pipelines. Once the data architecture is established, the organization can see what data resides where and ensure that the data is secured, stored efficiently, and processed accurately. Also, when one database or a component is changed, the data architecture can allow the organization to assess the impact quickly and guides all relevant teams on the designs and implementations. Lastly, the data architecture is a live document of the enterprise systems, which is guaranteed to be up-to-date and gives a clear end-to-end picture. In summary, a holistic data architecture that reflects the end-to-end business process and operations is essential for a company to advance quickly and efficiently while undergoing significant changes such as acquisitions, digital transformation, or migration to the next-gen platform."
A Framework for Modern Data Architecture, Framework for Modern Data Architectur,"Today is the age of data. Every organization is becoming a ‚Äúdata company.‚Äù Today, if an organization is not thinking about its data as a strategic asset, then it has already missed the bus. Data has evolved over the past decade. The rate of evolution has been exponential. But have the Data Architecture practices managed to keep up with the same pace? McKinsey recently published an  article  that formulates the building of modern data architecture to drive innovation. The article explains the foundational shifts for modern data architecture. Being a practitioner in this field, I found Mckinsey‚Äôs view interesting. The goal of this article is two-fold: The article elaborates on the six foundational shifts that enable modern data architecture. The article elaborates on the six foundational shifts that enable modern data architecture. Shift #1: From on-premise to cloud-based platforms Cloud enables organizational agility. Shift #2: From batch to real-time processing Organizations will pivot from batch to real-time data ingestion and processing. Shift #3: From pre-integrated commercial solutions to modular, best-of-breed platforms Cloud enables modular components focused on functionalities rather than technologies. Shift #4: From point-to-point to decoupled data access Co-existence of point-to-point and decouple data access is required to get the right data to the right stakeholder and at the right time. Shift #5: From an enterprise warehouse to domain-based architecture A single repository of Enterprise Data Warehouse impedes the required agility for transforming data into insights. Shift #6: From rigid data models toward flexible, extensible data schemas Data models used in the right context serve as the blueprint for the modern data architecture. Having discussed these shifts, let us see which components in a cloud platform can enable these shifts. Microsoft‚Äôs Azure cloud platform will be used to exemplify the features. However, these capabilities can be mapped to other cloud platforms as well. In conclusion, there are three key takeaways:"
A Data Scientist‚Äôs Guide to Data Architecture, Data Scientist‚Äôs Guide to Data Architectur,"You should always prepare your own data. This quote comes from a senior executive in the AI space when I asked him about his view on data a few years ago. He is not the only one thinking this way. The head of the department during my first job couldn‚Äôt trust another team to build the data pipeline and ended up writing a big portion of the ETL code himself and had the data team reporting to him directly. Why do these senior executives put so much emphasis on data? Because without a good grasp of data, you won‚Äôt (and shouldn‚Äôt) have much confidence in the insights and recommendations that you derive from the data. For data to move from its raw form all the way to being ready for analysis, it‚Äôs gone through a lot of processes. A good data scientist must question every step of the process relentlessly, and ask questions like: The purpose of this article is to go through some data architecture related topics using real-world examples so that you have some basic context and know what questions to ask your data team. As mentioned before, I started my career working for a major insurance company, in a team where all data processes are done by ourselves. Every month, we take a copy of a long list of tables from the production database and put it onto our SQL server which we call the staging database. Production database : this is the transaction database used by various departments to administer insurance policies and claims in real time. Staging database : this is a snapshot of the production database for all sorts of analysis work. Typically a snapshot is taken once a month, but sometimes it is taken more frequently upon ad-adhoc requests. The purpose of the staging database is so that we don‚Äôt interfere with the normal business operation. This should never happen, but it did happen once due to a data connection setting problem. We received phone calls from the IT department every 5 minutes, as everyone has been complaining the system is frozen‚Ä¶ The staging environment is safe for the analytics team to work in. Moreover, you are guaranteed to have the same result as long as you run your code against the staging database. Once all the tables have been copied onto the staging database, we perform a series of data processing steps to turn these into a handful of denormalized tables. The raw tables in the production system are called normalized tables which is a core concept behind relational databases. Each table contains data about different things and can be linked together via primary and foreign keys. For analytics purposes, a large number of table joins are required before you can have something useful. For example, say you want to predict the probability of having a building insurance claim given the characteristics of the building. Ideally, you need a flat table where To create the flat table you need to join the claim table (which tells you whether or not there is a claim), the policy table (which has the policy information) and the risk table (which has the building information) together. This process is called  denormalization . The result is a  denormalized  or  flat table  where the data is well organized and ready to be analyzed. You may think that this is no big deal. We just perform a few joins and it‚Äôs done, isn‚Äôt it? What could go wrong? A lot can go wrong. You may have duplicated records. You may unintentionally gain or lose records. This is what I was told to do when I started my first job, and I believe I still benefit from these good habits today. If we skip these steps, we could end up wasting more time down the track. This is a lesson I learned again and again in many different projects. After a lot of efforts, we end up a few core tables that satisfy the needs of the entire team, which include: These are all flat tables and are produced monthly and put onto a common server. Whoever needs to do any analysis just makes a copy of these flat tables, knowing that the data can be trusted. ETL stands for  E xtraction,  T ransform and  L oad. It may sound fancy, but you‚Äôve seen the process already. ‚ÄúExtract‚Äù is when we copy the data from the production database to the staging database. ‚ÄúLoad‚Äù is when we deliver the final core datasets. ‚ÄúTransform‚Äù is all the work in between. Throughout the entire process, we had robust data checks every step along the way: At that time, most of the data integrity checks, as well as the original ETL process was written in SAS programs. Typically after adding these checks, the size of SAS programs increases by a factor of 3. Indeed, checking takes more effort than doing! Is it worthwhile though? Absolutely. Everyone in the team can pick up the dataset and immediately work with it confidently knowing it‚Äôs been checked every step along the way. In addition to robust data checks, we also spent a lot of time learning how data is generated in the first place. This was done by sitting with the people who input the data every day. In our case, it was the sales department for policy data and claims department for claims data. We sat alongside the frontline staff, listening to them answer customer‚Äôs questions, typing data into the system. We tried our best to understand how they did their job and built a good deal of domain expertise. Check out  this article  I wrote on the importance of building up domain expertise. While the above process might be good for detecting data quality problems, it doesn‚Äôt fix them going forward. That‚Äôs why a feedback loop is needed. Every month, we publish a set of data quality reports and shared them with the management team. Some of the things we look for are The data quality performance is tied to the KPI of staff and forms part of the metrics that drive their remuneration. This provides a powerful incentive for everyone to care about data quality. This would never be possible without the support from the senior management of the company. I believe a data-driven decision-making culture can never be driven from bottom up. It needs to come from the top down. So far, I have explained the traditional approach. We were using SAS back then. The word data engineer or data scientist wasn‚Äôt used by people. Nowadays, a lot has changed, but these concepts are still relevant. Next, I will discuss a few topics that became increasingly relevant in recent years. The data extracted from the IT system is called internal data because it is internally generated. External data is really powerful and can significantly enrich and complement internal data. The type of external data varies significantly by context. In the case of building insurance, here are some examples of the external data that can be used: In the case of motor insurance, here are some examples of additional external data that can be used: Most of the data discussed above is structured data. Examples of unstructured data include: As you can see, unstructured data typically require some additional processing to become useful for our tasks, e.g. insurance risk modeling. So far I have only discussed batch processing. The ETL process takes at least 2‚Äì3 days. Adding another few days for analysis, you won‚Äôt have any output until at least a week after the data becomes available. Real-time analytics is different. The moment data comes in, it gets processed and analyzed. Can you imagine that the moment you buy a product, Amazon says to you: ‚ÄúDear customer, thanks for your purchase. Please wait for a week and we‚Äôll recommend some products that interest you.‚Äù That never happens. They provide a recommendation instantly. This allows them to generate 35% of their revenue on Amazon.com from  recommendations . Real-time analytics is often within the realms of data engineers. Take the COVID-19 as an example, these are batch processing based static dashboard results that I created. It was a pain to update the results. By the time I publish these articles, the numbers were already out of date. Status Update of the Coronavirus (COVID-19) Outbreak  ‚Äî published Mar 17 Status Update of the Coronavirus (COVID-19) Outbreak  ‚Äî published Feb 29 Coronavirus outbreak ‚Äî 5 questions to ask big data  ‚Äî published Feb 1 Compare the above to  this interactive dashboard  which shows the spread of the COVID-19 virus around the world and in particular in Singapore since late January. The entire process happens automatically as new data comes in. Real-world analytics delivers the result just when it is most needed ‚Äî who cares about what COVID-19 figures were a week ago? Let‚Äôs apply the same logic to the insurance sector. Why should the CEO of an insurance company care about the sales volume or loss ratios from 3 months ago? Yet, today most insurance companies still consume monthly or quarterly reports. With some exceptions such as real-time fraud analysis, the insurance industry tends to be slow to adopt these new technologies. As tools like R-Shiny and Dash make it much easier and more affordable to deploy real-time analytics I expect it to gain momentum in the near future. Within the data science world, there are a lot of sexy topics ‚Äî AI, deep learning, big data‚Ä¶ Data architecture is not one of these sexy topics. The company executives won‚Äôt ever attract any media attention by saying ‚Äúwe have built a world-class data integrity check‚Äù. It‚Äôs hard work. It takes a lot of discipline to get it right. However, if you do manage to get it right, it will probably deliver more value than any of those sexy topics. As a data scientist, it pays to spend some time learning about these concepts so that you can communicate effectively with your data team and ensure you have a robust data process that lays down a solid foundation for all your analytics work."
